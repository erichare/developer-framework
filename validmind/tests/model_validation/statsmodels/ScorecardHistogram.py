# Copyright © 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

from dataclasses import dataclass

import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from validmind.vm_models import Figure, Metric


@dataclass
class ScorecardHistogram(Metric):
    """
    Creates histograms of credit scores, from both default and non-default instances, generated by a credit-risk model.

    **Purpose**: The Scorecard Histogram test metric provides a visual interpretation of the credit scores generated by
    a machine learning model for credit-risk classification tasks. It aims to compare the alignment of the model's
    scoring decisions with the actual outcomes of credit loan applications. It helps in identifying potential
    discrepancies between the model's predictions and real-world risk levels.

    **Test Mechanism**: This metric uses logistic regression to generate a histogram of credit scores for both default
    (negative class) and non-default (positive class) instances. Using both training and test datasets, the metric
    calculates the credit score of each instance with a scorecard method, considering the impact of different features
    on the likelihood of default. İncludes the default point to odds (PDO) scaling factor and predefined target score
    and odds settings. Histograms for training and test sets are computed and plotted separately to offer insights into
    the model's generalizability to unseen data.

    **Signs of High Risk**:
    - Discrepancies between the distributions of training and testing data, indicating a model's poor generalisation
    ability
    - Skewed distributions favouring specific scores or classes, representing potential bias

    **Strengths**:
    - Provides a visual interpretation of the model's credit scoring system, enhancing comprehension of model behavior
    - Enables a direct comparison between actual and predicted scores for both training and testing data
    - Its intuitive visualization helps understand the model's ability to differentiate between positive and negative
    classes
    - Can unveil patterns or anomalies not easily discerned through numerical metrics alone

    **Limitations**:
    - Despite its value for visual interpretation, it doesn't quantify the performance of the model, and therefore may
    lack precision for thorough model evaluation
    - The quality of input data can strongly influence the metric, as bias or noise in the data will affect both the
    score calculation and resultant histogram
    - Its specificity to credit scoring models limits its applicability across a wider variety of machine learning
    tasks and models
    - The metric's effectiveness is somewhat tied to the subjective interpretation of the analyst, since it relies on
    the analyst's judgment of the characteristics and implications of the plot.
    """

    name = "scorecard_histogram"
    required_inputs = ["datasets"]
    metadata = {
        "task_types": ["classification"],
        "tags": ["tabular_data", "visualization", "credit_risk"],
    }
    default_params = {
        "title": "Histogram of Scores",
        "score_column": "score",
    }

    @staticmethod
    def plot_score_histogram(df_train, df_test, score_col, target_col, title):
        scores_train_0 = df_train[df_train[target_col] == 0][score_col]
        scores_train_1 = df_train[df_train[target_col] == 1][score_col]
        scores_test_0 = df_test[df_test[target_col] == 0][score_col]
        scores_test_1 = df_test[df_test[target_col] == 1][score_col]

        fig = make_subplots(rows=1, cols=2, subplot_titles=("Train Data", "Test Data"))

        trace_train_0 = go.Histogram(
            x=scores_train_0, opacity=0.75, name=f"Train {target_col} = 0"
        )
        trace_train_1 = go.Histogram(
            x=scores_train_1, opacity=0.75, name=f"Train {target_col} = 1"
        )
        trace_test_0 = go.Histogram(
            x=scores_test_0, opacity=0.75, name=f"Test {target_col} = 0"
        )
        trace_test_1 = go.Histogram(
            x=scores_test_1, opacity=0.75, name=f"Test {target_col} = 1"
        )

        fig.add_trace(trace_train_0, row=1, col=1)
        fig.add_trace(trace_train_1, row=1, col=1)
        fig.add_trace(trace_test_0, row=1, col=2)
        fig.add_trace(trace_test_1, row=1, col=2)

        fig.update_layout(barmode="overlay", title_text=title)

        return fig

    def run(self):

        title = self.params["title"]
        score_column = self.params["score_column"]

        df_train = self.inputs.datasets[0].df.copy()
        df_test = self.inputs.datasets[1].df.copy()

        target_column = self.inputs.datasets[0].target_column

        scores_train = self.inputs.datasets[0].get_extra_column(score_column)
        scores_test = self.inputs.datasets[1].get_extra_column(score_column)

        df_train["score"] = scores_train
        df_test["score"] = scores_test

        fig = self.plot_score_histogram(
            df_train, df_test, "score", target_column, title
        )

        return self.cache_results(
            metric_value={
                "score_histogram": {
                    "train_scores": list(df_train["score"]),
                    "test_scores": list(df_test["score"]),
                },
            },
            figures=[
                Figure(
                    for_object=self,
                    key="score_histogram",
                    figure=fig,
                )
            ],
        )
