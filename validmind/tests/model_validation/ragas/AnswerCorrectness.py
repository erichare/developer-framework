# Copyright Â© 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

import warnings

import plotly.express as px
from datasets import Dataset

from validmind import tags, tasks

from .utils import get_ragas_config, get_renamed_columns


@tags("ragas", "llm")
@tasks("text_qa", "text_generation", "text_summarization")
def AnswerCorrectness(
    dataset,
    question_column="question",
    answer_column="answer",
    ground_truth_column="ground_truth",
):
    """
    Assesses the correctness of answers generated by a text model, comparing them to ground truth answers.

    ### Purpose

    The Answer Correctness test evaluates the correctness of answers generated by a machine learning model by comparing
    them to provided ground truth answers. This test is essential for ensuring that the model produces accurate and
    reliable responses in tasks such as text generation, text summarization, and question-answering.

    ### Test Mechanism

    The test measures answer correctness based on both semantic and factual similarities between the generated answer
    and the ground truth. The correctness score ranges from 0 to 1, where a higher score signifies closer alignment.
    Factual correctness is determined using true positives (TP), false positives (FP), and false negatives (FN):

    - **True Positives (TP)**: Facts present in both the generated answer and the ground truth.
    - **False Positives (FP)**: Facts present in the generated answer but not in the ground truth.
    - **False Negatives (FN)**: Facts present in the ground truth but not in the generated answer.

    The evaluation results, including individual scores and aggregated metrics (mean, median, max, min, standard
    deviation), are visualized using histograms and box plots.

    ### Signs of High Risk

    - Significant occurrences of low correctness scores, indicating poor model performance.
    - High standard deviation, signifying inconsistency in the model's answers.
    - Presence of extreme outliers in the distribution of correctness scores.

    ### Strengths

    - Combines semantic and factual analysis for a comprehensive evaluation.
    - Provides detailed visualizations for easy interpretation of results.
    - Flexible configuration for datasets with varying column structures.

    ### Limitations

    - The correctness scores depend on the quality and accuracy of the provided ground truth.
    - May not fully capture nuanced errors or sophisticated semantic differences.
    - Sensitive to the definition and granularity of the facts considered in the FP, TP, and FN metrics.
    """
    try:
        from ragas import evaluate
        from ragas.metrics import answer_correctness
    except ImportError:
        raise ImportError("Please run `pip install validmind[llm]` to use LLM tests")

    warnings.filterwarnings(
        "ignore",
        category=FutureWarning,
        message="promote has been superseded by promote_options='default'.",
    )

    required_columns = {
        "question": question_column,
        "answer": answer_column,
        "ground_truth": ground_truth_column,
    }

    df = get_renamed_columns(dataset._df, required_columns)

    result_df = evaluate(
        Dataset.from_pandas(df), metrics=[answer_correctness], **get_ragas_config()
    ).to_pandas()

    fig_histogram = px.histogram(x=result_df["answer_correctness"].to_list(), nbins=10)
    fig_box = px.box(x=result_df["answer_correctness"].to_list())

    return (
        {
            "Scores (will not be uploaded to UI)": result_df[
                ["question", "answer", "ground_truth", "answer_correctness"]
            ],
            "Aggregate Scores": [
                {
                    "Mean Score": result_df["answer_correctness"].mean(),
                    "Median Score": result_df["answer_correctness"].median(),
                    "Max Score": result_df["answer_correctness"].max(),
                    "Min Score": result_df["answer_correctness"].min(),
                    "Standard Deviation": result_df["answer_correctness"].std(),
                    "Count": len(result_df),
                }
            ],
        },
        fig_histogram,
        fig_box,
    )
