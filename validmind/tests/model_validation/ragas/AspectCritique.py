# Copyright © 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

import warnings

import plotly.express as px
from datasets import Dataset

from validmind import tags, tasks

from .utils import get_ragas_config, get_renamed_columns


@tags("ragas", "llm", "qualitative")
@tasks("text_summarization", "text_generation", "text_qa")
def AspectCritique(
    dataset,
    question_column="question",
    answer_column="answer",
    contexts_column="contexts",
    aspects: list = [  # noqa: B006 this is fine as immutable default since it never gets modified
        "coherence",
        "conciseness",
        "correctness",
        "harmfulness",
        "maliciousness",
    ],
    additional_aspects: list = None,
):
    """
    Assesses generations against predefined and user-defined aspects such as harmfulness, maliciousness, coherence,
    correctness, and conciseness in text generation from LLMs.

    ### Purpose

    The Aspect Critique test aims to evaluate the quality of text generated by a Large Language Model (LLM) against
    specific predefined aspects like harmfulness, maliciousness, coherence, correctness, and conciseness. It ensures
    that the generated content adheres to these critical qualitative metrics, which are essential for maintaining
    ethical and high-quality outputs.

    ### Test Mechanism

    This test operates by using a judge LLM to critique generated text based on descriptions of each specified aspect.
    Here’s how it works:

    - The predefined aspects are harmfulness, maliciousness, coherence, correctness, and conciseness.
    - Custom aspects can be added by providing a list of tuples where each tuple includes an aspect name and a
    description.
    - The input dataset should contain columns for `question` (prompt to the LLM), `answer` (text generated by the
    LLM), and optional `contexts`.
    - Each aspect is evaluated, and the LLM gives a binary (0/1 = no/yes) score indicating whether the generated text
    meets the criteria.
    - The scores are aggregated to produce an aspect score, calculated as the number of "yes" scores divided by the
    total submissions.

    ### Signs of High Risk

    - High frequency of failed scores (0s) in any of the critical aspects like harmfulness or maliciousness.
    - Consistently low aspect scores across multiple aspects.
    - Disparities between predefined aspects and custom aspects, especially if custom aspects show more failures.

    ### Strengths

    - Provides a structured and quantifiable method to assess qualitative metrics.
    - Flexibility to add custom aspects relevant to specific use-cases.
    - Offers clear visualization of pass and fail counts for each aspect.

    ### Limitations

    - Restricted to binary scoring, which might oversimplify nuanced evaluations.
    - Relies on the credibility of the judge LLM, which may introduce its own biases.
    - May not capture the full context or intent behind the generated text, limiting deeper qualitative insights.
    """
    try:
        from ragas import evaluate
        from ragas.metrics.critique import AspectCritique as _AspectCritique
        from ragas.metrics.critique import (
            coherence,
            conciseness,
            correctness,
            harmfulness,
            maliciousness,
        )
    except ImportError:
        raise ImportError("Please run `pip install validmind[llm]` to use LLM tests")

    aspect_map = {
        "coherence": coherence,
        "conciseness": conciseness,
        "correctness": correctness,
        "harmfulness": harmfulness,
        "maliciousness": maliciousness,
    }

    warnings.filterwarnings(
        "ignore",
        category=FutureWarning,
        message="promote has been superseded by promote_options='default'.",
    )

    required_columns = {
        "question": question_column,
        "answer": answer_column,
        "contexts": contexts_column,
    }

    df = get_renamed_columns(dataset._df, required_columns)

    built_in_aspects = [aspect_map[aspect] for aspect in aspects]
    custom_aspects = (
        [
            _AspectCritique(name=name, definition=description)
            for name, description in additional_aspects
        ]
        if additional_aspects
        else []
    )
    all_aspects = [*built_in_aspects, *custom_aspects]

    result_df = evaluate(
        Dataset.from_pandas(df), metrics=all_aspects, **get_ragas_config()
    ).to_pandas()

    df_melted = result_df.melt(
        id_vars=["question", "answer", "contexts"],
        value_vars=[aspect.name for aspect in all_aspects],
        var_name="Metric",
        value_name="Result",
    )
    df_counts = df_melted.groupby(["Metric", "Result"]).size().reset_index(name="Count")
    df_counts["Result"] = df_counts["Result"].map({0: "Fail", 1: "Pass"})

    fig = px.bar(
        df_counts,
        x="Metric",
        y="Count",
        color="Result",
        color_discrete_map={"Fail": "red", "Pass": "green"},
        labels={"Count": "Pass vs Fail Count", "Metric": "Aspect Name"},
        barmode="group",
        title="Aspect Critique Results",
    )

    return {
        "Aspect Scores": [
            {"Aspect": aspect, "Score": result_df[aspect].mean()}
            for aspect in aspects + [aspect.name for aspect in custom_aspects]
        ]
    }, fig
