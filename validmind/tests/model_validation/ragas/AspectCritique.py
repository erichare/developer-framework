# Copyright Â© 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

from datasets import Dataset
from ragas import evaluate
from ragas.metrics.critique import (
    AspectCritique,
    coherence,
    conciseness,
    correctness,
    harmfulness,
    maliciousness,
)

from validmind import tags, tasks

from .utils import get_renamed_columns

aspect_map = {
    "coherence": coherence,
    "conciseness": conciseness,
    "correctness": correctness,
    "harmfulness": harmfulness,
    "maliciousness": maliciousness,
}


@tags("ragas", "llm", "qualitative")
@tasks("text_summarization", "text_generation", "text_qa")
def AspectCritique(
    dataset,
    question_column="question",
    answer_column="answer",
    contexts_column="contexts",
    aspects: list = [
        "coherence",
        "conciseness",
        "correctness",
        "harmfulness",
        "maliciousness",
    ],
    additional_aspects: list = [],
):
    """
    Evaluates generations against the following aspects: harmfulness, maliciousness,
    coherence, correctness, and conciseness.

    ### Overview:

    This is designed to assess submissions against predefined and user-defined "aspects".
    For each aspect, a judge LLM is prompted to critique a piece of generated text based
    on a description of the aspect. The output of this evaluation is a binary (0/1 = yes/no)
    score that indicates whether the submission aligns with the defined aspect or not.

    ### Inputs and Outputs:

    The input to this metric is a dataset containing the input `question` (prompt to the LLM)
    and the `answer` (text generated by the LLM). Any retrieved `contexts` can also be
    included to enhance the evaluation. The columns must be named `question`, `answer`, and
    `contexts` respectively or the name for each column must be provided as a parameter.
    Finally, by default, the aspects evaluated are harmfulness, maliciousness, coherence,
    correctness, and conciseness. To change the aspects evaluated, the `aspects` parameter
    can be set to a list containing any of these aspects. To add custom aspects, the
    `additional_aspects` parameter can be passed as a list of tuples where each tuple
    contains the aspect name and a description of the aspect that the judge LLM will use
    to critique the submission.

    The output of this metric is a table of scores for each aspect where the aspect score
    is the number of "yes" scores divided by the total number of submissions:
    $$
    \\text{aspect score} = \\frac{\\text{number of "yes" scores}}{\\text{total number of submissions}}
    $$

    ### Examples:

    - **Mapping Sub-Columns to Required Columns:** In the case where the VM Model outputs
    `contexts` and `answer` as a dictionary, these sub-columns can be mapped to the required
    column names using dot notation. For example, if the dataset has a column named
    `my_model_id_predictions` where the value is a dictionary with the keys `contexts` and
    `llm_output`, the `answer_column` parameter can be set like this:

    ```python
    run_test(
        "validmind.model_validation.ragas.AspectCritique",
        inputs={"dataset": my_vm_dataset},
        params={
            "question_column": "input_prompt",
            "answer_column": f"{my_vm_dataset.prediction_column(my_vm_model)}.llm_output",
            "contexts_column": f"{my_vm_dataset.prediction_column(my_vm_model)}.contexts",
        },
    )
    ```

    - **Custom Aspects:** To evaluate custom aspects, the `additional_aspects` parameter can
    be set to a list of tuples where each tuple contains the aspect name and a description
    of the aspect that the judge LLM will use to critique the submission. For example, to
    evaluate whether the LLM-generated text has a "professional tone", the `additional_aspects`
    parameter can be set like this:

    ```python
    run_test(
        "validmind.model_validation.ragas.AspectCritique",
        inputs={"dataset": my_vm_dataset},
        params={
            "additional_aspects": [
                ("professionalism", "Does the text have a professional tone?"),
            ],
        },
    )
    ```
    """
    required_columns = {
        question_column: "question",
        answer_column: "answer",
        contexts_column: "contexts",
    }

    df = get_renamed_columns(dataset.df, required_columns)

    built_in_aspects = [aspect_map[aspect] for aspect in aspects]
    custom_aspects = [
        AspectCritique(name=name, definition=description)
        for name, description in additional_aspects
    ]

    result_df = evaluate(
        Dataset.from_pandas(df[list(required_columns.values())]),
        metrics=[*built_in_aspects, *custom_aspects],
    ).to_pandas()

    print(result_df)

    score_columns = [*aspects, *[aspect.name for aspect in custom_aspects]]

    return {
        "Scores": result_df[["question", "contexts", "answer", *score_columns]],
        "Aggregate Scores": [
            {
                "Mean Score": result_df[column].mean(),
                "Median Score": result_df[column].median(),
                "Max Score": result_df[column].max(),
                "Min Score": result_df[column].min(),
                "Standard Deviation": result_df[column].std(),
                "Count": len(result_df),
            }
            for column in score_columns
        ],
    }
