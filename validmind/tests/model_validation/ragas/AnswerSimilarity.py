# Copyright Â© 2023-2024 ValidMind Inc. All rights reserved.
# See the LICENSE file in the root of this repository for details.
# SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial

import warnings

import plotly.express as px
from datasets import Dataset

from validmind import tags, tasks

from .utils import get_ragas_config, get_renamed_columns


@tags("ragas", "llm")
@tasks("text_qa", "text_generation", "text_summarization")
def AnswerSimilarity(
    dataset,
    answer_column="answer",
    ground_truth_column="ground_truth",
):
    """
    Assesses the semantic similarity between generated answers and ground truths to evaluate the quality of LLM
    responses.

    ### Purpose

    The Answer Similarity test aims to measure the semantic similarity between generated answers and their
    corresponding ground truths. This evaluation helps in understanding how closely the responses generated by a model
    align with the expected answers, providing insights into the model's performance in tasks like text generation,
    question answering, and summarization.

    ### Test Mechanism

    This test calculates the semantic similarity score between generated answers and ground truths using a
    cross-encoder model through the following steps:
    - Vectorizing the ground truth answers using a specified embedding model.
    - Vectorizing the generated answers using the same embedding model.
    - Computing the cosine similarity score between the two vectors, which ranges from 0 to 1, where a higher score
    indicates a better alignment.

    ### Signs of High Risk

    - Low average similarity scores across the test dataset.
    - High variance in similarity scores, indicating inconsistency in model performance.
    - Consistently poor performance on certain types of questions or summaries.

    ### Strengths

    - Provides a quantitative measure of semantic alignment between generated and ground-truth answers.
    - Utilizes advanced embedding techniques for precise similarity computation.
    - Offers detailed aggregate scores including mean, median, max, min, and standard deviation for comprehensive
    analysis.

    ### Limitations

    - Relies on the quality of the embedding model used for similarity computation.
    - The cosine similarity score may not capture all nuances of textual meaning and context.
    - Performance may vary depending on the domain and type of text used in the dataset.
    """
    try:
        from ragas import evaluate
        from ragas.metrics import answer_similarity
    except ImportError:
        raise ImportError("Please run `pip install validmind[llm]` to use LLM tests")

    warnings.filterwarnings(
        "ignore",
        category=FutureWarning,
        message="promote has been superseded by promote_options='default'.",
    )

    required_columns = {
        "answer": answer_column,
        "ground_truth": ground_truth_column,
    }

    df = get_renamed_columns(dataset._df, required_columns)

    result_df = evaluate(
        Dataset.from_pandas(df), metrics=[answer_similarity], **get_ragas_config()
    ).to_pandas()

    fig_histogram = px.histogram(x=result_df["answer_similarity"].to_list(), nbins=10)
    fig_box = px.box(x=result_df["answer_similarity"].to_list())

    return (
        {
            "Scores (will not be uploaded to UI)": result_df[
                ["answer", "ground_truth", "answer_similarity"]
            ],
            "Aggregate Scores": [
                {
                    "Mean Score": result_df["answer_similarity"].mean(),
                    "Median Score": result_df["answer_similarity"].median(),
                    "Max Score": result_df["answer_similarity"].max(),
                    "Min Score": result_df["answer_similarity"].min(),
                    "Standard Deviation": result_df["answer_similarity"].std(),
                    "Count": len(result_df),
                }
            ],
        },
        fig_histogram,
        fig_box,
    )
