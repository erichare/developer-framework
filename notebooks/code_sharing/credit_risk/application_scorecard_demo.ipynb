{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Scorecard Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "TBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(source_csv, column_types=None):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from a pickle file if available, or from a CSV file otherwise.\n",
    "    Automatically handles the creation and cleanup of a temporary directory for the pickle file.\n",
    "    \n",
    "    :param source_csv: Path to the CSV file.\n",
    "    :param column_types: Dictionary specifying columns and their types to prevent DtypeWarning.\n",
    "    :return: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    pkl_file = os.path.join(temp_dir, 'dataframe.pkl')\n",
    "\n",
    "    try:\n",
    "        # Try loading from the pickle file\n",
    "        df = pd.read_pickle(pkl_file)\n",
    "        print(\"Loaded DataFrame from pickle.\")\n",
    "    except (FileNotFoundError, IOError):\n",
    "        print(\"Pickle file not found. Loading CSV and creating pickle file...\")\n",
    "        # Load from CSV if pickle doesn't exist\n",
    "        df = pd.read_csv(source_csv, dtype=column_types)\n",
    "        # Save to pickle for future use\n",
    "        df.to_pickle(pkl_file)\n",
    "        print(\"DataFrame loaded from CSV and saved to pickle.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "Every documentation project in the Platform UI comes with a _code snippet_ that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place. \n",
    "\n",
    "Get your code snippet by creating a documentation project:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. Go to **Documentation Projects** and click **Create new project**.\n",
    "\n",
    "3. Select **`[Demo] Customer Churn Model`** and **`Initial Validation`** for the model name and type, give the project a unique  name to make it yours, and then click **Create project**.\n",
    "\n",
    "4. Go to **Documentation Projects** > **YOUR_UNIQUE_PROJECT_NAME** > **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  project = \"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.preview_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL to the Lending Club loan data set (2007-2014) hosted on AWS S3 for easy access.\n",
    "source = \"https://vmai.s3.us-west-1.amazonaws.com/datasets/lending_club_loan_data_2007_2014.csv\"\n",
    "\n",
    "# Load CSV with pandas, setting column 21 (index 20) to string data type to prevent DtypeWarning due to mixed types.\n",
    "df = load_dataframe(source, column_types={20: str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_raw_ds = vm.init_dataset(\n",
    "    input_id='raw_dataset',\n",
    "    dataset=df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.TabularDescriptionTables\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_raw_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.MissingValuesBarPlot\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_raw_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non relevant columns for building an application scorecard model\n",
    "COLS_TO_DROP = [\n",
    "    \"Unnamed: 0\",\n",
    "    \"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\", \"mths_since_last_major_derog\",\n",
    "    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\",\n",
    "    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n",
    "    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n",
    "    'earliest_cr_line', 'issue_d'\n",
    "]\n",
    "\n",
    "df.drop(columns=COLS_TO_DROP, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of missing values for each feature in the dataset.\n",
    "missing_fractions = df.isnull().mean()\n",
    "\n",
    "# Set a threshold for the minimum fraction of missing values to consider dropping a feature.\n",
    "min_missing_fraction = 0.8\n",
    "\n",
    "# Identify features where the missing value fraction exceeds the threshold.\n",
    "to_drop = missing_fractions[missing_fractions > min_missing_fraction].index.tolist()\n",
    "print(to_drop)\n",
    "\n",
    "# Remove identified features with too many missing values from the dataset.\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable for the model, representing loan default status.\n",
    "# Map 'loan_status' to a binary variable where 'Fully Paid' loans are 0 (no default)\n",
    "# and 'Charged Off' loans are 1 (default). Other statuses are treated as missing (NaN) and then removed.\n",
    "target_column = \"default\"\n",
    "\n",
    "df[target_column] = df[\"loan_status\"].apply(\n",
    "    lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan\n",
    ")\n",
    "\n",
    "# Remove rows with missing target variable values to ensure model integrity.\n",
    "df.dropna(subset=[target_column], inplace=True)\n",
    "\n",
    "# Convert the target variable to integer type for modeling.\n",
    "df[target_column] = df[target_column].astype(int)\n",
    "\n",
    "# Drop the original 'loan_status' column as it's now redundant with 'default'.\n",
    "df.drop(columns=[\"loan_status\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_preprocessed_ds = vm.init_dataset(\n",
    "    input_id='preprocessed_dataset',\n",
    "    dataset=df,\n",
    "    target_column=target_column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.TabularDescriptionTables\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_preprocessed_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.MissingValuesBarPlot\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_preprocessed_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.IQROutliersTable\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_preprocessed_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_preprocessed_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "# Split data into train and test\n",
    "X = df.drop(target_column, axis=1)\n",
    "y = df[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds = vm.init_dataset(\n",
    "    input_id='train_dataset',\n",
    "    dataset=df_train,\n",
    "    target_column=target_column\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    input_id='test_dataset',\n",
    "    dataset=df_test,\n",
    "    target_column=target_column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.TabularNumericalHistograms\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.TabularCategoricalBarPlots\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.TargetRateBarPlots\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_train_ds\n",
    "    },\n",
    "    params = {\n",
    "        \"default_column\": target_column,\n",
    "        \"columns\": None\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.PearsonCorrelationMatrix\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= vm.tests.run_test(\n",
    "    \"validmind.data_validation.WOEBinTable\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_DROP = ['addr_state', 'total_rec_int', 'loan_amnt',\n",
    "                    'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt',\n",
    "                    'total_pymnt_inv', 'last_pymnt_amnt', \"inq_last_6mths\"]\n",
    "\n",
    "\n",
    "# Keep rows where purpose is 'debt_consolidation' or 'credit_card'\n",
    "df = df[df[\"purpose\"].isin([\"debt_consolidation\", \"credit_card\"])]\n",
    "\n",
    "# Remove rows where grade is 'F' or 'G'\n",
    "df = df[~df[\"grade\"].isin([\"F\", \"G\"])]\n",
    "\n",
    "# Remove rows where sub_grade starts with 'F' or 'G'\n",
    "df = df[~df[\"sub_grade\"].str.startswith((\"F\", \"G\"))]\n",
    "\n",
    "# Remove rows where home_ownership is 'OTHER', 'NONE', or 'ANY'\n",
    "df = df[~df[\"home_ownership\"].isin([\"OTHER\", \"NONE\", \"ANY\"])]\n",
    "\n",
    "df.drop(FEATURES_TO_DROP, axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
