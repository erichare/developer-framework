{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of a RAG Model for RFP Question Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise Exception(\"OPENAI_API_KEY not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import HTML, display\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def _format_cell_text(text, width=50):\n",
    "    \"\"\"Private function to format a cell's text.\"\"\"\n",
    "    return \"\\n\".join([textwrap.fill(line, width=width) for line in text.split(\"\\n\")])\n",
    "\n",
    "\n",
    "def _format_dataframe_for_tabulate(df):\n",
    "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Format all string columns\n",
    "    for column in df_out.columns:\n",
    "        # Check if column is of type object (likely strings)\n",
    "        if df_out[column].dtype == object:\n",
    "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def _dataframe_to_html_table(df):\n",
    "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
    "    headers = df.columns.tolist()\n",
    "    table_data = df.values.tolist()\n",
    "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
    "\n",
    "\n",
    "def display_nice(df, num_rows=None):\n",
    "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
    "    if num_rows is not None:\n",
    "        df = df.head(num_rows)\n",
    "    formatted_df = _format_dataframe_for_tabulate(df)\n",
    "    html_table = _dataframe_to_html_table(formatted_df)\n",
    "    display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_keys(data, indent=0):\n",
    "    for key, value in data.items():\n",
    "        print(' ' * indent + str(key))\n",
    "        if isinstance(value, dict):  # if the value is another dictionary, recurse\n",
    "            print_dict_keys(value, indent + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing RFPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV file paths\n",
    "existing_rfp_paths = [\n",
    "    \"datasets/rag/rfp_existing_questions_client_1.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_2.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_3.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_4.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_5.csv\",\n",
    "]\n",
    "\n",
    "existing_rfp_df = [pd.read_csv(file_path) for file_path in existing_rfp_paths]\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "existing_rfp_df = pd.concat(existing_rfp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project_Title</th>\n",
       "      <th>RFP_Question_ID</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>Area</th>\n",
       "      <th>Last_Accessed_At</th>\n",
       "      <th>Requester</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>1</td>\n",
       "      <td>What is your experience in developing AI-based...</td>\n",
       "      <td>Our company has 15 years of experience in deve...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>2</td>\n",
       "      <td>How do you ensure your AI-based apps remain up...</td>\n",
       "      <td>We maintain a dedicated R&amp;D team focused on in...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>3</td>\n",
       "      <td>Can your AI-based applications be customized t...</td>\n",
       "      <td>Absolutely, customization is a core aspect of ...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>4</td>\n",
       "      <td>What measures do you take to ensure user priva...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>5</td>\n",
       "      <td>How do you approach user interface and experie...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Generative AI Solutions for Fraud Detection an...</td>\n",
       "      <td>19</td>\n",
       "      <td>What steps do you take to ensure the transpare...</td>\n",
       "      <td>We prioritize transparency by incorporating ex...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>18/10/2022</td>\n",
       "      <td>Bank E</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Generative AI Solutions for Fraud Detection an...</td>\n",
       "      <td>20</td>\n",
       "      <td>How do you monitor and assess AI risk exposure...</td>\n",
       "      <td>We have developed a set of Key Performance Ind...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>18/10/2022</td>\n",
       "      <td>Bank E</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Generative AI Solutions for Fraud Detection an...</td>\n",
       "      <td>21</td>\n",
       "      <td>How do you handle the management and mitigatio...</td>\n",
       "      <td>We implement and maintain robust risk manageme...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>18/10/2022</td>\n",
       "      <td>Bank E</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Generative AI Solutions for Fraud Detection an...</td>\n",
       "      <td>22</td>\n",
       "      <td>How do you ensure your AI solutions adhere to ...</td>\n",
       "      <td>We ensure compliance with U.S. regulations suc...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>18/10/2022</td>\n",
       "      <td>Bank E</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Generative AI Solutions for Fraud Detection an...</td>\n",
       "      <td>23</td>\n",
       "      <td>How do you contribute to the ongoing developme...</td>\n",
       "      <td>We actively participate in industry working gr...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>18/10/2022</td>\n",
       "      <td>Bank E</td>\n",
       "      <td>Under Review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Project_Title  RFP_Question_ID  \\\n",
       "0              Gen AI-Driven Financial Advisory System                1   \n",
       "1              Gen AI-Driven Financial Advisory System                2   \n",
       "2              Gen AI-Driven Financial Advisory System                3   \n",
       "3              Gen AI-Driven Financial Advisory System                4   \n",
       "4              Gen AI-Driven Financial Advisory System                5   \n",
       "..                                                 ...              ...   \n",
       "110  Generative AI Solutions for Fraud Detection an...               19   \n",
       "111  Generative AI Solutions for Fraud Detection an...               20   \n",
       "112  Generative AI Solutions for Fraud Detection an...               21   \n",
       "113  Generative AI Solutions for Fraud Detection an...               22   \n",
       "114  Generative AI Solutions for Fraud Detection an...               23   \n",
       "\n",
       "                                          RFP_Question  \\\n",
       "0    What is your experience in developing AI-based...   \n",
       "1    How do you ensure your AI-based apps remain up...   \n",
       "2    Can your AI-based applications be customized t...   \n",
       "3    What measures do you take to ensure user priva...   \n",
       "4    How do you approach user interface and experie...   \n",
       "..                                                 ...   \n",
       "110  What steps do you take to ensure the transpare...   \n",
       "111  How do you monitor and assess AI risk exposure...   \n",
       "112  How do you handle the management and mitigatio...   \n",
       "113  How do you ensure your AI solutions adhere to ...   \n",
       "114  How do you contribute to the ongoing developme...   \n",
       "\n",
       "                                            RFP_Answer           Area  \\\n",
       "0    Our company has 15 years of experience in deve...        General   \n",
       "1    We maintain a dedicated R&D team focused on in...        General   \n",
       "2    Absolutely, customization is a core aspect of ...        General   \n",
       "3    User privacy and data security are paramount. ...        General   \n",
       "4    Our design philosophy centers on simplicity an...        General   \n",
       "..                                                 ...            ...   \n",
       "110  We prioritize transparency by incorporating ex...  AI Regulation   \n",
       "111  We have developed a set of Key Performance Ind...  AI Regulation   \n",
       "112  We implement and maintain robust risk manageme...  AI Regulation   \n",
       "113  We ensure compliance with U.S. regulations suc...  AI Regulation   \n",
       "114  We actively participate in industry working gr...  AI Regulation   \n",
       "\n",
       "    Last_Accessed_At Requester        Status  \n",
       "0         18/12/2023    Bank A  Under Review  \n",
       "1         18/12/2023    Bank A  Under Review  \n",
       "2         18/12/2023    Bank A  Under Review  \n",
       "3         18/12/2023    Bank A  Under Review  \n",
       "4         18/12/2023    Bank A  Under Review  \n",
       "..               ...       ...           ...  \n",
       "110       18/10/2022    Bank E  Under Review  \n",
       "111       18/10/2022    Bank E  Under Review  \n",
       "112       18/10/2022    Bank E  Under Review  \n",
       "113       18/10/2022    Bank E  Under Review  \n",
       "114       18/10/2022    Bank E  Under Review  \n",
       "\n",
       "[115 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_rfp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Questions and Answers to Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project_Title</th>\n",
       "      <th>RFP_Question_ID</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>Area</th>\n",
       "      <th>Last_Accessed_At</th>\n",
       "      <th>Requester</th>\n",
       "      <th>Status</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>1</td>\n",
       "      <td>What is your experience in developing AI-based...</td>\n",
       "      <td>Our company has 15 years of experience in deve...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>2</td>\n",
       "      <td>How do you ensure your AI-based apps remain up...</td>\n",
       "      <td>We maintain a dedicated R&amp;D team focused on in...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>3</td>\n",
       "      <td>Can your AI-based applications be customized t...</td>\n",
       "      <td>Absolutely, customization is a core aspect of ...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>4</td>\n",
       "      <td>What measures do you take to ensure user priva...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>5</td>\n",
       "      <td>How do you approach user interface and experie...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Project_Title  RFP_Question_ID  \\\n",
       "0  Gen AI-Driven Financial Advisory System                1   \n",
       "1  Gen AI-Driven Financial Advisory System                2   \n",
       "2  Gen AI-Driven Financial Advisory System                3   \n",
       "3  Gen AI-Driven Financial Advisory System                4   \n",
       "4  Gen AI-Driven Financial Advisory System                5   \n",
       "\n",
       "                                        RFP_Question  \\\n",
       "0  What is your experience in developing AI-based...   \n",
       "1  How do you ensure your AI-based apps remain up...   \n",
       "2  Can your AI-based applications be customized t...   \n",
       "3  What measures do you take to ensure user priva...   \n",
       "4  How do you approach user interface and experie...   \n",
       "\n",
       "                                          RFP_Answer     Area  \\\n",
       "0  Our company has 15 years of experience in deve...  General   \n",
       "1  We maintain a dedicated R&D team focused on in...  General   \n",
       "2  Absolutely, customization is a core aspect of ...  General   \n",
       "3  User privacy and data security are paramount. ...  General   \n",
       "4  Our design philosophy centers on simplicity an...  General   \n",
       "\n",
       "  Last_Accessed_At Requester        Status unique_id  \n",
       "0       18/12/2023    Bank A  Under Review         0  \n",
       "1       18/12/2023    Bank A  Under Review         1  \n",
       "2       18/12/2023    Bank A  Under Review         2  \n",
       "3       18/12/2023    Bank A  Under Review         3  \n",
       "4       18/12/2023    Bank A  Under Review         4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add unique identifier to each row in the rfp df\n",
    "existing_rfp_df[\"unique_id\"] = existing_rfp_df.index.astype(str)\n",
    "existing_rfp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can import and use the openai client\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def get_embedding_OpenAI(text):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    return client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\",\n",
    "    ).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Embed a single text item and return the embedding\n",
    "def get_embedding_LC(text):\n",
    "    return embeddings_model.embed_query(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run:\n",
    "    # Apply the function to each question and answer and create new columns\n",
    "    existing_rfp_df['Question_Embeddings_LC'] = existing_rfp_df['RFP_Question'].apply(get_embedding_LC)\n",
    "    existing_rfp_df['Answer_Embeddings_LC'] = existing_rfp_df['RFP_Answer'].apply(get_embedding_LC)\n",
    "    #existing_rfp_df['Question_Embeddings_OpenAI'] = existing_rfp_df['RFP_Question'].apply(get_embedding_OpenAI)\n",
    "    #existing_rfp_df['Answer_Embeddings_OpenAI'] = existing_rfp_df['RFP_Answer'].apply(get_embedding_OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project_Title</th>\n",
       "      <th>RFP_Question_ID</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>Area</th>\n",
       "      <th>Last_Accessed_At</th>\n",
       "      <th>Requester</th>\n",
       "      <th>Status</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>Question_Embeddings_LC</th>\n",
       "      <th>Answer_Embeddings_LC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>1</td>\n",
       "      <td>What is your experience in developing AI-based...</td>\n",
       "      <td>Our company has 15 years of experience in deve...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.009233377394315142, -0.030979380265132028, ...</td>\n",
       "      <td>[-0.014074281457419971, -0.019768934230661707,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>2</td>\n",
       "      <td>How do you ensure your AI-based apps remain up...</td>\n",
       "      <td>We maintain a dedicated R&amp;D team focused on in...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.015557997606513376, 0.001083239429675231, ...</td>\n",
       "      <td>[0.010924095968986537, 0.0148562794873187, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>3</td>\n",
       "      <td>Can your AI-based applications be customized t...</td>\n",
       "      <td>Absolutely, customization is a core aspect of ...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.012713182022339392, 0.0019399791850565832,...</td>\n",
       "      <td>[0.0012248214130063306, 0.009834839798469275, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>4</td>\n",
       "      <td>What measures do you take to ensure user priva...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.00713508370007173, -0.016262165748897776, ...</td>\n",
       "      <td>[0.03035999154741477, 0.02386305199822589, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>5</td>\n",
       "      <td>How do you approach user interface and experie...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>General</td>\n",
       "      <td>18/12/2023</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.014495182214268135, 0.007043612831731077, ...</td>\n",
       "      <td>[0.008875330851434665, 0.0019440388876996923, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Project_Title  RFP_Question_ID  \\\n",
       "0  Gen AI-Driven Financial Advisory System                1   \n",
       "1  Gen AI-Driven Financial Advisory System                2   \n",
       "2  Gen AI-Driven Financial Advisory System                3   \n",
       "3  Gen AI-Driven Financial Advisory System                4   \n",
       "4  Gen AI-Driven Financial Advisory System                5   \n",
       "\n",
       "                                        RFP_Question  \\\n",
       "0  What is your experience in developing AI-based...   \n",
       "1  How do you ensure your AI-based apps remain up...   \n",
       "2  Can your AI-based applications be customized t...   \n",
       "3  What measures do you take to ensure user priva...   \n",
       "4  How do you approach user interface and experie...   \n",
       "\n",
       "                                          RFP_Answer     Area  \\\n",
       "0  Our company has 15 years of experience in deve...  General   \n",
       "1  We maintain a dedicated R&D team focused on in...  General   \n",
       "2  Absolutely, customization is a core aspect of ...  General   \n",
       "3  User privacy and data security are paramount. ...  General   \n",
       "4  Our design philosophy centers on simplicity an...  General   \n",
       "\n",
       "  Last_Accessed_At Requester        Status unique_id  \\\n",
       "0       18/12/2023    Bank A  Under Review         0   \n",
       "1       18/12/2023    Bank A  Under Review         1   \n",
       "2       18/12/2023    Bank A  Under Review         2   \n",
       "3       18/12/2023    Bank A  Under Review         3   \n",
       "4       18/12/2023    Bank A  Under Review         4   \n",
       "\n",
       "                              Question_Embeddings_LC  \\\n",
       "0  [0.009233377394315142, -0.030979380265132028, ...   \n",
       "1  [-0.015557997606513376, 0.001083239429675231, ...   \n",
       "2  [-0.012713182022339392, 0.0019399791850565832,...   \n",
       "3  [-0.00713508370007173, -0.016262165748897776, ...   \n",
       "4  [-0.014495182214268135, 0.007043612831731077, ...   \n",
       "\n",
       "                                Answer_Embeddings_LC  \n",
       "0  [-0.014074281457419971, -0.019768934230661707,...  \n",
       "1  [0.010924095968986537, 0.0148562794873187, 0.0...  \n",
       "2  [0.0012248214130063306, 0.009834839798469275, ...  \n",
       "3  [0.03035999154741477, 0.02386305199822589, 0.0...  \n",
       "4  [0.008875330851434665, 0.0019440388876996923, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_rfp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\n",
    "    name = \"rfp_qa_collection\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for batch addition\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_documents = []\n",
    "all_ids = []\n",
    "\n",
    "# Loop through the DataFrame rows\n",
    "for index, row in existing_rfp_df.iterrows():\n",
    "    # Append each piece of data to its respective list\n",
    "    all_embeddings.append(row['Question_Embeddings_LC'])\n",
    "    all_metadatas.append({\n",
    "        'Project_Title': row['Project_Title'],\n",
    "        'RFP_Question_ID': row['RFP_Question_ID'],\n",
    "        'RFP_Question': row['RFP_Question'],\n",
    "        'RFP_Answer': row['RFP_Answer'],\n",
    "        'Area': row['Area'],\n",
    "        'Last_Accessed_At': row['Last_Accessed_At'],\n",
    "        'Requester': row['Requester'],\n",
    "        'Status': row['Status'],\n",
    "        'hnsw:space': 'cosine'\n",
    "    })\n",
    "    all_documents.append(row['RFP_Question'])\n",
    "    all_ids.append(row['unique_id'])\n",
    "\n",
    "# Add all data to the collection in a single operation\n",
    "collection.add(\n",
    "    ids=all_ids, \n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 115 documents in the collection\n"
     ]
    }
   ],
   "source": [
    "langchain_chroma = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"rfp_qa_collection\",\n",
    ")\n",
    "\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"documents in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = existing_rfp_df['RFP_Question'][0]\n",
    "documents = langchain_chroma.similarity_search_by_vector_with_relevance_scores(\n",
    "    get_embedding_LC(query), k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RFP Question:\n",
      "What is your experience in developing AI-based applications, and can you provide examples of successful projects?\n",
      "\n",
      "Top 10 most similar existing RFP questions:\n",
      "\n",
      "Document 1\n",
      "Question: What is your experience in developing AI-based applications, and can you provide examples of successful projects?\n",
      "Answer: Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For instance, our project MediAI Insight for the healthcare industry demonstrated significant achievements in patient data analysis, resulting in a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. Our platform has engaged over 200 healthcare facilities, achieving a user satisfaction rate of 95%.\n",
      "Score: 1.0 (cosine)\n",
      "\n",
      "Document 2\n",
      "Question: Please share your experience with developing AI-enabled applications and provide examples of notable projects.\n",
      "Answer: Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For instance, our project MediAI Insight for the healthcare industry demonstrated significant achievements in patient data analysis, resulting in a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. Our platform has engaged over 200 healthcare facilities, achieving a user satisfaction rate of 95%.\n",
      "Score: 0.7660512030124664 (cosine)\n",
      "\n",
      "Document 3\n",
      "Question: Can you discuss your expertise in creating AI-driven applications and share examples of your successful implementations?\n",
      "Answer: Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For instance, our project MediAI Insight for the healthcare industry demonstrated significant achievements in patient data analysis, resulting in a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. Our platform has engaged over 200 healthcare facilities, achieving a user satisfaction rate of 95%.\n",
      "Score: 0.7358323633670807 (cosine)\n",
      "\n",
      "Document 4\n",
      "Question: What is your track record in developing AI-powered applications, and could you cite some examples of your achievements in this area?\n",
      "Answer: Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For instance, our project MediAI Insight for the healthcare industry demonstrated significant achievements in patient data analysis, resulting in a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. Our platform has engaged over 200 healthcare facilities, achieving a user satisfaction rate of 95%.\n",
      "Score: 0.7070538997650146 (cosine)\n",
      "\n",
      "Document 5\n",
      "Question: What expertise do you have in creating applications based on artificial intelligence, and can you list some key projects?\n",
      "Answer: Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For instance, our project MediAI Insight for the healthcare industry demonstrated significant achievements in patient data analysis, resulting in a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. Our platform has engaged over 200 healthcare facilities, achieving a user satisfaction rate of 95%.\n",
      "Score: 0.6411957740783691 (cosine)\n",
      "\n",
      "Document 6\n",
      "Question: What strategies do you employ to update your AI applications with the most recent developments in AI technology?\n",
      "Answer: We maintain a dedicated R&D team focused on integrating the latest AI advancements into our applications. This includes regular updates and feature enhancements based on cutting-edge technologies such as GPT (Generative Pre-trained Transformer) for natural language understanding, CNNs (Convolutional Neural Networks) for advanced image recognition tasks, and DQN (Deep Q-Networks) for decision-making processes in complex environments. Our commitment to these AI methodologies ensures that our applications remain innovative, with capabilities that adapt to evolving market demands and client needs. This approach has enabled us to enhance the predictive accuracy of our financial forecasting tools by 25% and improve the efficiency of our educational content personalization by 40%\n",
      "Score: 0.2678478956222534 (cosine)\n",
      "\n",
      "Document 7\n",
      "Question: How do you maintain your AI applications with the newest AI technologies and advancements?\n",
      "Answer: We maintain a dedicated R&D team focused on integrating the latest AI advancements into our applications. This includes regular updates and feature enhancements based on cutting-edge technologies such as GPT (Generative Pre-trained Transformer) for natural language understanding, CNNs (Convolutional Neural Networks) for advanced image recognition tasks, and DQN (Deep Q-Networks) for decision-making processes in complex environments. Our commitment to these AI methodologies ensures that our applications remain innovative, with capabilities that adapt to evolving market demands and client needs. This approach has enabled us to enhance the predictive accuracy of our financial forecasting tools by 25% and improve the efficiency of our educational content personalization by 40%\n",
      "Score: 0.26053619384765625 (cosine)\n",
      "\n",
      "Document 8\n",
      "Question: How do you evaluate the success of your AI applications in fulfilling client goals?\n",
      "Answer: Success measurement is tailored to each project's objectives. We establish key performance indicators (KPIs) in collaboration with our clients, such as user engagement rates, efficiency improvements, or return on investment (ROI). We then regularly review these metrics using advanced analytics platforms and business intelligence tools to assess the app’s impact. Our approach includes monthly performance analysis meetings where we provide detailed reports and insights on metrics like session duration, user retention rates, and cost savings achieved through automation. We also implement A/B testing to continuously refine and optimize the application based on real-world usage data, ensuring that we make data-driven improvements that align closely with our clients' strategic goals.\n",
      "Score: 0.24846011400222778 (cosine)\n",
      "\n",
      "Document 9\n",
      "Question: How do you assess the effectiveness and success of your AI applications in meeting client goals?\n",
      "Answer: Success measurement is tailored to each project's objectives. We establish key performance indicators (KPIs) in collaboration with our clients, such as user engagement rates, efficiency improvements, or return on investment (ROI). We then regularly review these metrics using advanced analytics platforms and business intelligence tools to assess the app’s impact. Our approach includes monthly performance analysis meetings where we provide detailed reports and insights on metrics like session duration, user retention rates, and cost savings achieved through automation. We also implement A/B testing to continuously refine and optimize the application based on real-world usage data, ensuring that we make data-driven improvements that align closely with our clients' strategic goals.\n",
      "Score: 0.23720306158065796 (cosine)\n",
      "\n",
      "Document 10\n",
      "Question: How do you keep your AI applications current with ongoing advancements in artificial intelligence?\n",
      "Answer: We maintain a dedicated R&D team focused on integrating the latest AI advancements into our applications. This includes regular updates and feature enhancements based on cutting-edge technologies such as GPT (Generative Pre-trained Transformer) for natural language understanding, CNNs (Convolutional Neural Networks) for advanced image recognition tasks, and DQN (Deep Q-Networks) for decision-making processes in complex environments. Our commitment to these AI methodologies ensures that our applications remain innovative, with capabilities that adapt to evolving market demands and client needs. This approach has enabled us to enhance the predictive accuracy of our financial forecasting tools by 25% and improve the efficiency of our educational content personalization by 40%\n",
      "Score: 0.23682653903961182 (cosine)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_documents = 10\n",
    "\n",
    "print(f\"New RFP Question:\\n{query}\")\n",
    "print()\n",
    "print(f\"Top {number_of_documents} most similar existing RFP questions:\")\n",
    "print()\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    page_content = document[0].page_content  # This is where the content of the page is stored.\n",
    "    metadata = document[0].metadata  # This is where the metadata of the document is stored.\n",
    "    score = document[1]  # This is the score at the end of the tuple.\n",
    "\n",
    "    # Extracting the metadata\n",
    "    rfp = metadata['Project_Title']\n",
    "    question = metadata['RFP_Question']\n",
    "    answer = metadata['RFP_Answer']\n",
    "    metric = metadata['hnsw:space']\n",
    "\n",
    "    # Print formatted output\n",
    "    print(f\"Document {i + 1}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Score: {1-score} ({metric})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Documents from CSV Files\n",
    "\n",
    "- Load each row as its own document\n",
    "- Represent the text of each document as a list of Column: value pairs, each on their own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Iterate through each file path in the list\n",
    "for file_path in existing_rfp_paths:\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "        metadata_columns=[\"Area\"]\n",
    "    )\n",
    "\n",
    "    # Load a document from the current CSV file\n",
    "    doc = loader.load()\n",
    "    \n",
    "    # Append documents\n",
    "    documents.extend(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `CSVLoader`, each document represents a single row and includes its respective contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 50\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Document {i + 1}: {document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the page content of each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 2\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Page content for document {i + 1}:\")\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when adding metadata, it is appended to the default metadata, which consists of the `row` number and the `source`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = 5\n",
    "\n",
    "for i, document in enumerate(documents[:number_of_documents]):\n",
    "    print(f\"Metadata for document {i + 1}: {document.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=10, add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some general information about the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the length of the bigger and smaller chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chunk_length = max([len(chunk.page_content) for chunk in chunks])\n",
    "min_chunk_length = min([len(chunk.page_content) for chunk in chunks])\n",
    "mean_chunk_length = sum([len(chunk.page_content) for chunk in chunks]) / len(chunks)\n",
    "\n",
    "print(f\"Maximum chunk length: {max_chunk_length}\")\n",
    "print(f\"Minimum chunk length: {min_chunk_length}\")\n",
    "print(f\"Mean chunk length: {mean_chunk_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Calculate lengths of each chunk's page_content\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "# Creating a histogram of chunk lengths\n",
    "fig = px.histogram(chunk_lengths, nbins=50, title=\"Distribution of Chunk Lengths\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Chunk Length\",\n",
    "    yaxis_title=\"Count\",\n",
    "    bargap=0.2,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add summary statistics as text on the plot\n",
    "fig.add_annotation(\n",
    "    x=max(chunk_lengths),\n",
    "    y=0,\n",
    "    showarrow=False,\n",
    "    yshift=10\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for index, chunk in enumerate(chunks[:i]):\n",
    "    print(f\"Chunk {index + 1}: {chunk}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the page content of each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5\n",
    "\n",
    "for i, document in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Page content for chunk {i + 1}:\")\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the metadata for individual chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Metadata for chunk {i + 1}: {chunk.metadata}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the source of each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(chunks[:number_of_chunks]):\n",
    "    print(f\"Source for chunk {i + 1}: {chunk.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store chunks into a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Question-Answer Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RAG Chain\n",
    "\n",
    "1. Get the question from the user\n",
    "2. Pass the question to the retriever \n",
    "3. Get the context from the retriever \n",
    "4. Combine the context and the question to format the prompt\n",
    "5. Pass the prompt to the LLM to get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Step 1: \"question\": Retrieved from the \"question\" key.\n",
    "# Step 2: \"context\": Retrieved from the \"question\" key and fed into the retriever.\n",
    "# Step 3: \"context\": Assigned to a RunnablePassthrough object using the \"context\" key from the previous step.\n",
    "# Step 4: \"answer\": \"context\" and \"question\" are combined to format the prompt, then sent to the LLM and stored under the \"answer\" key.\n",
    "# Step 5: \"context\": Repopulated using the \"context\" key from the previous step.\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    \n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"answer\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question to test the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Find a similar question as this one: 'What is your experience in developing AI-based applications?'\"\n",
    "response = rag_chain.invoke({\"question\" : question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined in the earlier chat prompt, the RAG response includes two fields: `answer` and `context`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_keys(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the answer, we see that the `rag_chain` is functioning correctly and identifies the most similar question in the `vectorstore`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question:\")\n",
    "print(question)\n",
    "print()\n",
    "print(f\"Answer:\")\n",
    "print(response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inspect the content of the `answer` and the `context` retrieved based on the `question`. The context should contain `k` chunks, the most relevant based on the question. Remember that we set`k` in the `retriever` earlier. These `k` chunks are pasted into the prompt as text, informing the LLM to generate an answer that is closer in the embedding space to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 5  \n",
    "\n",
    "for i, chunk in enumerate(response[\"context\"][:number_of_chunks]):\n",
    "    print(f\"Content for chunk {i + 1}:\")  # i + 1 to start counting from 1 instead of 0\n",
    "    print(chunk.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now inspect the `response_metadata` object to understand its contents and identify what could be useful to incorporate in our RAG evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"].response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dict_keys(response[\"answer\"].response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the LLM used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {response['answer'].response_metadata['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed earlier, we can also extract some token usage statistics that can help us understand and optimize our interactions with the language model for cost-effectiveness and efficiency.\n",
    "\n",
    "- **Prompt tokens**: tokens that form the input text sent to the language model. This includes all the text provided to the LLM to generate a response.\n",
    "- **Completion tokens**: number of tokens in the generated text or output from the model.\n",
    "- **Total tokens**: total number of tokens processed by the model. It is the sum of both `prompt_tokens` and `completion_tokens`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Completion tokens: {response['answer'].response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"Prompt tokens: {response['answer'].response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"Total tokens: {response['answer'].response_metadata['token_usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new RFP questions \n",
    "rag_evaluation_df = pd.read_csv(\"datasets/rag/rag_evaluation_dataset_01.csv\")\n",
    "\n",
    "# Set the constant variable to the number of rows in the DataFrame\n",
    "NUM_OF_NEW_RFP_QUESTIONS = len(rag_evaluation_df)\n",
    "\n",
    "print(\"Number of New RFP Questions:\", NUM_OF_NEW_RFP_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now expand our evaluation dataset to capture some metadata generated by the LLM, which will be used later when validating our RAG pipeline. We will add the following additional columns to our dataframe: `context`, `model_name`, `completion_tokens`, prompt_tokens, and `total_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df['context'] = ''\n",
    "\n",
    "rag_evaluation_df['question_embeddings'] = ''\n",
    "rag_evaluation_df['answer_embeddings'] = ''\n",
    "rag_evaluation_df['context_embeddings'] = ''\n",
    "\n",
    "rag_evaluation_df['similarity_score_question_vs_context'] = ''\n",
    "rag_evaluation_df['similarity_score_question_vs_answer'] = ''\n",
    "rag_evaluation_df['similarity_score_context_vs_answer'] = ''\n",
    "\n",
    "rag_evaluation_df['model'] = ''\n",
    "\n",
    "rag_evaluation_df['completion_tokens'] = ''\n",
    "rag_evaluation_df['prompt_tokens'] = ''\n",
    "rag_evaluation_df['total_tokens'] = ''\n",
    "\n",
    "rag_evaluation_df['response_time'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to also compute few similarity metrics between embeddings such as cosine similaruty or euclidean distance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_score(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding1 (array-like): Embedding of the first entity.\n",
    "    - embedding2 (array-like): Embedding of the second entity.\n",
    "\n",
    "    Returns:\n",
    "    - float: Cosine similarity score between the two embeddings.\n",
    "\n",
    "    Note: The order of the embeddings does not affect the result as cosine similarity is symmetric.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings are reshaped to 2D arrays for sklearn's cosine_similarity\n",
    "    embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "    embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # Calculate and return the cosine similarity\n",
    "    return cosine_similarity(embedding1, embedding2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between two embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding1 (array-like): First embedding vector.\n",
    "    - embedding2 (array-like): Second embedding vector.\n",
    "\n",
    "    Returns:\n",
    "    - float: Euclidean distance between the two embeddings.\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays if they aren't already\n",
    "    embedding1 = np.array(embedding1)\n",
    "    embedding2 = np.array(embedding2)\n",
    "    \n",
    "    # Calculate and return the Euclidean distance\n",
    "    return np.linalg.norm(embedding1 - embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Number of questions to process by the RAG model\n",
    "number_of_rows_to_process = NUM_OF_NEW_RFP_QUESTIONS\n",
    "\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing row {i}...\")\n",
    "\n",
    "    # Check if the 'answer' field is 'None' (as a string) for the current row\n",
    "    if row[\"answer\"] == \"None\":\n",
    "        print(f\"Answer is 'None' for question ID {index}. Invoking RAG model...\")\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        # Invoke the RAG model with the question from the current row\n",
    "        response = rag_chain.invoke({\"question\": row[\"question_to_llm\"]})\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "\n",
    "        # Calculate the response time and store it\n",
    "        rag_evaluation_df.at[index, 'response_time'] = round(end_time - start_time, 1)\n",
    "\n",
    "        # Store whatever response comes from the LLM\n",
    "        rag_evaluation_df.at[index, \"answer\"] = response[\"answer\"].content\n",
    "        print(f\"Question ID {index} answer updated with the response from the RAG model.\")\n",
    "    \n",
    "        # Store the context included in the prompt\n",
    "        context = \"\\n\\n\".join(chunk.page_content for chunk in response[\"context\"])\n",
    "        rag_evaluation_df.at[index, \"context\"] = context\n",
    "        \n",
    "        # Compute and store embeddings for the question, context and answer\n",
    "        print(\"Computing embeddings for the question...\")\n",
    "        question_embeddings = np.array(embeddings_model.embed_query(row[\"question_to_llm\"]))\n",
    "        rag_evaluation_df.at[index, 'question_embeddings'] = question_embeddings\n",
    "        \n",
    "        print(\"Computing embeddings for the context...\")\n",
    "        context_embeddings = np.array(embeddings_model.embed_query(context))\n",
    "        rag_evaluation_df.at[index, 'context_embeddings'] = context_embeddings\n",
    "        \n",
    "        print(\"Computing embeddings for the answer...\")\n",
    "        answer_embeddings = np.array(embeddings_model.embed_query(response[\"answer\"].content))\n",
    "        rag_evaluation_df.at[index, 'answer_embeddings'] = answer_embeddings\n",
    "        \n",
    "        # Compute similarity measures between embeddings \n",
    "        print(\"Computing cosine similarity between question and context...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_question_vs_context'] = cosine_similarity_score(question_embeddings, context_embeddings)\n",
    "        \n",
    "        print(\"Computing cosine similarity between question and answer...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_question_vs_answer'] = cosine_similarity_score(question_embeddings, answer_embeddings)\n",
    "\n",
    "        print(\"Computing cosine similarity between context and answer...\")\n",
    "        rag_evaluation_df.at[index, 'similarity_score_context_vs_answer'] = cosine_similarity_score(context_embeddings, answer_embeddings)\n",
    "        \n",
    "        # Store some metadata such as model name and tokens statistics\n",
    "        rag_evaluation_df.at[index, \"model\"] = response[\"answer\"].response_metadata[\"model_name\"]\n",
    "        rag_evaluation_df.at[index, \"completion_tokens\"] = response['answer'].response_metadata['token_usage']['completion_tokens']\n",
    "        rag_evaluation_df.at[index, \"prompt_tokens\"] = response['answer'].response_metadata['token_usage']['prompt_tokens']\n",
    "        rag_evaluation_df.at[index, \"total_tokens\"] = response['answer'].response_metadata['token_usage']['total_tokens']\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if all responses have been generated by the RAG pipeline or if there are any `None` values in the answers column. If there are any rows with `None` answers, remove these before they are passed to the RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluation_df = rag_evaluation_df[rag_evaluation_df['answer'] != 'None']\n",
    "rag_evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the results in a CSV file for convenience to avoid having to execute the entire RAG pipeline every time we want to test our RAG evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "rag_evaluation_df.to_csv('datasets/rag/rag_evaluation_dataset_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation with RAGAS\n",
    "\n",
    "RETRIEVAL \n",
    "- **Context Precision**:  how relevant is the `context` to the `question`.\n",
    " - **Context Recall**: given the `ground truth`, is the retriever able to retrieve all the relevant `context`.\n",
    "\n",
    "GENERATION\n",
    " - **Answer Relevancy**: how relevant is the generated `answer` to the `question`. \n",
    " - **Faithfulness**: is the `answer` fact-checkable. The number of correct statements from the given `contexts` divided by the total number of statements in the generated answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to evaluate our RAG pipeline using RAGAS metrics from the `ragas` package. The `evaluate()` function expects a Dataset with specific column names: `question`, `contexts`, `ground_truth`, and `answer`. We will now rename these columns to conform to the expected column names in RAGAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe for RAGAS evaluation\n",
    "ragas_results_df = rag_evaluation_df.copy()\n",
    "\n",
    "# Rename the columns to match ragas convention\n",
    "ragas_results_df.rename(\n",
    "    columns={\n",
    "        \"question_to_llm\": \"question\",\n",
    "        \"context\": \"contexts\"}, \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the 'contexts' column from a string to a list of strings for each row\n",
    "ragas_results_df['contexts'] = ragas_results_df['contexts'].apply(lambda x: [x])\n",
    "\n",
    "ragas_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the RAGAS evaluation metrics row by row, adding the results to corresponding columns for each metric in our evaluation dataset. We first initialize the columns where the evaluation metrics will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df['context_precision'] = ''\n",
    "ragas_results_df['faithfulness'] = ''\n",
    "ragas_results_df['answer_relevancy'] = ''\n",
    "ragas_results_df['context_recall'] = ''\n",
    "ragas_results_df['context_relevancy'] = ''\n",
    "ragas_results_df['answer_correctness'] = ''\n",
    "ragas_results_df['answer_similarity'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "required_fields = [\"question\", \"answer\", \"contexts\", \"ground_truth\"]\n",
    "metrics = [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"context_relevancy\", \"answer_correctness\", \"answer_similarity\"]\n",
    "\n",
    "# Set the variable to the number of rows, limited to a maximum of NUM_OF_NEW_RFP_QUESTIONS\n",
    "number_of_rows_to_process = min(len(ragas_results_df), NUM_OF_NEW_RFP_QUESTIONS)\n",
    "\n",
    "# Mapping of metric names to their respective functions, assuming these functions are predefined\n",
    "metrics_functions = {\n",
    "    \"context_precision\": context_precision,\n",
    "    \"faithfulness\": faithfulness,\n",
    "    \"answer_relevancy\": answer_relevancy,\n",
    "    \"context_recall\": context_recall,\n",
    "    \"context_relevancy\": context_relevancy,\n",
    "    \"answer_correctness\": answer_correctness,\n",
    "    \"answer_similarity\": answer_similarity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This loop processes each row up to a predefined number of rows, evaluating them with specified metrics and storing the results\n",
    "for i, (index, row) in enumerate(rag_evaluation_df.iloc[:number_of_rows_to_process].iterrows()):\n",
    "    print(f\"Processing RFP question {i+1}...\")\n",
    "    print(f\"Question: {ragas_results_df.iloc[i]['question']}\")\n",
    "    print(f\"Answer: {ragas_results_df.iloc[i]['answer']}\")\n",
    "\n",
    "    # Create a temporary Dataset for the current row\n",
    "    ragas_dataset = Dataset.from_pandas(ragas_results_df.iloc[i: i + 1][required_fields])\n",
    "\n",
    "    # Evaluate using RAGAS metrics\n",
    "    evaluation_result = evaluate(\n",
    "        ragas_dataset, \n",
    "        [metrics_functions[metric] for metric in metrics if metric in metrics_functions])\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    # Store evaluation results back into the DataFrame\n",
    "    for metric in metrics:\n",
    "        if metric in evaluation_result:\n",
    "            ragas_results_df.at[i, metric] = evaluation_result[metric]\n",
    "            print(f\"{metric}: {evaluation_result[metric]}\")\n",
    "\n",
    "print(\"All RFP questions processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "ragas_results_df.to_csv('datasets/rag/rag_evaluation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
