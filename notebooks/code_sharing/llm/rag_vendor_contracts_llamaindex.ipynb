{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index llama-index-embeddings-huggingface llama-index-vector-stores-chroma -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore contracts datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the contracts by first loading them as pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contract ID</th>\n",
       "      <th>Vendor Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Services Provided</th>\n",
       "      <th>Annual Spend</th>\n",
       "      <th>Features</th>\n",
       "      <th>Software Provided</th>\n",
       "      <th>Category</th>\n",
       "      <th>Preferred Vendor</th>\n",
       "      <th>Historical Spend</th>\n",
       "      <th>Exclusivity</th>\n",
       "      <th>Engagement Terms</th>\n",
       "      <th>Supported BSLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C001</td>\n",
       "      <td>SirionLabs</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>Contract management for legal teams</td>\n",
       "      <td>$20,000</td>\n",
       "      <td>AI analytics, Contract lifecycle</td>\n",
       "      <td>ContractHub</td>\n",
       "      <td>Contract Management</td>\n",
       "      <td>Yes</td>\n",
       "      <td>$40,000</td>\n",
       "      <td>No</td>\n",
       "      <td>Full lifecycle management</td>\n",
       "      <td>Legal, Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C002</td>\n",
       "      <td>Evisort</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>AI-driven contract analysis</td>\n",
       "      <td>$18,000</td>\n",
       "      <td>Data extraction, Compliance tracking</td>\n",
       "      <td>Evisort Analytics</td>\n",
       "      <td>Data Analytics</td>\n",
       "      <td>No</td>\n",
       "      <td>$36,000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Data-driven contract management</td>\n",
       "      <td>Legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C003</td>\n",
       "      <td>DocuSign</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>Digital signature and workflow management</td>\n",
       "      <td>$12,000</td>\n",
       "      <td>Secure signature, Integration</td>\n",
       "      <td>DocuSign Platform</td>\n",
       "      <td>Document Management</td>\n",
       "      <td>Yes</td>\n",
       "      <td>$24,000</td>\n",
       "      <td>No</td>\n",
       "      <td>Digital signing and workflow</td>\n",
       "      <td>All Departments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C004</td>\n",
       "      <td>Icertis</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>Enterprise contract management</td>\n",
       "      <td>$25,000</td>\n",
       "      <td>Compliance, Contract authoring</td>\n",
       "      <td>Icertis Management</td>\n",
       "      <td>Contract Management</td>\n",
       "      <td>Yes</td>\n",
       "      <td>$50,000</td>\n",
       "      <td>No</td>\n",
       "      <td>Enterprise-wide contract management</td>\n",
       "      <td>Operations, Legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C005</td>\n",
       "      <td>Concord</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>End-to-end contract lifecycle management</td>\n",
       "      <td>$30,000</td>\n",
       "      <td>Automated workflows, Collaboration</td>\n",
       "      <td>Concord Suite</td>\n",
       "      <td>Contract Management</td>\n",
       "      <td>No</td>\n",
       "      <td>$60,000</td>\n",
       "      <td>No</td>\n",
       "      <td>Comprehensive contract management</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Contract ID Vendor Name  Start Date    End Date  \\\n",
       "0        C001  SirionLabs  2023-01-01  2025-12-31   \n",
       "1        C002     Evisort  2023-02-01  2024-01-31   \n",
       "2        C003    DocuSign  2023-03-01  2025-02-28   \n",
       "3        C004     Icertis  2022-07-01  2024-06-30   \n",
       "4        C005     Concord  2023-05-01  2024-04-30   \n",
       "\n",
       "                           Services Provided Annual Spend  \\\n",
       "0        Contract management for legal teams      $20,000   \n",
       "1                AI-driven contract analysis      $18,000   \n",
       "2  Digital signature and workflow management      $12,000   \n",
       "3             Enterprise contract management      $25,000   \n",
       "4   End-to-end contract lifecycle management      $30,000   \n",
       "\n",
       "                               Features   Software Provided  \\\n",
       "0      AI analytics, Contract lifecycle         ContractHub   \n",
       "1  Data extraction, Compliance tracking   Evisort Analytics   \n",
       "2         Secure signature, Integration   DocuSign Platform   \n",
       "3        Compliance, Contract authoring  Icertis Management   \n",
       "4    Automated workflows, Collaboration       Concord Suite   \n",
       "\n",
       "              Category Preferred Vendor Historical Spend Exclusivity  \\\n",
       "0  Contract Management              Yes          $40,000          No   \n",
       "1       Data Analytics               No          $36,000         Yes   \n",
       "2  Document Management              Yes          $24,000          No   \n",
       "3  Contract Management              Yes          $50,000          No   \n",
       "4  Contract Management               No          $60,000          No   \n",
       "\n",
       "                      Engagement Terms     Supported BSLs  \n",
       "0            Full lifecycle management     Legal, Finance  \n",
       "1      Data-driven contract management              Legal  \n",
       "2         Digital signing and workflow    All Departments  \n",
       "3  Enterprise-wide contract management  Operations, Legal  \n",
       "4    Comprehensive contract management           Business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CONTRACT_FILES = [\n",
    "    \"datasets/rag/vendor_contracts_001_020.csv\",\n",
    "    \"datasets/rag/vendor_contracts_021_040.csv\",\n",
    "    \"datasets/rag/vendor_contracts_041_060.csv\",\n",
    "]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "contracts_df = pd.concat(\n",
    "    [pd.read_csv(file) for file in CONTRACT_FILES], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "contracts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore questions and answers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we explore the questions and answers dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question #</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What software do we have available for data an...</td>\n",
       "      <td>Evisort Analytics, Kira QuickSearch, Seal Disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What taxonomy category is Tableau in?</td>\n",
       "      <td>Not listed. The dataset doesn't include Tableau.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Who are the BSLs that support Asset Servicing?</td>\n",
       "      <td>Not specifically mentioned. BSLs include Legal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Is Cisco a preferred vendor?</td>\n",
       "      <td>Cisco is not listed as a vendor in the provide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Which active vendors are covering comparable c...</td>\n",
       "      <td>HighQ might offer collaborative features simil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question #                                           Question  \\\n",
       "0           1  What software do we have available for data an...   \n",
       "1           2              What taxonomy category is Tableau in?   \n",
       "2           3     Who are the BSLs that support Asset Servicing?   \n",
       "3           4                       Is Cisco a preferred vendor?   \n",
       "4           5  Which active vendors are covering comparable c...   \n",
       "\n",
       "                                              Answer  \n",
       "0  Evisort Analytics, Kira QuickSearch, Seal Disc...  \n",
       "1   Not listed. The dataset doesn't include Tableau.  \n",
       "2  Not specifically mentioned. BSLs include Legal...  \n",
       "3  Cisco is not listed as a vendor in the provide...  \n",
       "4  HighQ might offer collaborative features simil...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTION_FILES = [\n",
    "    \"datasets/rag/vendor_contracts_questions.csv\",\n",
    "]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "questions_df = pd.concat(\n",
    "    [pd.read_csv(file) for file in QUESTION_FILES], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_questions_df`, which contains questions and answers, will be stored in the vector store. This will simulate questions that have already been answered and are stored in the database. The `test_questions_df`, on the other hand, will act as a set of new questions posed by the user. The answers in this dataset will be used as ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "questions_df = pd.read_csv(\"datasets/rag/vendor_contracts_questions.csv\")\n",
    "train_questions_df, test_questions_df = train_test_split(questions_df, test_size=0.20)\n",
    "\n",
    "# Rename columns \n",
    "train_questions_df = train_questions_df.rename(columns={\n",
    "    'Question #': 'question_id',\n",
    "    'Question': 'question', \n",
    "    'Answer': 'answer'})\n",
    "test_questions_df = test_questions_df.rename(columns={\n",
    "    'Question #': 'question_id',\n",
    "    'Question': 'question', \n",
    "    'Answer': 'ground_truth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What software do we have available for data an...</td>\n",
       "      <td>Evisort Analytics, Kira QuickSearch, Seal Disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What taxonomy category is Tableau in?</td>\n",
       "      <td>Not listed. The dataset doesn't include Tableau.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Which contracts are valued at over $20,000 but...</td>\n",
       "      <td>SirionLabs, Concord, Agiloft, Zycus, Seal Soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>How much was spent on legal management softwar...</td>\n",
       "      <td>$112,000 (Sum of annual spends for CobbleStone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>List the vendors that offer Electronic Signatu...</td>\n",
       "      <td>DocuSign, PandaDoc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id                                           question  \\\n",
       "0             1  What software do we have available for data an...   \n",
       "1             2              What taxonomy category is Tableau in?   \n",
       "23           24  Which contracts are valued at over $20,000 but...   \n",
       "17           18  How much was spent on legal management softwar...   \n",
       "38           39  List the vendors that offer Electronic Signatu...   \n",
       "\n",
       "                                               answer  \n",
       "0   Evisort Analytics, Kira QuickSearch, Seal Disc...  \n",
       "1    Not listed. The dataset doesn't include Tableau.  \n",
       "23  SirionLabs, Concord, Agiloft, Zycus, Seal Soft...  \n",
       "17  $112,000 (Sum of annual spends for CobbleStone...  \n",
       "38                                 DocuSign, PandaDoc  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Which contract has the most diverse set of fea...</td>\n",
       "      <td>Mitratech, offering comprehensive legal operat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>How many contracts support the IT department?</td>\n",
       "      <td>1 (Agiloft)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>Which contracts include machine learning and c...</td>\n",
       "      <td>None of the contracts include both features to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>What is the average duration of contracts focu...</td>\n",
       "      <td>2.14 years (Average duration for CobbleStone, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>How many contracts provide automated workflows...</td>\n",
       "      <td>5 (SirionLabs, Concord, Onit, Mitratech, Preci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id                                           question  \\\n",
       "20           21  Which contract has the most diverse set of fea...   \n",
       "14           15      How many contracts support the IT department?   \n",
       "48           49  Which contracts include machine learning and c...   \n",
       "44           45  What is the average duration of contracts focu...   \n",
       "26           27  How many contracts provide automated workflows...   \n",
       "\n",
       "                                         ground_truth  \n",
       "20  Mitratech, offering comprehensive legal operat...  \n",
       "14                                        1 (Agiloft)  \n",
       "48  None of the contracts include both features to...  \n",
       "44  2.14 years (Average duration for CobbleStone, ...  \n",
       "26  5 (SirionLabs, Concord, Onit, Mitratech, Preci...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore different document loaders for contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the `CSVReader` from `llama_index` and review the properties of the documents it loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import CSVReader\n",
    "from pathlib import Path\n",
    "\n",
    "reader = CSVReader(concat_rows=False)\n",
    "llama_documents = []\n",
    "\n",
    "# Iterate over each file path in the FILES list\n",
    "for file_path in CONTRACT_FILES:\n",
    "    # Convert string file path to Path object\n",
    "    path_obj = Path(file_path)\n",
    "    \n",
    "    # Load data from each file and append to contract_docs list\n",
    "    llama_documents.extend(reader.load_data(path_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 0b731406-bb47-457c-b4f4-44764993d9ea\n",
      "File Name: vendor_contracts_001_020.csv\n",
      "Text Content:\n",
      " Contract ID, Vendor Name, Start Date, End Date, Services Provided, Annual Spend, Features, Software Provided, Category, Preferred Vendor, Historical Spend, Exclusivity, Engagement Terms, Supported BSLs\n",
      "\n",
      "Document ID: 2d72c5ba-b660-41d4-8a65-265f4664b515\n",
      "File Name: vendor_contracts_001_020.csv\n",
      "Text Content:\n",
      " C001, SirionLabs, 2023-01-01, 2025-12-31, Contract management for legal teams, $20,000, AI analytics, Contract lifecycle, ContractHub, Contract Management, Yes, $40,000, No, Full lifecycle management, Legal, Finance\n",
      "\n",
      "Document ID: 04f852d2-5fd7-4563-862a-658291fa4339\n",
      "File Name: vendor_contracts_001_020.csv\n",
      "Text Content:\n",
      " C002, Evisort, 2023-02-01, 2024-01-31, AI-driven contract analysis, $18,000, Data extraction, Compliance tracking, Evisort Analytics, Data Analytics, No, $36,000, Yes, Data-driven contract management, Legal\n",
      "\n",
      "Document ID: 62bbf011-5e49-41fd-a808-b5e700ae8291\n",
      "File Name: vendor_contracts_001_020.csv\n",
      "Text Content:\n",
      " C003, DocuSign, 2023-03-01, 2025-02-28, Digital signature and workflow management, $12,000, Secure signature, Integration, DocuSign Platform, Document Management, Yes, $24,000, No, Digital signing and workflow, All Departments\n",
      "\n",
      "Document ID: f5cfbda1-e86c-4ff3-85fc-ea58a4eb5bba\n",
      "File Name: vendor_contracts_001_020.csv\n",
      "Text Content:\n",
      " C004, Icertis, 2022-07-01, 2024-06-30, Enterprise contract management, $25,000, Compliance, Contract authoring, Icertis Management, Contract Management, Yes, $50,000, No, Enterprise-wide contract management, Operations, Legal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_to_print = 5\n",
    "\n",
    "for i, doc in enumerate(llama_documents[:number_to_print]):\n",
    "    print(\"Document ID:\", doc.id_)\n",
    "    print(\"File Name:\", doc.metadata['filename']) \n",
    "    print(\"Text Content:\\n\", doc.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try the `CSVLoader` from `langchain` and check the properties of the documents it loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "lc_documents = [] \n",
    "\n",
    "# Iterate through each file path in the list\n",
    "for file_path in CONTRACT_FILES:\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "    )\n",
    "\n",
    "    # Load a document from the current CSV file\n",
    "    doc = loader.load()\n",
    "    \n",
    "    # Append documents\n",
    "    lc_documents.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Page Content:\n",
      "Contract ID: C001\n",
      "Vendor Name: SirionLabs\n",
      "Start Date: 2023-01-01\n",
      "End Date: 2025-12-31\n",
      "Services Provided: Contract management for legal teams\n",
      "Annual Spend: $20,000\n",
      "Features: AI analytics, Contract lifecycle\n",
      "Software Provided: ContractHub\n",
      "Category: Contract Management\n",
      "Preferred Vendor: Yes\n",
      "Historical Spend: $40,000\n",
      "Exclusivity: No\n",
      "Engagement Terms: Full lifecycle management\n",
      "Supported BSLs: Legal, Finance\n",
      "Metadata:\n",
      "source: datasets/rag/vendor_contracts_001_020.csv\n",
      "row: 0\n",
      "\n",
      "Document 2:\n",
      "Page Content:\n",
      "Contract ID: C002\n",
      "Vendor Name: Evisort\n",
      "Start Date: 2023-02-01\n",
      "End Date: 2024-01-31\n",
      "Services Provided: AI-driven contract analysis\n",
      "Annual Spend: $18,000\n",
      "Features: Data extraction, Compliance tracking\n",
      "Software Provided: Evisort Analytics\n",
      "Category: Data Analytics\n",
      "Preferred Vendor: No\n",
      "Historical Spend: $36,000\n",
      "Exclusivity: Yes\n",
      "Engagement Terms: Data-driven contract management\n",
      "Supported BSLs: Legal\n",
      "Metadata:\n",
      "source: datasets/rag/vendor_contracts_001_020.csv\n",
      "row: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_to_print = 2\n",
    "\n",
    "for index, doc in enumerate(lc_documents[:number_to_print]):\n",
    "    print(f\"Document {index + 1}:\")\n",
    "    print(\"Page Content:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling what is stored as metadata and what is stored as page content, which will be converted into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_documents = [] \n",
    "\n",
    "# Iterate through each file path in the list\n",
    "for file_path in CONTRACT_FILES:\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path,\n",
    "        metadata_columns=[\"Contract ID\",\"Supported BSLs\", \"Engagement Terms\"]\n",
    "    )\n",
    "\n",
    "    # Load a document from the current CSV file\n",
    "    doc = loader.load()\n",
    "    \n",
    "    # Append documents\n",
    "    lc_documents.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Page Content:\n",
      "Vendor Name: SirionLabs\n",
      "Start Date: 2023-01-01\n",
      "End Date: 2025-12-31\n",
      "Services Provided: Contract management for legal teams\n",
      "Annual Spend: $20,000\n",
      "Features: AI analytics, Contract lifecycle\n",
      "Software Provided: ContractHub\n",
      "Category: Contract Management\n",
      "Preferred Vendor: Yes\n",
      "Historical Spend: $40,000\n",
      "Exclusivity: No\n",
      "Metadata:\n",
      "source: datasets/rag/vendor_contracts_001_020.csv\n",
      "row: 0\n",
      "Contract ID: C001\n",
      "Supported BSLs: Legal, Finance\n",
      "Engagement Terms: Full lifecycle management\n",
      "\n",
      "Document 2:\n",
      "Page Content:\n",
      "Vendor Name: Evisort\n",
      "Start Date: 2023-02-01\n",
      "End Date: 2024-01-31\n",
      "Services Provided: AI-driven contract analysis\n",
      "Annual Spend: $18,000\n",
      "Features: Data extraction, Compliance tracking\n",
      "Software Provided: Evisort Analytics\n",
      "Category: Data Analytics\n",
      "Preferred Vendor: No\n",
      "Historical Spend: $36,000\n",
      "Exclusivity: Yes\n",
      "Metadata:\n",
      "source: datasets/rag/vendor_contracts_001_020.csv\n",
      "row: 1\n",
      "Contract ID: C002\n",
      "Supported BSLs: Legal\n",
      "Engagement Terms: Data-driven contract management\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_to_print = 2\n",
    "\n",
    "for index, doc in enumerate(lc_documents[:number_to_print]):\n",
    "    print(f\"Document {index + 1}:\")\n",
    "    print(\"Page Content:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert contract documents into vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='2eb390b6-5397-4a6c-9951-6393145d885d', embedding=None, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b731406-bb47-457c-b4f4-44764993d9ea', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, hash='056619adc42f470bce860c49a55866399590bc2e7aa6326f34af4a770b0f7ea7')}, text='Contract ID, Vendor Name, Start Date, End Date, Services Provided, Annual Spend, Features, Software Provided, Category, Preferred Vendor, Historical Spend, Exclusivity, Engagement Terms, Supported BSLs', start_char_idx=0, end_char_idx=201, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='0de47516-f3cc-46d3-840a-1477c82e18cb', embedding=None, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2d72c5ba-b660-41d4-8a65-265f4664b515', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, hash='3a1a3668c5bb99b78f2ba5cf22f85692e5985d470d26f06bcab59bfd57feb7cb')}, text='C001, SirionLabs, 2023-01-01, 2025-12-31, Contract management for legal teams, $20,000, AI analytics, Contract lifecycle, ContractHub, Contract Management, Yes, $40,000, No, Full lifecycle management, Legal, Finance', start_char_idx=0, end_char_idx=215, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='aa729907-3f22-44e9-86ae-e275519b24b3', embedding=None, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='04f852d2-5fd7-4563-862a-658291fa4339', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, hash='fb1619dcb3cc52e416352c7b037b37bc290fe8a8194904f6710c4637d3c0cd4a')}, text='C002, Evisort, 2023-02-01, 2024-01-31, AI-driven contract analysis, $18,000, Data extraction, Compliance tracking, Evisort Analytics, Data Analytics, No, $36,000, Yes, Data-driven contract management, Legal', start_char_idx=0, end_char_idx=206, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='5c860598-d111-4f82-ab20-b127623e5ac6', embedding=None, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='62bbf011-5e49-41fd-a808-b5e700ae8291', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, hash='45085821289919815cc79cc2b67e2c0aed3ed45ccc895f3fc16475d4df8f9fdb')}, text='C003, DocuSign, 2023-03-01, 2025-02-28, Digital signature and workflow management, $12,000, Secure signature, Integration, DocuSign Platform, Document Management, Yes, $24,000, No, Digital signing and workflow, All Departments', start_char_idx=0, end_char_idx=226, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='ad2c7de5-b93d-401e-812e-3abb183b5a0d', embedding=None, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f5cfbda1-e86c-4ff3-85fc-ea58a4eb5bba', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'vendor_contracts_001_020.csv', 'extension': '.csv'}, hash='525143d36999433dbc78086e949b16c8d8d445fb751116a7e6d91183126c22e5')}, text='C004, Icertis, 2022-07-01, 2024-06-30, Enterprise contract management, $25,000, Compliance, Contract authoring, Icertis Management, Contract Management, Yes, $50,000, No, Enterprise-wide contract management, Operations, Legal', start_char_idx=0, end_char_idx=225, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(llama_documents)\n",
    "nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1:\n",
      "Metadata:\n",
      "filename: vendor_contracts_001_020.csv\n",
      "extension: .csv\n",
      "Text Content:\n",
      "Contract ID, Vendor Name, Start Date, End Date, Services Provided, Annual Spend, Features, Software Provided, Category, Preferred Vendor, Historical Spend, Exclusivity, Engagement Terms, Supported BSLs\n",
      "Embeddings: None\n",
      "\n",
      "Node 2:\n",
      "Metadata:\n",
      "filename: vendor_contracts_001_020.csv\n",
      "extension: .csv\n",
      "Text Content:\n",
      "C001, SirionLabs, 2023-01-01, 2025-12-31, Contract management for legal teams, $20,000, AI analytics, Contract lifecycle, ContractHub, Contract Management, Yes, $40,000, No, Full lifecycle management, Legal, Finance\n",
      "Embeddings: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_to_print = 2\n",
    "\n",
    "for index, node in enumerate(nodes[:num_to_print]):\n",
    "    print(f\"Node {index + 1}:\")\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in node.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"Text Content:\")\n",
    "    print(node.text)\n",
    "    print(f\"Embeddings: {node.embedding}\")\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert nodes into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, node in enumerate(nodes):\n",
    "    data.append({\n",
    "        \"Node ID\": index + 1,\n",
    "        \"Text Content\": node.text,\n",
    "    })\n",
    "contracts_nodes_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 17:14:38,545 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node ID</th>\n",
       "      <th>Text Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contract ID, Vendor Name, Start Date, End Date...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C001, SirionLabs, 2023-01-01, 2025-12-31, Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C002, Evisort, 2023-02-01, 2024-01-31, AI-driv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C003, DocuSign, 2023-03-01, 2025-02-28, Digita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C004, Icertis, 2022-07-01, 2024-06-30, Enterpr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Node ID                                       Text Content\n",
       "0        1  Contract ID, Vendor Name, Start Date, End Date...\n",
       "1        2  C001, SirionLabs, 2023-01-01, 2025-12-31, Cont...\n",
       "2        3  C002, Evisort, 2023-02-01, 2024-01-31, AI-driv...\n",
       "3        4  C003, DocuSign, 2023-03-01, 2025-02-28, Digita...\n",
       "4        5  C004, Icertis, 2022-07-01, 2024-06-30, Enterpr..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm_contracts_ds = vm.init_dataset(\n",
    "    input_id=\"contracts_nodes\",\n",
    "    dataset=contracts_nodes_df,\n",
    "    text_column=\"Text Content\", \n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_contracts_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from validmind.models import FunctionModel\n",
    "\n",
    "client = OpenAIEmbedding()\n",
    "\n",
    "def embed(input):\n",
    "    model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "    return model.get_text_embedding(input[\"Text Content\"])\n",
    "\n",
    "vm_embedder = FunctionModel(input_id=\"text-embedding-3-small\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 20:16:43,315 - WARNING(validmind.vm_models.dataset.dataset): Model predictions already assigned... Overwriting.\n",
      "2024-05-08 20:16:43,316 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-08 20:16:43,316 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-08 20:16:43,317 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-08 20:18:00,216 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    }
   ],
   "source": [
    "vm_contracts_ds.assign_predictions(vm_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node ID</th>\n",
       "      <th>Text Content</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "      <th>text-embedding-3-small_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Contract ID, Vendor Name, Start Date, End Date...</td>\n",
       "      <td>[-0.0532541498541832, -0.01645655557513237, 0....</td>\n",
       "      <td>[-0.0532541498541832, -0.01645655557513237, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C001, SirionLabs, 2023-01-01, 2025-12-31, Cont...</td>\n",
       "      <td>[-0.020622380077838898, 0.010668843053281307, ...</td>\n",
       "      <td>[-0.020622380077838898, 0.010668843053281307, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C002, Evisort, 2023-02-01, 2024-01-31, AI-driv...</td>\n",
       "      <td>[-0.019044026732444763, 0.011319427751004696, ...</td>\n",
       "      <td>[-0.019044026732444763, 0.011319427751004696, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C003, DocuSign, 2023-03-01, 2025-02-28, Digita...</td>\n",
       "      <td>[-0.011594770476222038, 0.008915002457797527, ...</td>\n",
       "      <td>[-0.011594770476222038, 0.008915002457797527, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C004, Icertis, 2022-07-01, 2024-06-30, Enterpr...</td>\n",
       "      <td>[0.000986601458862424, 0.0023517629597336054, ...</td>\n",
       "      <td>[0.000986601458862424, 0.0023517629597336054, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Node ID                                       Text Content  \\\n",
       "0        1  Contract ID, Vendor Name, Start Date, End Date...   \n",
       "1        2  C001, SirionLabs, 2023-01-01, 2025-12-31, Cont...   \n",
       "2        3  C002, Evisort, 2023-02-01, 2024-01-31, AI-driv...   \n",
       "3        4  C003, DocuSign, 2023-03-01, 2025-02-28, Digita...   \n",
       "4        5  C004, Icertis, 2022-07-01, 2024-06-30, Enterpr...   \n",
       "\n",
       "                          embedding_model_prediction  \\\n",
       "0  [-0.0532541498541832, -0.01645655557513237, 0....   \n",
       "1  [-0.020622380077838898, 0.010668843053281307, ...   \n",
       "2  [-0.019044026732444763, 0.011319427751004696, ...   \n",
       "3  [-0.011594770476222038, 0.008915002457797527, ...   \n",
       "4  [0.000986601458862424, 0.0023517629597336054, ...   \n",
       "\n",
       "                   text-embedding-3-small_prediction  \n",
       "0  [-0.0532541498541832, -0.01645655557513237, 0....  \n",
       "1  [-0.020622380077838898, 0.010668843053281307, ...  \n",
       "2  [-0.019044026732444763, 0.011319427751004696, ...  \n",
       "3  [-0.011594770476222038, 0.008915002457797527, ...  \n",
       "4  [0.000986601458862424, 0.0023517629597336054, ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_contracts_ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a7b728849149669ca323f0f1ce012e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Cosine Similarity Heatmap</h1>'), HTML(value='<p>Plots an interactive heatmap o…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "test= run_test(\n",
    "    \"validmind.model_validation.embeddings.CosineSimilarityHeatmap\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_contracts_ds,\n",
    "        \"model\": vm_embedder,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885b0881f11f426ca5c8cefe73d6eb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Euclidean Distance Heatmap</h1>'), HTML(value='<p>Plots an interactive heatmap …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test= run_test(\n",
    "    \"validmind.model_validation.embeddings.EuclideanDistanceHeatmap\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_contracts_ds,\n",
    "        \"model\": vm_embedder,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116b2c11ba15451e9dcf0c7c739ca243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>PCA Components Pairwise Plots</h1>'), HTML(value=\"<p>Plots individual scatter p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test= run_test(\n",
    "    \"validmind.model_validation.embeddings.PCAComponentsPairwisePlots\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_contracts_ds,\n",
    "        \"model\": vm_embedder,\n",
    "    },\n",
    "    params = {\n",
    "        \"n_components\": 3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347b5d78bae14f279bbea6a9961ea4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>TSNE Components Pairwise Plots</h1>'), HTML(value=\"<p>Plots individual scatter …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test= run_test(\n",
    "    \"validmind.model_validation.embeddings.TSNEComponentsPairwisePlots\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_contracts_ds,\n",
    "        \"model\": vm_embedder,\n",
    "    },\n",
    "    params = {\n",
    "        \"n_components\": 3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_questions_df`, which contains questions and answers, will be stored in the vector store. This will simulate questions that have already been answered and are stored in the database. The `test_questions_df`, on the other hand, will act as a set of new questions posed by the user. The answers in this dataset will be used as ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "questions_df = pd.read_csv(\"datasets/rag/vendor_contracts_questions.csv\")\n",
    "train_questions_df, test_questions_df = train_test_split(questions_df, test_size=0.20)\n",
    "\n",
    "# Rename columns \n",
    "train_questions_df = train_questions_df.rename(columns={\n",
    "    'Question #': 'question_id',\n",
    "    'Question': 'question', \n",
    "    'Answer': 'answer'})\n",
    "test_questions_df = test_questions_df.rename(columns={\n",
    "    'Question #': 'question_id',\n",
    "    'Question': 'question', \n",
    "    'Answer': 'ground_truth'})\n",
    "\n",
    "contracts_df = pd.read_csv(\"datasets/rag/vendor_contracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data into the vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Store vendor contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"chroma_collection\")\n",
    "\n",
    "# Initialize lists to store data for batch addition\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_documents = []\n",
    "all_ids = []\n",
    "\n",
    "# Loop through the DataFrame rows\n",
    "for index, row in train_df.iterrows():\n",
    "\n",
    "    all_embeddings.append(row[vm_embedder_openai.predict_col])\n",
    "    all_metadatas.append({\n",
    "        'ground_truth': row['ground_truth'],\n",
    "        'hnsw:space': 'cosine'\n",
    "    })\n",
    "    all_documents.append(row['question'])\n",
    "    all_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "# Add all data to the collection in a single operation\n",
    "collection.add(\n",
    "    ids=all_ids, \n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First, we test both the `sentence-transformers` and `openai` embedding models using their native interfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "text = train_questions_df['question'].iloc[0]\n",
    "embeddings = embed_model.get_text_embedding(text)\n",
    "\n",
    "print(f\"Question: {text}\")\n",
    "print(f\"Dimension: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "embeddings = client.embeddings.create(\n",
    "    input=text, \n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding\n",
    "\n",
    "print(f\"Question: {text}\")\n",
    "print(f\"Dimension: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `validmind` embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import EmbeddingModel\n",
    "\n",
    "def embed(question):\n",
    "    \"\"\"Returns an embedding vector for the given text\"\"\"\n",
    "    \n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return embed_model.get_text_embedding(question)\n",
    "\n",
    "\n",
    "vm_embedder_st = EmbeddingModel(\n",
    "    input_id=\"embedding_model_st\",\n",
    "    predict_col=\"embedding_st\",\n",
    "    predict_fn=embed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(question):\n",
    "    \"\"\"Returns an embedding vector for the given text\"\"\"\n",
    "\n",
    "    return (\n",
    "        client.embeddings.create(\n",
    "            input=question,\n",
    "            model=\"text-embedding-3-small\",\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "\n",
    "vm_embedder_openai = EmbeddingModel(\n",
    "    input_id=\"embedding_model_openai\", \n",
    "    predict_col=\"embedding_openai\",\n",
    "    predict_fn=embed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings from the text in the `question` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[vm_embedder_openai.predict_col] = vm_embedder_openai.predict(test_df)\n",
    "#test_df[vm_embedder_st.predict_col] = vm_embedder_st.predict(test_df)\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[vm_embedder_openai.predict_col] = vm_embedder_openai.predict(train_df)\n",
    "#train_df[vm_embedder_st.predict_col] = vm_embedder_st.predict(train_df)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `validmind` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "#vm_train_ds = vm.init_dataset(train_df, text_column=\"question\", __log=False)\n",
    "#vm_test_ds = vm.init_dataset(test_df, text_column=\"question\", __log=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an embedding test for both models to ensure that the embedding models function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "run = False\n",
    "if run:\n",
    "    result = run_test(\n",
    "        \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise:HFEmbeddingModel\",\n",
    "        inputs={\"model\": vm_embedder_st, \"dataset\": vm_test_ds},\n",
    "        params={\"probability\": 0.3},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False\n",
    "if run:\n",
    "    result = run_test(\n",
    "        \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise:OpenAIEmbeddingModel\",\n",
    "        inputs={\"model\": vm_embedder_openai, \"dataset\": vm_test_ds},\n",
    "        params={\"probability\": 0.3},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    result = run_test(\n",
    "        \"validmind.model_validation.embeddings.EmbeddingsDistances\",\n",
    "        inputs={\"dataset\": vm_test_ds},\n",
    "        params={\n",
    "            \"embedding_col_A\": vm_embedder_st.predict_col,\n",
    "            \"embedding_col_B\": vm_embedder_openai.predict_col\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"rfp_rag_collection\")\n",
    "\n",
    "# Initialize lists to store data for batch addition\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_documents = []\n",
    "all_ids = []\n",
    "\n",
    "# Loop through the DataFrame rows\n",
    "for index, row in train_df.iterrows():\n",
    "\n",
    "    all_embeddings.append(row[vm_embedder_openai.predict_col])\n",
    "    all_metadatas.append({\n",
    "        'ground_truth': row['ground_truth'],\n",
    "        'hnsw:space': 'cosine'\n",
    "    })\n",
    "    all_documents.append(row['question'])\n",
    "    all_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "# Add all data to the collection in a single operation\n",
    "collection.add(\n",
    "    ids=all_ids, \n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimensionality of the collection by examining the dimensions of any embedding within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = collection.get(include=['embeddings'])\n",
    "len(res['embeddings'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the retriever by directly querying using the pre-computed embedding corresponding to the question. We expect the vector store to return the top k most similar questions, along with the metadata associated with each of these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "query = VectorStoreQuery(query_embedding=train_df['embedding_openai'][0], similarity_top_k=10)\n",
    "result = chroma_vector_store.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question: {train_df['question'][0]}\\n\")\n",
    "\n",
    "for node, similarity, id_ in zip(result.nodes, result.similarities, result.ids):\n",
    "    print(\"Node ID:\", id_)\n",
    "    print(\"Question:\", node.text)\n",
    "    print(\"Answer:\", node.metadata['ground_truth'])\n",
    "    print(\"Similarity:\", similarity)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import RetrievalModel\n",
    "\n",
    "def retrieve(embedding):\n",
    "\n",
    "    contexts = []\n",
    "    \n",
    "    query = VectorStoreQuery(query_embedding=embedding, similarity_top_k=10)\n",
    "\n",
    "    result = chroma_vector_store.query(query)\n",
    "\n",
    "    for node, similarity, id_ in zip(result.nodes, result.similarities, result.ids):\n",
    "\n",
    "        context = f\"Node ID: {id_}\\n\"\n",
    "        context = f\"Question: {node.text}\\n\"\n",
    "        context += f\"Answer: {node.metadata['ground_truth']}\\n\"\n",
    "        context += f\"Similarity: {similarity}\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "vm_retriever = RetrievalModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_retriever.predict_col] = vm_retriever.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Prompt\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = Prompt(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(\n",
    "    context=test_df[vm_retriever.predict_col][0], \n",
    "    question=test_df['question'][0]\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from validmind.models import GenerationModel\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate(question, contexts):\n",
    "\n",
    "    formatted_prompt = prompt.format(\n",
    "        context=contexts, \n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = GenerationModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_generator.predict_col] = vm_generator.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a ValidMind RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import RAGModel\n",
    "\n",
    "vm_rag_model = RAGModel(\n",
    "    embedder=vm_embedder_openai,\n",
    "    retriever=vm_retriever,\n",
    "    generator=vm_generator,\n",
    "    input_id=\"rag_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = vm_rag_model.predict(test_df)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ragas_ds = vm.init_dataset(result_df, __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
