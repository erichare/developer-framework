{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Model using Langchain library\n",
    "\n",
    "In this notebook, we are going to use LangChain to implement a simple RAG Model for automating the process of answering RFP questions using GenAI. We will see how we can initialize an embedding model, a retrieval model and a generator model with LangChain components and use them within the ValidMind developer framework to run tests against them. Finally, we will see how we can put them together in a Pipeline and run that to get e2e results and run tests against that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_'></a>\n",
    "\n",
    "## About ValidMind\n",
    "\n",
    "ValidMind is a platform for managing model risk, including risk associated with AI and statistical models.\n",
    "\n",
    "You use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "<a id='toc2_1_'></a>\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language. \n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "<a id='toc2_2_'></a>\n",
    "\n",
    "### New to ValidMind?\n",
    "\n",
    "If you haven't already seen our [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html), we recommend you explore the available resources for developers at some point. There, you can learn more about documenting models, find code samples, or read our developer reference.\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "For access to all features available in this notebook, create a free ValidMind account.\n",
    "\n",
    "Signing up is FREE — [**Sign up now!**](https://app.prod.validmind.ai)\n",
    "\n",
    ":::\n",
    "\n",
    "<a id='toc2_3_'></a>\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n",
    "\n",
    "**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n",
    "\n",
    "**Tests**: A function contained in the ValidMind Developer Framework, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n",
    "\n",
    "**Metrics**: A subset of tests that do not have thresholds. In the context of this notebook, metrics and tests can be thought of as interchangeable concepts.\n",
    "\n",
    "**Custom metrics**: Custom metrics are functions that you define to evaluate your model or dataset. These functions can be registered with ValidMind to be used in the platform.\n",
    "\n",
    "**Inputs**: Objects to be evaluated and documented in the ValidMind framework. They can be any of the following:\n",
    "\n",
    "  - **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\n",
    "  - **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\n",
    "  - **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom metric.\n",
    "  - **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom metric. See this [example](https://docs.validmind.ai/notebooks/how_to/run_tests_that_require_multiple_datasets.html) for more information.\n",
    "\n",
    "**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a metric, customize its behavior, or provide additional context.\n",
    "\n",
    "**Outputs**: Custom metrics can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n",
    "\n",
    "**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\n",
    "\n",
    "Example: the [`classifier_full_suite`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html#ClassifierFullSuite) test suite runs tests from the [`tabular_dataset`](https://docs.validmind.ai/validmind/validmind/test_suites/tabular_datasets.html) and [`classifier`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html) test suites to fully document the data and model sections for binary classification model use-cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "Let's go ahead and install the `validmind` library if its not already installed... Then we can install the `qdrant-client` library for our vector store and `langchain` for everything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q qdrant-client langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ValidMind Initialization\n",
    "\n",
    "Now we will import and initialize the ValidMind framework so we can connect to our project in the ValidMind platform. This will let us log inputs, plots, and test results to our model documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 11:14:18,448 - INFO(validmind.api_client): Connected to ValidMind. Project: [Demo] Customer Churn Model - Initial Validation (clnt1f4qc00ap15lfts8ur7lw)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"...\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  project = \"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read openai key\n",
    "\n",
    "We will need to have an OpenAI API key to be able to use their `text-embedding-3-small` model for our embeddings, `gpt-3.5-turbo` model for our generator and `gpt-4-turbo` model for our LLM-as-Judge tests. If you don't have an OpenAI API key, you can get one by signing up at [OpenAI](https://platform.openai.com/signup). Then you can create a `.env` file in the root of your project and the following cell will load it from there. Alternatively, you can just uncomment the line below to directly set the key (not recommended for security reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader\n",
    "\n",
    "Great, now that we have all of our dependencies installed, the developer framework initialized and connected to our model documentation project and our OpenAI API key setup, we can go ahead and load our datasets. We will use the synthetic `RFP` dataset included with ValidMind for this notebook. This dataset contains a variety of RFP questions and ground truth answers that we can use both as the source where our Retriever will search for similar question-answer pairs as well as our test set for evaluating the performance of our RAG model. To do this, we just have to load it and call the preprocess function to get a split of the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample dataset from the library\n",
    "from validmind.datasets.llm.rag import rfp\n",
    "\n",
    "raw_df = rfp.load_data()\n",
    "train_df, test_df = rfp.preprocess(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 11:14:18,553 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2024-05-10 11:14:19,706 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project_Title</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Area</th>\n",
       "      <th>Requester</th>\n",
       "      <th>Status</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Implementation of AI Chatbots for Enhanced Cus...</td>\n",
       "      <td>How do you contribute to the ongoing improveme...</td>\n",
       "      <td>We actively participate in industry working gr...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>Bank C</td>\n",
       "      <td>Awarded</td>\n",
       "      <td>57b8b70d-02c5-4d8e-b69e-cf14de02f7de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Generative AI Solutions for Fraud Detection an...</td>\n",
       "      <td>What support and maintenance do you provide fo...</td>\n",
       "      <td>Post-launch, we offer comprehensive support an...</td>\n",
       "      <td>General</td>\n",
       "      <td>Bank E</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>2b8c98e4-de6a-4ff6-b271-b71cafaab517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Automated Document Processing System Using AI ...</td>\n",
       "      <td>How is user interface and experience considere...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>General</td>\n",
       "      <td>Bank D</td>\n",
       "      <td>Awarded</td>\n",
       "      <td>f767bd61-7de4-439f-a44d-ede488e63d00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>How do you ensure your AI-based apps remain up...</td>\n",
       "      <td>We maintain a dedicated R&amp;D team focused on in...</td>\n",
       "      <td>General</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>edf039c1-e518-45eb-bc8f-d60d0c1466c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Implementation of AI Chatbots for Enhanced Cus...</td>\n",
       "      <td>What measures do you implement to ensure the e...</td>\n",
       "      <td>We prioritize transparency by incorporating ex...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>Bank C</td>\n",
       "      <td>Awarded</td>\n",
       "      <td>d9a098b3-fd94-4069-87eb-7a3a8c24132e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Project_Title  \\\n",
       "91  Implementation of AI Chatbots for Enhanced Cus...   \n",
       "5   Generative AI Solutions for Fraud Detection an...   \n",
       "27  Automated Document Processing System Using AI ...   \n",
       "47            Gen AI-Driven Financial Advisory System   \n",
       "87  Implementation of AI Chatbots for Enhanced Cus...   \n",
       "\n",
       "                                             question  \\\n",
       "91  How do you contribute to the ongoing improveme...   \n",
       "5   What support and maintenance do you provide fo...   \n",
       "27  How is user interface and experience considere...   \n",
       "47  How do you ensure your AI-based apps remain up...   \n",
       "87  What measures do you implement to ensure the e...   \n",
       "\n",
       "                                         ground_truth           Area  \\\n",
       "91  We actively participate in industry working gr...  AI Regulation   \n",
       "5   Post-launch, we offer comprehensive support an...        General   \n",
       "27  Our design philosophy centers on simplicity an...        General   \n",
       "47  We maintain a dedicated R&D team focused on in...        General   \n",
       "87  We prioritize transparency by incorporating ex...  AI Regulation   \n",
       "\n",
       "   Requester        Status                                    id  \n",
       "91    Bank C       Awarded  57b8b70d-02c5-4d8e-b69e-cf14de02f7de  \n",
       "5     Bank E  Under Review  2b8c98e4-de6a-4ff6-b271-b71cafaab517  \n",
       "27    Bank D       Awarded  f767bd61-7de4-439f-a44d-ede488e63d00  \n",
       "47    Bank A  Under Review  edf039c1-e518-45eb-bc8f-d60d0c1466c3  \n",
       "87    Bank C       Awarded  d9a098b3-fd94-4069-87eb-7a3a8c24132e  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_train_ds = vm.init_dataset(\n",
    "    train_df,\n",
    "    text_column=\"question\",\n",
    "    target_column=\"ground_truth\",\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    test_df,\n",
    "    text_column=\"question\",\n",
    "    target_column=\"ground_truth\",\n",
    ")\n",
    "\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data validation\n",
    "\n",
    "Now that we have loaded our dataset, we can go ahead and run some data validation tests right away to start assessing and documenting the quality of our data. Since we are using a text dataset, we can use ValidMind's built-in array of text data quality tests to check that things like number of duplicates, missing values, and other common text data issues are not present in our dataset. We can also run some tests to check the sentiment and toxicity of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "First, let's check for duplicates in our dataset. We can use the `validmind.data_validation.Duplicates` test and pass our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfafc1fa7fc48ff87d915686a0ff8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h1>Duplicates ✅</h1>\\n            <p>Tests dataset for duplicate ent…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.Duplicates\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Next, let's check for stop words in our dataset. We can use the `validmind.data_validation.StopWords` test and pass our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jwalz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba689572b6b14907a96b4295c8ae25ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h1>Stop Words ❌</h1>\\n            <p>Evaluates and visualizes the fr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.StopWords\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations\n",
    "\n",
    "Next, let's check for punctuations in our dataset. We can use the `validmind.data_validation.Punctuations` test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61911d9fdbb4f5db338abb5c9d82384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Punctuations</h1>'), HTML(value=\"<p>Analyzes and visualizes the frequency distr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.Punctuations\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Words\n",
    "\n",
    "Next, let's check for common words in our dataset. We can use the `validmind.data_validation.CommonWord` test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jwalz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0a28c0ae48471e9eaf7e427514dc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Common Words</h1>'), HTML(value=\"<p>Identifies and visualizes the 40 most frequ…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.CommonWords\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Detection\n",
    "\n",
    "For documentation purposes, we can detect and log the languages used in the dataset with the `validmind.data_validation.LanguageDetection` test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3470346c2e1f4a0881e74a7d6fdab0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Language Detection</h1>'), HTML(value=\"<p>Detects the language of each text ent…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.nlp.LanguageDetection\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds\n",
    "    }\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity Score\n",
    "\n",
    "Now, let's go ahead and run the `validmind.data_validation.nlp.Toxicity` test to compute a toxicity score for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d6cbe7532e4b2f8dc8a19ed4a0698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Toxicity</h1>'), HTML(value=\"<p>Analyzes the toxicity of text data within a dat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.Toxicity\", inputs={\"dataset\": vm_train_ds}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity and Subjectivity\n",
    "\n",
    "We can also run the `validmind.data_validation.nlp.PolarityAndSubjectivity` test to compute the polarity and subjectivity of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e308b2ea4b294b5d886dfc64668dd304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Polarity And Subjectivity</h1>'), HTML(value='<p>Analyzes the polarity and subj…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.PolarityAndSubjectivity\",\n",
    "    inputs={\"dataset\": vm_train_ds},\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment\n",
    "\n",
    "Finally, we can run the `validmind.data_validation.nlp.Sentiment` test to plot the sentiment of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32c78fa56214962a5b9a0013d7ac064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Sentiment</h1>'), HTML(value=\"<p>Analyzes the sentiment of text data within a d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.Sentiment\", inputs={\"dataset\": vm_train_ds}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model\n",
    "\n",
    "Now that we have our dataset loaded and have run some data validation tests to assess and document the quality of our data, we can go ahead and initialize our embedding model. We will use the `text-embedding-3-small` model from OpenAI for this purpose wrapped in the `OpenAIEmbeddings` class from LangChain. This model will be used to \"embed\" our questions both for inserting the question-answer pairs from the \"train\" set into the vector store and for embedding the question from inputs when making predictions with our RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_client = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def embed(input):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    return embedding_client.embed_query(input[\"question\"])\n",
    "\n",
    "vm_embedder = vm.init_model(input_id=\"embedding_model\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done here is to initialize the `OpenAIEmbeddings` class so it uses OpenAI's `text-embedding-3-small` model. We then created an `embed` function that takes in an `input` dictionary and uses the `embed_query` method of the embedding client to compute the embeddings of the `question`. We use an `embed` function since that is how ValidMind supports any custom model. We will use this strategy for the retrieval and generator models as well but you could also use, say, a HuggingFace model directly. See the documentation for more information on which model types are directly supported - [ValidMind Documentation](https://docs.validmind.ai/validmind/validmind.html)... Finally, we use the `init_model` function from the ValidMind framework to create a `VMModel` object that can be used in ValidMind tests. This also logs the model to our model documentation project and any test that uses the model will be linked to the logged model and its metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Predictions\n",
    "\n",
    "To precompute the embeddings for our test set, we can call the `assign_predictions` method of our `vm_test_ds` object we created above. This will compute the embeddings for each question in the test set and store them in the a special prediction column of the test set thats linked to our `vm_embedder` model. This will allow us to use these embeddings later when we run tests against our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 11:14:32,422 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-10 11:14:32,423 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-10 11:14:32,423 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-10 11:14:37,283 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "VMDataset object: \n",
      "=================\n",
      "Input ID: dataset\n",
      "Target Column: ground_truth\n",
      "Feature Columns: ['Project_Title', 'question', 'Area', 'Requester', 'Status', 'id']\n",
      "Text Column: question\n",
      "Extra Columns: ExtraColumns(extras=set(), group_by_column=None, prediction_columns={'embedding_model': 'embedding_model_prediction'}, probability_columns={})\n",
      "Target Class Labels: None\n",
      "Columns: ['Project_Title', 'question', 'ground_truth', 'Area', 'Requester', 'Status', 'id', 'embedding_model_prediction']\n",
      "Index: [ 91   5  27  47  87  30  97  60  84  52 111  34  24  93  73  86  57 108\n",
      "  78  39 107  99  38]\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(vm_embedder)\n",
    "print(vm_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tests\n",
    "\n",
    "Now that everything is setup for the embedding model, we can go ahead and run some tests to assess and document the quality of our embeddings. We will use the `validmind.model_validation.embeddings.*` tests to compute a variety of metrics against our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539d409d3b034c18b8028fb7fd69b197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h1>Stability Analysis Random Noise ✅</h1>\\n            <p>Evaluate r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params={\"probability\": 0.3},\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9a6d63fe8e4de9bcd72fa1eca9306b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Cosine Similarity Heatmap</h1>'), HTML(value='<p>Plots an interactive heatmap o…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.CosineSimilarityHeatmap\",\n",
    "    inputs = {\"model\": vm_embedder, \"dataset\": vm_test_ds}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8515b11b60744a6a9b28e1bad7d0a3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Euclidean Distance Heatmap</h1>'), HTML(value='<p>Plots an interactive heatmap …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.EuclideanDistanceHeatmap\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c135c2a9a7745d28e413fe10fdbe09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>PCA Components Pairwise Plots</h1>'), HTML(value=\"<p>Plots individual scatter p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.PCAComponentsPairwisePlots\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params = {\"n_components\": 3}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3176b8dffa42b3ad22df10cf7714fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>TSNE Components Pairwise Plots</h1>'), HTML(value=\"<p>Plots individual scatter …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.TSNEComponentsPairwisePlots\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params = {\"n_components\": 3, \"perplexity\": 20}\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Vector Store\n",
    "\n",
    "Great, so now that we have assessed our embedding model and verified that it is performing well, we can go ahead and use it to compute embeddings for our question-answer pairs in the \"train\" set. We will then use these embeddings to insert the question-answer pairs into a vector store. We will use an in-memory `qdrant` vector database for demo purposes but any option would work just as well here. We will use the `QdrantClient` class from LangChain to interact with the vector store. This class will allow us to insert and search for embeddings in the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for the Train Set\n",
    "\n",
    "We can use the same `assign_predictions` method from earlier except this time we will use the `vm_train_ds` object to compute the embeddings for the question-answer pairs in the \"train\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 11:14:48,231 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-10 11:14:48,232 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-10 11:14:48,232 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-10 11:15:06,723 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "VMDataset object: \n",
      "=================\n",
      "Input ID: dataset\n",
      "Target Column: ground_truth\n",
      "Feature Columns: ['Project_Title', 'question', 'Area', 'Requester', 'Status', 'id']\n",
      "Text Column: question\n",
      "Extra Columns: ExtraColumns(extras=set(), group_by_column=None, prediction_columns={'embedding_model': 'embedding_model_prediction'}, probability_columns={})\n",
      "Target Class Labels: None\n",
      "Columns: ['Project_Title', 'question', 'ground_truth', 'Area', 'Requester', 'Status', 'id', 'embedding_model_prediction']\n",
      "Index: [ 16  71  20  42  18  32 109  68  85  51 113  21  17  80  36  14 100   9\n",
      "  94  59  82  81 104  48  77  67  46  40   1  11  75  83  44  43  63  90\n",
      "  56 105 102  89  65  55 112  50  31  66   3  15 101  41  70 103   8  33\n",
      " 110  35  37 114  72  92  79  19  53  13  76  61  10  29  26   0   6  49\n",
      "  22 106  45  62  25  28   4  54   7   2  95  69  88  12  96  74  64  98\n",
      "  23  58]\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm_train_ds.assign_predictions(vm_embedder)\n",
    "print(vm_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert embeddings and questions into Vector DB\n",
    "\n",
    "Now that we have computed the embeddings for our question-answer pairs in the \"train\" set, we can go ahead and insert them into the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# load documents from dataframe\n",
    "loader = DataFrameLoader(train_df, page_content_column=\"question\")\n",
    "docs = loader.load()\n",
    "# choose model using embedding client\n",
    "embedding_client = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# setup vector datastore\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embedding_client,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"rfp_rag_collection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Model\n",
    "\n",
    "Now that we have an embedding model and a vector database setup and loaded with our data, we need a Retrieval model that can search for similar question-answer pairs for a given input question. Once created, we can initialize this as a ValidMind model and `assign_predictions` to it just like our embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(input):\n",
    "    contexts = []\n",
    "\n",
    "    for result in qdrant.similarity_search_with_score(input[\"question\"]):\n",
    "        document, score = result\n",
    "        context = f\"Q: {document.page_content}\\n\"\n",
    "        context += f\"A: {document.metadata['ground_truth']}\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "vm_retriever = vm.init_model(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 11:15:08,632 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-10 11:15:08,636 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-10 11:15:08,637 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-10 11:15:14,288 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "VMDataset object: \n",
      "=================\n",
      "Input ID: dataset\n",
      "Target Column: ground_truth\n",
      "Feature Columns: ['Project_Title', 'question', 'Area', 'Requester', 'Status', 'id']\n",
      "Text Column: question\n",
      "Extra Columns: ExtraColumns(extras=set(), group_by_column=None, prediction_columns={'embedding_model': 'embedding_model_prediction', 'retrieval_model': 'retrieval_model_prediction'}, probability_columns={})\n",
      "Target Class Labels: None\n",
      "Columns: ['Project_Title', 'question', 'ground_truth', 'Area', 'Requester', 'Status', 'id', 'embedding_model_prediction', 'retrieval_model_prediction']\n",
      "Index: [ 91   5  27  47  87  30  97  60  84  52 111  34  24  93  73  86  57 108\n",
      "  78  39 107  99  38]\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(model=vm_retriever)\n",
    "print(vm_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Model\n",
    "\n",
    "As the final piece of this simple RAG pipeline, we can create and initialize a generation model that will use the retrieved context to generate an answer to the input question. We will use the `gpt-3.5-turbo` model from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "Ignore your pre-existing knowledge and answer the question based on the provided context.\n",
    "\"\"\".strip()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def generate(input):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n\".join(input[\"retrieval_model\"])},\n",
    "            {\"role\": \"user\", \"content\": input[\"question\"]},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = vm.init_model(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your name is Anil.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vm_generator.predict(pd.DataFrame({\"retrieval_model\": [[\"My name is anil\"]], \"question\": [\"what is my name\"]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup RAG Pipeline Model\n",
    "\n",
    "Now that we have all of our individual \"component\" models setup and initialized we need some way to put them all together in a single \"pipeline\". We can use the `PipelineModel` class to do this. This ValidMind model type simply wraps any number of other ValidMind models and runs them in sequence. We can use a pipe(`|`) operator - in Python this is normally an `or` operator but we have overloaded it for easy pipeline creation - to chain together our models. We can then initialize this pipeline model and assign predictions to it just like any other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_rag_model = vm.init_model(vm_retriever | vm_generator, input_id=\"rag_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can `assign_predictions` to the pipeline model just like we did with the individual models. This will run the pipeline on the test set and store the results in the test set for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 11:15:15,328 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-10 11:15:15,329 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-10 11:15:15,329 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-10 11:16:29,442 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "VMDataset object: \n",
      "=================\n",
      "Input ID: dataset\n",
      "Target Column: ground_truth\n",
      "Feature Columns: ['Project_Title', 'question', 'Area', 'Requester', 'Status', 'id']\n",
      "Text Column: question\n",
      "Extra Columns: ExtraColumns(extras=set(), group_by_column=None, prediction_columns={'embedding_model': 'embedding_model_prediction', 'retrieval_model': 'retrieval_model_prediction', 'rag_model': 'rag_model_prediction'}, probability_columns={})\n",
      "Target Class Labels: None\n",
      "Columns: ['Project_Title', 'question', 'ground_truth', 'Area', 'Requester', 'Status', 'id', 'embedding_model_prediction', 'retrieval_model_prediction', 'rag_model_prediction']\n",
      "Index: [ 91   5  27  47  87  30  97  60  84  52 111  34  24  93  73  86  57 108\n",
      "  78  39 107  99  38]\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(model=vm_rag_model)\n",
    "print(vm_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project_Title</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>Area</th>\n",
       "      <th>Requester</th>\n",
       "      <th>Status</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "      <th>retrieval_model_prediction</th>\n",
       "      <th>rag_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Implementation of AI Chatbots for Enhanced Cus...</td>\n",
       "      <td>How do you contribute to the ongoing improveme...</td>\n",
       "      <td>We actively participate in industry working gr...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>Bank C</td>\n",
       "      <td>Awarded</td>\n",
       "      <td>57b8b70d-02c5-4d8e-b69e-cf14de02f7de</td>\n",
       "      <td>[-0.0050807861365839, 0.026934581241044576, 0....</td>\n",
       "      <td>[Q: How do you contribute to the ongoing devel...</td>\n",
       "      <td>We actively participate in industry working gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Generative AI Solutions for Fraud Detection an...</td>\n",
       "      <td>What support and maintenance do you provide fo...</td>\n",
       "      <td>Post-launch, we offer comprehensive support an...</td>\n",
       "      <td>General</td>\n",
       "      <td>Bank E</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>2b8c98e4-de6a-4ff6-b271-b71cafaab517</td>\n",
       "      <td>[-0.018275110017933974, 0.016940619765829782, ...</td>\n",
       "      <td>[Q: What types of post-launch support and main...</td>\n",
       "      <td>Post-launch, we offer comprehensive support an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Automated Document Processing System Using AI ...</td>\n",
       "      <td>How is user interface and experience considere...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>General</td>\n",
       "      <td>Bank D</td>\n",
       "      <td>Awarded</td>\n",
       "      <td>f767bd61-7de4-439f-a44d-ede488e63d00</td>\n",
       "      <td>[-0.020680216227465752, 0.008899854934113305, ...</td>\n",
       "      <td>[Q: What considerations do you take into accou...</td>\n",
       "      <td>Our design philosophy prioritizes simplicity a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Gen AI-Driven Financial Advisory System</td>\n",
       "      <td>How do you ensure your AI-based apps remain up...</td>\n",
       "      <td>We maintain a dedicated R&amp;D team focused on in...</td>\n",
       "      <td>General</td>\n",
       "      <td>Bank A</td>\n",
       "      <td>Under Review</td>\n",
       "      <td>edf039c1-e518-45eb-bc8f-d60d0c1466c3</td>\n",
       "      <td>[-0.015557997606513376, 0.001083239429675231, ...</td>\n",
       "      <td>[Q: How do you maintain your AI applications w...</td>\n",
       "      <td>We maintain a dedicated R&amp;D team focused on in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Implementation of AI Chatbots for Enhanced Cus...</td>\n",
       "      <td>What measures do you implement to ensure the e...</td>\n",
       "      <td>We prioritize transparency by incorporating ex...</td>\n",
       "      <td>AI Regulation</td>\n",
       "      <td>Bank C</td>\n",
       "      <td>Awarded</td>\n",
       "      <td>d9a098b3-fd94-4069-87eb-7a3a8c24132e</td>\n",
       "      <td>[0.0071566046813090175, -0.012519419623633132,...</td>\n",
       "      <td>[Q: What steps do you take to ensure the trans...</td>\n",
       "      <td>We prioritize transparency by incorporating ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Project_Title  \\\n",
       "91  Implementation of AI Chatbots for Enhanced Cus...   \n",
       "5   Generative AI Solutions for Fraud Detection an...   \n",
       "27  Automated Document Processing System Using AI ...   \n",
       "47            Gen AI-Driven Financial Advisory System   \n",
       "87  Implementation of AI Chatbots for Enhanced Cus...   \n",
       "\n",
       "                                             question  \\\n",
       "91  How do you contribute to the ongoing improveme...   \n",
       "5   What support and maintenance do you provide fo...   \n",
       "27  How is user interface and experience considere...   \n",
       "47  How do you ensure your AI-based apps remain up...   \n",
       "87  What measures do you implement to ensure the e...   \n",
       "\n",
       "                                         ground_truth           Area  \\\n",
       "91  We actively participate in industry working gr...  AI Regulation   \n",
       "5   Post-launch, we offer comprehensive support an...        General   \n",
       "27  Our design philosophy centers on simplicity an...        General   \n",
       "47  We maintain a dedicated R&D team focused on in...        General   \n",
       "87  We prioritize transparency by incorporating ex...  AI Regulation   \n",
       "\n",
       "   Requester        Status                                    id  \\\n",
       "91    Bank C       Awarded  57b8b70d-02c5-4d8e-b69e-cf14de02f7de   \n",
       "5     Bank E  Under Review  2b8c98e4-de6a-4ff6-b271-b71cafaab517   \n",
       "27    Bank D       Awarded  f767bd61-7de4-439f-a44d-ede488e63d00   \n",
       "47    Bank A  Under Review  edf039c1-e518-45eb-bc8f-d60d0c1466c3   \n",
       "87    Bank C       Awarded  d9a098b3-fd94-4069-87eb-7a3a8c24132e   \n",
       "\n",
       "                           embedding_model_prediction  \\\n",
       "91  [-0.0050807861365839, 0.026934581241044576, 0....   \n",
       "5   [-0.018275110017933974, 0.016940619765829782, ...   \n",
       "27  [-0.020680216227465752, 0.008899854934113305, ...   \n",
       "47  [-0.015557997606513376, 0.001083239429675231, ...   \n",
       "87  [0.0071566046813090175, -0.012519419623633132,...   \n",
       "\n",
       "                           retrieval_model_prediction  \\\n",
       "91  [Q: How do you contribute to the ongoing devel...   \n",
       "5   [Q: What types of post-launch support and main...   \n",
       "27  [Q: What considerations do you take into accou...   \n",
       "47  [Q: How do you maintain your AI applications w...   \n",
       "87  [Q: What steps do you take to ensure the trans...   \n",
       "\n",
       "                                 rag_model_prediction  \n",
       "91  We actively participate in industry working gr...  \n",
       "5   Post-launch, we offer comprehensive support an...  \n",
       "27  Our design philosophy prioritizes simplicity a...  \n",
       "47  We maintain a dedicated R&D team focused on in...  \n",
       "87  We prioritize transparency by incorporating ex...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_test_ds.df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests\n",
    "\n",
    "Let's go ahead and run some of our new RAG tests against our model...\n",
    "\n",
    "> Note: these tests are still being developed and are not yet in a stable state. We are using advanced tests here that use LLM-as-Judge and other strategies to assess things like the relevancy of the retrieved context to the input question and the correctness of the generated answer when compared to the ground truth. There is more to come in this area so stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we are just plotting the results of the tests here for demo purposes, but these will get moved into the test so we can log the plots as well to use them in our model documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Similarity\n",
    "\n",
    "The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.\n",
    "\n",
    "Measuring the semantic similarity between answers can offer valuable insights into the quality of the generated response. This evaluation utilizes a cross-encoder model to calculate the semantic similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fba5450a8747f3a9ca21293180c4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb56a293b754743aab09c81b674d3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Answer Similarity</h1>'), HTML(value='<p>Calculates the answer similarity metri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Entity Recall\n",
    "\n",
    "This metric gives the measure of recall of the retrieved context, based on the number of entities present in both ground_truths and contexts relative to the number of entities present in the ground_truths alone. Simply put, it is a measure of what fraction of entities are recalled from ground_truths. This metric is useful in fact-based use cases like tourism help desk, historical QA, etc. This metric can help evaluate the retrieval mechanism for entities, based on comparison with entities present in ground_truths, because in cases where entities matter, we need the contexts which cover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1849fe229cb046c8a74de71e40e1b692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492346a8568d40f1a40d86938358631d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Context Entity Recall</h1>'), HTML(value='<p>Evaluates the context entity recal…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextEntityRecall\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Precision\n",
    "\n",
    "Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground_truth and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c82908116d94995991d297382cd9e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f33db84f73b4c60a189b4e9d0fe0e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Context Precision</h1>'), HTML(value='<p>Evaluates the context precision metric…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextPrecision\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Relevancy\n",
    "\n",
    "This metric gauges the relevancy of the retrieved context, calculated based on both the question and contexts. The values fall within the range of (0, 1), with higher values indicating better relevancy.\n",
    "\n",
    "Ideally, the retrieved context should exclusively contain essential information to address the provided query. To compute this, we initially estimate the value of by identifying sentences within the retrieved context that are relevant for answering the given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7750e14881f04ed0af271962dcfceacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e88240e5ec346cdaa25c653439e3eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Context Relevancy</h1>'), HTML(value='<p>Evaluates the context relevancy metric…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRelevancy\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.Faithfulness\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerRelevance\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRecall\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerCorrectness\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AspectCritique\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    params= {\n",
    "        \"question_column\":\"question\",\n",
    "        \"answer_column\":\"rag_model_prediction\",\n",
    "        \"ground_truth_column\":\"ground_truth\",\n",
    "        \"contexts_column\":\"retrieval_model_prediction\"\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-pPj8dHa5-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
