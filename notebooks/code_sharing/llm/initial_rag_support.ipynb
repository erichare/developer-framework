{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Framework RAG Model Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how the developer framework supports RAG Models. It introduces a new `RAGModel` class which is a pipeline model made up of 3 component models: `EmbeddingModel`, `RetrievalModel` and `GenerationModel`. This allows developers to test the individual component models as well as the entire e2e pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install -q qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "import os\n",
    "from csv import DictReader\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_map = {\"RFP_Question\": \"question\", \"RFP_Answer\": \"ground_truth\"}\n",
    "\n",
    "\n",
    "def load_documents(prefix):\n",
    "    documents = []\n",
    "    root_dir = \"datasets/rag/\"\n",
    "    for file in os.listdir(root_dir):\n",
    "        if file.startswith(prefix) and file.endswith(\".csv\"):\n",
    "            # use csv dict reader to load the csv file\n",
    "            with open(os.path.join(root_dir, file)) as f:\n",
    "                reader = DictReader(f)\n",
    "                for row in reader:\n",
    "                    # add a unique id to the row\n",
    "                    row[\"id\"] = str(uuid4())\n",
    "                    documents.append(row)\n",
    "\n",
    "    df = pd.DataFrame(documents)\n",
    "    df = df[[\"id\", \"RFP_Question\", \"RFP_Answer\"]]\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_split(limit=None):\n",
    "    df = load_documents(\"rfp_existing_questions\")\n",
    "\n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "\n",
    "    # split the dataset into a \"train\" - which gets inserted into the vector store\n",
    "    # and a \"test\" - which is used to evaluate the search results\n",
    "    train_df = df.sample(frac=0.8)\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First let's setup our embedding model and run some tests to make sure its working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from validmind.models import EmbeddingModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def embed(question):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    return (\n",
    "        client.embeddings.create(\n",
    "            input=question,\n",
    "            model=\"text-embedding-3-small\",\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "\n",
    "vm_embedder = EmbeddingModel(input_id=\"embedding_model\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at whats going on above... The `EmbeddingModel` we just instatiated is a subclass of `FunctionModel` type. `FunctionModel`s allow you to pass a `predict_fn` that will be called to compute 'predictions' for an input. By default, the `FunctionModel` will look at the `predict_fn`'s signature and pass in columns that match the argument names. So in the above model, it will look in the input dataframe for a column named `question` and pass that into the function. Once the `predict_fn` returns the prediction, it will be stored in the column thats specified by the `predict_col` property. This is set by default to `'embedding'` for `EmbeddingModel` instances. Other model types may set a default as well but this can always be customized by passing `predict_col` as an argument to any `VMModel` class or to `vm.init_model()` function. This column name is important since predictions are cached/stored in the validmind dataset that gets passed into tests and its also used in the `RAGModel` to store intermediate predictions before they are passed to the next model in the pipeline.\n",
    "\n",
    "RAG Pipeline:\n",
    "`input` ?-> `EmbeddingModel()` -> `RetrievalModel()` -> `GenerationModel()` -> `output`\n",
    "\n",
    "Below we'll see more about how to customize the `predict_col` and more advanced ways to use `predict_args` to pass parameters to the `predict_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm_embedder.predict_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our test dataset so we can run it through our different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "train_df, test_df = load_dataset_split()\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    test_df,\n",
    "    text_column=\"question\", # some NLP and Embedding tests which work with text data require a `text_column` to be specified\n",
    "    target_column=\"ground_truth\",\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's run our embeddings model on the test dataframe and save the results back into the dataframe.\n",
    "\n",
    "> Normally, you would not do this, but for sake of demonstration we are going to call the `predict` method and manually pass in the dataframe. This would normally be done using `vm_test_ds.assign_predictions(embedding_model)` or, if calling the `RAGModel`, it would be done as part of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm_test_ds.df[vm_embedder.predict_col] = vm_embedder.predict(vm_test_ds.df)\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and run one of the ValidMind embeddings stability analysis tests to make sure our embeddings model is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params={\"probability\": 0.3},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_df[vm_embedder.predict_col] = vm_embedder.predict(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "qdrant.recreate_collection(\n",
    "    \"rfp_rag_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "qdrant.upsert(\n",
    "    \"rfp_rag_collection\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=row[\"id\"],\n",
    "            vector=row[vm_embedder.predict_col],\n",
    "            payload={\"question\": row[\"question\"], \"ground_truth\": row[\"ground_truth\"]},\n",
    "        )\n",
    "        for _, row in train_df.iterrows()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.models import RetrievalModel\n",
    "\n",
    "\n",
    "def retrieve(query_embedding, limit=10):\n",
    "    contexts = []\n",
    "\n",
    "    for result in qdrant.search(\n",
    "        \"rfp_rag_collection\",\n",
    "        query_vector=query_embedding,\n",
    "        limit=limit,\n",
    "    ):\n",
    "        context = f\"Q: {result.payload['question']}\\n\"\n",
    "        context += f\"A: {result.payload['ground_truth']}\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "vm_retriever = RetrievalModel(\n",
    "    input_id=\"retrieval_model\",\n",
    "    predict_fn=retrieve,  # function to call to retrieve the contexts\n",
    "    predict_args={\n",
    "        \"query_embedding\": vm_embedder.predict_col,\n",
    "        \"limit\": lambda row: row[\"limit\"] if \"limit\" in row else 10,\n",
    "    },  # argument mapping for the predict_fn\n",
    "    predict_col=\"retrieved_contexts\",  # column name where the retrieved contexts will be stored\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we are passing the `predict_args` dictionary which contains keys matching the argument names to the `retrieve()` function. This is how you can customize the inputs. In the above case, there are two arguments, `query_embedding` and `limit`. For `query_embedding` we are setting the value to the column name where the embeddings are stored - in this case, the `predict_col` of the `EmbeddingModel`. For `limit`, we are passing a lambda function that takes the row (a dictionary of the columns) and returns the value of the `limit` column if it exists, otherwise it returns 10. This would actually be automatically handled since the `limit` argument for the `retrieve()` function already sets a default but we are explicitly setting it here to demonstrate dynamic arguments.\n",
    "\n",
    "Let's see how the arguments are built from a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# this is called internally whenever the `model.predict()` method is called\n",
    "inputs = vm_retriever._get_args(vm_test_ds.df.to_dict(orient=\"records\")[0])\n",
    "\n",
    "# lets print out the inputs and their types\n",
    "print(\"Inputs for `retrieve()` from the first row of our test dataset:\")\n",
    "print(\"\\n\".join([f\"{k}: {type(v)}\" for k, v in inputs.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm_test_ds.df[vm_retriever.predict_col] = vm_retriever.predict(vm_test_ds.df)\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.models import GenerationModel\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "Ignore your pre-existing knowledge and answer the question based on the provided context.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate(question, contexts):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n\".join(contexts)},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = GenerationModel(\n",
    "    input_id=\"generation_model\",\n",
    "    predict_fn=generate,\n",
    "    predict_args={\n",
    "        \"contexts\": vm_retriever.predict_col,\n",
    "        # question will automatically be inferred from the `generate()` function signature\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm_test_ds.df[vm_generator.predict_col] = vm_generator.predict(vm_test_ds.df)\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG Model (Pipeline of \"Component\" Models)\n",
    "\n",
    "Now that we have our individual models setup, let's create a `RAGModel` instance that will chain them together and give us a single model that can be evalated end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.models import RAGModel\n",
    "\n",
    "vm_rag_model = RAGModel(\n",
    "    embedder=vm_embedder,\n",
    "    retriever=vm_retriever,\n",
    "    generator=vm_generator,\n",
    "    input_id=\"rag_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the test dataset through the entire pipeline. It will overwrite the current predictions that we generated from the individual models, but the key here is that calling `predict` on the `RAGModel` will run the entire pipeline and store the intermediate predictions in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result_df = vm_rag_model.predict(vm_test_ds.df)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with some RAGAS Metrics\n",
    "\n",
    "Below I am just experimenting to see how the RAGAS metrics can work with the `RAGModel` instance. This is not a full implementation of the RAGAS metrics but just a poc. We'll want to make this work in a more general way so that the columns can be properly mapped from the user-provided `predict_col` or the default `predict_col` to the column names that RAGAS expects i.e. `question`, `contexts`, `answer`, `ground_truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm_ragas_ds = vm.init_dataset(result_df, __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextEntityRecall\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextPrecision\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRelevancy\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
