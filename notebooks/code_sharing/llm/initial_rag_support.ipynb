{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Framework RAG Model Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "import os\n",
    "from csv import DictReader\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_map = {\"RFP_Question\": \"question\", \"RFP_Answer\": \"ground_truth\"}\n",
    "\n",
    "\n",
    "def load_documents(prefix):\n",
    "    documents = []\n",
    "    root_dir = \"datasets/rag/\"\n",
    "    for file in os.listdir(root_dir):\n",
    "        if file.startswith(prefix) and file.endswith(\".csv\"):\n",
    "            # use csv dict reader to load the csv file\n",
    "            with open(os.path.join(root_dir, file)) as f:\n",
    "                reader = DictReader(f)\n",
    "                for row in reader:\n",
    "                    # add a unique id to the row\n",
    "                    row[\"id\"] = str(uuid4())\n",
    "                    documents.append(row)\n",
    "\n",
    "    df = pd.DataFrame(documents)\n",
    "    df = df[[\"id\", \"RFP_Question\", \"RFP_Answer\"]]\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_split():\n",
    "    df = load_documents(\"rfp_existing_questions\")\n",
    "\n",
    "    # split the dataset into a \"train\" - which gets inserted into the vector store\n",
    "    # and a \"test\" - which is used to evaluate the search results\n",
    "    train_df = df.sample(frac=0.8)\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First let's setup our embedding model and run some tests to make sure its working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from validmind.models import EmbeddingModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def embed(question):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    return (\n",
    "        client.embeddings.create(\n",
    "            input=question,\n",
    "            model=\"text-embedding-3-small\",\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "\n",
    "vm_embedder = EmbeddingModel(\n",
    "    input_id=\"embedding_model\",\n",
    "    predict_fn=embed,\n",
    "    input_map={\"question\": \"question\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `EmbeddingModel` class sets an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmbeddingModel' object has no attribute 'prediction_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvm_embedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_column\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EmbeddingModel' object has no attribute 'prediction_column'"
     ]
    }
   ],
   "source": [
    "vm_embedder.prediction_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "train_df, test_df = load_dataset_split()\n",
    "\n",
    "vm_test_ds = vm.init_dataset(test_df, text_column=\"question\", __log=False)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_embedder.prediction_column] = vm_embedder.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params={\"probability\": 0.3},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[vm_embedder.prediction_column] = vm_embedder.predict(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "qdrant.recreate_collection(\n",
    "    \"rfp_rag_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "qdrant.upsert(\n",
    "    \"rfp_rag_collection\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=row[\"id\"],\n",
    "            vector=row[vm_embedder.prediction_column],\n",
    "            payload={\"question\": row[\"question\"], \"ground_truth\": row[\"ground_truth\"]},\n",
    "        )\n",
    "        for _, row in train_df.iterrows()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import RetrievalModel\n",
    "\n",
    "def retrieve(embedding):\n",
    "    contexts = []\n",
    "\n",
    "    for result in qdrant.search(\n",
    "        \"rfp_rag_collection\",\n",
    "        query_vector=embedding,\n",
    "        limit=10,\n",
    "    ):\n",
    "        context = f\"Q: {result.payload['question']}\\n\"\n",
    "        context += f\"A: {result.payload['ground_truth']}\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "vm_retriever = RetrievalModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_retriever.prediction_column] = vm_retriever.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import GenerationModel\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "Ignore your pre-existing knowledge and answer the question based on the provided context.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate(question, contexts):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n\".join(contexts)},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = GenerationModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_generator.prediction_column] = vm_generator.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG Model (Pipeline of \"Component\" Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import RAGModel\n",
    "\n",
    "vm_rag_model = RAGModel(\n",
    "    embedder=vm_embedder,\n",
    "    retriever=vm_retriever,\n",
    "    generator=vm_generator,\n",
    "    input_id=\"rag_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = vm_rag_model.predict(test_df)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ragas_ds = vm.init_dataset(result_df, __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextEntityRecall\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextPrecision\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRelevancy\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
