{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Framework RAG Model Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "import os\n",
    "from csv import DictReader\n",
    "from uuid import uuid4\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_map = {\"RFP_Question\": \"question\", \"RFP_Answer\": \"ground_truth\"}\n",
    "\n",
    "\n",
    "def load_documents(prefix):\n",
    "    documents = []\n",
    "    root_dir = \"datasets/rag/\"\n",
    "    for file in os.listdir(root_dir):\n",
    "        if file.startswith(prefix) and file.endswith(\".csv\"):\n",
    "            # use csv dict reader to load the csv file\n",
    "            with open(os.path.join(root_dir, file)) as f:\n",
    "                reader = DictReader(f)\n",
    "                for row in reader:\n",
    "                    # add a unique id to the row\n",
    "                    row[\"id\"] = str(uuid4())\n",
    "                    documents.append(row)\n",
    "\n",
    "    df = pd.DataFrame(documents)\n",
    "    df = df[[\"id\", \"RFP_Question\", \"RFP_Answer\"]]\n",
    "    # df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_dataset_split(limit=None):\n",
    "    df = load_documents(\"rfp_existing_questions\")\n",
    "\n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "\n",
    "    # split the dataset into a \"train\" - which gets inserted into the vector store\n",
    "    # and a \"test\" - which is used to evaluate the search results\n",
    "    train_df = df.sample(frac=0.8)\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First let's setup our embedding model and run some tests to make sure its working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from validmind.models import FunctionModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def embed(input):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    input[\"embedding\"] = (\n",
    "        client.embeddings.create(\n",
    "            input=input[\"RFP_Question\"],\n",
    "            model=\"text-embedding-3-small\",\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "    return input\n",
    "\n",
    "vm_embedder = FunctionModel(input_id=\"embedding_model\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our test dataset so we can run it through our different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:42:58,375 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>227914ef-6a4a-4dba-913d-36c26de755a6</td>\n",
       "      <td>What considerations do you take into account f...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6594068a-f252-4443-888b-cd1252cab55d</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e8dd0378-ebe2-47c6-8006-b6a6c787328f</td>\n",
       "      <td>Describe your strategy for integrating LLMs in...</td>\n",
       "      <td>Our approach involves conducting a thorough an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b37ae0ec-b7ce-41f3-8779-991caf8b428e</td>\n",
       "      <td>How does your AI strategy align with the NIST ...</td>\n",
       "      <td>Our AI solution is meticulously designed to al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "4   227914ef-6a4a-4dba-913d-36c26de755a6   \n",
       "9   6594068a-f252-4443-888b-cd1252cab55d   \n",
       "13  e8dd0378-ebe2-47c6-8006-b6a6c787328f   \n",
       "15  b37ae0ec-b7ce-41f3-8779-991caf8b428e   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "4   What considerations do you take into account f...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "13  Describe your strategy for integrating LLMs in...   \n",
       "15  How does your AI strategy align with the NIST ...   \n",
       "\n",
       "                                           RFP_Answer  \n",
       "4   Our design philosophy centers on simplicity an...  \n",
       "9   We implement advanced continuous learning mech...  \n",
       "13  Our approach involves conducting a thorough an...  \n",
       "15  Our AI solution is meticulously designed to al...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "train_df, test_df = load_dataset_split(20)\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    test_df,\n",
    "    text_column=\"RFP_Question\", # some NLP which work with text data require a `text_column` to be specified\n",
    "    target_column=\"RFP_Answer\",\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:42:58,382 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-07 13:42:58,383 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-07 13:42:58,383 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-07 13:42:59,108 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(vm_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>227914ef-6a4a-4dba-913d-36c26de755a6</td>\n",
       "      <td>What considerations do you take into account f...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>{'embedding': [-0.002952731214463711, -0.00326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6594068a-f252-4443-888b-cd1252cab55d</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "      <td>{'embedding': [-0.010829819366335869, 0.029368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e8dd0378-ebe2-47c6-8006-b6a6c787328f</td>\n",
       "      <td>Describe your strategy for integrating LLMs in...</td>\n",
       "      <td>Our approach involves conducting a thorough an...</td>\n",
       "      <td>{'embedding': [0.010061484761536121, 0.0223792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b37ae0ec-b7ce-41f3-8779-991caf8b428e</td>\n",
       "      <td>How does your AI strategy align with the NIST ...</td>\n",
       "      <td>Our AI solution is meticulously designed to al...</td>\n",
       "      <td>{'embedding': [-0.0057174162939190865, 0.02251...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "4   227914ef-6a4a-4dba-913d-36c26de755a6   \n",
       "9   6594068a-f252-4443-888b-cd1252cab55d   \n",
       "13  e8dd0378-ebe2-47c6-8006-b6a6c787328f   \n",
       "15  b37ae0ec-b7ce-41f3-8779-991caf8b428e   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "4   What considerations do you take into account f...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "13  Describe your strategy for integrating LLMs in...   \n",
       "15  How does your AI strategy align with the NIST ...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "4   Our design philosophy centers on simplicity an...   \n",
       "9   We implement advanced continuous learning mech...   \n",
       "13  Our approach involves conducting a thorough an...   \n",
       "15  Our AI solution is meticulously designed to al...   \n",
       "\n",
       "                           embedding_model_prediction  \n",
       "4   {'embedding': [-0.002952731214463711, -0.00326...  \n",
       "9   {'embedding': [-0.010829819366335869, 0.029368...  \n",
       "13  {'embedding': [0.010061484761536121, 0.0223792...  \n",
       "15  {'embedding': [-0.0057174162939190865, 0.02251...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and run one of the ValidMind embeddings stability analysis tests to make sure our embeddings model is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "# result = run_test(\n",
    "#     \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "#     inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "#     params={\"probability\": 0.3},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions\n",
    "\n",
    "> Note: We use the name `train_df` to refer to the dataset that is loaded into the vector store and used as context. This is not a great name but its consistent with data science terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b2e16d39-1e04-42d7-8867-ebc19fb55934</td>\n",
       "      <td>How do you maintain your AI applications with ...</td>\n",
       "      <td>We maintain a dedicated R&amp;D team focused on in...</td>\n",
       "      <td>[0.011783392168581486, 0.010354681871831417, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1cc1d530-0143-421a-9481-adf4d906006d</td>\n",
       "      <td>What is your approach to maintaining and suppo...</td>\n",
       "      <td>Our post-deployment support is designed to ens...</td>\n",
       "      <td>[-0.004095795564353466, 0.04930035024881363, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bc286719-ba2c-400b-a9bb-738d10ae5409</td>\n",
       "      <td>What steps do you take to ensure the transpare...</td>\n",
       "      <td>We prioritize transparency by incorporating ex...</td>\n",
       "      <td>[-0.0011347628897055984, -0.009663973934948444...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16dd571d-1e83-4545-92b1-77de4e013313</td>\n",
       "      <td>Can you discuss your governance framework for ...</td>\n",
       "      <td>We have established an AI Risk Council that pl...</td>\n",
       "      <td>[0.014880189672112465, 0.03505474328994751, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b1b93f5f-c16d-4cd3-b0e1-4e792e148ad4</td>\n",
       "      <td>What actions do you undertake to secure user d...</td>\n",
       "      <td>User privacy and data security are paramount. ...</td>\n",
       "      <td>[0.007698851637542248, 0.007591660600155592, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "1   b2e16d39-1e04-42d7-8867-ebc19fb55934   \n",
       "14  1cc1d530-0143-421a-9481-adf4d906006d   \n",
       "18  bc286719-ba2c-400b-a9bb-738d10ae5409   \n",
       "16  16dd571d-1e83-4545-92b1-77de4e013313   \n",
       "3   b1b93f5f-c16d-4cd3-b0e1-4e792e148ad4   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "1   How do you maintain your AI applications with ...   \n",
       "14  What is your approach to maintaining and suppo...   \n",
       "18  What steps do you take to ensure the transpare...   \n",
       "16  Can you discuss your governance framework for ...   \n",
       "3   What actions do you undertake to secure user d...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "1   We maintain a dedicated R&D team focused on in...   \n",
       "14  Our post-deployment support is designed to ens...   \n",
       "18  We prioritize transparency by incorporating ex...   \n",
       "16  We have established an AI Risk Council that pl...   \n",
       "3   User privacy and data security are paramount. ...   \n",
       "\n",
       "                                            embedding  \n",
       "1   [0.011783392168581486, 0.010354681871831417, 0...  \n",
       "14  [-0.004095795564353466, 0.04930035024881363, 0...  \n",
       "18  [-0.0011347628897055984, -0.009663973934948444...  \n",
       "16  [0.014880189672112465, 0.03505474328994751, 0....  \n",
       "3   [0.007698851637542248, 0.007591660600155592, 0...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"embedding\"] = [embed(row)[\"embedding\"] for _, row in train_df.iterrows()]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "qdrant.recreate_collection(\n",
    "    \"rfp_rag_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "qdrant.upsert(\n",
    "    \"rfp_rag_collection\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=row[\"id\"],\n",
    "            vector=row[\"embedding\"],\n",
    "            payload={\"RFP_Question\": row[\"RFP_Question\"], \"RFP_Answer\": row[\"RFP_Answer\"]},\n",
    "        )\n",
    "        for _, row in train_df.iterrows()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def retrieve(input):\n",
    "    input[\"contexts\"] = []\n",
    "\n",
    "    for result in qdrant.search(\n",
    "        \"rfp_rag_collection\",\n",
    "        query_vector=input[\"embedding\"],\n",
    "        limit=input.get(\n",
    "            \"limit\", 10\n",
    "        ),  # we could add a row to the dataset to specify a limit\n",
    "    ):\n",
    "        context = f\"Q: {result.payload['RFP_Question']}\\n\"\n",
    "        context += f\"A: {result.payload['RFP_Answer']}\\n\"\n",
    "\n",
    "        input[\"contexts\"].append(context)\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "vm_retriever = FunctionModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:43:03,648 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-07 13:43:03,648 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-07 13:43:03,648 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-07 13:43:03,650 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(vm_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "      <th>retrieval_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>227914ef-6a4a-4dba-913d-36c26de755a6</td>\n",
       "      <td>What considerations do you take into account f...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>{'embedding': [-0.002952731214463711, -0.00326...</td>\n",
       "      <td>{'contexts': ['Q: How do you maintain your AI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6594068a-f252-4443-888b-cd1252cab55d</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "      <td>{'embedding': [-0.010829819366335869, 0.029368...</td>\n",
       "      <td>{'contexts': ['Q: How do you ensure your LLMs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e8dd0378-ebe2-47c6-8006-b6a6c787328f</td>\n",
       "      <td>Describe your strategy for integrating LLMs in...</td>\n",
       "      <td>Our approach involves conducting a thorough an...</td>\n",
       "      <td>{'embedding': [0.010061484761536121, 0.0223792...</td>\n",
       "      <td>{'contexts': ['Q: How do you ensure your LLMs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b37ae0ec-b7ce-41f3-8779-991caf8b428e</td>\n",
       "      <td>How does your AI strategy align with the NIST ...</td>\n",
       "      <td>Our AI solution is meticulously designed to al...</td>\n",
       "      <td>{'embedding': [-0.0057174162939190865, 0.02251...</td>\n",
       "      <td>{'contexts': ['Q: Can you discuss your governa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "4   227914ef-6a4a-4dba-913d-36c26de755a6   \n",
       "9   6594068a-f252-4443-888b-cd1252cab55d   \n",
       "13  e8dd0378-ebe2-47c6-8006-b6a6c787328f   \n",
       "15  b37ae0ec-b7ce-41f3-8779-991caf8b428e   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "4   What considerations do you take into account f...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "13  Describe your strategy for integrating LLMs in...   \n",
       "15  How does your AI strategy align with the NIST ...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "4   Our design philosophy centers on simplicity an...   \n",
       "9   We implement advanced continuous learning mech...   \n",
       "13  Our approach involves conducting a thorough an...   \n",
       "15  Our AI solution is meticulously designed to al...   \n",
       "\n",
       "                           embedding_model_prediction  \\\n",
       "4   {'embedding': [-0.002952731214463711, -0.00326...   \n",
       "9   {'embedding': [-0.010829819366335869, 0.029368...   \n",
       "13  {'embedding': [0.010061484761536121, 0.0223792...   \n",
       "15  {'embedding': [-0.0057174162939190865, 0.02251...   \n",
       "\n",
       "                           retrieval_model_prediction  \n",
       "4   {'contexts': ['Q: How do you maintain your AI ...  \n",
       "9   {'contexts': ['Q: How do you ensure your LLMs ...  \n",
       "13  {'contexts': ['Q: How do you ensure your LLMs ...  \n",
       "15  {'contexts': ['Q: Can you discuss your governa...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "Ignore your pre-existing knowledge and answer the question based on the provided context.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate(input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n\".join(input[\"contexts\"])},\n",
    "            {\"role\": \"user\", \"content\": input[\"RFP_Question\"]},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    input[\"answer\"] = response.choices[0].message.content\n",
    "\n",
    "    return input\n",
    "\n",
    "vm_generator = FunctionModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 13:43:03,763 - INFO(validmind.vm_models.dataset.utils): Running predict_proba()... This may take a while\n",
      "2024-05-07 13:43:03,765 - INFO(validmind.vm_models.dataset.utils): Not running predict_proba() for unsupported models.\n",
      "2024-05-07 13:43:03,767 - INFO(validmind.vm_models.dataset.utils): Running predict()... This may take a while\n",
      "2024-05-07 13:43:17,842 - INFO(validmind.vm_models.dataset.utils): Done running predict()\n"
     ]
    }
   ],
   "source": [
    "vm_test_ds.assign_predictions(vm_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RFP_Question</th>\n",
       "      <th>RFP_Answer</th>\n",
       "      <th>embedding_model_prediction</th>\n",
       "      <th>retrieval_model_prediction</th>\n",
       "      <th>generation_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>227914ef-6a4a-4dba-913d-36c26de755a6</td>\n",
       "      <td>What considerations do you take into account f...</td>\n",
       "      <td>Our design philosophy centers on simplicity an...</td>\n",
       "      <td>{'embedding': [-0.002952731214463711, -0.00326...</td>\n",
       "      <td>{'contexts': ['Q: How do you maintain your AI ...</td>\n",
       "      <td>{'answer': 'We prioritize user interface and u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6594068a-f252-4443-888b-cd1252cab55d</td>\n",
       "      <td>How do your LLMs continuously learn and update...</td>\n",
       "      <td>We implement advanced continuous learning mech...</td>\n",
       "      <td>{'embedding': [-0.010829819366335869, 0.029368...</td>\n",
       "      <td>{'contexts': ['Q: How do you ensure your LLMs ...</td>\n",
       "      <td>{'answer': 'Our LLMs are designed to continuou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e8dd0378-ebe2-47c6-8006-b6a6c787328f</td>\n",
       "      <td>Describe your strategy for integrating LLMs in...</td>\n",
       "      <td>Our approach involves conducting a thorough an...</td>\n",
       "      <td>{'embedding': [0.010061484761536121, 0.0223792...</td>\n",
       "      <td>{'contexts': ['Q: How do you ensure your LLMs ...</td>\n",
       "      <td>{'answer': 'Our strategy for integrating Large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b37ae0ec-b7ce-41f3-8779-991caf8b428e</td>\n",
       "      <td>How does your AI strategy align with the NIST ...</td>\n",
       "      <td>Our AI solution is meticulously designed to al...</td>\n",
       "      <td>{'embedding': [-0.0057174162939190865, 0.02251...</td>\n",
       "      <td>{'contexts': ['Q: Can you discuss your governa...</td>\n",
       "      <td>{'answer': 'Our AI strategy aligns closely wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "4   227914ef-6a4a-4dba-913d-36c26de755a6   \n",
       "9   6594068a-f252-4443-888b-cd1252cab55d   \n",
       "13  e8dd0378-ebe2-47c6-8006-b6a6c787328f   \n",
       "15  b37ae0ec-b7ce-41f3-8779-991caf8b428e   \n",
       "\n",
       "                                         RFP_Question  \\\n",
       "4   What considerations do you take into account f...   \n",
       "9   How do your LLMs continuously learn and update...   \n",
       "13  Describe your strategy for integrating LLMs in...   \n",
       "15  How does your AI strategy align with the NIST ...   \n",
       "\n",
       "                                           RFP_Answer  \\\n",
       "4   Our design philosophy centers on simplicity an...   \n",
       "9   We implement advanced continuous learning mech...   \n",
       "13  Our approach involves conducting a thorough an...   \n",
       "15  Our AI solution is meticulously designed to al...   \n",
       "\n",
       "                           embedding_model_prediction  \\\n",
       "4   {'embedding': [-0.002952731214463711, -0.00326...   \n",
       "9   {'embedding': [-0.010829819366335869, 0.029368...   \n",
       "13  {'embedding': [0.010061484761536121, 0.0223792...   \n",
       "15  {'embedding': [-0.0057174162939190865, 0.02251...   \n",
       "\n",
       "                           retrieval_model_prediction  \\\n",
       "4   {'contexts': ['Q: How do you maintain your AI ...   \n",
       "9   {'contexts': ['Q: How do you ensure your LLMs ...   \n",
       "13  {'contexts': ['Q: How do you ensure your LLMs ...   \n",
       "15  {'contexts': ['Q: Can you discuss your governa...   \n",
       "\n",
       "                          generation_model_prediction  \n",
       "4   {'answer': 'We prioritize user interface and u...  \n",
       "9   {'answer': 'Our LLMs are designed to continuou...  \n",
       "13  {'answer': 'Our strategy for integrating Large...  \n",
       "15  {'answer': 'Our AI strategy aligns closely wit...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_test_ds.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG Model (Pipeline of \"Component\" Models)\n",
    "\n",
    "Now that we have our individual models setup, let's create a `RAGModel` instance that will chain them together and give us a single model that can be evalated end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.models import PipelineModel\n",
    "\n",
    "vm_output_parser = FunctionModel(\n",
    "    input_id=\"output_parser\",\n",
    "    predict_fn=lambda input: input[\"answer\"],\n",
    ")\n",
    "vm_rag_model = PipelineModel(vm_embedder | vm_retriever | vm_generator | vm_output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the test dataset through the entire pipeline. It will overwrite the current predictions that we generated from the individual models, but the key here is that calling `predict` on the `RAGModel` will run the entire pipeline and store the intermediate predictions in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FunctionModel `predict_fn` must return the input dictionary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mvm_rag_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/ValidMind/validmind-python/validmind/models/pipeline.py:61\u001b[0m, in \u001b[0;36mPipelineModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m---> 61\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         X[model\u001b[38;5;241m.\u001b[39minput_id] \u001b[38;5;241m=\u001b[39m predictions\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/Code/ValidMind/validmind-python/validmind/models/function.py:65\u001b[0m, in \u001b[0;36mFunctionModel.predict\u001b[0;34m(self, X, return_alias)\u001b[0m\n\u001b[1;32m     62\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_fn(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m!=\u001b[39m output:\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctionModel `predict_fn` must return the input dictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         )\n\u001b[1;32m     69\u001b[0m     Y\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mget_new())\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# if return_alias:\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#     return alias, Y\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: FunctionModel `predict_fn` must return the input dictionary"
     ]
    }
   ],
   "source": [
    "result_df = vm_rag_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'We prioritize user interface and user experience design in our AI applications by focusing on intuitive navigation, clear information presentation, and seamless interaction flows. Our design process incorporates user feedback and usability testing to ensure that the interface is user-friendly and that the overall experience is engaging and efficient. Additionally, we leverage technologies such as React for responsive design, allowing for a consistent experience across different devices. By emphasizing user-centric design principles, we aim to create AI applications that not only deliver powerful functionalities but also provide a satisfying and intuitive user experience.\\n'},\n",
       " {'answer': 'Our LLMs are equipped with continuous learning mechanisms that enable them to adapt to new data and evolving user expectations. We implement reinforcement learning algorithms that allow the models to adjust their behaviors based on real-time feedback and interactions. By leveraging techniques like online learning and active learning, our LLMs can ingest fresh data streams and integrate updated user preferences seamlessly. This iterative learning loop ensures that our models stay current and reflective of the latest trends and user demands, enabling them to deliver accurate and relevant outputs consistently.'},\n",
       " {'answer': \"Our strategy for integrating Large Language Models (LLMs) into existing organizational frameworks and systems is centered around seamless compatibility and optimal performance. We begin by conducting a thorough assessment of the organization's current infrastructure, workflows, and data processing systems to identify integration points for LLM implementation. Through close collaboration with stakeholders, we develop a tailored integration plan that takes into account the unique requirements and goals of the organization.\\n\\nKey components of our integration strategy include:\\n\\n1. API Integration: We leverage robust Application Programming Interfaces (APIs) to facilitate the communication between LLMs and existing systems, enabling smooth data exchange and interoperability.\\n\\n2. Customization: We customize the LLMs to align with the specific needs of the organization, incorporating features and functionalities that enhance productivity and efficiency within the existing frameworks.\\n\\n3. Training and Support: We provide comprehensive training sessions for users and administrators to ensure a smooth transition to LLM usage. Our dedicated support team is also available to address any integration challenges and provide ongoing assistance post-implementation.\\n\\n4. Data Integration: We optimize data pipelines to seamlessly feed relevant data into the LLMs, ensuring that the models receive accurate and up-to-date information for effective decision-making.\\n\\n5. Testing and Validation: Rigorous testing protocols are employed to validate the integration of LLMs with existing systems, verifying performance, accuracy, and compliance with organizational standards.\\n\\nOverall, our strategy aims to harmoniously embed LLM capabilities into the organization's workflows and systems, enhancing operational efficiency and unlocking new opportunities for leveraging AI technologies effectively.\"},\n",
       " {'answer': \"Our AI strategy aligns closely with the principles outlined in the NIST AI RMF for ensuring trustworthy and responsible AI use. We have established an AI Risk Council that oversees AI risk management, defines clear roles and responsibilities, integrates AI risk management into broader governance frameworks, and promotes collaboration between business units and IT. This framework ensures that we approach AI risks proactively and ethically. We also track AI risk exposure using specific metrics and KPIs in line with the NIST AI RMF's 'Measure' function, utilizing automated tools for monitoring and analysis. In conducting risk assessments and identification, we prioritize identifying potential risks such as data privacy, security, bias, and legal compliance, enabling us to prioritize and mitigate risks effectively. Additionally, we emphasize transparency and explainability in our AI decision-making processes, ensuring that stakeholders can trust and understand our AI systems' outputs. Our commitment to integrating the latest AI technologies and advancements further demonstrates our dedication to innovation and staying abreast of evolving industry standards.\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with some RAGAS Metrics\n",
    "\n",
    "Below I am just experimenting to see how the RAGAS metrics can work with the `RAGModel` instance. This is not a full implementation of the RAGAS metrics but just a poc. We'll want to make this work in a more general way so that the columns can be properly mapped from the user-provided `predict_col` or the default `predict_col` to the column names that RAGAS expects i.e. `question`, `contexts`, `answer`, `ground_truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "UnsupportedDatasetError",
     "evalue": "Only Pandas datasets and Tensor Datasets are supported at the moment.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedDatasetError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vm_ragas_ds \u001b[38;5;241m=\u001b[39m \u001b[43mvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/ValidMind/validmind-python/validmind/client.py:162\u001b[0m, in \u001b[0;36minit_dataset\u001b[0;34m(dataset, model, index, index_name, date_time_index, columns, options, text_column, target_column, feature_columns, extra_columns, class_labels, type, input_id, __log)\u001b[0m\n\u001b[1;32m    148\u001b[0m     vm_dataset \u001b[38;5;241m=\u001b[39m TorchDataset(\n\u001b[1;32m    149\u001b[0m         input_id\u001b[38;5;241m=\u001b[39minput_id,\n\u001b[1;32m    150\u001b[0m         raw_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         target_class_labels\u001b[38;5;241m=\u001b[39mclass_labels,\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedDatasetError(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly Pandas datasets and Tensor Datasets are supported at the moment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __log:\n\u001b[1;32m    166\u001b[0m     log_input(\n\u001b[1;32m    167\u001b[0m         name\u001b[38;5;241m=\u001b[39minput_id,\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    169\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mget_dataset_info(vm_dataset),\n\u001b[1;32m    170\u001b[0m     )\n",
      "\u001b[0;31mUnsupportedDatasetError\u001b[0m: Only Pandas datasets and Tensor Datasets are supported at the moment."
     ]
    }
   ],
   "source": [
    "vm_ragas_ds = vm.init_dataset(result_df, __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextEntityRecall\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextPrecision\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.ContextRelevancy\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
