{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Framework RAG Model Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First let's setup our embedding model and run some tests to make sure its working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from validmind.models import FunctionalModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def embed(x):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    return client.embeddings.create(\n",
    "        input=x,\n",
    "        model=\"text-embedding-3-small\",\n",
    "    ).data[0].embedding\n",
    "\n",
    "vm_embedder = FunctionalModel(input_id=\"vm_embedder\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 01:45:11,372 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I am a software engineer\",\n",
    "        \"Hello, world!\",\n",
    "        \"I am a data scientist\",\n",
    "        \"I am a data scientist. This is a really long example... Just to test what happens.\",\n",
    "    ]\n",
    "})\n",
    "vm_test_ds = vm.init_dataset(test_df, text_column=\"text\", __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9d2c068e29403a8b4817a35e40d61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h1>Stability Analysis Random Noise ✅</h1>\\n            <p>Evaluate r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "    inputs={\"model\": vm_embedder, \"dataset\": vm_test_ds},\n",
    "    params={\"probability\": 0.8},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load RFP Question/Answer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "import os\n",
    "from csv import DictReader\n",
    "from uuid import uuid4\n",
    "\n",
    "## grab all csv files with the name  \"rfp_existing_questions_<client_name>.csv\"\n",
    "documents = []\n",
    "root_dir = \"datasets/rag/\"\n",
    "for file in os.listdir(root_dir):\n",
    "    if file.startswith(\"rfp_existing_questions_\") and file.endswith(\".csv\"):\n",
    "        # use csv dict reader to load the csv file\n",
    "        with open(os.path.join(root_dir, file)) as f:\n",
    "            reader = DictReader(f)\n",
    "            for row in reader:\n",
    "                # add a unique id to the row\n",
    "                row[\"id\"] = str(uuid4())\n",
    "                documents.append(row)\n",
    "\n",
    "documents = documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    doc[\"embedding_question\"] = embed(doc[\"RFP_Question\"])\n",
    "    doc[\"embedding_answer\"] = embed(doc[\"RFP_Answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    \"my_rag_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "qdrant.upsert(\n",
    "    \"my_rag_collection\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=doc[\"id\"],\n",
    "            vector=doc[\"embedding_question\"],\n",
    "            payload={\n",
    "                k: v\n",
    "                for k, v in doc.items()\n",
    "                if k not in [\"embedding_question\", \"embedding_answer\"]\n",
    "            },\n",
    "        )\n",
    "        for doc in documents\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(x_embed, limit=10):\n",
    "    context = \"\"\n",
    "\n",
    "    for result in qdrant.search(\n",
    "        \"my_rag_collection\",\n",
    "        query_vector=x_embed,\n",
    "        limit=limit,\n",
    "    ):\n",
    "        context += f\"Q: {result.payload['RFP_Question']}\\n\"\n",
    "        context += f\"A: {result.payload['RFP_Answer']}\\n\"\n",
    "        context += f\"Project: {result.payload['Project_Title']}\\n\\n\"\n",
    "\n",
    "    return context\n",
    "\n",
    "vm_retriever = FunctionalModel(input_id=\"vm_retriever\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert RFP AI assistant.\n",
    "You are tasked with answering new RFP questions based on existing RFP questions and answers.\n",
    "You will be provided with the existing RFP questions and answer pairs that are the most relevant to the new RFP question.\n",
    "After that you will be provided with a new RFP question.\n",
    "You will generate an answer and respond only with the answer.\n",
    "Ignore your pre-existing knowledge and answer the question based on the provided context.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate(x, context):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": x},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = FunctionalModel(input_id=\"vm_generator\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG Model (Pipeline of \"Component\" Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import RAGModel\n",
    "\n",
    "vm_rag_model = RAGModel(\n",
    "    embedder=vm_embedder,\n",
    "    retriever=vm_retriever,\n",
    "    generator=vm_generator,\n",
    "    input_id=\"vm_rag_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Our company has 15 years of experience in developing AI-based applications, with a strong portfolio in sectors such as healthcare, finance, and education. For example, one of our notable projects, MediAI Insight for the healthcare industry, showcased significant advancements in patient data analysis, leading to a 30% reduction in diagnostic errors and a 40% improvement in treatment personalization. With over 200 healthcare facilities utilizing our platform and achieving a user satisfaction rate of 95%, we have a proven track record of success in AI application development.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm_rag_model.predict([[\"What is your experience with AI?\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
