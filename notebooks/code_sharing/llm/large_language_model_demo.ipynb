{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Sentiment Analysis using LLMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_KEY = '...'\n",
    "\n",
    "def available_openai_engines(filters=None):\n",
    "    \"\"\"Get all available OpenAI engine IDs, sorted by version number.\"\"\"\n",
    "    try:\n",
    "        openai.api_key = OPEN_AI_KEY\n",
    "        response = openai.Engine.list()\n",
    "        engines = response['data']\n",
    "\n",
    "        # If a filter is provided, only keep engines that include any of the filter keywords\n",
    "        if filters:\n",
    "            engines = [engine for engine in engines if any(keyword in engine['id'] for keyword in filters)]\n",
    "\n",
    "        # Extract version numbers and sort\n",
    "        versioned_engines = sorted(engines, key=lambda e: list(map(int, re.findall(r'\\d+', e['id']))), reverse=True)\n",
    "\n",
    "        # Extract only the IDs and create a DataFrame\n",
    "        ids = [engine['id'] for engine in versioned_engines]\n",
    "        df = pd.DataFrame(ids, columns=['Engine ID'])\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while retrieving engines: {e}\")\n",
    "\n",
    "\n",
    "def query_gpt(prompt, model_name):\n",
    "    \"\"\"Query the specified GPT model with the provided prompt.\"\"\"\n",
    "    try:\n",
    "        openai.api_key = OPEN_AI_KEY\n",
    "        response = openai.Completion.create(\n",
    "          engine=model_name,\n",
    "          prompt=prompt,\n",
    "          max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_prompts(input_text, instructions):\n",
    "    \"\"\"Create a list of prompts by concatenating the instructions and adding to each line of input text.\"\"\"\n",
    "    instruction = \" \".join(instructions)\n",
    "    prompts = [instruction + \" \" + text for text in input_text]\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def process_output(output):\n",
    "    \"\"\"\n",
    "    This function takes the raw output from GPT-3, extracts the sentiment, and standardizes it to be in\n",
    "    uppercase and one of \"NEGATIVE\", \"POSITIVE\", or \"NEUTRAL\".\n",
    "    \"\"\"\n",
    "    output = output.lower()\n",
    "    if 'negative' in output:\n",
    "        return 'NEGATIVE'\n",
    "    elif 'positive' in output:\n",
    "        return 'POSITIVE'\n",
    "    elif 'neutral' in output:\n",
    "        return 'NEUTRAL'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "\n",
    "def longest_common_starting_substring(strs):\n",
    "    \"\"\"\n",
    "    Given a list of strings, this function returns the longest common starting substring.\n",
    "    \"\"\"\n",
    "    prefix = strs[0]\n",
    "    for string in strs[1:]:\n",
    "        while string[:len(prefix)] != prefix:\n",
    "            prefix = prefix[:-1]\n",
    "    return prefix\n",
    "\n",
    "def predict_sentiment(models, prompts):\n",
    "    \"\"\"\n",
    "    This function takes a list of models and prompts as input,\n",
    "    and outputs a pandas DataFrame containing the Input Text (obtained by removing the instructions from the prompt) and\n",
    "    Predicted Sentiment for each model, organized in columns by model.\n",
    "    \"\"\"\n",
    "    # Placeholder for results\n",
    "    results = []\n",
    "\n",
    "    # Determine the common instruction from the prompts\n",
    "    instruction = longest_common_starting_substring(prompts)\n",
    "\n",
    "    # Iterate over the models and prompts and query the models\n",
    "    for model in models:\n",
    "        for prompt in prompts:\n",
    "            # Split the instruction from the input text\n",
    "            input_text = prompt.replace(instruction, '').strip()\n",
    "\n",
    "            # Check if we've already queried this model with this input_text\n",
    "            if not any((result['ID'] == model and result['Input Text'] == input_text) for result in results):\n",
    "                sentiment_raw = query_gpt(prompt, model)  # Replace `query_gpt` with the function you're using to query the model\n",
    "                sentiment = process_output(sentiment_raw)\n",
    "                results.append({'ID': model, 'Input Text': input_text, 'Predicted Sentiment': sentiment})\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Pivot the DataFrame to organize by input text and model\n",
    "    results_pivot = results_df.pivot(index='Input Text', columns='ID', values='Predicted Sentiment')\n",
    "\n",
    "    # Reset the index to return a \"regular\" dataframe\n",
    "    return results_pivot.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_multiclass_confusion_matrix(pred_sentiment, true_sentiment, label_names=['NEGATIVE', 'NEUTRAL', 'POSITIVE']):\n",
    "    # Get model's name from column name\n",
    "    model_name = pred_sentiment.columns[0]\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_sentiment, pred_sentiment[model_name], labels=label_names)\n",
    "\n",
    "    # Create a dataframe for better annotation handling\n",
    "    cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "\n",
    "    # Create a heatmap\n",
    "    heat_map = go.Heatmap(\n",
    "        z=cm_df.values,\n",
    "        x=list(cm_df.columns),\n",
    "        y=list(cm_df.index),\n",
    "        colorscale='Blues',\n",
    "        showscale=True,\n",
    "    )\n",
    "\n",
    "    # Create annotations (hover text)\n",
    "    annotations = []\n",
    "    for n, row in enumerate(cm):\n",
    "        for m, val in enumerate(row):\n",
    "            var_text = f'{cm[n][m]}'\n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    showarrow=False,\n",
    "                    text=var_text,\n",
    "                    xref='x',\n",
    "                    yref='y',\n",
    "                    x=m,\n",
    "                    y=n,\n",
    "                    font=dict(color='white'),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Create layout\n",
    "    layout = go.Layout(\n",
    "        title_text='Confusion matrix for ' + model_name,  # Use model's name in title\n",
    "        height=500,\n",
    "        width=500,\n",
    "        annotations=annotations\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=heat_map, layout=layout)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = [\n",
    "    \"Determine the sentiment of the financial news as negative, neutral or positive.\",\n",
    "    \"Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'.\"\n",
    "]\n",
    "input_text = [\n",
    "    'Consumer credit $18.9BN, Exp. $16BN, Last $9.6BN.',\n",
    "    'Estee Lauder Q2 adj. EPS $2.11; FactSet consensus $1.90.',\n",
    "    'The situation of coated magazine printing paper will continue to be weak',\n",
    "    'Pre-tax loss totaled euro 0.3 million, compared to a loss of euro 2.2 million in the first quarter of 2005.',\n",
    "    'Madison Square Garden Q2 EPS $3.93 vs. $3.42.',\n",
    "    'Boeing announces additional order for 737 MAX planes.',\n",
    "    'Boeing: Deliveries 24 Jets in November',\n",
    "    'PPDâ€™s stock indicated in early going to open at $30, or 11% above $27 IPO price.'\n",
    "]\n",
    "true_sentiment = ['POSITIVE',\n",
    "                  'NEUTRAL',\n",
    "                  'NEGATIVE',\n",
    "                  'POSITIVE',\n",
    "                  'POSITIVE',\n",
    "                  'NEUTRAL',\n",
    "                  'POSITIVE',\n",
    "                  'NEUTRAL']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = create_prompts(input_text, instruction)\n",
    "prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Available LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = available_openai_engines(filters=['curie', 'davinci'])\n",
    "display(engines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Numerical Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = ['text-davinci-003', 'text-curie-001']\n",
    "pred_sentiment = predict_sentiment(llms, prompts)\n",
    "\n",
    "# Add true_sentiment to DataFrame\n",
    "results = pred_sentiment.copy()\n",
    "results['true_sentiment'] = true_sentiment\n",
    "\n",
    "# Display the DataFrame\n",
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Reasoning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pred_sentiment_curie\n",
    "pred_sentiment_curie = results[['text-curie-001']].copy()\n",
    "\n",
    "# Create pred_sentiment_davinci\n",
    "pred_sentiment_davinci = results[['text-davinci-003']].copy()\n",
    "\n",
    "# Create true_sentiment\n",
    "true_sentiment = results[['true_sentiment']].copy()\n",
    "\n",
    "#Â reasoning_curie = prediction_reasoning(pred_sentiment_curie, prompts)\n",
    "# reasoning_davinci = prediction_reasoning(pred_sentiment_davinci, prompts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_confusion_matrix(pred_sentiment_curie, true_sentiment)\n",
    "plot_multiclass_confusion_matrix(pred_sentiment_davinci, true_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
