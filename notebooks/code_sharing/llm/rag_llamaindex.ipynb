{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-embeddings-huggingface llama-index-vector-stores-chroma -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai api key\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not 'OPENAI_API_KEY' in os.environ:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"datasets/rag/rfp_existing_questions_client_1.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_2.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_3.csv\",\n",
    "]\n",
    "\n",
    "TEST_FILES = [\n",
    "    \"datasets/rag/rfp_existing_questions_client_4.csv\",\n",
    "    \"datasets/rag/rfp_existing_questions_client_5.csv\",\n",
    "]\n",
    "\n",
    "def load(file_paths):\n",
    "    # Load each file into a DataFrame and concatenate them into one\n",
    "    df_list = [pd.read_csv(file) for file in file_paths]\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Rename columns to match the expected format \n",
    "    concatenated_df = concatenated_df.rename(\n",
    "        columns={\"RFP_Question\": \"question\", \"RFP_Answer\": \"ground_truth\"}\n",
    "    )\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "# Load and concatenate training and testing data\n",
    "train_df = load(TRAIN_FILES)\n",
    "test_df = load(TEST_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Selection\n",
    "\n",
    "First, we test both the `sentence-transformers` and `openai` embedding models using their native interfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "embeddings = client.embeddings.create(\n",
    "    input=\"Hello World!\", \n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `validmind` embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import EmbeddingModel\n",
    "\n",
    "def embed(question):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "    \n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return embed_model.get_text_embedding(question)\n",
    "\n",
    "\n",
    "vm_embedder_st = EmbeddingModel(input_id=\"embedding_model_st\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(question):\n",
    "    \"\"\"Returns a text embedding for the given text\"\"\"\n",
    "\n",
    "    return (\n",
    "        client.embeddings.create(\n",
    "            input=question,\n",
    "            model=\"text-embedding-3-small\",\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "\n",
    "vm_embedder_openai = EmbeddingModel(input_id=\"embedding_model_openai\", predict_fn=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings from the text in the `question` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_embedder_openai.output_column] = vm_embedder_openai.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_embedder_st.output_column] = vm_embedder_st.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `validmind` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm_train_ds = vm.init_dataset(train_df, text_column=\"question\", __log=False)\n",
    "vm_test_ds = vm.init_dataset(test_df, text_column=\"question\", __log=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an embedding test for both models to ensure that the embedding models function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    result = run_test(\n",
    "        \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise:HFEmbeddingModel\",\n",
    "        inputs={\"model\": vm_embedder_st, \"dataset\": vm_test_ds},\n",
    "        params={\"probability\": 0.3},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run:\n",
    "    result = run_test(\n",
    "        \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise:OpenAIEmbeddingModel\",\n",
    "        inputs={\"model\": vm_embedder_openai, \"dataset\": vm_test_ds},\n",
    "        params={\"probability\": 0.3},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[vm_embedder_st.output_column] = vm_embedder_st.predict(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert embeddings and questions into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"rfp_rag_collection\")\n",
    "\n",
    "# Initialize lists to store data for batch addition\n",
    "all_embeddings = []\n",
    "all_metadatas = []\n",
    "all_documents = []\n",
    "all_ids = []\n",
    "\n",
    "# Loop through the DataFrame rows\n",
    "for index, row in train_df.iterrows():\n",
    "\n",
    "    all_embeddings.append(row[vm_embedder_openai.output_column])\n",
    "    all_metadatas.append({\n",
    "        'ground_truth': row['ground_truth'],\n",
    "        'hnsw:space': 'cosine'\n",
    "    })\n",
    "    all_documents.append(row['question'])\n",
    "    all_ids.append(str(uuid.uuid4()))\n",
    "\n",
    "# Add all data to the collection in a single operation\n",
    "collection.add(\n",
    "    ids=all_ids, \n",
    "    documents=all_documents,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadatas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the retriever by directly querying using the pre-computed embedding corresponding to the question. We expect the vector store to return the top k most similar questions, along with the metadata associated with each of these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "query = VectorStoreQuery(query_embedding=test_df['embedding'][0], similarity_top_k=10)\n",
    "result = chroma_vector_store.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, similarity, id_ in zip(result.nodes, result.similarities, result.ids):\n",
    "    print(\"Node ID:\", id_)\n",
    "    print(\"Question:\", node.text)\n",
    "    print(\"Answer:\", node.metadata['ground_truth'])\n",
    "    print(\"Similarity:\", similarity)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import RetrievalModel\n",
    "\n",
    "def retrieve(embedding):\n",
    "\n",
    "    contexts = []\n",
    "    \n",
    "    query = VectorStoreQuery(query_embedding=embedding, similarity_top_k=10)\n",
    "\n",
    "    result = chroma_vector_store.query(query)\n",
    "\n",
    "    for node, similarity, id_ in zip(result.nodes, result.similarities, result.ids):\n",
    "\n",
    "        context = f\"Node ID: {id_}\\n\"\n",
    "        context = f\"Question: {node.text}\\n\"\n",
    "        context += f\"Answer: {node.metadata['ground_truth']}\\n\"\n",
    "        context += f\"Similarity: {similarity}\\n\"\n",
    "\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "vm_retriever = RetrievalModel(input_id=\"retrieval_model\", predict_fn=retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_retriever.output_column] = vm_retriever.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Prompt\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context. \n",
    "If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = Prompt(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.format(\n",
    "    context=test_df[vm_retriever.output_column][0], \n",
    "    question=test_df['question'][0]\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from validmind.models import GenerationModel\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate(question, contexts):\n",
    "\n",
    "    formatted_prompt = prompt.format(\n",
    "        context=contexts, \n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "vm_generator = GenerationModel(input_id=\"generation_model\", predict_fn=generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[vm_generator.output_column] = vm_generator.predict(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a ValidMind RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import RAGModel\n",
    "\n",
    "vm_rag_model = RAGModel(\n",
    "    embedder=vm_embedder_openai,\n",
    "    retriever=vm_retriever,\n",
    "    generator=vm_generator,\n",
    "    input_id=\"rag_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = vm_rag_model.predict(test_df)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ragas_ds = vm.init_dataset(result_df, __log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(scores):\n",
    "    # plot distribution of scores (0-1) from ragas metric\n",
    "    # scores is a list of floats\n",
    "    fig = px.histogram(x=scores, nbins=10)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.ragas.AnswerSimilarity\",\n",
    "    inputs={\"dataset\": vm_ragas_ds},\n",
    "    show=False,\n",
    ")\n",
    "plot_distribution(result.metric.summary.results[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
