{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi class Sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loading libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#for displaying 500 results in pandas dataframe\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "from tqdm import tqdm\n",
    "from statistics import *\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"../datasets/nlp/classification/covid_tweets/Corona_NLP_train.csv\",encoding='latin1')\n",
    "test=pd.read_csv(\"../datasets/nlp/classification/covid_tweets/Corona_NLP_test.csv\",encoding='latin1')\n",
    "\n",
    "\n",
    "df=pd.concat([train,test])\n",
    "df['OriginalTweet']=df['OriginalTweet'].astype(str)\n",
    "df['Sentiment']=df['Sentiment'].astype(str)\n",
    "\n",
    "train['OriginalTweet']=train['OriginalTweet'].astype(str)\n",
    "train['Sentiment']=train['Sentiment'].astype(str)\n",
    "\n",
    "test['OriginalTweet']=test['OriginalTweet'].astype(str)\n",
    "test['Sentiment']=test['Sentiment'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Shape of dataframe after dropping duplicates: \", df.shape)\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "print(\" Shape of dataframe after dropping duplicates: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Sentiment.unique())\n",
    "print(df.Sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will copy the text in another column so that the original text is also there for comparison\n",
    "\n",
    "df['text'] = df.OriginalTweet\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "train['text'] = train.OriginalTweet\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "\n",
    "test['text'] = test.OriginalTweet\n",
    "test[\"text\"] = test[\"text\"].astype(str)\n",
    "\n",
    "# Data has 5 classes, let's convert them to 3\n",
    "\n",
    "def classes_def(x):\n",
    "    if x ==  \"Extremely Positive\":\n",
    "        return \"positive\"\n",
    "    elif x == \"Extremely Negative\":\n",
    "        return \"negative\"\n",
    "    elif x == \"Negative\":\n",
    "        return \"negative\"\n",
    "    elif x ==  \"Positive\":\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df['sentiment']=df['Sentiment'].apply(lambda x:classes_def(x))\n",
    "train['sentiment']=train['Sentiment'].apply(lambda x:classes_def(x))\n",
    "test['sentiment']=test['Sentiment'].apply(lambda x:classes_def(x))\n",
    "target=df['sentiment']\n",
    "\n",
    "df.sentiment.value_counts(normalize= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
    "class_df.style.background_gradient(cmap='winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_class=class_df.text\n",
    "labels= class_df.sentiment\n",
    "\n",
    "colors = ['#17C37B','#F92969','#FACA0C']\n",
    "\n",
    "my_pie,_,_ = plt.pie(percent_class,radius = 1.2,labels=labels,colors=colors,autopct=\"%.1f%%\")\n",
    "\n",
    "plt.setp(my_pie, width=0.6, edgecolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_subplots(1,2,subplot_titles=('Train set','Test set'))\n",
    "x=train.sentiment.value_counts()\n",
    "fig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#17C37B','#F92969','#FACA0C'],name='train'),row=1,col=1)\n",
    "x=test.sentiment.value_counts()\n",
    "fig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#17C37B','#F92969','#FACA0C'],name='test'),row=1,col=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "tweet_len=train[train['sentiment']==\"positive\"]['text'].str.len()\n",
    "ax1.hist(tweet_len,color='#17C37B')\n",
    "ax1.set_title('Positive Sentiments')\n",
    "\n",
    "tweet_len=train[train['sentiment']==\"negative\"]['text'].str.len()\n",
    "ax2.hist(tweet_len,color='#F92969')\n",
    "ax2.set_title('Negative Sentiments')\n",
    "\n",
    "tweet_len=train[train['sentiment']==\"neutral\"]['text'].str.len()\n",
    "ax3.hist(tweet_len,color='#FACA0C')\n",
    "ax3.set_title('Neutral Sentiments')\n",
    "\n",
    "fig.suptitle('Characters in tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(text):\n",
    "    '''a function which returns the length of text'''\n",
    "    return len(text)\n",
    "df['length'] = df['text'].apply(length)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "bins = 150\n",
    "plt.hist(df[df['sentiment'] == \"neutral\"]['length'], alpha = 0.5, bins=bins, label='neutral')\n",
    "plt.hist(df[df['sentiment'] == \"positive\"]['length'], alpha = 0.7, bins=bins, label='positive')\n",
    "plt.hist(df[df['sentiment'] == \"negative\"]['length'], alpha = 0.8, bins=bins, label='negative')\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('numbers')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0,150)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "tweet_len=train[train['sentiment']==\"positive\"]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(tweet_len,color='#17C37B')\n",
    "ax1.set_title('Positive Sentiments')\n",
    "\n",
    "\n",
    "tweet_len=train[train['sentiment']==\"negative\"]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(tweet_len,color='#F92969')\n",
    "ax2.set_title('Negative Sentiments')\n",
    "\n",
    "tweet_len=train[train['sentiment']==\"neutral\"]['text'].str.split().map(lambda x: len(x))\n",
    "ax3.hist(tweet_len,color='#FACA0C')\n",
    "ax3.set_title('Neutral Sentiments')\n",
    "\n",
    "fig.suptitle('Words in a tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,(ax1,ax2, ax3)=plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "word=train[train['sentiment']==\"positive\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='#17C37B')\n",
    "ax1.set_title('Positive')\n",
    "\n",
    "\n",
    "word=train[train['sentiment']==\"negative\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='#F92969')\n",
    "ax2.set_title('Negative')\n",
    "\n",
    "word=train[train['sentiment']==\"neutral\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='#FACA0C')\n",
    "ax3.set_title('Neutral')\n",
    "\n",
    "\n",
    "fig.suptitle('Average word length in each tweet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(target):\n",
    "    corpus=[]\n",
    "\n",
    "    for x in train[train['sentiment']==target ]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "\n",
    "for val in stop:\n",
    "\n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "\n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (8, 8), facecolor = \"white\")\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(\"positive\")\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y, color='#17C37B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(\"negative\")\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y, color='#F92969')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(\"neutral\")\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y, color='#FACA0C')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "corpus=create_corpus(\"positive\")\n",
    "\n",
    "dic=defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "\n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y,color='#17C37B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "corpus=create_corpus(\"negative\")\n",
    "\n",
    "dic=defaultdict(int)\n",
    "import string\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "\n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y, color='#F92969')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop) :\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=y,y=x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hash(text):\n",
    "    line=re.findall(r'(?<=#)\\w+',text)\n",
    "    return \" \".join(line)\n",
    "df['hash']=df['text'].apply(lambda x:find_hash(x))\n",
    "temp=df['hash'].value_counts()[:][1:11]\n",
    "temp= temp.to_frame().reset_index().rename(columns={'index':'Hashtag','hash':'count'})\n",
    "sns.barplot(x=\"Hashtag\",y=\"count\", data = temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from math import log10\n",
    "\n",
    "labels = df['hash'].value_counts()[:][2:11].index.tolist()\n",
    "data = df['hash'].value_counts()[:][2:11]\n",
    "\n",
    "df['hash'].value_counts()[:][1:11].index.tolist()\n",
    "#number of data points\n",
    "n = len(data)\n",
    "#find max value for full ring\n",
    "k = 10 ** int(log10(max(data)))\n",
    "m = k * (1 + max(data) // k)\n",
    "\n",
    "#radius of donut chart\n",
    "r = 1.5\n",
    "#calculate width of each ring\n",
    "w = r / n\n",
    "\n",
    "#create colors along a chosen colormap\n",
    "colors = [cm.terrain(i / n) for i in range(n)]\n",
    "\n",
    "#create figure, axis\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "#create rings of donut chart\n",
    "for i in range(n):\n",
    "    #hide labels in segments with textprops: alpha = 0 - transparent, alpha = 1 - visible\n",
    "    innerring, _ = ax.pie([m - data[i], data[i]], radius = r - i * w, startangle = 90, labels = [\"\", labels[i]], labeldistance = 1 - 1 / (1.5 * (n - i)), textprops = {\"alpha\": 0}, colors = [\"white\", colors[i]])\n",
    "    plt.setp(innerring, width = w, edgecolor = \"white\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentions(text):\n",
    "    line=re.findall(r'(?<=@)\\w+',text)\n",
    "    return \" \".join(line)\n",
    "df['mentions']=df['text'].apply(lambda x:mentions(x))\n",
    "\n",
    "temp=df['mentions'].value_counts()[:][1:11]\n",
    "temp =temp.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n",
    "\n",
    "sns.barplot(x=\"Mentions\",y=\"count\", data = temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df['mentions'].value_counts()[:][1:11].index.tolist()\n",
    "a = df['mentions'].value_counts()[:][1:11].tolist()\n",
    "row = pd.DataFrame({'scenario' : []})\n",
    "row[\"scenario\"] = b\n",
    "row[\"Percentage\"] = a\n",
    "fig = px.treemap(row, path= [\"scenario\"], values=\"Percentage\",title='Tree of Mentions')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Urls and HTML links\n",
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "df['text_new']=df['text'].apply(lambda x:remove_urls(x))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "df['text']=df['text_new'].apply(lambda x:remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing\n",
    "def lower(text):\n",
    "    low_text= text.lower()\n",
    "    return low_text\n",
    "df['text_new']=df['text'].apply(lambda x:lower(x))\n",
    "\n",
    "\n",
    "# Number removal\n",
    "def remove_num(text):\n",
    "    remove= re.sub(r'\\d+', '', text)\n",
    "    return remove\n",
    "df['text']=df['text_new'].apply(lambda x:remove_num(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords & Punctuations\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct\n",
    "df['text_new']=df['text'].apply(lambda x:punct_remove(x))\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "df['text']=df['text_new'].apply(lambda x:remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove mentions and hashtags\n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "df['text_new']=df['text'].apply(lambda x:remove_mention(x))\n",
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "df['text']=df['text_new'].apply(lambda x:remove_hash(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extra white space left while removing stuff\n",
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n",
    "    return space_remove\n",
    "df['text_new']=df['text'].apply(lambda x:remove_space(x))\n",
    "\n",
    "df = df.drop(columns=['text_new'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\n",
    "\n",
    "df_pos = df[df[\"sentiment\"]==\"positive\"]\n",
    "df_neg = df[df[\"sentiment\"]==\"negative\"]\n",
    "df_neu = df[df[\"sentiment\"]==\"neutral\"]\n",
    "\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for val in df_pos.text:\n",
    "\n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "\n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "\n",
    "wordcloud1 = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                colormap=\"Greens\",\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "\n",
    "ax1.imshow(wordcloud1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Positive Sentiment',fontsize=35);\n",
    "\n",
    "comment_words = ''\n",
    "\n",
    "for val in df_neg.text:\n",
    "\n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "\n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wordcloud2 = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                colormap=\"Reds\",\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "ax2.imshow(wordcloud2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Negative Sentiment',fontsize=35);\n",
    "\n",
    "\n",
    "\n",
    "comment_words = ''\n",
    "for val in df_neu.text:\n",
    "\n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "\n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud3 = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                colormap=\"Greys\",\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "ax3.imshow(wordcloud3)\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Neutal Sentiment',fontsize=35);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unigram\n",
    "\n",
    "# Define functions\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "N = 30\n",
    "\n",
    "\n",
    "\n",
    "positive= train[\"sentiment\"]== \"positive\"\n",
    "negative= train[\"sentiment\"]== \"negative\"\n",
    "neutral= train[\"sentiment\"]== \"neutral\"\n",
    "\n",
    "positive_unigrams = defaultdict(int)\n",
    "neutral_unigrams = defaultdict(int)\n",
    "negative_unigrams = defaultdict(int)\n",
    "\n",
    "# Unigrams\n",
    "for tweet in train[positive]['text']:\n",
    "    for word in generate_ngrams(tweet):\n",
    "        positive_unigrams[word] += 1\n",
    "\n",
    "for tweet in train[negative]['text']:\n",
    "    for word in generate_ngrams(tweet):\n",
    "        negative_unigrams[word] += 1\n",
    "\n",
    "for tweet in train[neutral]['text']:\n",
    "    for word in generate_ngrams(tweet):\n",
    "        neutral_unigrams[word] += 1\n",
    "\n",
    "df_positive_unigrams = pd.DataFrame(sorted(positive_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_negative_unigrams = pd.DataFrame(sorted(negative_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_neutral_unigrams = pd.DataFrame(sorted(neutral_unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=150)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_positive_unigrams[0].values[:N], x=df_positive_unigrams[1].values[:N], ax=axes[0], color='#17C37B')\n",
    "sns.barplot(y=df_negative_unigrams[0].values[:N], x=df_negative_unigrams[1].values[:N], ax=axes[1], color='#F92969')\n",
    "sns.barplot(y=df_neutral_unigrams[0].values[:N], x=df_neutral_unigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common unigrams in Postive Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common unigrams in Negative Tweets', fontsize=15)\n",
    "axes[2].set_title(f'Top {N} most common unigrams in Neutral Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bi grams\n",
    "# Bigrams\n",
    "positive_bigrams = defaultdict(int)\n",
    "neutral_bigrams = defaultdict(int)\n",
    "negative_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train[positive]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        positive_bigrams[word] += 1\n",
    "\n",
    "for tweet in train[negative]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        negative_bigrams[word] += 1\n",
    "\n",
    "for tweet in train[neutral]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        neutral_bigrams[word] += 1\n",
    "\n",
    "df_positive_bigrams = pd.DataFrame(sorted(positive_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_negative_bigrams = pd.DataFrame(sorted(negative_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_neutral_bigrams = pd.DataFrame(sorted(neutral_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_positive_bigrams[0].values[:N], x=df_positive_bigrams[1].values[:N], ax=axes[0], color='#17C37B')\n",
    "sns.barplot(y=df_negative_bigrams[0].values[:N], x=df_negative_bigrams[1].values[:N], ax=axes[1], color='#F92969')\n",
    "sns.barplot(y=df_neutral_bigrams[0].values[:N], x=df_neutral_bigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common bigrams in Postive Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common bigrams in Negative Tweets', fontsize=15)\n",
    "axes[2].set_title(f'Top {N} most common bigrams in Neutral Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tri-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "positive_trigrams = defaultdict(int)\n",
    "neutral_trigrams = defaultdict(int)\n",
    "negative_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train[positive]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        positive_trigrams[word] += 1\n",
    "\n",
    "for tweet in train[negative]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        negative_trigrams[word] += 1\n",
    "\n",
    "for tweet in train[neutral]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        neutral_trigrams[word] += 1\n",
    "\n",
    "df_positive_trigrams = pd.DataFrame(sorted(positive_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_negative_trigrams = pd.DataFrame(sorted(negative_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_neutral_trigrams = pd.DataFrame(sorted(neutral_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_positive_trigrams[0].values[:N], x=df_positive_trigrams[1].values[:N], ax=axes[0], color='#17C37B')\n",
    "sns.barplot(y=df_negative_trigrams[0].values[:N], x=df_negative_trigrams[1].values[:N], ax=axes[1], color='#F92969')\n",
    "sns.barplot(y=df_neutral_trigrams[0].values[:N], x=df_neutral_trigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common trigrams in Postive Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common trigrams in Negative Tweets', fontsize=15)\n",
    "axes[2].set_title(f'Top {N} most common trigrams in Neutral Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_hub\n",
    "!pip install tensorflow_text\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_encoder = \\\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "tfhub_handle_preprocess = \\\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_classifier_model():\n",
    "\n",
    "    text_input = tf.keras.layers.Input(\n",
    "        shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "    preprocessing_layer = hub.KerasLayer(\n",
    "        tfhub_handle_preprocess, name='preprocessing')\n",
    "\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(\n",
    "        tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(\n",
    "        3, activation='softmax', name='classifier')(net)\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.metrics.CategoricalAccuracy('accuracy')\n",
    "    optimizer = Adam(\n",
    "        learning_rate=5e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=loss, metrics=metric)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train = train[['text','sentiment']]\n",
    "test = test[['text','sentiment']]\n",
    "\n",
    "train, valid = train_test_split(\n",
    "    train,\n",
    "    train_size=0.7,\n",
    "    random_state=0,\n",
    "    stratify=train['sentiment'])\n",
    "y_train, X_train = \\\n",
    "    train['sentiment'], train.drop(['sentiment'], axis=1)\n",
    "y_valid, X_valid = \\\n",
    "    valid['sentiment'], valid.drop(['sentiment'], axis=1)\n",
    "\n",
    "y_test, X_test= \\\n",
    "    test['sentiment'], test.drop(['sentiment'], axis=1)\n",
    "\n",
    "y_train_c = tf.keras.utils.to_categorical(\n",
    "    y_train.astype('category').cat.codes.values, num_classes=3)\n",
    "y_valid_c = tf.keras.utils.to_categorical(\n",
    "    y_valid.astype('category').cat.codes.values, num_classes=3)\n",
    "y_test_c = tf.keras.utils.to_categorical(\n",
    "    y_test.astype('category').cat.codes.values, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "def fit_model(train_pool, test_pool, **kwargs):\n",
    "    model = CatBoostClassifier(\n",
    "        task_type='CPU',\n",
    "        iterations=5000,\n",
    "        eval_metric='Accuracy',\n",
    "        od_type='Iter',\n",
    "        od_wait=500,\n",
    "        **kwargs\n",
    "    )\n",
    "    return model.fit(\n",
    "        train_pool,\n",
    "        eval_set=test_pool,\n",
    "        verbose=100,\n",
    "        plot=True,\n",
    "        use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    text_features=['text']\n",
    ")\n",
    "valid_pool = Pool(\n",
    "    data=X_valid,\n",
    "    label=y_valid,\n",
    "    text_features=['text']\n",
    ")\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=X_test,\n",
    "   label=y_test,\n",
    "    text_features=['text']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_model(\n",
    "    train_pool, valid_pool,\n",
    "    learning_rate=0.35,\n",
    "    tokenizers=[\n",
    "        {\n",
    "            'tokenizer_id': 'Sense',\n",
    "            'separator_type': 'BySense',\n",
    "            'lowercasing': 'True',\n",
    "            'token_types':['Word', 'Number', 'SentenceBreak'],\n",
    "            'sub_tokens_policy':'SeveralTokens'\n",
    "        }\n",
    "    ],\n",
    "    dictionaries = [\n",
    "        {\n",
    "            'dictionary_id': 'Word',\n",
    "            'max_dictionary_size': '50000'\n",
    "        }\n",
    "    ],\n",
    "    feature_calcers = [\n",
    "        'BoW:top_tokens_count=10000'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.utils import get_confusion_matrix\n",
    "\n",
    "cm = get_confusion_matrix(model, train_pool)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = get_confusion_matrix(model, test_pool)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
