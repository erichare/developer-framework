{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC Loan Rates Forecast Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Data Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import glob\n",
    "\n",
    "# ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from arch.unitroot import PhillipsPerron, DFGLS\n",
    "import xgboost as xgb\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load FRED Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.datasets.regression import fred as demo_dataset\n",
    "\n",
    "target_column = demo_dataset.target_column\n",
    "feature_columns = demo_dataset.feature_columns\n",
    "\n",
    "# Split the dataset into test and training\n",
    "df = demo_dataset.load_data()\n",
    "df.tail(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, cols_to_plot=None, title=''):\n",
    "    \"\"\"\n",
    "    Plot multiple time-series in the same axes using seaborn.\n",
    "\n",
    "    :param df: DataFrame with time-series data\n",
    "    :param cols_to_plot: List of column names to plot. If None, plot all columns in df.\n",
    "    :param title: Title of the plot, default is ''\n",
    "    \"\"\"\n",
    "    if cols_to_plot is None:\n",
    "        cols_to_plot = df.columns.tolist()\n",
    "\n",
    "    # Create a new DataFrame with the columns to plot\n",
    "    plot_df = df[cols_to_plot]\n",
    "\n",
    "    # Set seaborn plot style\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    # Plot the time-series data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in plot_df.columns:\n",
    "        sns.lineplot(data=plot_df[col], label=col)\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(df_diff[['GS10']], title='All Variables')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Data Quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of Series "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of frequencies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_difference_frequency(df):\n",
    "    # Calculate the time differences between consecutive entries\n",
    "    time_diff = df.index.to_series().diff().dropna()\n",
    "\n",
    "    # Convert the time differences to a suitable unit (e.g., days)\n",
    "    time_diff_days = time_diff.dt.total_seconds() / (60 * 60 * 24)\n",
    "\n",
    "    # Create a DataFrame with the time differences\n",
    "    time_diff_df = pd.DataFrame({'Time Differences (Days)': time_diff_days})\n",
    "\n",
    "    # Plot the frequency distribution of the time differences\n",
    "    sns.histplot(data=time_diff_df, x='Time Differences (Days)', bins=50, kde=False)\n",
    "    plt.xlabel('Time Differences (Days)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_difference_frequency(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify frequencies for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_frequencies(df):\n",
    "    \"\"\"\n",
    "    Identify the frequency of each series in the DataFrame.\n",
    "\n",
    "    :param df: Time-series DataFrame\n",
    "    :return: DataFrame with two columns: 'Variable' and 'Frequency'\n",
    "    \"\"\"\n",
    "    frequencies = []\n",
    "    for column in df.columns:\n",
    "        series = df[column].dropna()\n",
    "        if not series.empty:\n",
    "            freq = pd.infer_freq(series.index)\n",
    "            if freq == 'MS' or freq == 'M':\n",
    "                label = 'Monthly'\n",
    "            elif freq == 'Q':\n",
    "                label = 'Quarterly'\n",
    "            elif freq == 'A':\n",
    "                label = 'Yearly'\n",
    "            else:\n",
    "                label = freq\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        frequencies.append({'Variable': column, 'Frequency': label})\n",
    "\n",
    "    freq_df = pd.DataFrame(frequencies)\n",
    "\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = identify_frequencies(df)\n",
    "display(frequencies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('MS').last()\n",
    "frequencies = identify_frequencies(df)\n",
    "display(frequencies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Identify Missing Values**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values_bar(df):\n",
    "    \"\"\"\n",
    "    Plot a bar chart displaying the total number of missing values per variable (column) in a time-series DataFrame using seaborn.\n",
    "\n",
    "    :param df: Time-series DataFrame\n",
    "    \"\"\"\n",
    "    # Calculate the total number of missing values per column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Set seaborn plot style\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=missing_values.index, y=missing_values.values)\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Variables (Columns)')\n",
    "    plt.ylabel('Number of Missing Values')\n",
    "    plt.title('Total Number of Missing Values per Variable')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_values_bar(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values_heatmap(df, start_year=None, end_year=None):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of missing values with actual years in rows using seaborn.\n",
    "\n",
    "    :param df: Time-series DataFrame\n",
    "    :param start_year: Start year for zooming in, defaults to None\n",
    "    :param end_year: End year for zooming in, defaults to None\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame based on the specified start_year and end_year\n",
    "    if start_year:\n",
    "        df = df[df.index.year >= start_year]\n",
    "    if end_year:\n",
    "        df = df[df.index.year <= end_year]\n",
    "\n",
    "    # Create a boolean mask for missing values\n",
    "    missing_mask = df.isnull()\n",
    "\n",
    "    # Set seaborn plot style\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(missing_mask.T, cmap='viridis', cbar=False, xticklabels=False)\n",
    "\n",
    "    # Add actual years on the x-axis\n",
    "    years = df.index.year.unique()\n",
    "    xticks = [df.index.get_loc(df.index[df.index.year == year][0]) for year in years]\n",
    "    plt.xticks(xticks, years, rotation=45, ha='right')\n",
    "\n",
    "    plt.ylabel('Columns')\n",
    "    plt.xlabel('Rows (Years)')\n",
    "    plt.title('Missing Values Heatmap with Actual Years in Rows')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_values_heatmap(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Handling Missing Values**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_values_bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_values_heatmap(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Identify Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(df, threshold=3):\n",
    "    z_scores = pd.DataFrame(stats.zscore(df), index=df.index, columns=df.columns)\n",
    "    outliers = z_scores[(z_scores.abs() > threshold).any(axis=1)]\n",
    "\n",
    "    outlier_table = []\n",
    "    for idx, row in outliers.iterrows():\n",
    "        for col in df.columns:\n",
    "            if abs(row[col]) > threshold:\n",
    "                outlier_table.append({\"Variable\": col, \"z-score\": row[col], \"Threshold\": threshold, \"Date\": idx})\n",
    "\n",
    "    return pd.DataFrame(outlier_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_table = identify_outliers(df, threshold=3)\n",
    "display(outliers_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_outliers(df, outliers_table, use_subplots=False):\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    if use_subplots:\n",
    "        n_variables = len(df.columns)\n",
    "        fig, axes = plt.subplots(n_variables, 1, figsize=(12, 3 * n_variables), sharex=True)\n",
    "\n",
    "        for i, col in enumerate(df.columns):\n",
    "            sns.lineplot(data=df, x=df.index, y=col, ax=axes[i], label=col)\n",
    "\n",
    "            variable_outliers = outliers_table[outliers_table[\"Variable\"] == col]\n",
    "            for idx, row in variable_outliers.iterrows():\n",
    "                date = row[\"Date\"]\n",
    "                outlier_value = df.loc[date, col]\n",
    "                axes[i].scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\", label=\"Outlier\" if idx == 0 else \"\")\n",
    "\n",
    "            axes[i].legend()\n",
    "            axes[i].set_ylabel(\"Value\")\n",
    "            axes[i].set_title(f\"Time Series with Outliers for {col}\")\n",
    "\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    else:\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        for col in df.columns:\n",
    "            sns.lineplot(data=df, x=df.index, y=col, label=col)\n",
    "\n",
    "        plotted_outlier_variables = set()\n",
    "        for idx, row in outliers_table.iterrows():\n",
    "            date = row[\"Date\"]\n",
    "            variable = row[\"Variable\"]\n",
    "            outlier_value = df.loc[date, variable]\n",
    "            if variable not in plotted_outlier_variables:\n",
    "                plt.scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\", label=f\"Outlier ({variable})\")\n",
    "                plotted_outlier_variables.add(variable)\n",
    "            else:\n",
    "                plt.scatter(date, outlier_value, marker=\"o\", s=100, c=\"red\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.title(\"Time Series with Outliers\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(df, outliers_table, use_subplots=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Inspection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(df, title='All Variables')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonality "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Compute Seasonal Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seasonal_decomposition(data, model='additive'):\n",
    "    \"\"\"\n",
    "    Compute seasonal decomposition for all time-series in a DataFrame and store all the components in a new DataFrame.\n",
    "\n",
    "    :param data: DataFrame with time-series data\n",
    "    :param period: Number of observations in each seasonal period\n",
    "    :return: DataFrame with seasonal, trend, and residual components for all time-series in the input DataFrame\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store the components for each time-series\n",
    "    decomp_df = pd.DataFrame()\n",
    "\n",
    "    # Loop over each column in the input DataFrame and perform seasonal decomposition\n",
    "    for col in data.columns:\n",
    "        res = seasonal_decompose(data[col], model=model)\n",
    "        decomp_df[f'{col}_seasonal'] = res.seasonal\n",
    "        decomp_df[f'{col}_trend'] = res.trend\n",
    "        decomp_df[f'{col}_residual'] = res.resid\n",
    "\n",
    "    # Set the index of the decomposed DataFrame to be the same as the input DataFrame\n",
    "    decomp_df.index = data.index\n",
    "\n",
    "    return decomp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_df = compute_seasonal_decomposition(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Visualize Seasonal Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal_components(decomp_df):\n",
    "    \"\"\"\n",
    "    Plot all seasonal, trend, and residual components for each variable in a DataFrame.\n",
    "\n",
    "    :param decomp_df: DataFrame with seasonal, trend, and residual components for each variable\n",
    "    \"\"\"\n",
    "    # Initialize a figure with subplots for each variable and component\n",
    "    fig, axs = plt.subplots(nrows=len(decomp_df.columns) // 3, ncols=3, figsize=(12, 4 * (len(decomp_df.columns) // 3)))\n",
    "\n",
    "    # Loop over each variable in the input DataFrame and plot the seasonal, trend, and residual components\n",
    "    for i, col in enumerate(decomp_df.columns[::3]):\n",
    "        axs[i, 0].plot(decomp_df.index, decomp_df[f'{col}'])\n",
    "        axs[i, 0].set_title(f'Seasonal: {col[:-9]}')\n",
    "        axs[i, 1].plot(decomp_df.index, decomp_df[f'{col[:-9]}_trend'])\n",
    "        axs[i, 1].set_title(f'Trend: {col[:-9]}')\n",
    "        axs[i, 2].plot(decomp_df.index, decomp_df[f'{col[:-9]}_residual'])\n",
    "        axs[i, 2].set_title(f'Residual: {col[:-9]}')\n",
    "\n",
    "    # Set the figure title\n",
    "    fig.suptitle('Seasonal Decomposition', fontsize=16)\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seasonal_components(decomp_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Residual Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Auto Stationarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(data, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Perform multiple stationarity tests on each time series in a DataFrame.\n",
    "\n",
    "    :param data: DataFrame with time-series data\n",
    "    :return: DataFrame with test results (Variable, Test, p-value, Threshold, Pass/Fail, Decision)\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store the test results\n",
    "    test_results = pd.DataFrame(columns=['Variable', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n",
    "\n",
    "    # Loop over each column in the input DataFrame and perform stationarity tests\n",
    "    for col in data.columns:\n",
    "        # Perform the ADF test\n",
    "        adf_result = adfuller(data[col], autolag='AIC')\n",
    "        adf_pvalue = adf_result[1]\n",
    "        adf_pass_fail = adf_pvalue < threshold\n",
    "        adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\n",
    "        test_results = test_results.append({\n",
    "            'Variable': col,\n",
    "            'Test': 'ADF',\n",
    "            'p-value': adf_pvalue,\n",
    "            'Threshold': threshold,\n",
    "            'Pass/Fail': adf_pass_fail,\n",
    "            'Decision': adf_decision\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Perform the KPSS test\n",
    "        kpss_result = kpss(data[col], regression='c', nlags='auto')\n",
    "        kpss_pvalue = kpss_result[1]\n",
    "        kpss_pass_fail = kpss_pvalue > threshold\n",
    "        kpss_decision = 'Stationary' if kpss_pass_fail else 'Non-stationary'\n",
    "        test_results = test_results.append({\n",
    "            'Variable': col,\n",
    "            'Test': 'KPSS',\n",
    "            'p-value': kpss_pvalue,\n",
    "            'Threshold': threshold,\n",
    "            'Pass/Fail': kpss_pass_fail,\n",
    "            'Decision': kpss_decision\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Perform the Phillips-Perron test\n",
    "        pp_result = PhillipsPerron(data[col], trend='ct')\n",
    "        pp_pvalue = pp_result.pvalue\n",
    "        pp_threshold = threshold\n",
    "        pp_pass_fail = pp_pvalue < pp_threshold\n",
    "        pp_decision = 'Stationary' if pp_pass_fail else 'Non-stationary'\n",
    "        test_results = test_results.append({\n",
    "            'Variable': col,\n",
    "            'Test': 'PhillipsPerron',\n",
    "            'p-value': pp_pvalue,\n",
    "            'Threshold': pp_threshold,\n",
    "            'Pass/Fail': pp_pass_fail,\n",
    "            'Decision': pp_decision\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Perform the DF-GLS test\n",
    "        dfgls_result = DFGLS(data[col], trend='ct')\n",
    "        dfgls_pvalue = dfgls_result.pvalue\n",
    "        dfgls_threshold = threshold\n",
    "        dfgls_pass_fail = dfgls_pvalue < dfgls_threshold\n",
    "        dfgls_decision = 'Stationary' if dfgls_pass_fail else 'Non-stationary'\n",
    "        test_results = test_results.append({\n",
    "            'Variable': col,\n",
    "            'Test': 'DFGLS',\n",
    "            'p-value': dfgls_pvalue,\n",
    "            'Threshold': dfgls_threshold,\n",
    "            'Pass/Fail': dfgls_pass_fail,\n",
    "            'Decision': dfgls_decision\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_stationarity(data, max_order=5, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Perform the Augmented Dickey-Fuller (ADF) stationarity test on each time series in a DataFrame,\n",
    "    testing for different integration orders until the series is stationary.\n",
    "\n",
    "    :param data: DataFrame with time-series data\n",
    "    :param max_order: Maximum integration order to test\n",
    "    :param threshold: Significance level for the ADF test\n",
    "    :return: DataFrame with test results (Variable, Integration Order, Test, p-value, Threshold, Pass/Fail, Decision)\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store the test results\n",
    "    test_results = pd.DataFrame(columns=['Variable', 'Integration Order', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n",
    "\n",
    "    # Loop over each column in the input DataFrame and perform stationarity tests\n",
    "    for col in data.columns:\n",
    "        is_stationary = False\n",
    "        order = 0\n",
    "\n",
    "        while not is_stationary and order <= max_order:\n",
    "            series = data[col]\n",
    "\n",
    "            if order == 0:\n",
    "                adf_result = adfuller(series)\n",
    "            else:\n",
    "                adf_result = adfuller(np.diff(series, n=order-1))\n",
    "\n",
    "            adf_pvalue = adf_result[1]\n",
    "            adf_pass_fail = adf_pvalue < threshold\n",
    "            adf_decision = 'Stationary' if adf_pass_fail else 'Non-stationary'\n",
    "\n",
    "            test_results = test_results.append({\n",
    "                'Variable': col,\n",
    "                'Integration Order': order,\n",
    "                'Test': 'ADF',\n",
    "                'p-value': adf_pvalue,\n",
    "                'Threshold': threshold,\n",
    "                'Pass/Fail': 'Pass' if adf_pass_fail else 'Fail',\n",
    "                'Decision': adf_decision\n",
    "            }, ignore_index=True)\n",
    "\n",
    "            if adf_pass_fail:\n",
    "                is_stationary = True\n",
    "\n",
    "            order += 1\n",
    "\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_stationarity(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Rolling Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_statistics(df, window_size=12):\n",
    "    \"\"\"\n",
    "    Plot rolling mean and rolling standard deviation in different subplots for each variable.\n",
    "\n",
    "    :param df: DataFrame with time-series data\n",
    "    :param window_size: Window size for the rolling calculations\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        rolling_mean = df[col_name].rolling(window=window_size).mean()\n",
    "        rolling_std = df[col_name].rolling(window=window_size).std()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 6))\n",
    "\n",
    "        ax1.plot(rolling_mean, label=f'{col_name} Rolling Mean')\n",
    "        ax1.legend()\n",
    "        ax1.set_ylabel('Value')\n",
    "        ax1.set_title(f'Rolling Mean for {col_name}')\n",
    "\n",
    "        ax2.plot(rolling_std, label=f'{col_name} Rolling Standard Deviation', color='orange')\n",
    "        ax2.legend()\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Value')\n",
    "        ax2.set_title(f'Rolling Standard Deviation for {col_name}')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = df.diff().dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AR Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Calculate AR Orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ar_orders(dataset, max_order=3):\n",
    "    \"\"\"\n",
    "    This function calculates the autoregressive order of all time series in a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (pd.DataFrame): The dataset containing the time series.\n",
    "    max_order (int): The maximum order to consider for the autoregressive models.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A table with the autoregressive order, AIC, and BIC for orders 0 up to max_order.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each column (time series) in the dataset\n",
    "    for col in dataset.columns:\n",
    "        time_series = dataset[col]\n",
    "\n",
    "        # Test for stationarity using Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(time_series)\n",
    "        if adf_result[1] > 0.05:\n",
    "            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\n",
    "\n",
    "        # Test different autoregressive orders and store the AIC and BIC values\n",
    "        for order in range(max_order + 1):\n",
    "            model = AutoReg(time_series, lags=order, old_names=False)\n",
    "            result = model.fit()\n",
    "\n",
    "            # Add the current time series, order, AIC, and BIC to the results list\n",
    "            results.append({'Variable': col, 'AR order': order, 'AIC': result.aic, 'BIC': result.bic})\n",
    "\n",
    "    # Convert the results list to a DataFrame and return it\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_ar_orders(df_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Selection of AR Order**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MA Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Calculate MA Orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ma_orders(dataset, max_order=3):\n",
    "    \"\"\"\n",
    "    This function calculates the moving average order of all time series in a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (pd.DataFrame): The dataset containing the time series.\n",
    "    max_order (int): The maximum order to consider for the moving average models.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A table with the moving average order, AIC, and BIC for orders 0 up to max_order.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each column (time series) in the dataset\n",
    "    for col in dataset.columns:\n",
    "        time_series = dataset[col]\n",
    "\n",
    "        # Test for stationarity using Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(time_series)\n",
    "        if adf_result[1] > 0.05:\n",
    "            time_series = time_series.diff().dropna()  # Apply first difference to make the series stationary\n",
    "\n",
    "        # Test different moving average orders and store the AIC and BIC values\n",
    "        for order in range(max_order + 1):\n",
    "            model = ARIMA(time_series, order=(0, 0, order))\n",
    "            result = model.fit()\n",
    "\n",
    "            # Add the current time series, order, AIC, and BIC to the results list\n",
    "            results.append({'Variable': col, 'MA order': order, 'AIC': result.aic, 'BIC': result.bic})\n",
    "\n",
    "    # Convert the results list to a DataFrame and return it\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_ma_orders(df_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Selection of MA Order**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Compute Correlation Matrix on Levels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_heatmap(df):\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrrelations across Levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Compute Correlation Matrix on First Difference**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations across First Differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(df_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Reasoning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Compute Scatter Plots on Levels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_pairs(df):\n",
    "    # Compute pairwise scatter plots\n",
    "    sns.pairplot(df, kind='scatter')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_pairs(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Compute Scatter Plots on First Difference**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute first difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_pairs(df_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Reasoning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Compute Correlations at Multiple Lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_correlations(df, target_col, independent_vars, num_lags=10):\n",
    "    \"\"\"\n",
    "    Calculate the correlation between the target variable and the lags of independent variables in the dataset,\n",
    "    and plot a heatmap of these correlations.\n",
    "    :param df: DataFrame containing the target variable and independent variables of interest\n",
    "    :param target_col: Column name of the target variable in the DataFrame\n",
    "    :param independent_vars: List of column names of the independent variables in the DataFrame\n",
    "    :param num_lags: Number of lags to calculate (default is 10)\n",
    "    \"\"\"\n",
    "\n",
    "    correlations = np.zeros((len(independent_vars), num_lags + 1))\n",
    "\n",
    "    for i, ind_var_col in enumerate(independent_vars):\n",
    "        for lag in range(num_lags + 1):\n",
    "            # Create a new DataFrame with the original and lagged variable\n",
    "            temp_df = pd.DataFrame({target_col: df[target_col],\n",
    "                                    f'{ind_var_col}_lag{lag}': df[ind_var_col].shift(lag)})\n",
    "\n",
    "            # Drop NaN rows\n",
    "            temp_df = temp_df.dropna()\n",
    "\n",
    "            # Calculate the correlation between the target variable and the lagged independent variable\n",
    "            corr = temp_df[target_col].corr(temp_df[f'{ind_var_col}_lag{lag}'])\n",
    "\n",
    "            # Store the correlation in the correlations matrix\n",
    "            correlations[i, lag] = corr\n",
    "\n",
    "    # Create a DataFrame with the correlations matrix\n",
    "    correlation_df = pd.DataFrame(correlations, columns=[f'lag_{i}' for i in range(num_lags + 1)], index=independent_vars)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Heatmap of Correlations between Target Variable and Lags of Independent Variables')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Independent Variables')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 'MORTGAGE30US'\n",
    "independent_vars = ['GS10', 'UNRATE', 'FEDFUNDS']\n",
    "plot_heatmap_correlations(df_diff, target_col=target_var, independent_vars=independent_vars, num_lags=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Reasoning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colinearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cointegration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Compute Cointegration Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cointegration for each pair of variables in a DataFrame\n",
    "def calculate_cointegration(dataframe, test=\"Engle-Granger\", threshold=0.05):\n",
    "    coint_df = pd.DataFrame(columns=['Variable 1', 'Variable 2', 'Test', 'p-value', 'Threshold', 'Pass/Fail', 'Decision'])\n",
    "    for i in range(len(dataframe.columns)):\n",
    "        for j in range(i+1, len(dataframe.columns)):\n",
    "            var1 = dataframe.columns[i]\n",
    "            var2 = dataframe.columns[j]\n",
    "            _, p_value, _ = coint(dataframe[var1], dataframe[var2])\n",
    "            pass_fail = \"Pass\" if p_value <= threshold else \"Fail\"\n",
    "            decision = \"Cointegrated\" if pass_fail == \"Pass\" else \"Not Cointegrated\"\n",
    "            coint_df = coint_df.append({\n",
    "                'Variable 1': var1,\n",
    "                'Variable 2': var2,\n",
    "                'Test': test,\n",
    "                'p-value': p_value,\n",
    "                'Threshold': threshold,\n",
    "                'Pass/Fail': pass_fail,\n",
    "                'Decision': decision\n",
    "            }, ignore_index=True)\n",
    "    return coint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cointegration for pairs of variables in the DataFrame\n",
    "coint_results = calculate_cointegration(df)\n",
    "display(coint_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Plot Spread between Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the spread between all pairs of variables in a DataFrame\n",
    "def plot_spread(dataframe):\n",
    "    num_vars = len(dataframe.columns)\n",
    "\n",
    "    for i in range(num_vars):\n",
    "        for j in range(i+1, num_vars):\n",
    "            var1 = dataframe.columns[i]\n",
    "            var2 = dataframe.columns[j]\n",
    "\n",
    "            # Calculate the spread between the two variables\n",
    "            dataframe['spread'] = dataframe[var1] - dataframe[var2]\n",
    "\n",
    "            # Plot the difference (spread) using seaborn\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.lineplot(data=dataframe['spread'], label=f'Spread ({var1} - {var2})')\n",
    "            plt.title(f'Spread ({var1} - {var2})')\n",
    "            plt.legend()\n",
    "\n",
    "            # Display the plot\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spread(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Model Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. Sampling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Split dataset into Training and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(df) * 0.8)   # use 80% of the data for training\n",
    "df_train, df_test = df[:split_index], df[split_index:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Create a Stationary Train and Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply first difference to both training and test df\n",
    "df_train_diff = df_train.diff().dropna()\n",
    "df_test_diff = df_test.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_diff.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Loan Rates and FEDFUNDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the independent variables with no intercept\n",
    "X = df_train_diff['FEDFUNDS']\n",
    "\n",
    "# Define the dependent variable\n",
    "y = df_train_diff['MORTGAGE30US']\n",
    "\n",
    "# Fit the linear regression model\n",
    "model_1 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(model_1.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Reasoning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Loan Rates, constant and FEDFUNDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the independent variables for the linear regression model\n",
    "X = sm.add_constant(df_train_diff['FEDFUNDS'])\n",
    "\n",
    "# Define the dependent variable\n",
    "y = df_train_diff['MORTGAGE30US']\n",
    "\n",
    "# Fit the linear regression model\n",
    "model_2 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(model_2.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Reasoning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Loan Rates and GS10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the independent variables for the linear regression model\n",
    "X = df_train_diff['GS10']\n",
    "\n",
    "# Define the dependent variable\n",
    "y = df_train_diff['MORTGAGE30US']\n",
    "\n",
    "# Fit the linear regression model\n",
    "model_3 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(model_3.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Loan Rates, FEDFUNDS and GS10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the independent variables for the linear regression model\n",
    "X = df_train_diff[['GS10', 'FEDFUNDS']]\n",
    "\n",
    "# Define the dependent variable\n",
    "y = df_train_diff['MORTGAGE30US']\n",
    "\n",
    "# Fit the linear regression model\n",
    "model_4 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(model_4.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Reasoning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Loan Rates, FEDFUNDS, GS10 and UNRATE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the independent variables for the linear regression model\n",
    "X = df_train_diff[['GS10', 'FEDFUNDS', 'UNRATE']]\n",
    "\n",
    "# Define the dependent variable\n",
    "y = df_train_diff['MORTGAGE30US']\n",
    "\n",
    "# Fit the linear regression model\n",
    "model_5 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(model_5.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: In-Sample Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_sample_performance_ols(models):\n",
    "    evaluation_results = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        X = model.model.exog\n",
    "        X_columns = model.model.exog_names\n",
    "        y = model.model.endog\n",
    "\n",
    "        # Calculate the predicted values using the model\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        # Calculate the residuals\n",
    "        residuals = y - y_pred\n",
    "\n",
    "        # Extract R-squared and Adjusted R-squared\n",
    "        r2 = model.rsquared\n",
    "        adj_r2 = model.rsquared_adj\n",
    "\n",
    "        # Calculate the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
    "        mse = model.mse_resid\n",
    "        rmse = mse ** 0.5\n",
    "\n",
    "        # Append the results to the evaluation_results list\n",
    "        evaluation_results.append({\n",
    "            'Model': f'Model_{i + 1}',\n",
    "            'Independent Variables': ', '.join(X_columns),\n",
    "            'R-Squared': r2,\n",
    "            'Adjusted R-Squared': adj_r2,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse\n",
    "        })\n",
    "\n",
    "    # Convert the evaluation_results list to a DataFrame\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "results_df = in_sample_performance_ols(models)\n",
    "display(results_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: In-Sample Forecast First Difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_sample_forecast(models, observed_data, separate_subplots=False):\n",
    "    # Extract the observed data and dates\n",
    "    y = observed_data\n",
    "    x = observed_data.index\n",
    "\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    if separate_subplots:\n",
    "        # Calculate the number of rows and columns for the subplots\n",
    "        n_models = len(models)\n",
    "        n_cols = 2\n",
    "        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n",
    "\n",
    "        # Set up the plot\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        # Plot the observed data and in-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            ax = axes[i]\n",
    "            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n",
    "\n",
    "            X = model.model.exog\n",
    "            y_pred = model.predict(X)\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\n",
    "\n",
    "            # Get the independent variable names\n",
    "            ind_var_names = ', '.join(model.model.exog_names)\n",
    "\n",
    "            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\n",
    "            ax.legend()\n",
    "\n",
    "        # Remove unused subplots\n",
    "        for j in range(i+1, n_rows * n_cols):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        # Customize the plot\n",
    "        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n",
    "\n",
    "    else:\n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot the observed data\n",
    "        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n",
    "\n",
    "        # Plot the in-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            X = model.model.exog\n",
    "            y_pred = model.predict(X)\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Observed Data and In-sample Predictions')\n",
    "        plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "observed_data = df_train_diff['MORTGAGE30US']\n",
    "in_sample_forecast(models, observed_data, separate_subplots=True)  # For separate subplots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: In-Sample Forecast Levels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False):\n",
    "    # Extract the observed data (levels) and dates\n",
    "    y = original_data\n",
    "    x = original_data.index\n",
    "\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    if separate_subplots:\n",
    "        # Calculate the number of rows and columns for the subplots\n",
    "        n_models = len(models)\n",
    "        n_cols = 2\n",
    "        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n",
    "\n",
    "        # Set up the plot\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        # Plot the observed data (levels) and in-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            ax = axes[i]\n",
    "            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n",
    "\n",
    "            X = model.model.exog\n",
    "            y_diff_pred = model.predict(X)\n",
    "            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5, ax=ax)\n",
    "\n",
    "            # Get the independent variable names\n",
    "            ind_var_names = ', '.join(model.model.exog_names)\n",
    "\n",
    "            ax.set_title(f'Model_{i + 1} ({ind_var_names})')\n",
    "            ax.legend()\n",
    "\n",
    "        # Remove unused subplots\n",
    "        for j in range(i+1, n_rows * n_cols):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        # Customize the plot\n",
    "        fig.text(0.5, 0.04, 'DATE', ha='center')\n",
    "        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n",
    "\n",
    "    else:\n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot the observed data (levels)\n",
    "        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n",
    "\n",
    "        # Plot the in-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            X = model.model.exog\n",
    "            y_diff_pred = model.predict(X)\n",
    "            y_pred = np.concatenate(([y.iloc[0]], y.iloc[0] + np.cumsum(y_diff_pred)))\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'Model_{i + 1}', linewidth=1.5)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.xlabel('DATE')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Observed Data and In-sample Predictions (Levels)')\n",
    "        plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "original_data = df_train['MORTGAGE30US']\n",
    "diff_data = df_train_diff['MORTGAGE30US']\n",
    "in_sample_forecast_levels(models, original_data, diff_data, separate_subplots=False)  # For a single plot with all series\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1. Out-of-Sample Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Sample Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_sample_performance(model_list, model_names, test_data, target_col):\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    for fitted_model, model_name in zip(model_list, model_names):\n",
    "        # Extract the column names of the independent variables from the model\n",
    "        independent_vars = fitted_model.model.exog_names\n",
    "\n",
    "        # Separate the target variable and features in the test dataset\n",
    "        X_test = test_data[independent_vars]\n",
    "        y_test = test_data[target_col]\n",
    "\n",
    "        # Predict the test data\n",
    "        y_pred = fitted_model.predict(X_test)\n",
    "\n",
    "        # Calculate the residuals\n",
    "        residuals = y_test - y_pred\n",
    "\n",
    "        # Calculate the mean squared error and root mean squared error\n",
    "        mse = np.mean(residuals ** 2)\n",
    "        rmse_val = np.sqrt(mse)\n",
    "\n",
    "        # Store the results\n",
    "        model_name_with_vars = f\"{model_name} ({', '.join(independent_vars)})\"\n",
    "        results.append([model_name_with_vars, mse, rmse_val])\n",
    "\n",
    "    # Create a DataFrame to display the results\n",
    "    results_df = pd.DataFrame(results, columns=['Model', 'MSE', 'RMSE'])\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_3, model_4]\n",
    "model_names = ['model_3', 'model_4']\n",
    "results_df = out_of_sample_performance(model_list, model_names=model_names, test_data=df_test_diff, target_col='MORTGAGE30US')\n",
    "display(results_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Sample Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_sample_forecast(models, model_names, test_data, target_col, separate_subplots=False):\n",
    "    # Extract the observed data and dates\n",
    "    y = test_data[target_col]\n",
    "    x = test_data.index\n",
    "\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    if separate_subplots:\n",
    "        # Calculate the number of rows and columns for the subplots\n",
    "        n_models = len(models)\n",
    "        n_cols = 2\n",
    "        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n",
    "\n",
    "        # Set up the plot\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        # Plot the observed data and out-of-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            ax = axes[i]\n",
    "            sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n",
    "\n",
    "            exog_names = model.model.exog_names\n",
    "            if 'const' in exog_names and 'const' not in test_data.columns:\n",
    "                X_test = test_data[[name for name in exog_names if name != 'const']]\n",
    "                X_test.insert(0, 'const', 1)\n",
    "            else:\n",
    "                X_test = test_data[exog_names]\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\n",
    "\n",
    "            ax.set_title(f'{model_names[i]} ({\", \".join(exog_names)})')\n",
    "            ax.legend()\n",
    "\n",
    "        # Remove unused subplots\n",
    "        for j in range(i + 1, n_rows * n_cols):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        # Customize the plot\n",
    "        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n",
    "\n",
    "    else:\n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot the observed data\n",
    "        sns.lineplot(x=x, y=y, label='Observed', linewidth=2, color='lightgrey')\n",
    "\n",
    "        # Plot the out-of-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            exog_names = model.model.exog_names\n",
    "            if 'const' in exog_names and 'const' not in test_data.columns:\n",
    "                X_test = test_data[[name for name in exog_names if name != 'const']]\n",
    "                X_test.insert(0, 'const', 1)\n",
    "            else:\n",
    "                X_test = test_data[exog_names]\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Observed Data and Out-of-sample Predictions')\n",
    "        plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_3, model_4]\n",
    "out_of_sample_forecast(models, model_names=['model_3', 'model_4'], test_data=df_test_diff, target_col='MORTGAGE30US', separate_subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_sample_forecast_levels(models, model_names, test_data, original_data, target_col, separate_subplots=False):\n",
    "    # Extract the observed data and dates\n",
    "    y_test = test_data[target_col]\n",
    "    y_orig = original_data[original_data.index.isin(test_data.index)][target_col]\n",
    "    x = y_orig.index\n",
    "\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    if separate_subplots:\n",
    "        # Calculate the number of rows and columns for the subplots\n",
    "        n_models = len(models)\n",
    "        n_cols = 2\n",
    "        n_rows = n_models // n_cols + (n_models % n_cols > 0)\n",
    "\n",
    "        # Set up the plot\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5), sharex=True, sharey=True)\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        # Plot the observed data and out-of-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            ax = axes[i]\n",
    "            sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey', ax=ax)\n",
    "\n",
    "            exog_names = model.model.exog_names\n",
    "            if 'const' in exog_names and 'const' not in test_data.columns:\n",
    "                X_test = test_data[[name for name in exog_names if name != 'const']]\n",
    "                X_test.insert(0, 'const', 1)\n",
    "            else:\n",
    "                X_test = test_data[exog_names]\n",
    "\n",
    "            y_pred_diff = model.predict(X_test)\n",
    "            y_pred = y_pred_diff + y_orig.shift(1).values\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5, ax=ax)\n",
    "\n",
    "            ax.set_title(f'{model_names[i]} ({\", \".join(exog_names)})')\n",
    "            ax.legend()\n",
    "\n",
    "        # Remove unused subplots\n",
    "        for j in range(i + 1, n_rows * n_cols):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        # Customize the plot\n",
    "        fig.text(0.04, 0.5, 'Value', va='center', rotation='vertical')\n",
    "\n",
    "    else:\n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot the observed data\n",
    "        sns.lineplot(x=x, y=y_orig, label='Observed', linewidth=2, color='lightgrey')\n",
    "\n",
    "        # Plot the out-of-sample predictions for each model\n",
    "        for i, model in enumerate(models):\n",
    "            exog_names = model.model.exog_names\n",
    "            if 'const' in exog_names and 'const' not in test_data.columns:\n",
    "                X_test = test_data[[name for name in exog_names if name != 'const']]\n",
    "                X_test.insert(0, 'const', 1)\n",
    "            else:\n",
    "                X_test = test_data[exog_names]\n",
    "\n",
    "            y_pred_diff = model.predict(X_test)\n",
    "            y_pred = y_pred_diff + y_orig.shift(1).values\n",
    "            sns.lineplot(x=x, y=y_pred, label=f'{model_names[i]}', linewidth=1.5)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Observed Data and Out-of-sample Predictions')\n",
    "        plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_3, model_4]\n",
    "model_names = ['model_3', 'model_4']\n",
    "original_data = df_test\n",
    "out_of_sample_forecast_levels(models, model_names, test_data=df_test_diff, original_data=original_data, target_col='MORTGAGE30US', separate_subplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2. Forecast Performance "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Step Ahead Forecast "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Five-Step Ahead Forecast "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3. Scenario Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Interest Rates Shocks "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.4. Stress Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.5. Uncertainty Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-eEL8LtKG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
