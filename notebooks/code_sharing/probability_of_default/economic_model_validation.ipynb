{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "To use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready. When running this notebook locally, this includes installing any missing prerequisite modules that you discover with `pip install`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "Every documentation project in the Platform UI comes with a _code snippet_ that lets the client library associate your documentation and tests with the right project on the Platform UI when you run this notebook. As you will see later, documentation projects are useful because they act as containers for model documentation and validation reports and they enable you to organize all of your documentation work in one place. \n",
    "\n",
    "Get your code snippet by creating a documentation project:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. Go to **Documentation Projects** and click **Create new project**.\n",
    "\n",
    "<!--- NR TO DO this notebook didn't specify what model to use --->\n",
    "3. Select **`MODEL_NAME`** and **`Initial Validation`** for the model name and type, give the project a unique  name to make it yours, and then click **Create project**.\n",
    "\n",
    "4. Go to **Documentation Projects** > **YOUR_UNIQUE_PROJECT_NAME** > **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 14:10:48,147 - INFO(validmind.api_client): Connected to ValidMind. Project: [9] Credit Risk Scorecard - Initial Validation (cllaz74gb067dszy6donpqm98)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  project = \"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "from IPython.display import HTML\n",
    "from notebooks.probability_of_default.helpers.Developer import Developer\n",
    "from notebooks.probability_of_default.helpers.economic_model import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"DRSFRMACBS\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded 8 objects from datasets/macroeconomic_data_and_models.pkl\n"
     ]
    }
   ],
   "source": [
    "developer = Developer()\n",
    "macro_model = developer.load_objects_from_pickle(\"datasets/macroeconomic_data_and_models.pkl\")\n",
    "\n",
    "df_raw = macro_model[\"df_raw\"]\n",
    "df_preparation = macro_model[\"df_prepared\"]\n",
    "\n",
    "df_train = macro_model[\"df_train\"]\n",
    "df_test = macro_model[\"df_test\"]\n",
    "model_fit = macro_model[\"model_fit\"]\n",
    "\n",
    "df_train_final = macro_model[\"df_train_final\"]\n",
    "df_test_final = macro_model[\"df_test_final\"]\n",
    "model_fit_final = macro_model[\"model_fit_final\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ValidMind Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 14:10:48,336 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-21 14:10:48,343 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-21 14:10:48,347 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-21 14:10:48,351 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-21 14:10:48,354 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-08-21 14:10:48,357 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "# Create ValidMind Datasets\n",
    "vm_df_raw = vm.init_dataset(\n",
    "    dataset=df_raw, \n",
    "    target_column=target_column)\n",
    "vm_df_preparation = vm.init_dataset(\n",
    "    dataset=df_preparation, \n",
    "    target_column=target_column)\n",
    "vm_df_train = vm.init_dataset(\n",
    "    dataset=df_train, \n",
    "    target_column=target_column)\n",
    "vm_df_test = vm.init_dataset(\n",
    "    dataset=df_test, \n",
    "    target_column=target_column)\n",
    "vm_df_train_final = vm.init_dataset(\n",
    "    dataset=df_train_final, \n",
    "    target_column=target_column)\n",
    "vm_df_test_final = vm.init_dataset(\n",
    "    dataset=df_test_final, \n",
    "    target_column=target_column)\n",
    "\n",
    "# Create ValidMind Models\n",
    "vm_model_fit = vm.init_model(\n",
    "    model = model_fit, \n",
    "    train_ds=vm_df_train, \n",
    "    test_ds=vm_df_test)\n",
    "vm_model_fit_final = vm.init_model(\n",
    "    model = model_fit_final, \n",
    "    train_ds=vm_df_train_final, \n",
    "    test_ds=vm_df_test_final)\n",
    "\n",
    "# Create test contexts \n",
    "test_context_raw = TestContext(dataset=vm_df_raw)\n",
    "test_context_preparation = TestContext(dataset=vm_df_preparation)\n",
    "test_context_train = TestContext(dataset=vm_df_train)\n",
    "test_context_test = TestContext(dataset=vm_df_test)\n",
    "test_context_train_final = TestContext(dataset=vm_df_train_final)\n",
    "test_context_test_final = TestContext(dataset=vm_df_test_final)\n",
    "\n",
    "test_context_model = TestContext(model = vm_model_fit)\n",
    "test_context_models = TestContext(models = [vm_model_fit, vm_model_fit_final])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23e4bbaebec431f93908b7a636714a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Time Series Missing Values ❌</h2>\\n            <p>Test that the n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesMissingValues import TimeSeriesMissingValues\n",
    "\n",
    "params = {\"min_threshold\": 2}\n",
    "\n",
    "metric = TimeSeriesMissingValues(test_context_raw, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "INFO: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bd212784ad43bf901d41344f7f187d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Time Series Outliers ❌</h2>\\n            <p>Test that find outlie…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesOutliers import TimeSeriesOutliers\n",
    "\n",
    "params = {\"zscore_threshold\": 3}\n",
    "\n",
    "metric = TimeSeriesOutliers(test_context_raw, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TimeSeriesFrequency.__init__() missing 1 required positional argument: 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_94743/4188207080.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvalidmind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeSeriesFrequency\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesFrequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesFrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_context_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TimeSeriesFrequency.__init__() missing 1 required positional argument: 'params'"
     ]
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesFrequency import TimeSeriesFrequency\n",
    "\n",
    "metric = TimeSeriesFrequency(test_context_raw)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepared Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesMissingValues import TimeSeriesMissingValues\n",
    "\n",
    "params = {\"min_threshold\": 2}\n",
    "\n",
    "metric = TimeSeriesMissingValues(test_context_preparation, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesFrequency import TimeSeriesFrequency\n",
    "\n",
    "metric = TimeSeriesFrequency(test_context_preparation)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TimeSeriesLinePlot import TimeSeriesLinePlot\n",
    "\n",
    "metric = TimeSeriesLinePlot(test_context_train)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = TimeSeriesLinePlot(test_context_test)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.LaggedCorrelationHeatmap import LaggedCorrelationHeatmap\n",
    "\n",
    "metric = LaggedCorrelationHeatmap(test_context_train)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.EngleGrangerCoint import EngleGrangerCoint\n",
    "\n",
    "metric = EngleGrangerCoint(test_context_train)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.DatasetSplit import DatasetSplit\n",
    "\n",
    "metric = DatasetSplit(test_context_model)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.ModelMetadata import ModelMetadata\n",
    "\n",
    "metric = ModelMetadata(test_context_model)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionCoeffsPlot import RegressionCoeffsPlot\n",
    "\n",
    "metric = RegressionCoeffsPlot(test_context_models)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n",
    "\n",
    "metric = RegressionModelsCoeffs(test_context_models)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsPerformance import RegressionModelsPerformance\n",
    "\n",
    "metric = RegressionModelsPerformance(test_context_models)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
