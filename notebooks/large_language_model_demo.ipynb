{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Sentiment Analysis using LLMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_KEY = '...'\n",
    "\n",
    "def available_openai_engines(filters=None):\n",
    "    \"\"\"Get all available OpenAI engine IDs, sorted by version number.\"\"\"\n",
    "    try:\n",
    "        openai.api_key = OPEN_AI_KEY\n",
    "        response = openai.Engine.list()\n",
    "        engines = response['data']\n",
    "\n",
    "        # If a filter is provided, only keep engines that include any of the filter keywords\n",
    "        if filters:\n",
    "            engines = [engine for engine in engines if any(keyword in engine['id'] for keyword in filters)]\n",
    "\n",
    "        # Extract version numbers and sort\n",
    "        versioned_engines = sorted(engines, key=lambda e: list(map(int, re.findall(r'\\d+', e['id']))), reverse=True)\n",
    "\n",
    "        # Extract only the IDs and create a DataFrame\n",
    "        ids = [engine['id'] for engine in versioned_engines]\n",
    "        df = pd.DataFrame(ids, columns=['Engine ID'])\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while retrieving engines: {e}\")\n",
    "\n",
    "\n",
    "def query_gpt(prompt, model_name):\n",
    "    \"\"\"Query the specified GPT model with the provided prompt.\"\"\"\n",
    "    try:\n",
    "        openai.api_key = OPEN_AI_KEY\n",
    "        response = openai.Completion.create(\n",
    "          engine=model_name,\n",
    "          prompt=prompt,\n",
    "          max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying model {model_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def create_prompts(input_text, instructions):\n",
    "    \"\"\"Create a list of prompts by concatenating the instructions and adding to each line of input text.\"\"\"\n",
    "    instruction = \" \".join(instructions)\n",
    "    prompts = [instruction + \" \" + text for text in input_text]\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def process_output(output):\n",
    "    \"\"\"\n",
    "    This function takes the raw output from GPT-3, extracts the sentiment, and standardizes it to be in\n",
    "    uppercase and one of \"NEGATIVE\", \"POSITIVE\", or \"NEUTRAL\".\n",
    "    \"\"\"\n",
    "    output = output.lower()\n",
    "    if 'negative' in output:\n",
    "        return 'NEGATIVE'\n",
    "    elif 'positive' in output:\n",
    "        return 'POSITIVE'\n",
    "    elif 'neutral' in output:\n",
    "        return 'NEUTRAL'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "\n",
    "def longest_common_starting_substring(strs):\n",
    "    \"\"\"\n",
    "    Given a list of strings, this function returns the longest common starting substring.\n",
    "    \"\"\"\n",
    "    prefix = strs[0]\n",
    "    for string in strs[1:]:\n",
    "        while string[:len(prefix)] != prefix:\n",
    "            prefix = prefix[:-1]\n",
    "    return prefix\n",
    "\n",
    "def predict_sentiment(models, prompts):\n",
    "    \"\"\"\n",
    "    This function takes a list of models and prompts as input, \n",
    "    and outputs a pandas DataFrame containing the Input Text (obtained by removing the instructions from the prompt) and\n",
    "    Predicted Sentiment for each model, organized in columns by model.\n",
    "    \"\"\"\n",
    "    # Placeholder for results\n",
    "    results = []\n",
    "\n",
    "    # Determine the common instruction from the prompts\n",
    "    instruction = longest_common_starting_substring(prompts)\n",
    "\n",
    "    # Iterate over the models and prompts and query the models\n",
    "    for model in models:\n",
    "        for prompt in prompts:\n",
    "            # Split the instruction from the input text\n",
    "            input_text = prompt.replace(instruction, '').strip()\n",
    "            \n",
    "            # Check if we've already queried this model with this input_text\n",
    "            if not any((result['ID'] == model and result['Input Text'] == input_text) for result in results):\n",
    "                sentiment_raw = query_gpt(prompt, model)  # Replace `query_gpt` with the function you're using to query the model\n",
    "                sentiment = process_output(sentiment_raw)\n",
    "                results.append({'ID': model, 'Input Text': input_text, 'Predicted Sentiment': sentiment})\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Pivot the DataFrame to organize by input text and model\n",
    "    results_pivot = results_df.pivot(index='Input Text', columns='ID', values='Predicted Sentiment')\n",
    "\n",
    "    # Reset the index to return a \"regular\" dataframe\n",
    "    return results_pivot.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_multiclass_confusion_matrix(pred_sentiment, true_sentiment, label_names=['NEGATIVE', 'NEUTRAL', 'POSITIVE']):\n",
    "    # Get model's name from column name\n",
    "    model_name = pred_sentiment.columns[0]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_sentiment, pred_sentiment[model_name], labels=label_names)\n",
    "\n",
    "    # Create a dataframe for better annotation handling\n",
    "    cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "\n",
    "    # Create a heatmap\n",
    "    heat_map = go.Heatmap(\n",
    "        z=cm_df.values,\n",
    "        x=list(cm_df.columns),\n",
    "        y=list(cm_df.index),\n",
    "        colorscale='Blues',\n",
    "        showscale=True,\n",
    "    )\n",
    "\n",
    "    # Create annotations (hover text)\n",
    "    annotations = []\n",
    "    for n, row in enumerate(cm):\n",
    "        for m, val in enumerate(row):\n",
    "            var_text = f'{cm[n][m]}'\n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    showarrow=False,\n",
    "                    text=var_text, \n",
    "                    xref='x', \n",
    "                    yref='y',\n",
    "                    x=m,\n",
    "                    y=n,\n",
    "                    font=dict(color='white'),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Create layout\n",
    "    layout = go.Layout(\n",
    "        title_text='Confusion matrix for ' + model_name,  # Use model's name in title\n",
    "        height=500,\n",
    "        width=500,\n",
    "        annotations=annotations\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=heat_map, layout=layout)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = [\n",
    "    \"Determine the sentiment of the financial news as negative, neutral or positive.\",\n",
    "    \"Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'.\"\n",
    "] \n",
    "input_text = [\n",
    "    'Consumer credit $18.9BN, Exp. $16BN, Last $9.6BN.',\n",
    "    'Estee Lauder Q2 adj. EPS $2.11; FactSet consensus $1.90.',\n",
    "    'The situation of coated magazine printing paper will continue to be weak',\n",
    "    'Pre-tax loss totaled euro 0.3 million, compared to a loss of euro 2.2 million in the first quarter of 2005.',\n",
    "    'Madison Square Garden Q2 EPS $3.93 vs. $3.42.',\n",
    "    'Boeing announces additional order for 737 MAX planes.',\n",
    "    'Boeing: Deliveries 24 Jets in November',\n",
    "    'PPDâ€™s stock indicated in early going to open at $30, or 11% above $27 IPO price.'\n",
    "]\n",
    "true_sentiment = ['POSITIVE', \n",
    "                  'NEUTRAL',\n",
    "                  'NEGATIVE',\n",
    "                  'POSITIVE',\n",
    "                  'POSITIVE',\n",
    "                  'NEUTRAL',\n",
    "                  'POSITIVE',\n",
    "                  'NEUTRAL']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. Consumer credit $18.9BN, Exp. $16BN, Last $9.6BN.\",\n",
       " \"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. Estee Lauder Q2 adj. EPS $2.11; FactSet consensus $1.90.\",\n",
       " \"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. The situation of coated magazine printing paper will continue to be weak\",\n",
       " \"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. Pre-tax loss totaled euro 0.3 million, compared to a loss of euro 2.2 million in the first quarter of 2005.\",\n",
       " \"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. Madison Square Garden Q2 EPS $3.93 vs. $3.42.\",\n",
       " \"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. Boeing announces additional order for 737 MAX planes.\",\n",
       " \"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. Boeing: Deliveries 24 Jets in November\",\n",
       " \"Determine the sentiment of the financial news as negative, neutral or positive. Please respond with a single word indicating the sentiment of the financial news: 'negative', 'neutral', or 'positive'. PPDâ€™s stock indicated in early going to open at $30, or 11% above $27 IPO price.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = create_prompts(input_text, instruction)\n",
    "prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Available LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Engine ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-davinci-003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-davinci-002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-davinci-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text-search-curie-query-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text-search-davinci-query-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text-search-curie-doc-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text-search-davinci-doc-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text-davinci-edit-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text-similarity-davinci-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text-curie-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>davinci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>curie-instruct-beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>davinci-similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>curie-search-document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>davinci-instruct-beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>davinci-search-query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>davinci-search-document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>curie-search-query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>curie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>curie-similarity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Engine ID\n",
       "0                text-davinci-003\n",
       "1                text-davinci-002\n",
       "2                text-davinci-001\n",
       "3     text-search-curie-query-001\n",
       "4           code-davinci-edit-001\n",
       "5       text-similarity-curie-001\n",
       "6   text-search-davinci-query-001\n",
       "7       text-search-curie-doc-001\n",
       "8     text-search-davinci-doc-001\n",
       "9           text-davinci-edit-001\n",
       "10    text-similarity-davinci-001\n",
       "11                 text-curie-001\n",
       "12                        davinci\n",
       "13            curie-instruct-beta\n",
       "14             davinci-similarity\n",
       "15          curie-search-document\n",
       "16          davinci-instruct-beta\n",
       "17           davinci-search-query\n",
       "18        davinci-search-document\n",
       "19             curie-search-query\n",
       "20                          curie\n",
       "21               curie-similarity"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engines = available_openai_engines(filters=['curie', 'davinci'])\n",
    "display(engines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Numerical Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ID</th>\n",
       "      <th>Input Text</th>\n",
       "      <th>text-curie-001</th>\n",
       "      <th>text-davinci-003</th>\n",
       "      <th>true_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boeing announces additional order for 737 MAX ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boeing: Deliveries 24 Jets in November</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consumer credit $18.9BN, Exp. $16BN, Last $9.6BN.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estee Lauder Q2 adj. EPS $2.11; FactSet consen...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madison Square Garden Q2 EPS $3.93 vs. $3.42.</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PPDâ€™s stock indicated in early going to open a...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pre-tax loss totaled euro 0.3 million, compare...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The situation of coated magazine printing pape...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ID                                         Input Text text-curie-001  \\\n",
       "0   Boeing announces additional order for 737 MAX ...       NEGATIVE   \n",
       "1              Boeing: Deliveries 24 Jets in November       POSITIVE   \n",
       "2   Consumer credit $18.9BN, Exp. $16BN, Last $9.6BN.       NEGATIVE   \n",
       "3   Estee Lauder Q2 adj. EPS $2.11; FactSet consen...       NEGATIVE   \n",
       "4       Madison Square Garden Q2 EPS $3.93 vs. $3.42.       POSITIVE   \n",
       "5   PPDâ€™s stock indicated in early going to open a...       NEGATIVE   \n",
       "6   Pre-tax loss totaled euro 0.3 million, compare...       NEGATIVE   \n",
       "7   The situation of coated magazine printing pape...       NEGATIVE   \n",
       "\n",
       "ID text-davinci-003 true_sentiment  \n",
       "0          POSITIVE       POSITIVE  \n",
       "1          NEGATIVE        NEUTRAL  \n",
       "2           NEUTRAL       NEGATIVE  \n",
       "3           NEUTRAL       POSITIVE  \n",
       "4           NEUTRAL       POSITIVE  \n",
       "5          POSITIVE        NEUTRAL  \n",
       "6          NEGATIVE       POSITIVE  \n",
       "7          NEGATIVE        NEUTRAL  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llms = ['text-davinci-003', 'text-curie-001']\n",
    "pred_sentiment = predict_sentiment(llms, prompts)\n",
    "\n",
    "# Add true_sentiment to DataFrame\n",
    "results = pred_sentiment.copy()\n",
    "results['true_sentiment'] = true_sentiment\n",
    "\n",
    "# Display the DataFrame\n",
    "display(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Reasoning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pred_sentiment_curie\n",
    "pred_sentiment_curie = results[['text-curie-001']].copy()\n",
    "\n",
    "# Create pred_sentiment_davinci\n",
    "pred_sentiment_davinci = results[['text-davinci-003']].copy()\n",
    "\n",
    "# Create true_sentiment\n",
    "true_sentiment = results[['true_sentiment']].copy()\n",
    "\n",
    "#Â reasoning_curie = prediction_reasoning(pred_sentiment_curie, prompts)\n",
    "# reasoning_davinci = prediction_reasoning(pred_sentiment_davinci, prompts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiclass_confusion_matrix(pred_sentiment_curie, true_sentiment)\n",
    "plot_multiclass_confusion_matrix(pred_sentiment_davinci, true_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
