{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Customm Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 17:15:36,629 - INFO(validmind.api_client): Connected to ValidMind. Project: Customer Churn - Initial Validation (clkvhtg6g0005q08h5h9uhtjl)\n"
     ]
    }
   ],
   "source": [
    "## Replace the code below with the code snippet from your project ## \n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key = \"...\",\n",
    "    api_secret = \"...\",\n",
    "    project = \"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Custom Metrics\n",
    "\n",
    "It is possible to implement custom metrics or threshold test classes. The are only two requirements for getting this to work:\n",
    "\n",
    "- We need to build a `TestPlan` that can execute the custom metric or threshold test (or add it to an existing `TestPlan`, TBD).\n",
    "- We need to implement a `run` method on the custom metric or threshold test class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Custom Metric\n",
    "\n",
    "The following example shows how to implement a custom metric that calculates the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Metric\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "        \n",
    "        return self.cache_results(mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Custom Metric\n",
    "\n",
    "It is possible to run a custom metric without running an entire test suite. This is useful for testing the metric before integrating it into a test suite.\n",
    "\n",
    "The key idea is to create a `TestContext` object and pass it to the metric initializer. When a test suite is executed, the `TestContext` is created by the `TestSuite` class and passed down to every associated metric and threshold test. However, when we want to test a metric in isolation, we need to create the `TestContext` ourselves.\n",
    "\n",
    "In this example we don't need to pass any arguments to the `TestContext` initializer, but it is possible to pass any arguments as required by `required_inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestSuiteMetricResult(result_id=\"mean_of_values\", metric, figures)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the results of the metric by accessing the `result` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9e9f42aecd417ea7337d8e2e7c3ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Calculates the mean of the provided values</p>'), HTML(value='\\n        <style>\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a `summary()` Method to the Custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "from validmind.vm_models import Metric, ResultSummary, ResultTable, ResultTableMetadata\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def summary(self, metric_value):\n",
    "        # Create a dataframe structure that can be rendered as a table\n",
    "        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n",
    "\n",
    "        return ResultSummary(\n",
    "            results=[\n",
    "                ResultTable(\n",
    "                    data=simple_df,\n",
    "                    metadata=ResultTableMetadata(title=\"Example Table\"),\n",
    "                ),                \n",
    "            ]\n",
    "        )        \n",
    "        \n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "        return self.cache_results(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestSuiteMetricResult(result_id=\"mean_of_values\", metric, figures)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fecdecade84725a2fb652ff1458df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Calculates the mean of the provided values</p>'), HTML(value='<h3>Example Table<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Figures to a Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from validmind.vm_models import Figure, Metric, ResultSummary, ResultTable, ResultTableMetadata\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    name = \"mean_of_values\"\n",
    "\n",
    "    def description(self):\n",
    "        return \"Calculates the mean of the provided values\"\n",
    "\n",
    "    def summary(self, metric_value):\n",
    "        # Create a dataframe structure that can be rendered as a table\n",
    "        simple_df = pd.DataFrame({\"Mean of Values\": [metric_value]})\n",
    "\n",
    "        return ResultSummary(\n",
    "            results=[\n",
    "                ResultTable(\n",
    "                    data=simple_df,\n",
    "                    metadata=ResultTableMetadata(title=\"Example Table\"),\n",
    "                ),\n",
    "            ]\n",
    "        )        \n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "\n",
    "        # Create a random histogram with matplotlib\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n",
    "        ax.set_title(\"Histogram of random numbers\")\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Do this if you want to prevent the figure from being displayed\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        figure = Figure(\n",
    "            for_object=self,\n",
    "            key=self.key,\n",
    "            figure=fig\n",
    "        )\n",
    "\n",
    "        return self.cache_results(mean, figures=[figure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestSuiteMetricResult(result_id=\"mean_of_values\", metric, figures)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3653bfec797e4ed0b98fab3047ff75bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Calculates the mean of the provided values</p>'), HTML(value='<h3>Example Table<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c66b265c2c4df19aed485da0b83ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Running test plan...'), IntProgress(value=0, max=2)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0338745e3cc742f99b07c4beae2f1250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Results for <i>My Custom Test Plan</i> Test Plan:</h2><hr>'), HTML(value='<div …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.vm_models import TestSuite\n",
    "\n",
    "class MyCustomTestSuite(TestSuite):\n",
    "    \"\"\"\n",
    "    Custom test suite\n",
    "    \"\"\"\n",
    "\n",
    "    suite_id = \"my_custom_test_suite\"\n",
    "    tests = [MeanMetric]\n",
    "\n",
    "my_custom_test_suite = MyCustomTestSuite(config={\n",
    "    \"mean_of_values\": {\n",
    "        \"values\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    },\n",
    "})\n",
    "my_custom_test_suite.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-mI3jzOkk-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
