{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick hack to load local library code\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Load API key and secret from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "# For test environment use api_host=\"https://api.dev.vm.validmind.ai/api/v1/tracking\"\n",
    "vm.init(\n",
    "    # project=\"cl2r3k1ri000009jweny7ba1g\"\n",
    "    project=\"cl1jyv16o000809lg98gi9tie\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas dataset detected. Initializing VM Dataset instance...\n",
      "Inferring dataset types...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n",
    "\n",
    "vm_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    target_column=\"Exited\",\n",
    "    class_labels={\n",
    "        \"0\": \"Did not exit\",\n",
    "        \"1\": \"Exited\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test plan 'tabular_dataset'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b79fb6a00b4b378b5489ce3253169a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending results of test plan execution 'tabular_dataset' to ValidMind...\n",
      "|-- Running sub test plan - tabular_dataset_description\n",
      "Running test plan 'tabular_dataset_description'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c10ce4ff0c430aa16c4b18cf73f9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DatasetMetadata: dataset_metadata\n",
      "Running Metric: dataset_description\n",
      "Preparing in-memory dataset copy...\n",
      "Sending results of test plan execution 'tabular_dataset_description' to ValidMind...\n",
      "Successfully logged dataset metadata and statistics.\n",
      "Successfully logged metrics\n",
      "|-- Running sub test plan - tabular_data_quality\n",
      "Running test plan 'tabular_data_quality'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06022a66afa4dd09bf6e259c81357a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ThresholdTest: class_imbalance\n",
      "Running ThresholdTest: duplicates\n",
      "Running ThresholdTest: cardinality\n",
      "Running ThresholdTest: pearson_correlation\n",
      "Running ThresholdTest: missing\n",
      "Running ThresholdTest: skewness\n",
      "Running ThresholdTest: unique\n",
      "Running ThresholdTest: zeros\n",
      "Sending results of test plan execution 'tabular_data_quality' to ValidMind...\n",
      "Successfully logged test results for test: class_imbalance\n",
      "Successfully logged test results for test: duplicates\n",
      "Successfully logged test results for test: cardinality\n",
      "Successfully logged test results for test: pearson_correlation\n",
      "Successfully logged test results for test: missing\n",
      "Successfully logged test results for test: skewness\n",
      "Successfully logged test results for test: unique\n",
      "Successfully logged test results for test: zeros\n"
     ]
    }
   ],
   "source": [
    "dataset_tests = vm.test_plans.TabularDataset(\n",
    "    dataset=vm_dataset,\n",
    ")\n",
    "\n",
    "dataset_tests.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n",
    "\n",
    "genders = {\"Male\": 0, \"Female\": 1}\n",
    "df.replace({\"Gender\": genders}, inplace=True)\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n",
    "df.drop(\"Geography\", axis=1, inplace=True)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.20)\n",
    "\n",
    "# This guarantees a 60/20/20 split\n",
    "train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "# For training\n",
    "x_train = train_ds.drop(\"Exited\", axis=1)\n",
    "y_train = train_ds.loc[:, \"Exited\"].astype(int)\n",
    "x_val = val_ds.drop(\"Exited\", axis=1)\n",
    "y_val = val_ds.loc[:, \"Exited\"].astype(int)\n",
    "\n",
    "# For testing\n",
    "x_test = test_df.drop(\"Exited\", axis=1)\n",
    "y_test = test_df.loc[:, \"Exited\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_model = vm.init_model(model)\n",
    "vm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\n",
    "vm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = vm.test_plans.SKLearnClassifier(\n",
    "    model=vm_model,\n",
    "    train_ds=vm_train_ds,\n",
    "    test_ds=vm_test_ds,\n",
    ")\n",
    "\n",
    "model_metrics.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-Jp3s24zK-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5507f2e99c1cac96073e07e686bb64d511c5f1c7216ba7fc4306f43af6557f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
