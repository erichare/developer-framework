{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "da095bb3-4368-4154-9a6c-0911711790be",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## ValidMind Python Client Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-a91a41e8-53b4-4d72-ad25-b779163ecb2b",
    "deepnote_cell_height": 307.984375,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "The ValidMind Python client allows model developers and validators to automatically document different aspects of the model development lifecycle. \n",
    "\n",
    "For modelers, the client provides the following high level features:\n",
    "\n",
    "- Log qualitative data about the model's conceptual soundness\n",
    "- Log information about datasets and models\n",
    "- Log training and evaluation metrics about datasets and models\n",
    "- Run data quality checks\n",
    "- Run model evaluation tests\n",
    "\n",
    "For validators, the client also provides (TBD) the ability to effectively challenge the model's performance according to its objective, use case and specific project's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cb3d95065c634710ba06e460fe4e143c",
    "deepnote_cell_height": 114.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Notebook Requirements\n",
    "\n",
    "- This notebook and the ValidMind client must be executed on an environment running Python >= 3.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-c54403d9-f83d-4e24-96e0-facd959a9513",
    "deepnote_cell_height": 122.796875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Installing the client library\n",
    "\n",
    "While we finish the process of making the library publicly accessible `pip`, it can be installed with the following command that will direct `pip` to the S3 bucket that contains the latest version of the client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-0e021be3-b49b-4ae6-83cf-a8469e4e1bda",
    "deepnote_cell_height": 106.796875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 39571,
    "execution_start": 1659124511003,
    "is_output_hidden": true,
    "source_hash": "fd669f24"
   },
   "outputs": [],
   "source": [
    "# !pip install https://vmai.s3.us-west-1.amazonaws.com/sdk/validmind-0.4.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick hack to load local SDK code\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Load API key and secret from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new project\n",
    "\n",
    "Before we test the client library with a dataset and a model, we need to create a new project on the ValidMind dashboard:\n",
    "\n",
    "- Navigate to the dashboard and click on the \"Create new Project\" button\n",
    "- Provide a name and description for the project\n",
    "- Select a model use case\n",
    "- For modeling objective, we only support automated documentation of `Binary Clasification` models at the moment\n",
    "\n",
    "After creating the project you will be provided with client library setup instructions. We have provided similar instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4ce25e6e546c4637a95d39867964c175",
    "deepnote_cell_height": 271.78125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Initializing the client library\n",
    "\n",
    "Every validation project in the ValidMind dashboard has an associated `project identifier`. In order to initialize the client, we need to provide the following arguments:\n",
    "\n",
    "- `project`: project identifier. The project identifier can be found in the dashboard URL when navigating to a project page, e.g. for `/projects/cl1jyvh2c000909lg1rk0a0zb` the project identifier is `cl1jyvh2c000909lg1rk0a0zb`\n",
    "- `api_host`: Location of the ValidMind API. This value is already set on this notebook.\n",
    "- `api_key`: Account API key. This can be found in the settings page in the ValidMind dashboard\n",
    "- `api_secret`: Account Secret key. Also found in the settings page in the ValidMind dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7f0f01a0c7e441dbb23d5a3cfff8a3d6",
    "deepnote_cell_height": 76,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1659233470795,
    "source_hash": "ae3a62c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4b874a48418b4a5dbb7f987c8b83be86",
    "deepnote_cell_height": 76,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1659233478619,
    "source_hash": "d29c47be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "project='cl61pr35n001sv88h5hgws6bq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2c4983ef1e7b48b7b3f6d1d196adba29",
    "deepnote_cell_height": 76,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10499,
    "execution_start": 1659233494333,
    "source_hash": "c57f60f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# api_key = input()\n",
    "api_key = 'e22b89a6b9c2a27da47cb0a09febc001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "10abe9ec25544f76b67a96bfd70bec02",
    "deepnote_cell_height": 76,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9060,
    "execution_start": 1659233512772,
    "source_hash": "a8e3cf89",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# api_secret = input()\n",
    "api_secret = 'a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize the client library with the `vm.init` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bc1361cd166c4e59a6a461bf41420b62",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    # api_host=api_host,\n",
    "    api_key=api_key,\n",
    "    api_secret=api_secret,\n",
    "    project=\"cl649obbs0000aa8hkbbg7fhv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports for training our demo models\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidMind Client Library Functions\n",
    "\n",
    "As of version `0.4.x` of the client library, the following logging and testing functions are available:\n",
    "\n",
    "|Function|Description|\n",
    "|-|-|\n",
    "|`log_metadata`|Logs free-form metadata text for a given content ID in the model documentation|\n",
    "|`log_dataset`|Analyzes a dataset and logs its description, column definitions and summary statistics|\n",
    "|`run_dataset_tests`|Runs dataset quality tests on the input dataset|\n",
    "|`log_model`|Logs information about a model's framework, architecture, target objective and training parameters|\n",
    "|`log_training_metrics`|Extracts and logs training metrics from a pre-trained model|\n",
    "|`run_model_tests`|Runs model evaluation tests according to the model objective, use case and specific validation requirements|\n",
    "\n",
    "In the example model training code in this notebook, we will demonstrate each of the documented client library functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log_metadata`\n",
    "\n",
    "Logs free-form metadata text for a given content ID in the model documentation.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- `content_id`: Content ID of the model documentation. This is a unique identifier generated by the ValidMind dashboard\n",
    "- `text`: Free-form text to be logged. A text template can be specified in combination with `extra_json` (see below)\n",
    "- `extra_json`: (TBD support for this) JSON object containing variables to be substituted in the text template\n",
    "\n",
    "The list of available `content_id`s for documentation will be provided in a future release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log_dataset`\n",
    "\n",
    "Analyzes a dataset and logs its description, column definitions and summary statistics. The following information is extracted from the dataset:\n",
    "\n",
    "- Descriptive statistics for numerical and categorical columns\n",
    "- Histograms and value counts for summarizing distribution of values\n",
    "- Pearson correlation matrix for numerical columns\n",
    "- Corelation plots for top 15 correlated features\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- `dataset`: Input dataset. Only Pandas DataFrames are supported at the moment\n",
    "- `dataset_type`: Type of dataset, e.g. `training`, `test`, `validation`. Value needs to be set to `training` for now\n",
    "- `analyze`: Boolean flag indicating whether to analyze and extract descriptive statistics for the dataset (including computing correlations)\n",
    "- `targets`: `vm.DatasetTargets` describing the label column and its values\n",
    "- `features`: Optional list of properties to specify for some features in the dataset\n",
    "\n",
    "Returns:\n",
    "\n",
    "- `vm.Dataset`: VM dataset object that can be provided to `run_dataset_tests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `run_dataset_tests`\n",
    "\n",
    "Runs the dataset quality tests on the input dataset:\n",
    "\n",
    "- Class imbalance test on target column\n",
    "- Duplicate rows and duplicates based on primary key\n",
    "- High cardinality test on categorical columns\n",
    "- Missing values\n",
    "- Highly correlated column pairs\n",
    "- Skewness test\n",
    "- Zeros test (columns with too many zeros)\n",
    "\n",
    "Arguments: \n",
    "\n",
    "- `df`: Input dataset. Only Pandas DataFrames are supported at the moment\n",
    "- `dataset_type`: Type of dataset, e.g. `training`, `test`, `validation`. Value needs to be set to `training` for now\n",
    "- `vm_dataset`: `vm.Dataset` object returned by `log_dataset`\n",
    "- `send`: Boolean flag indicating whether to send the test results to the ValidMind dashboard. This flag is useful for debugging purposes\n",
    "\n",
    "Returns:\n",
    "\n",
    "- `results`: List of test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log_model`\n",
    "\n",
    "Logs the following information about a model:\n",
    "\n",
    "- Model framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\n",
    "- Model task details (e.g. binary classification, regression, etc.)\n",
    "- Model hyperparameters (e.g. number of trees, max depth, etc.)\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- `model_instance`: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log_training_metrics`\n",
    "\n",
    "Extracts and logs training metrics from a pre-trained model. The extracted metrics are dependent on the configured evaluation metrics setup by the model developer when training the model.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- `model`: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\n",
    "- `x_train`: Training dataset features\n",
    "- `y_train`: Training dataset labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `run_model_tests`\n",
    "\n",
    "Runs model evaluation tests according to the model objective, use case and specific validation requirements. The following tests are available for binary classification models at the moment:\n",
    "\n",
    "- Accuracy score\n",
    "- Precision score\n",
    "- Recall score\n",
    "- F1 score\n",
    "- ROC AUC score\n",
    "- ROC AUC curve\n",
    "- Confusion matrix\n",
    "- Precision Recall curve\n",
    "- Permutation feature importance\n",
    "- SHAP global importance\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- `model`: Trained model instance. Only Scikit-learn interface compatible models are supported at the moment\n",
    "- `x_test`: Test dataset features\n",
    "- `y_test`: Test dataset labels\n",
    "- `send`: Boolean flag indicating whether to send the test results to the ValidMind dashboard. This flag is useful for debugging purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Example Model\n",
    "\n",
    "We'll now train an example model to demonstrate the ValidMind client library functions. The following demo datasets are available to use, and on this notebook we'll train a model for the Bank Customer Churn dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank Customer Churn Dataset\n",
    "churn_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/bank_customer_churn.csv\")\n",
    "\n",
    "# Health Insurance Cross-Sell Dataset\n",
    "insurance_dataset = pd.read_csv(\"https://vmai.s3.us-west-1.amazonaws.com/datasets/health_insurance_cross_sell.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log_dataset`\n",
    "\n",
    "After loading the dataset, we can log metadata and summary statistics for it using `log_dataset`. Note that the `log_dataset` function expects a `targets` definition. Additional information about columns can be provided with the `features` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_targets = vm.DatasetTargets(\n",
    "    target_column=\"Exited\",\n",
    "    class_labels={\n",
    "        \"0\": \"Did not exit\",\n",
    "        \"1\": \"Exited\",\n",
    "    }\n",
    ")\n",
    "\n",
    "churn_features = [\n",
    "    {\n",
    "        \"id\": \"id\",\n",
    "        \"type_options\": {\n",
    "            \"primary_key\": True,\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "churn_vm_dataset = vm.log_dataset(\n",
    "    churn_dataset,\n",
    "    \"training\",\n",
    "    analyze=True,\n",
    "    targets=churn_targets,\n",
    "    features=churn_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `run_dataset_tests`\n",
    "\n",
    "With the returned `vm.Dataset` object by `log_dataset`, we can run the dataset quality tests on it using `run_dataset_tests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_results = vm.run_dataset_tests(\n",
    "    churn_dataset,\n",
    "    dataset_type=\"training\",\n",
    "    vm_dataset=churn_vm_dataset,    \n",
    "    send=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `log_dataset` and `run_dataset_tests`, we can open the ValidMind dashboard on the following section to verify that the dataset and its data quality checks have been documented correctly:\n",
    "\n",
    "`Dashboard -> Project Overview -> Documentation -> Data Description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training dataset\n",
    "\n",
    "We are now going to preprocess and prepare our training, validation and test datasets so we can train an example model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_churn_dataset(df):\n",
    "    # Drop columns with no correlation to target\n",
    "    df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)\n",
    "\n",
    "    # Encode binary features\n",
    "    genders = {\"Male\": 0, \"Female\": 1}\n",
    "    df.replace({\"Gender\": genders}, inplace=True)\n",
    "\n",
    "    # Encode categorical features\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n",
    "    df.drop(\"Geography\", axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_churn = preprocess_churn_dataset(churn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split_dataset(df):\n",
    "    train_df, test_df = train_test_split(df, test_size=0.20)\n",
    "\n",
    "    # This guarantees a 60/20/20 split\n",
    "    train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "    # For training\n",
    "    x_train = train_ds.drop(\"Exited\", axis=1)\n",
    "    y_train = train_ds.loc[:, \"Exited\"].astype(int)\n",
    "    x_val = val_ds.drop(\"Exited\", axis=1)\n",
    "    y_val = val_ds.loc[:, \"Exited\"].astype(int)\n",
    "\n",
    "    # For testing\n",
    "    x_test = test_df.drop(\"Exited\", axis=1)\n",
    "    y_test = test_df.loc[:, \"Exited\"].astype(int)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split_dataset(preprocessed_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_churn_dataset(x_train, y_train, x_val, y_val):\n",
    "    xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "\n",
    "    xgb_model.set_params(\n",
    "        eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    "    )    \n",
    "\n",
    "    xgb_model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = train_churn_dataset(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(model, x, y):\n",
    "    y_pred = model.predict_proba(x)[:, -1]\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(xgb_model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log_model`\n",
    "\n",
    "With our new trained model we can now call `log_model` and log its framework, architecture, target objective and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.log_model(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log_training_metrics`\n",
    "\n",
    "Our example model was trained with the following evaluation metrics: \"error\", \"logloss\", \"auc\". We can use `log_training_metrics` to log these metrics on the ValidMind dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.log_training_metrics(xgb_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `log_model` and `log_training_metrics`, we can open the ValidMind dashboard on the following section to verify that the model training summary has been documented correctly:\n",
    "\n",
    "`Dashboard -> Project Overview -> Documentation -> Model Development -> Model Training`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `run_model_tests`\n",
    "\n",
    "Finally, after training our model, we can run the model evaluation tests on it using `run_model_tests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = vm.run_model_tests(xgb_model, x_test, y_test, send=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `run_model_tests`, we can open the ValidMind dashboard on the following sections to verify that the model evaluation test results have been logged correctly:\n",
    "\n",
    "- `Dashboard -> Project Overview -> Documentation -> Model Development -> Model Evaluation`\n",
    "- `Dashboard -> Project Overview -> Documentation -> Model Development -> Model Explainability and Interpretability`"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "71b70118-96c8-463b-94e5-733fca330035",
  "kernelspec": {
   "display_name": "Python 3.8.6 ('validmind-sdk-Jp3s24zK-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e673a474f5e5c69884c79b5bbf26bd3f43f29ea8e4fba12a1f063946fe17172c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
