{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Tests in ValidMind:\n",
    "## A Comprehensive Guide to List and Describe Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this comprehensive guide through the ValidMind Developer Framework! In this notebook, we'll dive deep into the utilities available for managing and understanding the various tests that can be run against your models and datasets. Whether you're just getting started or looking for advanced tips, you'll find clear examples and explanations to assist you every step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72223379",
   "metadata": {},
   "source": [
    "Before we delve into the details, let's set up our environment by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind.tests as vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing All Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84757906",
   "metadata": {},
   "source": [
    "The `list_tests` function provides a convenient way to retrieve all available tests in the `validmind.tests` module. When invoked without any parameters, it returns a pandas DataFrame containing detailed information about each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1ac33 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1ac33_row0_col0, #T_1ac33_row0_col1, #T_1ac33_row0_col2, #T_1ac33_row0_col3, #T_1ac33_row1_col0, #T_1ac33_row1_col1, #T_1ac33_row1_col2, #T_1ac33_row1_col3, #T_1ac33_row2_col0, #T_1ac33_row2_col1, #T_1ac33_row2_col2, #T_1ac33_row2_col3, #T_1ac33_row3_col0, #T_1ac33_row3_col1, #T_1ac33_row3_col2, #T_1ac33_row3_col3, #T_1ac33_row4_col0, #T_1ac33_row4_col1, #T_1ac33_row4_col2, #T_1ac33_row4_col3, #T_1ac33_row5_col0, #T_1ac33_row5_col1, #T_1ac33_row5_col2, #T_1ac33_row5_col3, #T_1ac33_row6_col0, #T_1ac33_row6_col1, #T_1ac33_row6_col2, #T_1ac33_row6_col3, #T_1ac33_row7_col0, #T_1ac33_row7_col1, #T_1ac33_row7_col2, #T_1ac33_row7_col3, #T_1ac33_row8_col0, #T_1ac33_row8_col1, #T_1ac33_row8_col2, #T_1ac33_row8_col3, #T_1ac33_row9_col0, #T_1ac33_row9_col1, #T_1ac33_row9_col2, #T_1ac33_row9_col3, #T_1ac33_row10_col0, #T_1ac33_row10_col1, #T_1ac33_row10_col2, #T_1ac33_row10_col3, #T_1ac33_row11_col0, #T_1ac33_row11_col1, #T_1ac33_row11_col2, #T_1ac33_row11_col3, #T_1ac33_row12_col0, #T_1ac33_row12_col1, #T_1ac33_row12_col2, #T_1ac33_row12_col3, #T_1ac33_row13_col0, #T_1ac33_row13_col1, #T_1ac33_row13_col2, #T_1ac33_row13_col3, #T_1ac33_row14_col0, #T_1ac33_row14_col1, #T_1ac33_row14_col2, #T_1ac33_row14_col3, #T_1ac33_row15_col0, #T_1ac33_row15_col1, #T_1ac33_row15_col2, #T_1ac33_row15_col3, #T_1ac33_row16_col0, #T_1ac33_row16_col1, #T_1ac33_row16_col2, #T_1ac33_row16_col3, #T_1ac33_row17_col0, #T_1ac33_row17_col1, #T_1ac33_row17_col2, #T_1ac33_row17_col3, #T_1ac33_row18_col0, #T_1ac33_row18_col1, #T_1ac33_row18_col2, #T_1ac33_row18_col3, #T_1ac33_row19_col0, #T_1ac33_row19_col1, #T_1ac33_row19_col2, #T_1ac33_row19_col3, #T_1ac33_row20_col0, #T_1ac33_row20_col1, #T_1ac33_row20_col2, #T_1ac33_row20_col3, #T_1ac33_row21_col0, #T_1ac33_row21_col1, #T_1ac33_row21_col2, #T_1ac33_row21_col3, #T_1ac33_row22_col0, #T_1ac33_row22_col1, #T_1ac33_row22_col2, #T_1ac33_row22_col3, #T_1ac33_row23_col0, #T_1ac33_row23_col1, #T_1ac33_row23_col2, #T_1ac33_row23_col3, #T_1ac33_row24_col0, #T_1ac33_row24_col1, #T_1ac33_row24_col2, #T_1ac33_row24_col3, #T_1ac33_row25_col0, #T_1ac33_row25_col1, #T_1ac33_row25_col2, #T_1ac33_row25_col3, #T_1ac33_row26_col0, #T_1ac33_row26_col1, #T_1ac33_row26_col2, #T_1ac33_row26_col3, #T_1ac33_row27_col0, #T_1ac33_row27_col1, #T_1ac33_row27_col2, #T_1ac33_row27_col3, #T_1ac33_row28_col0, #T_1ac33_row28_col1, #T_1ac33_row28_col2, #T_1ac33_row28_col3, #T_1ac33_row29_col0, #T_1ac33_row29_col1, #T_1ac33_row29_col2, #T_1ac33_row29_col3, #T_1ac33_row30_col0, #T_1ac33_row30_col1, #T_1ac33_row30_col2, #T_1ac33_row30_col3, #T_1ac33_row31_col0, #T_1ac33_row31_col1, #T_1ac33_row31_col2, #T_1ac33_row31_col3, #T_1ac33_row32_col0, #T_1ac33_row32_col1, #T_1ac33_row32_col2, #T_1ac33_row32_col3, #T_1ac33_row33_col0, #T_1ac33_row33_col1, #T_1ac33_row33_col2, #T_1ac33_row33_col3, #T_1ac33_row34_col0, #T_1ac33_row34_col1, #T_1ac33_row34_col2, #T_1ac33_row34_col3, #T_1ac33_row35_col0, #T_1ac33_row35_col1, #T_1ac33_row35_col2, #T_1ac33_row35_col3, #T_1ac33_row36_col0, #T_1ac33_row36_col1, #T_1ac33_row36_col2, #T_1ac33_row36_col3, #T_1ac33_row37_col0, #T_1ac33_row37_col1, #T_1ac33_row37_col2, #T_1ac33_row37_col3, #T_1ac33_row38_col0, #T_1ac33_row38_col1, #T_1ac33_row38_col2, #T_1ac33_row38_col3, #T_1ac33_row39_col0, #T_1ac33_row39_col1, #T_1ac33_row39_col2, #T_1ac33_row39_col3, #T_1ac33_row40_col0, #T_1ac33_row40_col1, #T_1ac33_row40_col2, #T_1ac33_row40_col3, #T_1ac33_row41_col0, #T_1ac33_row41_col1, #T_1ac33_row41_col2, #T_1ac33_row41_col3, #T_1ac33_row42_col0, #T_1ac33_row42_col1, #T_1ac33_row42_col2, #T_1ac33_row42_col3, #T_1ac33_row43_col0, #T_1ac33_row43_col1, #T_1ac33_row43_col2, #T_1ac33_row43_col3, #T_1ac33_row44_col0, #T_1ac33_row44_col1, #T_1ac33_row44_col2, #T_1ac33_row44_col3, #T_1ac33_row45_col0, #T_1ac33_row45_col1, #T_1ac33_row45_col2, #T_1ac33_row45_col3, #T_1ac33_row46_col0, #T_1ac33_row46_col1, #T_1ac33_row46_col2, #T_1ac33_row46_col3, #T_1ac33_row47_col0, #T_1ac33_row47_col1, #T_1ac33_row47_col2, #T_1ac33_row47_col3, #T_1ac33_row48_col0, #T_1ac33_row48_col1, #T_1ac33_row48_col2, #T_1ac33_row48_col3, #T_1ac33_row49_col0, #T_1ac33_row49_col1, #T_1ac33_row49_col2, #T_1ac33_row49_col3, #T_1ac33_row50_col0, #T_1ac33_row50_col1, #T_1ac33_row50_col2, #T_1ac33_row50_col3, #T_1ac33_row51_col0, #T_1ac33_row51_col1, #T_1ac33_row51_col2, #T_1ac33_row51_col3, #T_1ac33_row52_col0, #T_1ac33_row52_col1, #T_1ac33_row52_col2, #T_1ac33_row52_col3, #T_1ac33_row53_col0, #T_1ac33_row53_col1, #T_1ac33_row53_col2, #T_1ac33_row53_col3, #T_1ac33_row54_col0, #T_1ac33_row54_col1, #T_1ac33_row54_col2, #T_1ac33_row54_col3, #T_1ac33_row55_col0, #T_1ac33_row55_col1, #T_1ac33_row55_col2, #T_1ac33_row55_col3, #T_1ac33_row56_col0, #T_1ac33_row56_col1, #T_1ac33_row56_col2, #T_1ac33_row56_col3, #T_1ac33_row57_col0, #T_1ac33_row57_col1, #T_1ac33_row57_col2, #T_1ac33_row57_col3, #T_1ac33_row58_col0, #T_1ac33_row58_col1, #T_1ac33_row58_col2, #T_1ac33_row58_col3, #T_1ac33_row59_col0, #T_1ac33_row59_col1, #T_1ac33_row59_col2, #T_1ac33_row59_col3, #T_1ac33_row60_col0, #T_1ac33_row60_col1, #T_1ac33_row60_col2, #T_1ac33_row60_col3, #T_1ac33_row61_col0, #T_1ac33_row61_col1, #T_1ac33_row61_col2, #T_1ac33_row61_col3, #T_1ac33_row62_col0, #T_1ac33_row62_col1, #T_1ac33_row62_col2, #T_1ac33_row62_col3, #T_1ac33_row63_col0, #T_1ac33_row63_col1, #T_1ac33_row63_col2, #T_1ac33_row63_col3, #T_1ac33_row64_col0, #T_1ac33_row64_col1, #T_1ac33_row64_col2, #T_1ac33_row64_col3, #T_1ac33_row65_col0, #T_1ac33_row65_col1, #T_1ac33_row65_col2, #T_1ac33_row65_col3, #T_1ac33_row66_col0, #T_1ac33_row66_col1, #T_1ac33_row66_col2, #T_1ac33_row66_col3, #T_1ac33_row67_col0, #T_1ac33_row67_col1, #T_1ac33_row67_col2, #T_1ac33_row67_col3, #T_1ac33_row68_col0, #T_1ac33_row68_col1, #T_1ac33_row68_col2, #T_1ac33_row68_col3, #T_1ac33_row69_col0, #T_1ac33_row69_col1, #T_1ac33_row69_col2, #T_1ac33_row69_col3, #T_1ac33_row70_col0, #T_1ac33_row70_col1, #T_1ac33_row70_col2, #T_1ac33_row70_col3, #T_1ac33_row71_col0, #T_1ac33_row71_col1, #T_1ac33_row71_col2, #T_1ac33_row71_col3, #T_1ac33_row72_col0, #T_1ac33_row72_col1, #T_1ac33_row72_col2, #T_1ac33_row72_col3, #T_1ac33_row73_col0, #T_1ac33_row73_col1, #T_1ac33_row73_col2, #T_1ac33_row73_col3, #T_1ac33_row74_col0, #T_1ac33_row74_col1, #T_1ac33_row74_col2, #T_1ac33_row74_col3, #T_1ac33_row75_col0, #T_1ac33_row75_col1, #T_1ac33_row75_col2, #T_1ac33_row75_col3, #T_1ac33_row76_col0, #T_1ac33_row76_col1, #T_1ac33_row76_col2, #T_1ac33_row76_col3, #T_1ac33_row77_col0, #T_1ac33_row77_col1, #T_1ac33_row77_col2, #T_1ac33_row77_col3, #T_1ac33_row78_col0, #T_1ac33_row78_col1, #T_1ac33_row78_col2, #T_1ac33_row78_col3, #T_1ac33_row79_col0, #T_1ac33_row79_col1, #T_1ac33_row79_col2, #T_1ac33_row79_col3, #T_1ac33_row80_col0, #T_1ac33_row80_col1, #T_1ac33_row80_col2, #T_1ac33_row80_col3, #T_1ac33_row81_col0, #T_1ac33_row81_col1, #T_1ac33_row81_col2, #T_1ac33_row81_col3, #T_1ac33_row82_col0, #T_1ac33_row82_col1, #T_1ac33_row82_col2, #T_1ac33_row82_col3, #T_1ac33_row83_col0, #T_1ac33_row83_col1, #T_1ac33_row83_col2, #T_1ac33_row83_col3, #T_1ac33_row84_col0, #T_1ac33_row84_col1, #T_1ac33_row84_col2, #T_1ac33_row84_col3, #T_1ac33_row85_col0, #T_1ac33_row85_col1, #T_1ac33_row85_col2, #T_1ac33_row85_col3, #T_1ac33_row86_col0, #T_1ac33_row86_col1, #T_1ac33_row86_col2, #T_1ac33_row86_col3, #T_1ac33_row87_col0, #T_1ac33_row87_col1, #T_1ac33_row87_col2, #T_1ac33_row87_col3, #T_1ac33_row88_col0, #T_1ac33_row88_col1, #T_1ac33_row88_col2, #T_1ac33_row88_col3, #T_1ac33_row89_col0, #T_1ac33_row89_col1, #T_1ac33_row89_col2, #T_1ac33_row89_col3, #T_1ac33_row90_col0, #T_1ac33_row90_col1, #T_1ac33_row90_col2, #T_1ac33_row90_col3, #T_1ac33_row91_col0, #T_1ac33_row91_col1, #T_1ac33_row91_col2, #T_1ac33_row91_col3, #T_1ac33_row92_col0, #T_1ac33_row92_col1, #T_1ac33_row92_col2, #T_1ac33_row92_col3, #T_1ac33_row93_col0, #T_1ac33_row93_col1, #T_1ac33_row93_col2, #T_1ac33_row93_col3, #T_1ac33_row94_col0, #T_1ac33_row94_col1, #T_1ac33_row94_col2, #T_1ac33_row94_col3, #T_1ac33_row95_col0, #T_1ac33_row95_col1, #T_1ac33_row95_col2, #T_1ac33_row95_col3, #T_1ac33_row96_col0, #T_1ac33_row96_col1, #T_1ac33_row96_col2, #T_1ac33_row96_col3, #T_1ac33_row97_col0, #T_1ac33_row97_col1, #T_1ac33_row97_col2, #T_1ac33_row97_col3, #T_1ac33_row98_col0, #T_1ac33_row98_col1, #T_1ac33_row98_col2, #T_1ac33_row98_col3, #T_1ac33_row99_col0, #T_1ac33_row99_col1, #T_1ac33_row99_col2, #T_1ac33_row99_col3, #T_1ac33_row100_col0, #T_1ac33_row100_col1, #T_1ac33_row100_col2, #T_1ac33_row100_col3, #T_1ac33_row101_col0, #T_1ac33_row101_col1, #T_1ac33_row101_col2, #T_1ac33_row101_col3, #T_1ac33_row102_col0, #T_1ac33_row102_col1, #T_1ac33_row102_col2, #T_1ac33_row102_col3, #T_1ac33_row103_col0, #T_1ac33_row103_col1, #T_1ac33_row103_col2, #T_1ac33_row103_col3, #T_1ac33_row104_col0, #T_1ac33_row104_col1, #T_1ac33_row104_col2, #T_1ac33_row104_col3, #T_1ac33_row105_col0, #T_1ac33_row105_col1, #T_1ac33_row105_col2, #T_1ac33_row105_col3, #T_1ac33_row106_col0, #T_1ac33_row106_col1, #T_1ac33_row106_col2, #T_1ac33_row106_col3, #T_1ac33_row107_col0, #T_1ac33_row107_col1, #T_1ac33_row107_col2, #T_1ac33_row107_col3, #T_1ac33_row108_col0, #T_1ac33_row108_col1, #T_1ac33_row108_col2, #T_1ac33_row108_col3, #T_1ac33_row109_col0, #T_1ac33_row109_col1, #T_1ac33_row109_col2, #T_1ac33_row109_col3, #T_1ac33_row110_col0, #T_1ac33_row110_col1, #T_1ac33_row110_col2, #T_1ac33_row110_col3, #T_1ac33_row111_col0, #T_1ac33_row111_col1, #T_1ac33_row111_col2, #T_1ac33_row111_col3, #T_1ac33_row112_col0, #T_1ac33_row112_col1, #T_1ac33_row112_col2, #T_1ac33_row112_col3, #T_1ac33_row113_col0, #T_1ac33_row113_col1, #T_1ac33_row113_col2, #T_1ac33_row113_col3, #T_1ac33_row114_col0, #T_1ac33_row114_col1, #T_1ac33_row114_col2, #T_1ac33_row114_col3, #T_1ac33_row115_col0, #T_1ac33_row115_col1, #T_1ac33_row115_col2, #T_1ac33_row115_col3, #T_1ac33_row116_col0, #T_1ac33_row116_col1, #T_1ac33_row116_col2, #T_1ac33_row116_col3, #T_1ac33_row117_col0, #T_1ac33_row117_col1, #T_1ac33_row117_col2, #T_1ac33_row117_col3, #T_1ac33_row118_col0, #T_1ac33_row118_col1, #T_1ac33_row118_col2, #T_1ac33_row118_col3, #T_1ac33_row119_col0, #T_1ac33_row119_col1, #T_1ac33_row119_col2, #T_1ac33_row119_col3, #T_1ac33_row120_col0, #T_1ac33_row120_col1, #T_1ac33_row120_col2, #T_1ac33_row120_col3, #T_1ac33_row121_col0, #T_1ac33_row121_col1, #T_1ac33_row121_col2, #T_1ac33_row121_col3, #T_1ac33_row122_col0, #T_1ac33_row122_col1, #T_1ac33_row122_col2, #T_1ac33_row122_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1ac33\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1ac33_level0_col0\" class=\"col_heading level0 col0\" >Test Type</th>\n",
       "      <th id=\"T_1ac33_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_1ac33_level0_col2\" class=\"col_heading level0 col2\" >Description</th>\n",
       "      <th id=\"T_1ac33_level0_col3\" class=\"col_heading level0 col3\" >ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row0_col0\" class=\"data row0 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row0_col1\" class=\"data row0 col1\" >Bias</td>\n",
       "      <td id=\"T_1ac33_row0_col2\" class=\"data row0 col2\" >**Purpose:** The Bias Evaluation test calculates if and how the order and distribution of exemplars (examples) in a few-shot learning prompt affect the output of a Language Learning Model (LLM). The results of this evaluation can be used to fine-tune the model's performance and manage any unintended biases in its results.\n",
       "\n",
       "**Test Mechanism:** This test uses two checks:\n",
       "\n",
       "1. *Distribution of Exemplars:* The number of positive vs. negative examples in a prompt is varied. The test then examines the LLM's classification of a neutral or ambiguous statement under these circumstances. 2. *Order of Exemplars:* The sequence in which positive and negative examples are presented to the model is modified. Their resultant effect on the LLM's response is studied.\n",
       "\n",
       "For each test case, the LLM grades the input prompt on a scale of 1 to 10. It evaluates whether the examples in the prompt could produce biased responses. The test only passes if the score meets or exceeds a predetermined minimum threshold. This threshold is set at 7 by default, but it can be modified as per the requirements via the test parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "\n",
       "- A skewed result favoring either positive or negative responses may suggest potential bias in the model. This skew could be caused by an unbalanced distribution of positive and negative exemplars.\n",
       "- If the score given by the model is less than the set minimum threshold, it might indicate a risk of high bias and hence poor performance.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "- This test provides a quantitative measure of potential bias, providing clear guidelines for developers about whether their Language Learning Model (LLM) contains significant bias.\n",
       "- It's useful in evaluating the impartiality of the model based on the distribution and sequence of examples.\n",
       "- The flexibility to adjust the minimum required threshold allows tailoring this test to stricter or more lenient bias standards.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "- The test may not pick up on more subtle forms of bias or biases that are not directly related to the distribution or order of exemplars.\n",
       "- The test's effectiveness will decrease if the quality or balance of positive and negative exemplars is not representative of the problem space the model is intended to solve.\n",
       "- The use of a grading mechanism to gauge bias may not be entirely accurate in every case, particularly when the difference between threshold and score is narrow.</td>\n",
       "      <td id=\"T_1ac33_row0_col3\" class=\"data row0 col3\" >validmind.prompt_validation.Bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row1_col0\" class=\"data row1 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row1_col1\" class=\"data row1 col1\" >Clarity</td>\n",
       "      <td id=\"T_1ac33_row1_col2\" class=\"data row1 col2\" >**Purpose:** The Clarity evaluation metric is used to assess how clear the prompts of a Language Learning Model (LLM) are. This assessment is particularly important because clear prompts assist the LLM in more accurately interpreting and responding to instructions.\n",
       "\n",
       "**Test Mechanism:** The evaluation uses an LLM to scrutinize the clarity of prompts, factoring in considerations such as the inclusion of relevant details, persona adoption, step-by-step instructions, usage of examples and specification of desired output length. Each prompt is rated on a clarity scale of 1 to 10, and any prompt scoring at or above the preset threshold (default of 7) will be marked as clear. It is important to note that this threshold can be adjusted via test parameters, providing flexibility in the evaluation process.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "\n",
       "- Prompts that consistently score below the clarity threshold\n",
       "- Repeated failure of prompts to adhere to guidelines for clarity. These guidelines could include detail inclusion, persona adoption, explicit step-by-step instructions, use of examples, and specification of output length.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "- Encourages the development of more effective prompts that aid the LLM in interpreting instructions accurately.\n",
       "- Applies a quantifiable measure (a score from 1 to 10) to evaluate the clarity of prompts.\n",
       "- Threshold for clarity is adjustable, allowing for flexible evaluation depending on the context.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "- Scoring system is subjective and relies on the AI’s interpretation of 'clarity'.\n",
       "- The test assumes that all required factors (detail inclusion, persona adoption, step-by-step instructions, use of examples, and specification of output length) contribute equally to clarity, which might not always be the case.\n",
       "- The evaluation may not be as effective if used on non-textual models.</td>\n",
       "      <td id=\"T_1ac33_row1_col3\" class=\"data row1 col3\" >validmind.prompt_validation.Clarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row2_col0\" class=\"data row2 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row2_col1\" class=\"data row2 col1\" >Specificity</td>\n",
       "      <td id=\"T_1ac33_row2_col2\" class=\"data row2 col2\" >**Purpose:** The Specificity Test evaluates the clarity, precision, and effectiveness of the prompts provided to a Language Learning Model (LLM). It aims to ensure that the instructions embedded in a prompt are indisputably clear and relevant, thereby helping to yank out ambiguity and steer the LLM towards desired outputs. This level of specificity significantly affects the accuracy and relevance of LLM outputs.\n",
       "\n",
       "**Test Mechanism:** The Specificity Test employs an LLM to grade each prompt based on clarity, detail, and relevance parameters within a specificity scale that extends from 1 to 10. On this scale, prompts scoring equal to or more than a predefined threshold (set to 7 by default) pass the evaluation, while those scoring below this threshold fail it. Users can adjust this threshold as per their requirements.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Prompts scoring consistently below the established threshold\n",
       "- Vague or ambiguous prompts that do not provide clear direction to the LLM\n",
       "- Overly verbose prompts that may confuse the LLM instead of providing clear guidance\n",
       "\n",
       "**Strengths:**\n",
       "- Enables precise and clear communication with the LLM to achieve desired outputs\n",
       "- Serves as a crucial means to measure the effectiveness of prompts\n",
       "- Highly customizable, allowing users to set their threshold based on specific use cases\n",
       "\n",
       "**Limitations:**\n",
       "- This test doesn't consider the content comprehension capability of the LLM\n",
       "- High specificity score doesn't guarantee a high-quality response from the LLM, as the model's performance is also dependent on various other factors\n",
       "- Striking a balance between specificity and verbosity can be challenging, as overly detailed prompts might confuse or mislead the model.</td>\n",
       "      <td id=\"T_1ac33_row2_col3\" class=\"data row2 col3\" >validmind.prompt_validation.Specificity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row3_col0\" class=\"data row3 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row3_col1\" class=\"data row3 col1\" >Robustness</td>\n",
       "      <td id=\"T_1ac33_row3_col2\" class=\"data row3 col2\" >**Purpose:** The Robustness test is meant to evaluate the resilience and reliability of prompts provided to a Language Learning Model (LLM). The aim of this test is to guarantee that the prompts consistently generate accurate and the expected outputs, despite being in diverse or challenging scenarios.\n",
       "\n",
       "**Test Mechanism:** The Robustness test appraises prompts under various conditions, alterations, and contexts to ascertain their stability in producing consistent responses from the LLM. Factors evaluated range from different phrasings, inclusion of potential distracting elements, and various input complexities. By default, the test generates 10 inputs for a prompt but can be adjusted according to test parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- If the output from the tests diverges extensively from the expected results, this indicates high risk.\n",
       "- When the prompt doesn't give a consistent performance across various tests.\n",
       "- A high risk is indicated when the prompt is susceptible to breaking, especially when the output is expected to be of a specific type.\n",
       "\n",
       "**Strengths:**\n",
       "- The robustness test helps to ensure stable performance of the LLM prompts and lowers the chances of generating unexpected or off-target outputs.\n",
       "- This test is vital for applications where predictability and reliability of the LLM’s output are crucial.\n",
       "\n",
       "**Limitations:**\n",
       "- Currently, the test only supports single-variable prompts, which restricts its application to more complex models.\n",
       "- When there are too many target classes (over 10), the test is skipped, which can leave potential vulnerabilities unchecked in complex multi-class models.\n",
       "- The test may not account for all potential conditions or alterations that could show up in practical use scenarios.</td>\n",
       "      <td id=\"T_1ac33_row3_col3\" class=\"data row3 col3\" >validmind.prompt_validation.Robustness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row4_col0\" class=\"data row4 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row4_col1\" class=\"data row4 col1\" >Negative Instruction</td>\n",
       "      <td id=\"T_1ac33_row4_col2\" class=\"data row4 col2\" >**Purpose:** The Negative Instruction test is utilized to scrutinize the prompts given to a Language Learning Model (LLM). The objective is to ensure these prompts are expressed using proactive, affirmative language. The focus is on instructions indicating what needs to be done rather than what needs to be avoided, thereby guiding the LLM more efficiently towards the desired output.\n",
       "\n",
       "**Test Mechanism:** An LLM is employed to evaluate each prompt. The prompt is graded based on its use of positive instructions with scores ranging between 1-10. This grade reflects how effectively the prompt leverages affirmative language while shying away from negative or restrictive instructions. A prompt that attains a grade equal to or above a predetermined threshold (7 by default) is regarded as adhering effectively to the best practices of positive instruction. This threshold can be custom-tailored through the test parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Low score obtained from the LLM analysis, indicating heavy reliance on negative instructions in the prompts.\n",
       "- Failure to surpass the preset minimum threshold.\n",
       "- The LLM generates ambiguous or undesirable outputs as a consequence of the negative instructions used in the prompt.\n",
       "\n",
       "**Strengths:**\n",
       "- Encourages the usage of affirmative, proactive language in prompts, aiding in more accurate and advantageous model responses.\n",
       "- The test result provides a comprehensible score, helping to understand how well a prompt follows the positive instruction best practices.\n",
       "\n",
       "**Limitations:**\n",
       "- Despite an adequate score, a prompt could still be misleading or could lead to undesired responses due to factors not covered by this test.\n",
       "- The test necessitates an LLM for evaluation, which might not be available or feasible in certain scenarios.\n",
       "- A numeric scoring system, while straightforward, may oversimplify complex issues related to prompt designing and instruction clarity.\n",
       "- The effectiveness of the test hinges significantly on the predetermined threshold level, which can be subjective and may need to be adjusted according to specific use-cases.</td>\n",
       "      <td id=\"T_1ac33_row4_col3\" class=\"data row4 col3\" >validmind.prompt_validation.NegativeInstruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row5_col0\" class=\"data row5 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row5_col1\" class=\"data row5 col1\" >Conciseness</td>\n",
       "      <td id=\"T_1ac33_row5_col2\" class=\"data row5 col2\" >**Purpose:** The Conciseness Assessment is designed to evaluate the brevity and succinctness of prompts provided to a Language Learning Model (LLM). A concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed.\n",
       "\n",
       "**Test Mechanism:** Using an LLM, this test conducts a conciseness analysis on input prompts. The analysis grades the prompt on a scale from 1 to 10, where the grade reflects how well the prompt delivers clear instructions without being verbose. Prompts that score equal to or above a predefined threshold (default set to 7) are deemed successfully concise. This threshold can be adjusted to meet specific requirements.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Prompts that consistently score below the predefined threshold.\n",
       "- Prompts that are overly wordy or contain unnecessary information.\n",
       "- Prompts that create confusion or ambiguity due to excess or unnecessary information.\n",
       "\n",
       "**Strengths:**\n",
       "- Ensures clarity and effectiveness of the prompts.\n",
       "- Promotes brevity and preciseness in prompts without sacrificing essential information.\n",
       "- Useful for models like LLMs, where input prompt length and clarity greatly influence model performance.\n",
       "- Provides a quantifiable measure of prompt conciseness.\n",
       "\n",
       "**Limitations:**\n",
       "- The conciseness score is based on an AI's assessment, which might not fully capture human interpretation of conciseness.\n",
       "- The predefined threshold for conciseness could be subjective and might need adjustment based on application.\n",
       "- The test is dependent on the LLM’s understanding of conciseness, which might vary from model to model.</td>\n",
       "      <td id=\"T_1ac33_row5_col3\" class=\"data row5 col3\" >validmind.prompt_validation.Conciseness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row6_col0\" class=\"data row6 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row6_col1\" class=\"data row6 col1\" >Delimitation</td>\n",
       "      <td id=\"T_1ac33_row6_col2\" class=\"data row6 col2\" >**Purpose:** This test, dubbed the \"Delimitation Test\", is engineered to assess whether prompts provided to the Language Learning Model (LLM) correctly use delimiters to mark different sections of the input. Well-delimited prompts simplify the interpretation process for LLM, ensuring responses are precise and accurate.\n",
       "\n",
       "**Test Mechanism:** The test employs an LLM to examine prompts for appropriate use of delimiters such as triple quotation marks, XML tags, and section titles. Each prompt is assigned a score from 1 to 10 based on its delimitation integrity. Those with scores equal to or above the preset threshold (which is 7 by default, although it can be adjusted as necessary) pass the test.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- The test identifies prompts where a delimiter is missing, improperly placed, or incorrect, which can lead to misinterpretation by the LLM.\n",
       "- A high-risk scenario may involve complex prompts with multiple tasks or diverse data where correct delimitation is integral to understanding.\n",
       "- Low scores (below the threshold) are a clear indicator of high risk.\n",
       "\n",
       "**Strengths:**\n",
       "- This test ensures clarity in the demarcation of different components of given prompts.\n",
       "- It helps reduce ambiguity in understanding prompts, particularly for complex tasks.\n",
       "- Scoring allows for quantified insight into the appropriateness of delimiter usage, aiding continuous improvement.\n",
       "\n",
       "**Limitations:**\n",
       "- The test only checks for the presence and placement of delimiter, not whether the correct delimiter type is used for the specific data or task.\n",
       "- It may not fully reveal the impacts of poor delimitation on LLM's final performance.\n",
       "- Depending on the complexity of the tasks and prompts, the preset score threshold may not be refined enough, requiring regular manual adjustment.</td>\n",
       "      <td id=\"T_1ac33_row6_col3\" class=\"data row6 col3\" >validmind.prompt_validation.Delimitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row7_col0\" class=\"data row7 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row7_col1\" class=\"data row7 col1\" >Bert Score</td>\n",
       "      <td id=\"T_1ac33_row7_col2\" class=\"data row7 col2\" >**Purpose**: The BERTScore metric is deployed to evaluate the competence of text generation models by focusing on the similarity between the reference and the generated text. It employs the contextual embeddings from BERT models to assess the similarity of the contents. This measures the extent to which a model has learned and can generate contextually relevant results.\n",
       "\n",
       "**Test Mechanism**: The true values derived from the model's test dataset and the model's predictions are employed in this metric. BERTScore calculates the precision, recall, and F1 score of the model considering the contextual similarity between the reference and the produced text. These scores are computed for each token in the predicted sentences as compared to the reference sentences, while considering the cosine similarity with BERT embeddings. A line plot depicting the score changes across row indexes is generated for each metric i.e., Precision, Recall, and F1 Score.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Observable downward trend in Precision, Recall, or F1 Score.\n",
       "- Noticeable instability or fluctuation in these metrics. Lower Precision implies that predictions often incorporate irrelevant contexts.\n",
       "- Declining Recall suggests that the model frequently omits relevant contexts during predictions.\n",
       "- Lower F1 score signals poor overall performance in both precision and recall.\n",
       "\n",
       "**Strengths**:\n",
       "- BERTScore efficiently detects the quality of text that requires to comprehend the context, a common requirement in natural language processing tasks.\n",
       "- This metric advances beyond the simple n-gram matching and considers the semantic similarity in the context, thereby providing more meaningful evaluation results.\n",
       "- The integrated visualization function allows tracking of the performance trends across different prediction sets.\n",
       "\n",
       "**Limitations**:\n",
       "- Dependence on BERT model embeddings for BERTScore implies that if the base BERT model is not suitable for a specific task, it might impair the accuracy of BERTScore.\n",
       "- Despite being good at understanding semantics, it might be incapable of capturing certain nuances in text similarity that other metrics like BLEU or ROUGE could detect.\n",
       "- Can be computationally expensive due to the utilization of BERT embeddings.</td>\n",
       "      <td id=\"T_1ac33_row7_col3\" class=\"data row7 col3\" >validmind.model_validation.BertScore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row8_col0\" class=\"data row8 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row8_col1\" class=\"data row8 col1\" >Bleu Score</td>\n",
       "      <td id=\"T_1ac33_row8_col2\" class=\"data row8 col2\" >**Purpose**: The Bilingual Evaluation Understudy (BLEU) metric measures the quality of machine-translated text by comparing it to human-translated text. This comparison is done at the sentence level and is designed to bring machine translations closer to the quality of a professional human translation. It is commonly used in the field of translation evaluation, and its purpose is to assess the accuracy of a model's output against that of a benchmark.\n",
       "\n",
       "**Test Mechanism**: The BLEU score is implemented using the NLTK's word_tokenize function to split the text into individual words. After tokenization, the evaluate library's BLEU metric calculates the BLEU score for each translated sentence by comparing the model's translations (predictions) against the actual, correct translations (references). The test algorithm then combines these individual scores into a single score that represents the average 'distance' between the generated translations and the human translations across the entire test set.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low BLEU scores suggest high model risk. This could indicate significant discrepancies between the machine translation and its human equivalent.\n",
       "- This could be due to ineffective model learning, overfitting of training data, or inadequate handling of the language's nuances.\n",
       "- Machine biases toward a certain language style or translation mode can result in lower scores.\n",
       "\n",
       "**Strengths**:\n",
       "- The BLEU score's primary strength lies in its simplicity and interpretability. It offers a straightforward way to assess translated text quality, and its calculations often align with human judgments.\n",
       "- The BLEU score breaks down its evaluations at the sentence level, offering granular insights into any errors.\n",
       "- The score consolidates the model’s performance into a single, comprehensive score, making it easy to compare and monitor.\n",
       "\n",
       "**Limitations**:\n",
       "- The BLEU score heavily favours exact matches, which can create a bias towards literal translations. Thus, it may fail to fully evaluate more complex or flexible translations that shy away from a word-for-word structure.\n",
       "- The score does not directly measure the intelligibility or grammatical correctness of the translations.\n",
       "- It may miss errors originating from subtle nuances in language, cultural contexts, or ambiguities.</td>\n",
       "      <td id=\"T_1ac33_row8_col3\" class=\"data row8 col3\" >validmind.model_validation.BleuScore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row9_col0\" class=\"data row9 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row9_col1\" class=\"data row9 col1\" >Contextual Recall</td>\n",
       "      <td id=\"T_1ac33_row9_col2\" class=\"data row9 col2\" >**Purpose**: The Contextual Recall metric is used to evaluate the ability of a natural language generation (NLG) model to generate text that appropriately reflects the given context or prompt. It measures the model's capability to remember and reproduce the main context in its resulting output. This metric is critical in natural language processing tasks, as the coherency and contextuality of the generated text are essential.\n",
       "\n",
       "**Test Mechanism**:\n",
       "\n",
       "1. **Preparation of Reference and Candidate Texts**:\n",
       "- **Reference Texts**: Gather the reference text(s) which exemplify the expected or ideal output for a specific context or prompt.\n",
       "- **Candidate Texts**: Generate candidate text(s) from the NLG model under evaluation using the same context. 2. **Tokenization and Preprocessing**:\n",
       "- Tokenize the reference and candidate texts into discernible words or tokens using libraries such as NLTK. 3. **Computation of Contextual Recall**:\n",
       "- Identify the token overlap between the reference and candidate texts.\n",
       "- The Contextual Recall score is computed by dividing the number of overlapping tokens by the total number of tokens in the reference text. Scores are calculated for each test dataset instance, resulting in an array of scores. These scores are then visualized using a line plot to show score variations across different rows.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Low contextual recall scores could indicate that the model is not effectively reflecting the original context in its output, leading to incoherent or contextually misaligned text.\n",
       "- A consistent trend of low recall scores could suggest underperformance of the model.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The Contextual Recall metric provides a quantifiable measure of a model's adherence to the context and factual elements of the generated narrative.\n",
       "- This metric finds particular value in applications requiring deep comprehension of context, such as text continuation or interactive dialogue systems.\n",
       "- The line plot visualization provides a clear and intuitive representation of score fluctuations.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Despite its effectiveness, the Contextual Recall could fail to comprehensively assess the performance of NLG models. Its focus on word overlap could result in high scores for texts that use many common words, even when these texts lack coherence or meaningful context.\n",
       "- This metric does not consider the order of words, which could lead to overestimated scores for scrambled outputs.\n",
       "- Models that effectively use infrequent words might be undervalued, as these words might not overlap as often.</td>\n",
       "      <td id=\"T_1ac33_row9_col3\" class=\"data row9 col3\" >validmind.model_validation.ContextualRecall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row10_col0\" class=\"data row10 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row10_col1\" class=\"data row10 col1\" >Rouge Metrics</td>\n",
       "      <td id=\"T_1ac33_row10_col2\" class=\"data row10 col2\" >**Purpose**: The ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a metric employed to assess the quality of machine-generated text. This evaluation technique is mainly used in natural language processing tasks, such as text summarization, machine translation, and text generation. Its goal is to measure how well the machine-generated text reflects the key information and concepts in the human-crafted reference text.\n",
       "\n",
       "**Test Mechanism**:\n",
       "\n",
       "1. **Comparison Procedure**: The testing mechanism involves comparing machine-generated content with a reference human-constructed text.\n",
       "\n",
       "2. **Integral Metrics**:\n",
       "- **ROUGE-N (N-gram Overlap)**: This evaluates the overlap of n-grams (sequences of n words) between the generated and reference texts. The common n-values are 1 (unigrams), 2 (bigrams), and 3 (trigrams). Each metric calculates precision, recall, and F1-score.\n",
       "\n",
       "- **ROUGE-L (Longest Common Subsequence)**: This identifies the longest shared word sequence in both the machine and reference texts, thus evaluating the capability of the generated text to mirror key phrases.\n",
       "\n",
       "- **ROUGE-S (Skip-bigram)**: This measures the concurrence of skip-bigrams — word pairings that appear within a predefined word window in the text. This metric maintains sensitivity to word order while allowing for sporadic word omissions.\n",
       "\n",
       "3. **Visual Representation**: Precision, recall, and F1-score for all the metrics are visually charted, which makes the results easier to comprehend.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Low scores across the suite of ROUGE metrics\n",
       "- Low precision might indicate redundant information in machine-produced text\n",
       "- Low recall may suggest the omission of important information from the reference text\n",
       "- Low F1 score could indicate an imbalanced performance between precision and recall\n",
       "- Persistent low scores could signal inherent flaws in the model\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Offers a multifaceted perspective on text quality using various evaluation metrics\n",
       "- Adapts to synonyms and rewording, thanks to n-gram-based evaluation\n",
       "- Encourages the retention of key word sequences using the longest common subsequence method\n",
       "- Visual representation of precision, recall, and F1-scores enhances understandability of model performance\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- May fail to fully address the semantic coherence, fluency, or grammatical quality of the generated text\n",
       "- Tends to evaluate isolated phrases or n-grams rather than comprehensive sentences\n",
       "- May prove challenging when reference texts are difficult or impractical to obtain due to its reliance on comparisons with human-made references.</td>\n",
       "      <td id=\"T_1ac33_row10_col3\" class=\"data row10 col3\" >validmind.model_validation.RougeMetrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row11_col0\" class=\"data row11 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row11_col1\" class=\"data row11 col1\" >Model Metadata</td>\n",
       "      <td id=\"T_1ac33_row11_col2\" class=\"data row11 col2\" >**Purpose:** This test is designed to collect and summarize important metadata related to a particular machine learning model. Such metadata includes the model's architecture (modeling technique), the version and type of modeling framework used, and the programming language the model is written in.\n",
       "\n",
       "**Test Mechanism:** The mechanism of this test consists of extracting information from the model instance. It tries to extract the model information such as the modeling technique used, the modeling framework version, and the programming language. It decorates this information into a data frame and returns a summary of the results.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- High risk could be determined by a lack of documentation or inscrutable metadata for the model.\n",
       "- Unidentifiable language, outdated or unsupported versions of modeling frameworks, or undisclosed model architectures reflect risky situations, as they could hinder future reproducibility, support, and debugging of the model.\n",
       "\n",
       "**Strengths:**\n",
       "- The strengths of this test lie in the increased transparency and understanding it brings regarding the model's setup.\n",
       "- Knowing the model's architecture, the specific modeling framework version used, and the language involved, provides multiple benefits: supports better error understanding and debugging, facilitates model reuse, aids compliance of software policies, and assists in planning for model obsolescence due to evolving or discontinuing software and dependencies.\n",
       "\n",
       "**Limitations:**\n",
       "- Notably, this test is largely dependent on the compliance and correctness of information provided by the model or the model developer.\n",
       "- If the model's built-in methods for describing its architecture, framework or language are incorrect or lack necessary information, this test will hold limitations.\n",
       "- Moreover, it is not designed to directly evaluate the performance or accuracy of the model, rather it provides supplementary information which aids in comprehensive analysis.</td>\n",
       "      <td id=\"T_1ac33_row11_col3\" class=\"data row11 col3\" >validmind.model_validation.ModelMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row12_col0\" class=\"data row12 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row12_col1\" class=\"data row12 col1\" >Token Disparity</td>\n",
       "      <td id=\"T_1ac33_row12_col2\" class=\"data row12 col2\" >**Purpose**: The Token Disparity metric is designed to assess the distributional congruence between the model's predicted outputs and the actual data. This is achieved by constructing histograms that illustrate the disparity in token count between the two columns. Additionally, this metric is used to measure the model's verbosity in comparison to the genuine dataset.\n",
       "\n",
       "**Test Mechanism**: The mechanism of running this test involves tokenizing both columns: one containing the actual data and the other containing the model's predictions. The BERT tokenizer is used for tokenizing the contents of each column. After tokenization, tokens in each column are counted and represented in two distinct histograms to facilitate the visualization of token count distribution in the actual and predicted data. To quantify the difference in distribution, the histogram of the actual tokens is compared with the histogram of the predicted tokens.\n",
       "\n",
       "**Signs of High Risk**: High risk or potential failure in model performance may be suggested by:\n",
       "\n",
       "- Significant incongruities in distribution patterns between the two histograms.\n",
       "- Marked divergence of the predicted histogram from the reference histogram, indicating that the model may be generating output with unexpected verbosity.\n",
       "- This might result in an output that has a significantly higher or lower number of tokens than expected.\n",
       "\n",
       "**Strengths**: Strengths of the Token Disparity metric include:\n",
       "\n",
       "- It provides a clear and visual comparison of predicted versus actual token distributions, enhancing understanding of the model's output consistency and verbosity.\n",
       "- It is able to detect potential issues with the model's output generation capability, such as over-production or under-production of tokens compared to the actual data set.\n",
       "\n",
       "**Limitations**: Limitations of the Token Disparity metric include:\n",
       "\n",
       "- The metric focuses solely on token count, disregarding the semantics behind those tokens. Consequently, it may miss out on issues related to relevance or meaningfulness of produced tokens.\n",
       "- The assumption that similar token count between predicted and actual data suggests accurate output, which is not always the case.\n",
       "- Dependence on the BERT tokenizer, which may not always be the optimum choice for all types of text data.</td>\n",
       "      <td id=\"T_1ac33_row12_col3\" class=\"data row12 col3\" >validmind.model_validation.TokenDisparity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row13_col0\" class=\"data row13 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row13_col1\" class=\"data row13 col1\" >Classifier Out Of Sample Performance</td>\n",
       "      <td id=\"T_1ac33_row13_col2\" class=\"data row13 col2\" >**Purpose**: This test is designed to assess the performance of a Machine Learning model on out-of-sample data, specifically data not utilized during the training phase. The performance metrics used in the test (accuracy, precision, recall, and F1 score) serve to measure the model's generalization capability towards unseen data. The primary goal is to ensure that the model has not overfitted to the training data and retains the ability to make accurate predictions on novel data.\n",
       "\n",
       "**Test Mechanism**: The mechanism for this test involves applying the performance metrics to the predictions made by the model on the test dataset. These are then compared with the actual outcomes. It is assumed that the test dataset remains unutilized during the model training phase, therefore providing an unbiased and fair evaluation of the model's generalization capabilities. The various metrics used include:\n",
       "- Accuracy: The ratio of correct predictions\n",
       "- Precision: The ratio of correct positive predictions\n",
       "- Recall: The ratio of actual positives that were correctly predicted\n",
       "- F1 Score: Harmonic mean of precision and recall, effectively balancing both.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low accuracy rate.\n",
       "- Low precision and recall rates.\n",
       "- Low F1 score.\n",
       "- Significant discrepancies between the model's performance on training data and testing data, indicating overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- The test provides a realistic assessment of a model's predictive performance on unseen data, thereby estimating its generalizability.\n",
       "- It incorporates several performance metrics into the evaluation, offering a comprehensive look at performance.\n",
       "- The test aids in the detection of overfitting, a crucial factor for all machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- The effectiveness of this test is significantly dependent on the quality and the representativeness of the test dataset. Performance metrics may not accurately reflect the true performance of the model if the test database is not a good representative of the real-world data the model will be working on.\n",
       "- The metrics used (accuracy, precision, recall and F1 score) make the assumption that all errors and misclassifications bear equal importance. This, however, may not align with certain real-world scenarios where some types of errors might have more significant implications than others.</td>\n",
       "      <td id=\"T_1ac33_row13_col3\" class=\"data row13 col3\" >validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row14_col0\" class=\"data row14 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row14_col1\" class=\"data row14 col1\" >Robustness Diagnosis</td>\n",
       "      <td id=\"T_1ac33_row14_col2\" class=\"data row14 col2\" >**Purpose**:\n",
       "\n",
       "The purpose of this test code is to evaluate the robustness of a machine learning model. Robustness refers to a model's ability to maintain a high level of performance in the face of perturbations or changes—particularly noise—added to its input data. This test is designed to help gauge how well the model can handle potential real-world scenarios where the input data might be incomplete or corrupted.\n",
       "\n",
       "**Test Mechanism**:\n",
       "\n",
       "This test is conducted by adding Gaussian noise, proportional to a particular standard deviation scale, to numeric input features of both the training and testing datasets. The model performance in the face of these perturbed features is then evaluated using metrics (default: 'accuracy'). This process is iterated over a range of scale factors. The resulting accuracy trend against the amount of noise introduced is illustrated with a line chart. A predetermined threshold determines what level of accuracy decay due to perturbation is considered acceptable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Substantial decreases in accuracy when noise is introduced to feature inputs.\n",
       "- The decay in accuracy surpasses the configured threshold, indicating that the model is not robust against input noise.\n",
       "- Instances where one or more elements provided in the features list don't match with the training dataset's numerical feature columns.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides an empirical measure of the model's performance in tackling noise or data perturbations, revealing insights into the model's stability.\n",
       "- Offers flexibility with the ability to choose specific features to perturb and control the level of noise applied.\n",
       "- Detailed results visualization helps in interpreting the outcome of robustness testing.\n",
       "\n",
       "**Limitations**:\n",
       "- Only numerical features are perturbed, leaving out non-numerical features, which can lead to an incomplete analysis of robustness.\n",
       "- The default metric used is accuracy, which might not always give the best measure of a model's success, particularly for imbalanced datasets.\n",
       "- The test is contingent on the assumption that the added Gaussian noise sufficiently represents potential data corruption or incompleteness in real-world scenarios.\n",
       "- There might be a requirement to fine-tune the set decay threshold for accuracy with the help of domain knowledge or specific project requisites.\n",
       "- The robustness test might not deliver the expected results for datasets with a text column.</td>\n",
       "      <td id=\"T_1ac33_row14_col3\" class=\"data row14 col3\" >validmind.model_validation.sklearn.RobustnessDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row15_col0\" class=\"data row15 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row15_col1\" class=\"data row15 col1\" >SHAP Global Importance</td>\n",
       "      <td id=\"T_1ac33_row15_col2\" class=\"data row15 col2\" >**Purpose:** The SHAP (SHapley Additive exPlanations) Global Importance metric aims to elucidate model outcomes by attributing them to the contributing features. It assigns a quantifiable global importance to each feature via their respective absolute Shapley values, thereby making it suitable for tasks like classification (both binary and multiclass). This metric forms an essential part of model risk management.\n",
       "\n",
       "**Test Mechanism:** The exam begins with the selection of a suitable explainer which aligns with the model's type. For tree-based models like XGBClassifier, RandomForestClassifier, CatBoostClassifier, TreeExplainer is used whereas for linear models like LogisticRegression, XGBRegressor, LinearRegression, it is the LinearExplainer. Once the explainer calculates the Shapley values, these values are visualized using two specific graphical representations:\n",
       "\n",
       "1. Mean Importance Plot: This graph portrays the significance of individual features based on their absolute Shapley values. It calculates the average of these absolute Shapley values across all instances to highlight the global importance of features.\n",
       "\n",
       "2. Summary Plot: This visual tool combines the feature importance with their effects. Every dot on this chart represents a Shapley value for a certain feature in a specific case. The vertical axis is denoted by the feature whereas the horizontal one corresponds to the Shapley value. A color gradient indicates the value of the feature, gradually changing from low to high. Features are systematically organized in accordance with their importance. These plots are generated by the function `_generate_shap_plot()`.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Overemphasis on certain features in SHAP importance plots, thus hinting at the possibility of model overfitting\n",
       "- Anomalies such as unexpected or illogical features showing high importance, which might suggest that the model's decisions are rooted in incorrect or undesirable reasoning\n",
       "- A SHAP summary plot filled with high variability or scattered data points, indicating a cause for concern\n",
       "\n",
       "**Strengths:**\n",
       "- SHAP does more than just illustrating global feature significance, it offers a detailed perspective on how different features shape the model's decision-making logic for each instance.\n",
       "- It provides clear insights into model behavior.\n",
       "- It demonstrates flexibility by supporting a wide array of model types, thereby promising uniform interpretations across different models.\n",
       "\n",
       "**Limitations:**\n",
       "- SHAP might demand considerable time and resources for large datasets or intricate models.\n",
       "- Its compatibility doesn't cover all model classes. Models from libraries like \"statsmodels\", \"pytorch\", \"catboost\", \"transformers\", \"FoundationModel\", and \"R\" can't be handled.\n",
       "- High-dimensional data can convolute interpretations.\n",
       "- Associating importance with tangible real-world impact still involves a certain degree of subjectivity.</td>\n",
       "      <td id=\"T_1ac33_row15_col3\" class=\"data row15 col3\" >validmind.model_validation.sklearn.SHAPGlobalImportance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row16_col0\" class=\"data row16 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row16_col1\" class=\"data row16 col1\" >Confusion Matrix</td>\n",
       "      <td id=\"T_1ac33_row16_col2\" class=\"data row16 col2\" >**Purpose**: The Confusion Matrix tester is designed to assess the performance of a classification Machine Learning model. This performance is evaluated based on how well the model is able to correctly classify True Positives, True Negatives, False Positives, and False Negatives\n",
       "- fundamental aspects of model accuracy.\n",
       "\n",
       "**Test Mechanism**: The mechanism used involves taking the predicted results (`y_test_predict`) from the classification model and comparing them against the actual values (`y_test_true`). A confusion matrix is built using the unique labels extracted from `y_test_true`, employing scikit-learn's metrics. The matrix is then visually rendered with the help of Plotly's `create_annotated_heatmap` function. A heatmap is created which provides a two-dimensional graphical representation of the model's performance, showcasing distributions of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
       "\n",
       "**Signs of High Risk**: Indicators of high risk related to the model include:\n",
       "- High numbers of False Positives (FP) and False Negatives (FN), depicting that the model is not effectively classifying the values.\n",
       "- Low numbers of True Positives (TP) and True Negatives (TN), implying that the model is struggling with correctly identifying class labels.\n",
       "\n",
       "**Strengths**: The Confusion Matrix tester brings numerous strengths:\n",
       "- It provides a simplified yet comprehensive visual snapshot of the classification model's predictive performance.\n",
       "- It distinctly brings out True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), thus, making it easier to focus on potential areas of improvement.\n",
       "- The matrix is beneficial in dealing with multi-class classification problems as it can provide a simple view of complex model performances.\n",
       "- It aids in understanding the different types of errors that the model could potentially make, as it provides in-depth insights into Type-I and Type-II errors.\n",
       "\n",
       "**Limitations**: Despite its various strengths, the Confusion Matrix tester does exhibit some limitations:\n",
       "- In cases of unbalanced classes, the effectiveness of the confusion matrix might be lessened. It may wrongly interpret the accuracy of a model that is essentially just predicting the majority class.\n",
       "- It does not provide a single unified statistic that could evaluate the overall performance of the model. Different aspects of the model's performance are evaluated separately instead.\n",
       "- It mainly serves as a descriptive tool and does not offer the capability for statistical hypothesis testing.\n",
       "- Risks of misinterpretation exist because the matrix doesn't directly provide precision, recall, or F1-score data. These metrics have to be computed separately.</td>\n",
       "      <td id=\"T_1ac33_row16_col3\" class=\"data row16 col3\" >validmind.model_validation.sklearn.ConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row17_col0\" class=\"data row17 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row17_col1\" class=\"data row17 col1\" >Classifier In Sample Performance</td>\n",
       "      <td id=\"T_1ac33_row17_col2\" class=\"data row17 col2\" >**Purpose**: The purpose of this metric is to evaluate the performance of the machine learning model on the training data. This test gauges the model's ability to generalize its predictions to new, unseen data and assesses the level of the model's overfitting on the training set by measuring commonly-used metrics such as accuracy, precision, recall, and F1 score.\n",
       "\n",
       "**Test Mechanism**: The implementation of this test incorporates various metrics including accuracy, precision, recall, and F1 score. These metrics are applied on the model's predictions of the training set and compared with the true output. The accuracy evaluates the proportion of correct predictions out of the total predictions. Meanwhile, precision measures the accurate positive predictions relative to the total number of positive predictions. The recall metric indicates the proportion of true positive predictions in relation to the overall number of actual positives in the dataset. Lastly, the F1 score represents the harmonic mean of precision and recall, thus providing a comprehensive appraisal of the model's performance.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A near-perfect performance on all metrics on the training data, coupled with inferior performance on unseen data, may be indicative of overfitting. This constitutes a high-risk scenario.\n",
       "- Low values on any of these metrics may signal an underperforming model, posing a potential risk for production-grade applications.\n",
       "\n",
       "**Strengths**:\n",
       "- Using conventional metrics such as accuracy, precision, recall, and F1 score allows for an all-encompassing evaluation of the model's performance.\n",
       "- The results are interpretable due to the widespread use and understanding of these metrics in the machine learning field.\n",
       "- Being applied to the training set, this test can detect overfitting early in the model's development stage.\n",
       "\n",
       "**Limitations**:\n",
       "- Although these metrics yield valuable insights, they are susceptible to biases inherent in the training data.\n",
       "- There's always a chance for disparity between the model's performance in the training set and performance with new, unseen data.\n",
       "- Therefore, this test should be supplemented with additional validation tactics, such as k-fold cross-validation or out-of-sample testing, to provide a more unbiased evaluation of the model's performance.</td>\n",
       "      <td id=\"T_1ac33_row17_col3\" class=\"data row17 col3\" >validmind.model_validation.sklearn.ClassifierInSamplePerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row18_col0\" class=\"data row18 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row18_col1\" class=\"data row18 col1\" >Overfit Diagnosis</td>\n",
       "      <td id=\"T_1ac33_row18_col2\" class=\"data row18 col2\" >**Purpose**: The OverfitDiagnosis test is devised to detect areas within a Machine Learning model that might be prone to overfitting. It achieves this by comparing the model's performance on both the training and testing datasets. These datasets are broken down into distinct sections defined by a Feature Space. Areas, where the model underperforms by displaying high residual values or a significant amount of overfitting, are highlighted, prompting actions for mitigation using regularization techniques such as L1 or L2 regularization, Dropout, Early Stopping or data augmentation.\n",
       "\n",
       "**Test Mechanism**: The metric conducts the test by executing the method 'run' on the default parameters and metrics with 'accuracy' as the specified metric. It segments the feature space by binning crucial feature columns from both the training and testing datasets. Then, the method computes the prediction results for each defined region. Subsequently, the prediction's efficacy is evaluated, i.e., the model's performance gap (defined as the discrepancy between the actual and the model's predictions) for both datasets is calculated and compared with a preset cut-off value for the overfitting condition. A test failure presents an overfit model, whereas a pass signifies a fit model. Meanwhile, the function also prepares figures further illustrating the regions with overfitting.\n",
       "\n",
       "**Signs of High Risk**: Indicators of a high-risk model are:\n",
       "- A high 'gap' value indicating discrepancies in the training and testing data accuracy signals an overfit model.\n",
       "- Multiple or vast overfitting zones within the feature space suggest overcomplication of the model.\n",
       "\n",
       "**Strengths**:\n",
       "- Presents a visual perspective by plotting regions with overfit issues, simplifying understanding of the model structure.\n",
       "- Permits a feature-focused assessment, which promotes specific, targeted modifications to the model.\n",
       "- Caters to modifications of the testing parameters such as 'cut_off_percentage' and 'features_column' enabling a personalized analysis.\n",
       "- Handles both numerical and categorical features.\n",
       "\n",
       "**Limitations**:\n",
       "- Does not currently support regression tasks and is limited to classification tasks only.\n",
       "- Ineffectual for text-based features, which in turn restricts its usage for Natural Language Processing models.\n",
       "- Primarily depends on the bins setting, responsible for segmenting the feature space. Different bin configurations might yield varying results.\n",
       "- Utilization of a fixed cut-off percentage for making overfitting decisions, set arbitrarily, leading to a possible risk of inaccuracy.\n",
       "- Limitation of performance metrics to accuracy alone might prove inadequate for detailed examination, especially for imbalanced datasets.</td>\n",
       "      <td id=\"T_1ac33_row18_col3\" class=\"data row18 col3\" >validmind.model_validation.sklearn.OverfitDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row19_col0\" class=\"data row19 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row19_col1\" class=\"data row19 col1\" >Permutation Feature Importance</td>\n",
       "      <td id=\"T_1ac33_row19_col2\" class=\"data row19 col2\" >**Purpose**: The purpose of the Permutation Feature Importance (PFI) metric is to assess the importance of each feature used by the Machine Learning model. The significance is measured by evaluating the decrease in the model's performance when the feature's values are randomly arranged.\n",
       "\n",
       "**Test Mechanism**: PFI is calculated via the `permutation_importance` method from the `sklearn.inspection` module. This method shuffles the columns of the feature dataset and measures the impact on the model's performance. A significant decrease in performance after permutating a feature's values deems the feature as important. On the other hand, if performance remains the same, the feature is likely not important. The output of the PFI metric is a figure illustrating the importance of each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The model heavily relies on a feature with highly variable or easily permutable values, indicating instability.\n",
       "- A feature, deemed unimportant by the model but based on domain knowledge should have a significant effect on the outcome, is not influencing the model's predictions.\n",
       "\n",
       "**Strengths**:\n",
       "- PFI provides insights into the importance of different features and may reveal underlying data structure.\n",
       "- It can indicate overfitting if a particular feature or set of features overly impacts the model's predictions.\n",
       "- The metric is model-agnostic and can be used with any classifier that provides a measure of prediction accuracy before and after feature permutation.\n",
       "\n",
       "**Limitations**:\n",
       "- The feature importance calculated does not imply causality, it only presents the amount of information that a feature provides for the prediction task.\n",
       "- The metric does not account for interactions between features. If features are correlated, the permutation importance may allocate importance to one and not the other.\n",
       "- PFI cannot interact with certain libraries like statsmodels, pytorch, catboost, etc, thus limiting its applicability.</td>\n",
       "      <td id=\"T_1ac33_row19_col3\" class=\"data row19 col3\" >validmind.model_validation.sklearn.PermutationFeatureImportance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row20_col0\" class=\"data row20 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row20_col1\" class=\"data row20 col1\" >Minimum ROCAUC Score</td>\n",
       "      <td id=\"T_1ac33_row20_col2\" class=\"data row20 col2\" >**Purpose**: This test metric, Minimum ROC AUC Score, is used to determine the model's performance by ensuring that the Receiver Operating Characteristic Area Under the Curve (ROC AUC) score on the validation dataset meets or exceeds a predefined threshold. The ROC AUC score is an indicator of how well the model is capable of distinguishing between different classes, making it a crucial measure in binary and multiclass classification tasks.\n",
       "\n",
       "**Test Mechanism**: This test implementation calculates the multiclass ROC AUC score on the true target values and the model's prediction. The test converts the multi-class target variables into binary format using `LabelBinarizer` before computing the score. If this ROC AUC score is higher than the predefined threshold (defaulted to 0.5), the test passes; otherwise, it fails. The results, including the ROC AUC score, the threshold, and whether the test passed or failed, are then stored in a `ThresholdTestResult` object.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk or failure in the model's performance as related to this metric would be represented by a low ROC AUC score, specifically any score lower than the predefined minimum threshold. This suggests that the model is struggling to distinguish between different classes effectively.\n",
       "\n",
       "**Strengths**:\n",
       "- The test considers both the true positive rate and false positive rate, providing a comprehensive performance measure.\n",
       "- ROC AUC score is threshold-independent meaning it measures the model's quality across various classification thresholds.\n",
       "- Works robustly with binary as well as multi-class classification problems.\n",
       "\n",
       "**Limitations**:\n",
       "- ROC AUC may not be useful if the class distribution is highly imbalanced; it could perform well in terms of AUC but still fail to predict the minority class.\n",
       "- The test does not provide insight into what specific aspects of the model are causing poor performance if the ROC AUC score is unsatisfactory.\n",
       "- The use of macro average for multiclass ROC AUC score implies equal weightage to each class, which might not be appropriate if the classes are imbalanced.</td>\n",
       "      <td id=\"T_1ac33_row20_col3\" class=\"data row20 col3\" >validmind.model_validation.sklearn.MinimumROCAUCScore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row21_col0\" class=\"data row21 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row21_col1\" class=\"data row21 col1\" >Precision Recall Curve</td>\n",
       "      <td id=\"T_1ac33_row21_col2\" class=\"data row21 col2\" >**Purpose**: The Precision Recall Curve metric is intended to evaluate the trade-off between precision and recall in classification models, particularly binary classification models. It assesses the model's capacity to produce accurate results (high precision), as well as its ability to capture a majority of all positive instances (high recall).\n",
       "\n",
       "**Test Mechanism**: The test extracts ground truth labels and prediction probabilities from the model's test dataset. It applies the precision_recall_curve method from the sklearn metrics module to these extracted labels and predictions, which computes a precision-recall pair for each possible threshold. This calculation results in an array of precision and recall scores that can be plotted against each other to form the Precision-Recall Curve. This curve is then visually represented by using Plotly's scatter plot.\n",
       "\n",
       "**Signs of High Risk**: * A lower area under the Precision-Recall Curve signifies high risk. * This corresponds to a model yielding a high amount of false positives (low precision) and/or false negatives (low recall). * If the curve is closer to the bottom left of the plot, rather than being closer to the top right corner, it can be a sign of high risk.\n",
       "\n",
       "**Strengths**: * This metric aptly represents the balance between precision (minimizing false positives) and recall (minimizing false negatives), which is especially critical in scenarios where both values are significant. * Through the graphic representation, it enables an intuitive understanding of the model's performance across different threshold levels.\n",
       "\n",
       "**Limitations**: * This metric is only applicable to binary classification models – it raises errors for multiclass classification models or Foundation models. * It may not fully represent the overall accuracy of the model if the cost of false positives and false negatives are extremely different, or if the dataset is heavily imbalanced.</td>\n",
       "      <td id=\"T_1ac33_row21_col3\" class=\"data row21 col3\" >validmind.model_validation.sklearn.PrecisionRecallCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row22_col0\" class=\"data row22 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row22_col1\" class=\"data row22 col1\" >Classifier Performance</td>\n",
       "      <td id=\"T_1ac33_row22_col2\" class=\"data row22 col2\" >**Purpose**: The supplied script is designed to evaluate the performance of Machine Learning classification models. It accomplishes this by computing precision, recall, F1-Score, and accuracy, as well as the ROC AUC (Receiver operating characteristic\n",
       "- Area under the curve) scores, thereby providing a comprehensive analytic view of the models' performance. The test is adaptable, handling binary and multiclass models equally effectively.\n",
       "\n",
       "**Test Mechanism**: The script produces a report that includes precision, recall, F1-Score, and accuracy, by leveraging the `classification_report` from the scikit-learn's metrics module. For multiclass models, macro and weighted averages for these scores are also calculated. Additionally, the ROC AUC scores are calculated and included in the report using the script's unique `multiclass_roc_auc_score` function. The outcome of the test (report format) differs based on whether the model is binary or multiclass.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low values for precision, recall, F1-Score, accuracy, and ROC AUC, indicating poor performance.\n",
       "- Imbalance in precision and recall scores. Precision highlights correct positive class predictions, while recall indicates the accurate identification of actual positive cases. Imbalance may indicate flawed model performance.\n",
       "- A low ROC AUC score, especially scores close to 0.5 or lower, strongly suggests a failing model.\n",
       "\n",
       "**Strengths**:\n",
       "- The script is versatile, capable of assessing both binary and multiclass models.\n",
       "- It uses a variety of commonly employed performance metrics, offering a comprehensive view of a model's performance.\n",
       "- The use of ROC-AUC as a metric aids in determining the most optimal threshold for classification, especially beneficial when evaluation datasets are unbalanced.\n",
       "\n",
       "**Limitations**:\n",
       "- The test assumes correctly identified labels for binary classification models and raises an exception if the positive class is not labeled as \"1\". However, this setup may not align with all practical applications.\n",
       "- This script is specifically designed for classification models and is not suited to evaluate regression models.\n",
       "- The metrics computed may provide limited insights in cases where the test dataset does not adequately represent the data the model will encounter in real-world scenarios.</td>\n",
       "      <td id=\"T_1ac33_row22_col3\" class=\"data row22 col3\" >validmind.model_validation.sklearn.ClassifierPerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row23_col0\" class=\"data row23 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row23_col1\" class=\"data row23 col1\" >Minimum F1 Score</td>\n",
       "      <td id=\"T_1ac33_row23_col2\" class=\"data row23 col2\" >**Purpose:** The main objective of this test is to ensure that the F1 score, a balanced measure of precision and recall, of the model meets or surpasses a predefined threshold on the validation dataset. The F1 score is highly useful for gauging model performance in classification tasks, especially in cases where the distribution of positive and negative classes is skewed.\n",
       "\n",
       "**Test Mechanism:** The F1 score for the validation dataset is computed through the scikit-learn's metrics in Python. The scoring mechanism differs based on the classification problem: for multi-class problems, macro averaging is used (metrics are calculated separately and their unweighted mean is found), and for binary classification, the built-in f1_score calculation is used. The obtained F1 score is then assessed against the predefined minimum F1 score that is expected from the model.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "\n",
       "- If a model returns an F1 score that is less than the established threshold, it is regarded as high risk.\n",
       "- A low F1 score might suggest that the model is not finding an optimal balance between precision and recall, see: it isn't successfully identifying positive classes while minimizing false positives.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "- This metric gives a balanced measure of a model's performance by accounting for both false positives and false negatives.\n",
       "- It has a particular advantage in scenarios with imbalanced class distribution, where an accuracy measure can be misleading.\n",
       "- The flexibility of setting the threshold value allows for tailoring the minimum acceptable performance.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "- The testing method may not be suitable for all types of models and machine learning tasks.\n",
       "- Although the F1 score gives a balanced view of a model's performance, it presupposes an equal cost for false positives and false negatives, which may not always be true in certain real-world scenarios. As a consequence, practitioners might have to rely on other metrics such as precision, recall, or the ROC-AUC score that align more closely with their specific requirements.</td>\n",
       "      <td id=\"T_1ac33_row23_col3\" class=\"data row23 col3\" >validmind.model_validation.sklearn.MinimumF1Score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row24_col0\" class=\"data row24 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row24_col1\" class=\"data row24 col1\" >ROC Curve</td>\n",
       "      <td id=\"T_1ac33_row24_col2\" class=\"data row24 col2\" >**Purpose**: The Receiver Operating Characteristic (ROC) curve is designed to evaluate the performance of binary classification models. This curve illustrates the balance between the True Positive Rate (TPR) and False Positive Rate (FPR) across various threshold levels. In combination with the Area Under the Curve (AUC), the ROC curve aims to measure the model's discrimination ability between the two defined classes in a binary classification problem (e.g., default vs non-default). Ideally, a higher AUC score signifies superior model performance in accurately distinguishing between the positive and negative classes.\n",
       "\n",
       "**Test Mechanism**: First, this script selects the target model and datasets that require binary classification. It then calculates the predicted probabilities for the test set, and uses this data, along with the true outcomes, to generate and plot the ROC curve. Additionally, it concludes a line signifying randomness (AUC of 0.5). The AUC score for the model's ROC curve is also computed, presenting a numerical estimation of the model's performance. If any Infinite values are detected in the ROC threshold, these are effectively eliminated. The resulting ROC curve, AUC score, and thresholds are consequently saved for future reference.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk is potentially linked to the model's performance if the AUC score drops below or nears 0.5.\n",
       "- Another warning sign would be the ROC curve lying closer to the line of randomness, indicating no discriminative ability.\n",
       "- For the model to be deemed competent at its classification tasks, it is crucial that the AUC score is significantly above 0.5.\n",
       "\n",
       "**Strengths**:\n",
       "- This ROC Curve offers an inclusive visual depiction of a model's discriminative power throughout all conceivable classification thresholds, unlike other metrics that solely disclose model performance at one fixed threshold.\n",
       "- Despite the proportions of the dataset, the AUC Score, which represents the entire ROC curve as a single data point, continues to be consistent, proving to be the ideal choice for such situations.\n",
       "\n",
       "**Limitations**:\n",
       "- The primary limitation is that this test is exclusively structured for binary classification tasks, thus limiting its application towards other model types.\n",
       "- Furthermore, its performance might be subpar with models that output probabilities highly skewed towards 0 or 1.\n",
       "- At the extreme, the ROC curve could reflect high performance even when the majority of classifications are incorrect, provided that the model's ranking format is retained. This phenomenon is commonly termed the \"Class Imbalance Problem\".</td>\n",
       "      <td id=\"T_1ac33_row24_col3\" class=\"data row24 col3\" >validmind.model_validation.sklearn.ROCCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row25_col0\" class=\"data row25 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row25_col1\" class=\"data row25 col1\" >Training Test Degradation</td>\n",
       "      <td id=\"T_1ac33_row25_col2\" class=\"data row25 col2\" >**Purpose**: The 'TrainingTestDegradation' class serves as a test to verify that the degradation in performance between the training and test datasets does not exceed a predefined threshold. This test serves as a measure to check the model's ability to generalize from its training data to unseen test data. It assesses key classification metric scores such as accuracy, precision, recall and f1 score, to verify the model's robustness and reliability.\n",
       "\n",
       "**Test Mechanism**: The code applies several predefined metrics including accuracy, precision, recall and f1 scores to the model's predictions for both the training and test datasets. It calculates the degradation as the difference between the training score and test score divided by the training score. The test is considered successful if the degradation for each metric is less than the preset maximum threshold of 10%. The results are summarized in a table showing each metric's train score, test score, degradation percentage, and pass/fail status.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A degradation percentage that exceeds the maximum allowed threshold of 10% for any of the evaluated metrics.\n",
       "- A high difference or gap between the metric scores on the training and the test datasets.\n",
       "- The 'Pass/Fail' column displaying 'Fail' for any of the evaluated metrics.\n",
       "\n",
       "**Strengths**:\n",
       "- This test provides a quantitative measure of the model's ability to generalize to unseen data, which is key for predicting its practical real-world performance.\n",
       "- By evaluating multiple metrics, it takes into account different facets of model performance and enables a more holistic evaluation.\n",
       "- The use of a variable predefined threshold allows the flexibility to adjust the acceptability criteria for different scenarios.\n",
       "\n",
       "**Limitations**:\n",
       "- The test compares raw performance on training and test data, but does not factor in the nature of the data. Areas with less representation in the training set, for instance, might still perform poorly on unseen data.\n",
       "- It requires good coverage and balance in the test and training datasets to produce reliable results, which may not always be available.\n",
       "- The test is currently only designed for classification tasks.</td>\n",
       "      <td id=\"T_1ac33_row25_col3\" class=\"data row25 col3\" >validmind.model_validation.sklearn.TrainingTestDegradation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row26_col0\" class=\"data row26 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row26_col1\" class=\"data row26 col1\" >Models Performance Comparison</td>\n",
       "      <td id=\"T_1ac33_row26_col2\" class=\"data row26 col2\" >**Purpose**: This metric test aims to evaluate and compare the performance of various Machine Learning models using test data. It employs multiple metrics such as accuracy, precision, recall, and the F1 score, among others, to assess model performance and assist in selecting the most effective model for the designated task.\n",
       "\n",
       "**Test Mechanism**: The test employs Scikit-learn’s performance metrics to evaluate each model's performance for both binary and multiclass classification tasks. To compare performances, the test runs each model against the test dataset, then produces a comprehensive classification report. This report includes metrics such as accuracy, precision, recall, and the F1 score. Based on whether the task at hand is binary or multiclass classification, it calculates metrics globally for the \"positive\" class or, alternatively, their weighted averages, macro averages, and per class metrics. The test will be skipped if no models are supplied.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low scores in accuracy, precision, recall, and F1 metrics indicate a potentially high risk.\n",
       "- A low area under the Receiver Operating Characteristic (ROC) curve (roc_auc score) is another possible indicator of high risk.\n",
       "- If the metrics scores are significantly lower than alternative models, this might suggest a high risk of failure.\n",
       "\n",
       "**Strengths**:\n",
       "- The test provides a simple way to compare the performance of multiple models, accommodating both binary and multiclass classification tasks.\n",
       "- It provides a holistic view of model performance through a comprehensive report of key performance metrics.\n",
       "- The inclusion of the ROC AUC score is advantageous, as this robust performance metric can effectively handle class imbalance issues.\n",
       "\n",
       "**Limitations**:\n",
       "- This test may not be suitable for more complex performance evaluations that consider factors such as prediction speed, computational cost, or business-specific constraints.\n",
       "- The test's reliability depends on the provided test dataset; hence, the selected models' performance could vary with unseen data or changes in the data distribution.\n",
       "- The ROC AUC score might not be as meaningful or easily interpretable for multilabel/multiclass tasks.</td>\n",
       "      <td id=\"T_1ac33_row26_col3\" class=\"data row26 col3\" >validmind.model_validation.sklearn.ModelsPerformanceComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row27_col0\" class=\"data row27 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row27_col1\" class=\"data row27 col1\" >Weakspots Diagnosis</td>\n",
       "      <td id=\"T_1ac33_row27_col2\" class=\"data row27 col2\" >**Purpose:** The weak spots test is applied to evaluate the performance of a machine learning model within specific regions of its feature space. This test slices the feature space into various sections, evaluating the model's outputs within each section against specific performance metrics (e.g., accuracy, precision, recall, and F1 scores). The ultimate aim is to identify areas where the model's performance falls below the set thresholds, thereby exposing its possible weaknesses and limitations.\n",
       "\n",
       "**Test Mechanism:** The test mechanism adopts an approach of dividing the feature space of the training dataset into numerous bins. The model's performance metrics (accuracy, precision, recall, F1 scores) are then computed for each bin on both the training and test datasets. A \"weak spot\" is identified if any of the performance metrics fall below a predetermined threshold for a particular bin on the test dataset. The test results are visually plotted as bar charts for each performance metric, indicating the bins which fail to meet the established threshold.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Any performance metric of the model dropping below the set thresholds.\n",
       "- Significant disparity in performance between the training and test datasets within a bin could be an indication of overfitting.\n",
       "- Regions or slices with consistently low performance metrics. Such instances could mean that the model struggles to handle specific types of input data adequately, resulting in potentially inaccurate predictions.\n",
       "\n",
       "**Strengths:**\n",
       "- The test helps pinpoint precise regions of the feature space where the model's performance is below par, allowing for more targeted improvements to the model.\n",
       "- The graphical presentation of the performance metrics offers an intuitive way to understand the model's performance across different feature areas.\n",
       "- The test exhibits flexibility, letting users set different thresholds for various performance metrics according to the specific requirements of the application.\n",
       "\n",
       "**Limitations:**\n",
       "- The binning system utilized for the feature space in the test could over-simplify the model's behavior within each bin. The granularity of this slicing depends on the chosen 'bins' parameter and can sometimes be arbitrary.\n",
       "- The effectiveness of this test largely hinges on the selection of thresholds for the performance metrics, which may not hold universally applicable and could be subjected to the specifications of a particular model and application.\n",
       "- The test is unable to handle datasets with a text column, limiting its application to numerical or categorical data types only.\n",
       "- Despite its usefulness in highlighting problematic regions, the test does not offer direct suggestions for model improvement.</td>\n",
       "      <td id=\"T_1ac33_row27_col3\" class=\"data row27 col3\" >validmind.model_validation.sklearn.WeakspotsDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row28_col0\" class=\"data row28 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row28_col1\" class=\"data row28 col1\" >Population Stability Index</td>\n",
       "      <td id=\"T_1ac33_row28_col2\" class=\"data row28 col2\" >**Purpose:** The Population Stability Index (PSI) serves as a quantitative assessment for evaluating the stability of a machine learning model's output distributions when comparing two different datasets. Typically, these would be a development and a validation dataset or two datasets collected at different periods. The PSI provides a measurable indication of any significant shift in the model's performance over time or noticeable changes in the characteristics of the population the model is making predictions for.\n",
       "\n",
       "**Test Mechanism:** The implementation of the PSI in this script involves calculating the PSI for each feature between the training and test datasets. Data from both datasets is sorted and placed into either a predetermined number of bins or quantiles. The boundaries for these bins are initially determined based on the distribution of the training data. The contents of each bin are calculated and their respective proportions determined. Subsequently, the PSI is derived for each bin through a logarithmic transformation of the ratio of the proportions of data for each feature in the training and test datasets. The PSI, along with the proportions of data in each bin for both datasets, are displayed in a summary table, a grouped bar chart, and a scatter plot.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- A high PSI value is a clear indicator of high risk. Such a value suggests a significant shift in the model predictions or severe changes in the characteristics of the underlying population.\n",
       "- This ultimately suggests that the model may not be performing as well as expected and that it may be less reliable for making future predictions.\n",
       "\n",
       "**Strengths:**\n",
       "- The PSI provides a quantitative measure of the stability of a model over time or across different samples, making it an invaluable tool for evaluating changes in a model's performance.\n",
       "- It allows for direct comparisons across different features based on the PSI value.\n",
       "- The calculation and interpretation of the PSI are straightforward, facilitating its use in model risk management.\n",
       "- The use of visual aids such as tables and charts further simplifies the comprehension and interpretation of the PSI.\n",
       "\n",
       "**Limitations:**\n",
       "- The PSI test does not account for the interdependence between features: features that are dependent on one another may show similar shifts in their distributions, which in turn may result in similar PSI values.\n",
       "- The PSI test does not inherently provide insights into why there are differences in distributions or why the PSI values may have changed.\n",
       "- The test may not handle features with significant outliers adequately.\n",
       "- Additionally, the PSI test is performed on model predictions, not on the underlying data distributions which can lead to misinterpretations. Any changes in PSI could be due to shifts in the model (model drift), changes in the relationships between features and the target variable (concept drift), or both. However, distinguishing between these causes is non-trivial.</td>\n",
       "      <td id=\"T_1ac33_row28_col3\" class=\"data row28 col3\" >validmind.model_validation.sklearn.PopulationStabilityIndex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row29_col0\" class=\"data row29 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row29_col1\" class=\"data row29 col1\" >Minimum Accuracy</td>\n",
       "      <td id=\"T_1ac33_row29_col2\" class=\"data row29 col2\" >**Purpose**: The Minimum Accuracy test’s objective is to verify whether the model's prediction accuracy on a specific dataset meets or surpasses a predetermined minimum threshold. Accuracy, which is simply the ratio of right predictions to total predictions, is a key metric for evaluating the model's performance. Considering binary as well as multiclass classifications, accurate labeling becomes indispensable.\n",
       "\n",
       "**Test Mechanism**: The test mechanism involves contrasting the model's accuracy score with a pre-set minimum threshold value, default value being 0.7. The accuracy score is computed utilizing sklearn’s `accuracy_score` method, where the true label `y_true` and predicted label `class_pred` are compared. If the accuracy score is above the threshold, the test gets a passing mark. The test returns the result along with the accuracy score and threshold used for the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The risk level for this test surges considerably when the model is unable to achieve or surpass the predefined score threshold.\n",
       "- When the model persistently scores below the threshold, it suggests a high risk of inaccurate predictions, which in turn affects the model’s efficiency and reliability.\n",
       "\n",
       "**Strengths**:\n",
       "- One of the key strengths of this test is its simplicity, presenting a straightforward measure of the holistic model performance across all classes.\n",
       "- This test is particularly advantageous when classes are balanced.\n",
       "- Another advantage of this test is its versatility as it can be implemented on both binary and multiclass classification tasks.\n",
       "\n",
       "**Limitations**:\n",
       "- When analyzing imbalanced datasets, certain limitations of this test emerge. The accuracy score can be misleading when classes in the dataset are skewed considerably.\n",
       "- This can result in favoritism towards the majority class, consequently giving an inaccurate perception of the model performance.\n",
       "- Another limitation is its inability to measure the model's precision, recall, or capacity to manage false positives or false negatives.\n",
       "- The test majorly focuses on overall correctness and may not be sufficient for all types of model analytics.</td>\n",
       "      <td id=\"T_1ac33_row29_col3\" class=\"data row29 col3\" >validmind.model_validation.sklearn.MinimumAccuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row30_col0\" class=\"data row30 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row30_col1\" class=\"data row30 col1\" >Regression Models Coeffs</td>\n",
       "      <td id=\"T_1ac33_row30_col2\" class=\"data row30 col2\" >**Purpose**: The 'RegressionModelsCoeffs' metric is utilized to evaluate and compare coefficients of different regression models trained on the same dataset. By examining how each model weighted the importance of different features during training, this metric provides key insights into which factors have the most impact on the model's predictions and how these patterns differ across models.\n",
       "\n",
       "**Test Mechanism**: The test operates by extracting the coefficients of each regression model using the 'regression_coefficients()' method. These coefficients are then consolidated into a dataframe, with each row representing a model and columns corresponding to each feature's coefficient. It must be noted that this test is exclusive to 'statsmodels' and 'R' models, other models will result in a 'SkipTestError'.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Discrepancies in how different models weight the same features\n",
       "- Unexpectedly high or low coefficients\n",
       "- The test is inapplicable to certain models because they are not from 'statsmodels' or 'R' libraries\n",
       "\n",
       "**Strengths**:\n",
       "- Enables insight into the training process of different models\n",
       "- Allows comparison of feature importance across models\n",
       "- Through the review of feature coefficients, the test provides a more transparent evaluation of the model and highlights significant weights and biases in the training procedure\n",
       "\n",
       "**Limitations**:\n",
       "- The test is only compatible with 'statsmodels' and 'R' regression models\n",
       "- While the test provides contrast in feature weightings among models, it does not establish the most appropriate or accurate weighting, thus remaining subject to interpretation\n",
       "- It does not account for potential overfitting or underfitting of models\n",
       "- The computed coefficients might not lead to effective performance on unseen data</td>\n",
       "      <td id=\"T_1ac33_row30_col3\" class=\"data row30 col3\" >validmind.model_validation.statsmodels.RegressionModelsCoeffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row31_col0\" class=\"data row31 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row31_col1\" class=\"data row31 col1\" >Box Pierce</td>\n",
       "      <td id=\"T_1ac33_row31_col2\" class=\"data row31 col2\" >**Purpose:** The Box-Pierce test is utilized to detect the presence of autocorrelation in a time-series dataset. Autocorrelation, or serial correlation, refers to the degree of similarity between observations based on the temporal spacing between them. This test is essential for affirming the quality of a time-series model by ensuring that the error terms in the model are random and do not adhere to a specific pattern.\n",
       "\n",
       "**Test Mechanism:** The implementation of the Box-Pierce test involves calculating a test statistic along with a corresponding p-value derived from the dataset features. These quantities are used to test the null hypothesis that posits the data to be independently distributed. This is achieved by iterating over every feature column in the time-series data and applying the `acorr_ljungbox` function of the statsmodels library. The function yields the Box-Pierce test statistic as well as the respective p-value, all of which are cached as test results.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- A low p-value, typically under 0.05 as per statistical convention, throws the null hypothesis of independence into question. This implies that the dataset potentially houses autocorrelations, thus indicating a high-risk scenario concerning model performance.\n",
       "- Large Box-Pierce test statistic values may indicate the presence of autocorrelation.\n",
       "\n",
       "**Strengths:**\n",
       "- Detects patterns in data that are supposed to be random, thereby ensuring no underlying autocorrelation.\n",
       "- Can be computed efficiently given its low computational complexity.\n",
       "- Can be widely applied to most regression problems, making it very versatile.\n",
       "\n",
       "**Limitations:**\n",
       "- Assumes homoscedasticity (constant variance) and normality of residuals, which may not always be the case in real-world datasets.\n",
       "- May exhibit reduced power for detecting complex autocorrelation schemes such as higher-order or negative correlations.\n",
       "- It only provides a general indication of the existence of autocorrelation, without providing specific insights into the nature or patterns of the detected autocorrelation.\n",
       "- In the presence of exhibits trends or seasonal patterns, the Box-Pierce test may yield misleading results.\n",
       "- Applicability is limited to time-series data, which limits its overall utility.</td>\n",
       "      <td id=\"T_1ac33_row31_col3\" class=\"data row31 col3\" >validmind.model_validation.statsmodels.BoxPierce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row32_col0\" class=\"data row32 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row32_col1\" class=\"data row32 col1\" >Regression Coeffs Plot</td>\n",
       "      <td id=\"T_1ac33_row32_col2\" class=\"data row32 col2\" >**Purpose**: The Regression Coefficients with Confidence Intervals plot and metric aims to understand the impact of predictor variables on the response variable in question. This understanding is achieved via the visualization and analysis of the regression model by presenting the coefficients derived from the model along with their associated 95% confidence intervals. By doing so, it offers insights into the variability and uncertainty associated with the model's estimates.\n",
       "\n",
       "**Test Mechanism**: The test begins by extracting the estimated coefficients and their related standard errors from the regression model under test. It then calculates and draws confidence intervals based on a 95% confidence level (a standard convention in statistics). These intervals provide a range wherein the true value can be expected to fall 95% of the time if the same regression were re-run multiple times with samples drawn from the same population. This information is then visualized as a bar plot, with the predictor variables and their coefficients on the x-axis and y-axis respectively and the confidence intervals represented as error bars.\n",
       "\n",
       "**Signs of High Risk**: * If the calculated confidence interval contains the zero value, it could mean the feature/coefficient in question doesn't significantly contribute to prediction in the model. * If there are multiple coefficients exhibiting this behavior, it might raise concerns about overall model reliability. * Very wide confidence intervals might indicate high uncertainty in the associated coefficient estimates.\n",
       "\n",
       "**Strengths**: * This metric offers a simple and easily comprehendible visualization of the significance and impact of individual predictor variables in a regression model. * By including confidence intervals, it enables an observer to evaluate the uncertainty around each coefficient estimate.\n",
       "\n",
       "**Limitations**: * The test is dependent on a few assumptions about the data, namely normality of residuals and independence of observations, which may not always be true for all types of datasets. * The test does not consider multi-collinearity (correlation among predictor variables), which can potentially distort the model and make interpretation of coefficients challenging. * The test's application is limited to regression tasks and tabular datasets and is not suitable for other types of machine learning assignments or data structures.</td>\n",
       "      <td id=\"T_1ac33_row32_col3\" class=\"data row32 col3\" >validmind.model_validation.statsmodels.RegressionCoeffsPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row33_col0\" class=\"data row33 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row33_col1\" class=\"data row33 col1\" >Regression Model Sensitivity Plot</td>\n",
       "      <td id=\"T_1ac33_row33_col2\" class=\"data row33 col2\" >**Purpose**: The Regression Sensitivity Plot metric is designed to perform sensitivity analysis on regression models. This metric aims to measure the impact of slight changes (shocks) applied to individual variables on the system's outcome while keeping all other variables constant. By doing so, it analyzes the effects of each independent variable on the dependent variable within the regression model and helps identify significant risk factors that could substantially influence the model's output.\n",
       "\n",
       "**Test Mechanism**: This metric operates by initially applying shocks of varying magnitudes, defined by specific parameters, to each of the model's features, one at a time. With all other variables held constant, a new prediction is made for each dataset subjected to shocks. Any changes in the model's predictions are directly attributed to the shocks applied. In the event that the transformation parameter is set to \"integrate\", initial predictions and target values undergo transformation via an integration function before being plotted. Lastly, a plot demonstrating observed values against predicted values for each model is generated, showcasing a distinct line graph illustrating predictions for each shock.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If the plot exhibits drastic alterations in model predictions consequent to minor shocks to an individual variable, it may indicate high risk. This underscores potentially high model sensitivity to changes in that variable, suggesting over-dependence on that variable for predictions.\n",
       "- Unusually high or unpredictable shifts in response to shocks may also denote potential model instability or overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric allows identification of variables strongly influencing the model outcomes, paving the way for understanding feature importance.\n",
       "- It generates visual plots which make the results easily interpretable even to non-technical stakeholders.\n",
       "- Beneficial in identifying overfitting and detecting unstable models that over-react to minor changes in variables.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric operates on the assumption that all other variables remain unchanged during the application of a shock. However, real-world situations where variables may possess intricate interdependencies may not always reflect this.\n",
       "- It is best compatible with linear models and may not effectively evaluate the sensitivity of non-linear model configurations.\n",
       "- The metric does not provide a numerical risk measure. It offers only a visual representation, which may invite subjectivity in interpretation.</td>\n",
       "      <td id=\"T_1ac33_row33_col3\" class=\"data row33 col3\" >validmind.model_validation.statsmodels.RegressionModelSensitivityPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row34_col0\" class=\"data row34 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row34_col1\" class=\"data row34 col1\" >Regression Models Performance</td>\n",
       "      <td id=\"T_1ac33_row34_col2\" class=\"data row34 col2\" >**Purpose**: This metric is used to evaluate and compare the performance of various regression models. Through the use of key statistical measures such as R-squared, Adjusted R-squared, and Mean Squared Error (MSE), the performance of different models in predicting dependent variables can be assessed both on the data used for training (in-sample) and new, unseen data (out-of-sample).\n",
       "\n",
       "**Test Mechanism**: The test evaluates a list of provided regression models. For each model, it calculates their in-sample and out-of-sample performance by deriving the model predictions for the training and testing datasets respectively, and then comparing these predictions to the actual values. In doing so, it calculates R-squared, Adjusted R-squared, and MSE for each model, stores the results, and returns them for comparison.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High Mean Squared Error (MSE) values.\n",
       "- Strikingly low values of R-squared and Adjusted R-squared.\n",
       "- A significant drop in performance when transitioning from in-sample to out-of-sample evaluations, signaling a potential overfitting issue.\n",
       "\n",
       "**Strengths**:\n",
       "- The test permits comparisons of multiple models simultaneously, providing an objective base for identifying the top-performing model.\n",
       "- It delivers both in-sample and out-of-sample evaluations, presenting performance data on unseen data.\n",
       "- The utilization of R-squared and Adjusted R-squared in conjunction with MSE allows for a detailed view of the model's explainability and error rate.\n",
       "\n",
       "**Limitations**:\n",
       "- This test is built around the assumption that the residuals of the regression model are normally distributed, which is a fundamental requirement for Ordinary Least Squares (OLS) regression; thus, it could be not suitable for models where this assumption is broken.\n",
       "- The test does not consider cases where higher R-squared or lower MSE values do not necessarily correlate with better predictive performance, particularly in instances of excessively complex models.</td>\n",
       "      <td id=\"T_1ac33_row34_col3\" class=\"data row34 col3\" >validmind.model_validation.statsmodels.RegressionModelsPerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row35_col0\" class=\"data row35 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row35_col1\" class=\"data row35 col1\" >Zivot Andrews Arch</td>\n",
       "      <td id=\"T_1ac33_row35_col2\" class=\"data row35 col2\" >**Purpose**: The Zivot-Andrews Arch metric is used to evaluate the order of integration for a time series data in a machine learning model. It's designed to test for stationarity, a crucial aspect in time series analysis where data points are not dependent on time. Stationarity means that the statistical properties such as mean, variance and autocorrelation are all constant over time.\n",
       "\n",
       "**Test Mechanism**: The Zivot-Andrews unit root test is performed on each feature in the dataset using the `ZivotAndrews` function from the `arch.unitroot` module. This function returns the Zivot-Andrews metric for each feature, which includes the statistical value, p-value (probability value), the number of used lags, and the number of observations. The p-value is later used to decide on the null hypothesis (the time series has a unit root and is non-stationary) based on a chosen level of significance.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high p-value can suggest high risk. This might indicate that there's insufficient evidence to reject the null hypothesis, which would mean the time series has a unit root and is therefore non-stationary.\n",
       "- Non-stationary time series data can lead to misleading statistics and unreliable machine learning models.\n",
       "\n",
       "**Strengths**:\n",
       "- The Zivot-Andrews Arch metric dynamically tests for stationarity against structural breaks in time series data, offering robust evaluation of stationarity in features.\n",
       "- This metric is especially beneficial with financial, economic, or other time-series data where data observations lack a consistent pattern and structural breaks may occur.\n",
       "\n",
       "**Limitations**:\n",
       "- The Zivot-Andrews Arch metric assumes that data is derived from a single-equation, autoregressive model. It may, therefore, not be appropriate for multivariate time series data or data which does not align with the autoregressive model assumption.\n",
       "- It might not take into account unexpected shocks or changes in the series trend which can both have a significant impact on the stationarity of the data.</td>\n",
       "      <td id=\"T_1ac33_row35_col3\" class=\"data row35 col3\" >validmind.model_validation.statsmodels.ZivotAndrewsArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row36_col0\" class=\"data row36 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row36_col1\" class=\"data row36 col1\" >Regression Model Outsample Comparison</td>\n",
       "      <td id=\"T_1ac33_row36_col2\" class=\"data row36 col2\" >**Purpose**: The RegressionModelOutsampleComparison test is designed to evaluate the predictive performance of multiple regression models by means of an out-of-sample test. The primary aim of this test is to validate the model's ability to generalize to unseen data, a common challenge in the context of overfitting. It does this by computing two critical metrics — Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), which provide a quantifiable measure of the model's prediction accuracy on the testing dataset.\n",
       "\n",
       "**Test Mechanism**: This test requires multiple models (specifically Ordinary Least Squares\n",
       "- OLS regression models) and a test dataset as inputs. Each model generates predictions using the test dataset. The residuals are then calculated and used to compute the MSE and RMSE for each model. The test outcomes, which include the model's name, its MSE, and RMSE, are recorded and returned in a structured dataframe format.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High values of MSE or RMSE indicate significant risk, signifying that the model's predictions considerably deviate from the actual values in the test dataset.\n",
       "- Consistently large discrepancies between training and testing performance across various models may indicate an issue with the input data itself or the model selection strategies employed.\n",
       "\n",
       "**Strengths**:\n",
       "- This test offers a comparative evaluation of multiple models' out-of-sample performance, enabling the selection of the best performing model.\n",
       "- The use of both MSE and RMSE provides insights into the model's prediction error. While MSE is sensitive to outliers, emphasizing larger errors, RMSE provides a more interpretable measure of average prediction error given that it's in the same unit as the dependent variable.\n",
       "\n",
       "**Limitations**:\n",
       "- The applicability of this test is limited to regression tasks, specifically OLS models.\n",
       "- The test operates under the assumption that the test dataset is a representative sample of the population. This might not always hold true and can result in less accurate insights.\n",
       "- The interpretability and the objectivity of the output (MSE and RMSE) can be influenced when the scale of the dependent variable varies significantly, or the distribution of residuals is heavily skewed or contains outliers.</td>\n",
       "      <td id=\"T_1ac33_row36_col3\" class=\"data row36 col3\" >validmind.model_validation.statsmodels.RegressionModelOutsampleComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row37_col0\" class=\"data row37 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row37_col1\" class=\"data row37 col1\" >Regression Model Forecast Plot Levels</td>\n",
       "      <td id=\"T_1ac33_row37_col2\" class=\"data row37 col2\" >**Purpose:** The `RegressionModelForecastPlotLevels` metric is designed to visually assess a series of regression models' performance. It achieves this by contrasting the models' forecasts with the observed data from the respective training and test datasets. The gauge of accuracy here involves determining the extent of closeness between forecasted and actual values. Accordingly, if any transformations are specified, the metric will handle transforming the data before making this comparison.\n",
       "\n",
       "**Test Mechanism:** The `RegressionModelForecastPlotLevels` class in Python initiates with a `transformation` parameter, which default aggregates to None. Initially, the class checks for the presence of model objects and raises a `ValueError` if none are found. Each model is then processed, creating predictive forecasts for both its training and testing datasets. These forecasts are then contrasted with the actual values and plotted. In situations where a specified transformation, like \"integrate,\" is specified, the class navigates the transformation steps (performing cumulative sums to generate a novel series, for instance). Finally, plots are produced that compare observed and forecasted values for both the raw and transformed datasets.\n",
       "\n",
       "**Signs of High Risk:** Indications of high risk or failure in the model's performance can be derived from checking the generated plots. When the forecasted values dramatically deviate from the observed values in either the training or testing datasets, it suggests a high risk situation. A significant deviation could be a symptom of either overfitting or underfitting, both scenarios are worrying. Such discrepancies could inhibit the model's ability to create precise, generalized results.\n",
       "\n",
       "**Strengths:**\n",
       "- Visual Evaluations: The metric provides a visual and comparative way of assessing multiple regression models at once. This allows easier interpretation and evaluation of their forecasting accuracy.\n",
       "- Transformation Handling: This metric can handle transformations like \"integrate,\" enhancing its breadth and flexibility in evaluating different models.\n",
       "- Detailed Perspective: By looking at the performance on both datasets (training and testing), the metric may give a detailed overview of the model.\n",
       "\n",
       "**Limitations:**\n",
       "- Subjectivity: Relying heavily on visual interpretations; assessments may differ from person to person.\n",
       "- Limited Transformation Capability: Currently, only the \"integrate\" transformation is supported, implying complex transformations might go unchecked or unhandled.\n",
       "- Overhead: The plotting mechanism may become computationally costly when applying to extensive datasets, increasing runtime.\n",
       "- Numerical Measurement: Although visualization is instrumental, a corresponding numerical measure would further reinforce the observations. However, this metric does not provide numerical measures.</td>\n",
       "      <td id=\"T_1ac33_row37_col3\" class=\"data row37 col3\" >validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row38_col0\" class=\"data row38 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row38_col1\" class=\"data row38 col1\" >Log Regression Confusion Matrix</td>\n",
       "      <td id=\"T_1ac33_row38_col2\" class=\"data row38 col2\" >**Purpose**: The Logistic Regression Confusion Matrix is a metric used to measure the performance of a logistic regression classification model. This metric is particularly useful for scenarios where a model's predictions are formulated by thresholding probabilities. The main advantage of this approach is that it includes true positives, true negatives, false positives, and false negatives in its assessment, providing a more comprehensive overview of the model's effectiveness in distinguishing between correct and incorrect classifications.\n",
       "\n",
       "**Test Mechanism**: The methodology behind the Logistic Regression Confusion Matrix uses the `sklearn.metrics.confusion_matrix` function from the Python library to generate a matrix. This matrix is created by comparing the model's predicted probabilities, which are initially converted to binary predictions using a predetermined cut-off threshold (default is 0.5), against the actual classes. The matrix's design consists of the predicted class labels forming the x-axis, and the actual class labels forming the y-axis, with each cell containing the record of true positives, true negatives, false positives, and false negatives respectively.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant number of false positives and false negatives, indicating that the model is incorrectly classifying instances.\n",
       "- The counts of true positives and true negatives being substantially lower than projected, positioning this as a potential high-risk indicator.\n",
       "\n",
       "**Strengths**:\n",
       "- Simple, intuitive, and provides a comprehensive understanding of the model's performance.\n",
       "- Provides a detailed breakdown of error types, improving transparency.\n",
       "- Offers flexible adaptation for diverse prediction scenarios by allowing adjustments to the cut-off threshold, and enabling exploration of trade-offs between precision (minimizing false positives) and recall (minimizing false negatives).\n",
       "\n",
       "**Limitations**:\n",
       "- Acceptable performance on majority classes but potential poor performance on minority classes in imbalanced datasets, as the confusion matrix may supply misleading results.\n",
       "- Lack of insight into the severity of the mistakes and the cost trade-off between different types of misclassification.\n",
       "- Selection of the cut-off threshold can significantly alter the interpretation, and a poorly chosen threshold may lead to erroneous conclusions.</td>\n",
       "      <td id=\"T_1ac33_row38_col3\" class=\"data row38 col3\" >validmind.model_validation.statsmodels.LogRegressionConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row39_col0\" class=\"data row39 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row39_col1\" class=\"data row39 col1\" >PD Rating Class Plot</td>\n",
       "      <td id=\"T_1ac33_row39_col2\" class=\"data row39 col2\" >**Purpose**: The purpose of the Probability of Default (PD) Rating Class Plot test is to measure and evaluate the distribution of calculated default probabilities across different rating classes. This is critical for understanding and inferring credit risk and can provide insights into how effectively the model is differentiating between different risk levels in a credit dataset.\n",
       "\n",
       "**Test Mechanism**: This metric is implemented via a visualization mechanism. It sorts the predicted probabilities of defaults into user-defined rating classes defined in \"rating_classes\" in default parameters. When it has classified the probabilities, it then calculates the average default rates within each rating class. Subsequently, it produces bar plots for each of these rating classes, illustrating the average likelihood of a default within each class. This process is executed separately for both the training and testing data sets. The classification of predicted probabilities utilizes the pandas \"cut\" function, sorting and sectioning the data values into bins.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- If lower rating classes present higher average likelihoods of default than higher rating classes\n",
       "- If there is poor differentiation between the averages across the different rating classes\n",
       "- If the model generates a significant contrast between the likelihoods for the training set and the testing set, suggestive of model overfitting\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Presents a clear visual representation of how efficient the model is at predicting credit risk across different risk levels\n",
       "- Allows for rapid identification and understanding of model performance per rating class\n",
       "- Highlights potential overfitting issues by including both training and testing datasets in the analysis\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Making an incorrect choice for the number of rating classes, either oversimplifying or overcomplicating the distribution of default rates\n",
       "- Relying on the assumption that the rating classes are effective at differentiating risk levels and that the boundaries between classes truly represent the risk distribution\n",
       "- Not accounting for data set class imbalance, which could cause skewed average probabilities\n",
       "- Inability to gauge the overall performance of the model only based on this metric, emphasizing the requirement of combining it with other evaluation metrics</td>\n",
       "      <td id=\"T_1ac33_row39_col3\" class=\"data row39 col3\" >validmind.model_validation.statsmodels.PDRatingClassPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row40_col0\" class=\"data row40 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row40_col1\" class=\"data row40 col1\" >Scorecard Histogram</td>\n",
       "      <td id=\"T_1ac33_row40_col2\" class=\"data row40 col2\" >**Purpose**: The Scorecard Histogram test metric provides a visual interpretation of the credit scores generated by a machine learning model for credit-risk classification tasks. It aims to compare the alignment of the model's scoring decisions with the actual outcomes of credit loan applications. It helps in identifying potential discrepancies between the model's predictions and real-world risk levels.\n",
       "\n",
       "**Test Mechanism**: This metric uses logistic regression to generate a histogram of credit scores for both default (negative class) and non-default (positive class) instances. Using both training and test datasets, the metric calculates the credit score of each instance with a scorecard method, considering the impact of different features on the likelihood of default. İncludes the default point to odds (PDO) scaling factor and predefined target score and odds settings. Histograms for training and test sets are computed and plotted separately to offer insights into the model's generalizability to unseen data.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Discrepancies between the distributions of training and testing data, indicating a model's poor generalisation ability\n",
       "- Skewed distributions favouring specific scores or classes, representing potential bias\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a visual interpretation of the model's credit scoring system, enhancing comprehension of model behavior\n",
       "- Enables a direct comparison between actual and predicted scores for both training and testing data\n",
       "- Its intuitive visualization helps understand the model's ability to differentiate between positive and negative classes\n",
       "- Can unveil patterns or anomalies not easily discerned through numerical metrics alone\n",
       "\n",
       "**Limitations**:\n",
       "- Despite its value for visual interpretation, it doesn't quantify the performance of the model, and therefore may lack precision for thorough model evaluation\n",
       "- The quality of input data can strongly influence the metric, as bias or noise in the data will affect both the score calculation and resultant histogram\n",
       "- Its specificity to credit scoring models limits its applicability across a wider variety of machine learning tasks and models\n",
       "- The metric's effectiveness is somewhat tied to the subjective interpretation of the analyst, since it relies on the analyst's judgment of the characteristics and implications of the plot.</td>\n",
       "      <td id=\"T_1ac33_row40_col3\" class=\"data row40 col3\" >validmind.model_validation.statsmodels.ScorecardHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row41_col0\" class=\"data row41 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row41_col1\" class=\"data row41 col1\" >Feature Importance And Significance</td>\n",
       "      <td id=\"T_1ac33_row41_col2\" class=\"data row41 col2\" >**Purpose**: The 'FeatureImportanceAndSignificance' test evaluates the statistical significance and the importance of features in the context of the machine learning model. By comparing the p-values from a regression model and the feature importances from a decision tree model, this test aids in determining the most significant variables from a statistical and a machine learning perspective, assisting in feature selection during the model development process.\n",
       "\n",
       "**Test Mechanism**: The test first compares the p-values from a regression model and the feature importances from a decision tree model. These values are normalized to ensure a uniform comparison. The 'p_threshold' parameter is used to determine what p-value is considered statistically significant and if the 'significant_only' parameter is true, only features with p-values below this threshold are included in the final output. The output from this test includes an interactive visualization displaying normalized p-values and the associated feature importances. The test throws an error if it does not receive both a regression model and a decision tree model.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Exceptionally high or low p-values, which suggest that a feature may not be significant or meaningful in the context of the model.\n",
       "- If many variables with small feature importance values have significant p-values, this could indicate that the model might be overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- Combines two perspectives statistical significance (p-values) and feature importance (decision tree model), making it a robust feature selection test.\n",
       "- Provides an interactive visualization making it easy to interpret and understand the results.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only works with a regression model and a decision tree model which may limit its applicability.\n",
       "- The test does not take into account potential correlations or causative relationships between features which may lead to misinterpretations of significance and importance.\n",
       "- Over-reliance on the p-value as a cut-off for feature significance can be seen as arbitrary and may not truly reflect the real-world importance of the feature.</td>\n",
       "      <td id=\"T_1ac33_row41_col3\" class=\"data row41 col3\" >validmind.model_validation.statsmodels.FeatureImportanceAndSignificance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row42_col0\" class=\"data row42 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row42_col1\" class=\"data row42 col1\" >L Jung Box</td>\n",
       "      <td id=\"T_1ac33_row42_col2\" class=\"data row42 col2\" >**Purpose**: The Ljung-Box test is a type of statistical test utilized to ascertain whether there are autocorrelations within a given dataset that differ significantly from zero. In the context of a machine learning model, this test is primarily used to evaluate data utilized in regression tasks, especially those involving time series and forecasting.\n",
       "\n",
       "**Test Mechanism**: The test operates by iterating over each feature within the training dataset and applying the `acorr_ljungbox` function from the `statsmodels.stats.diagnostic` library. This function calculates the Ljung-Box statistic and p-value for each feature. These results are then stored in a dictionary where the keys are the feature names and the values are dictionaries containing the statistic and p-value respectively. Generally, a lower p-value indicates a higher likelihood of significant autocorrelations within the feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk or failure in the model's performance relating to this test might be indicated by high Ljung-Box statistic values or low p-values.\n",
       "- These outcomes suggest the presence of significant autocorrelations in the respective features. If not properly consider or handle in the machine learning model, these can negatively affect model performance or bias.\n",
       "\n",
       "**Strengths**:\n",
       "- The Ljung-Box test is a powerful tool for detecting autocorrelations within datasets, especially in time series data.\n",
       "- It provides quantitative measures (statistic and p-value) that allow for precise evaluation of autocorrelation.\n",
       "- This test can be instrumental in avoiding issues related to autoregressive residuals and other challenges in regression models.\n",
       "\n",
       "**Limitations**:\n",
       "- The Ljung-Box test cannot detect all types of non-linearity or complex interrelationships among variables.\n",
       "- Testing individual features may not fully encapsulate the dynamics of the data if features interact with each other.\n",
       "- It is designed more for traditional statistical models and may not be fully compatible with certain types of complex machine learning models.</td>\n",
       "      <td id=\"T_1ac33_row42_col3\" class=\"data row42 col3\" >validmind.model_validation.statsmodels.LJungBox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row43_col0\" class=\"data row43 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row43_col1\" class=\"data row43 col1\" >Logistic Reg Prediction Histogram</td>\n",
       "      <td id=\"T_1ac33_row43_col2\" class=\"data row43 col2\" >**Purpose**: This code is designed to generate histograms that display the Probability of Default (PD) predictions for positive and negative classes in both the training and testing datasets. By doing so, it evaluates the performance of a logistic regression model, particularly in the context of credit risk prediction.\n",
       "\n",
       "**Test Mechanism**: The metric executes these steps to run the test:\n",
       "- Firstly, it extracts the target column from both the train and test datasets.\n",
       "- The model's predict function is then used to calculate probabilities.\n",
       "- These probabilities are added as a new column to the training and testing dataframes.\n",
       "- Histograms are generated for each class (0 or 1 in binary classification scenarios) within the training and testing datasets.\n",
       "- To enhance visualization, the histograms are set to have different opacities.\n",
       "- The four histograms (two for training data and two for testing) are overlaid on two different subplot frames (one for training and one for testing data).\n",
       "- The test returns a plotly graph object displaying the visualization.\n",
       "\n",
       "**Signs of High Risk**: Several indicators could suggest a high risk or failure in the model's performance. These include:\n",
       "- Significant discrepancies observed between the histograms of training and testing data.\n",
       "- Large disparities between the histograms for the positive and negative classes.\n",
       "- These issues could signal potential overfitting or bias in the model.\n",
       "- Unevenly distributed probabilities may also indicate that the model does not accurately predict outcomes.\n",
       "\n",
       "**Strengths**: This metric and test offer several benefits, including:\n",
       "- The visual representation of the PD predictions made by the model, which aids in understanding the model's behaviour.\n",
       "- The ability to assess both the training and testing datasets, adding depth to the validation of the model.\n",
       "- Highlighting disparities between multiple classes, providing potential insights into class imbalance or data skewness issues.\n",
       "- Particularly beneficial for credit risk prediction, it effectively visualizes the spread of risk across different classes.\n",
       "\n",
       "**Limitations**: Despite its strengths, the test has several limitations:\n",
       "- It is specifically tailored for binary classification scenarios, where the target variable only has two classes; as such, it isn't suited for multi-class classification tasks.\n",
       "- This metric is mainly applicable for logistic regression models. It might not be effective or accurate when used on other model types.\n",
       "- While the test provides a robust visual representation of the model's PD predictions, it does not provide a quantifiable measure or score to assess model performance.</td>\n",
       "      <td id=\"T_1ac33_row43_col3\" class=\"data row43 col3\" >validmind.model_validation.statsmodels.LogisticRegPredictionHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row44_col0\" class=\"data row44 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row44_col1\" class=\"data row44 col1\" >Jarque Bera</td>\n",
       "      <td id=\"T_1ac33_row44_col2\" class=\"data row44 col2\" >**Purpose**: The purpose of the Jarque-Bera test as implemented in this metric is to determine if the features in the dataset of a given Machine Learning model follows a normal distribution. This is crucial for understanding the distribution and behavior of the model's features, as numerous statistical methods assume normal distribution of the data.\n",
       "\n",
       "**Test Mechanism**: The test mechanism involves computing the Jarque-Bera statistic, p-value, skew, and kurtosis for each feature in the dataset. It utilizes the 'jarque_bera' function from the 'statsmodels' library in Python, storing the results in a dictionary. The test evaluates the skewness and kurtosis to ascertain whether the dataset follows a normal distribution. A significant p-value (typically less than 0.05) implies that the data does not possess normal distribution.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high Jarque-Bera statistic and a low p-value (usually less than 0.05) indicates high-risk conditions.\n",
       "- Such results suggest the data significantly deviates from a normal distribution. If a machine learning model expects feature data to be normally distributed, these findings imply that it may not function as intended.\n",
       "\n",
       "**Strengths**:\n",
       "- This test provides insights into the shape of the data distribution, helping determine whether a given set of data follows a normal distribution.\n",
       "- This is particularly useful for risk assessment for models that assume a normal distribution of data.\n",
       "- By measuring skewness and kurtosis, it provides additional insights into the nature and magnitude of a distribution's deviation.\n",
       "\n",
       "**Limitations**:\n",
       "- The Jarque-Bera test only checks for normality in the data distribution. It cannot provide insights into other types of distributions.\n",
       "- Datasets that aren't normally distributed but follow some other distribution might lead to inaccurate risk assessments.\n",
       "- The test is highly sensitive to large sample sizes, often rejecting the null hypothesis (that data is normally distributed) even for minor deviations in larger datasets.</td>\n",
       "      <td id=\"T_1ac33_row44_col3\" class=\"data row44 col3\" >validmind.model_validation.statsmodels.JarqueBera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row45_col0\" class=\"data row45 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row45_col1\" class=\"data row45 col1\" >Phillips Perron Arch</td>\n",
       "      <td id=\"T_1ac33_row45_col2\" class=\"data row45 col2\" >**Purpose**: The Phillips-Perron (PP) test is used to establish the order of integration in time series data, testing a null hypothesis that a time series is unit-root non-stationary. This is vital in forecasting and understanding the stochastic behavior of data within machine learning models. Essentially, the PP test aids in confirming the robustness of results and generating valid predictions from regression analysis models.\n",
       "\n",
       "**Test Mechanism**: The PP test is conducted for each feature in the dataset. A data frame is created from the dataset, and for each column in this frame, the PhillipsPerron method calculates the statistic value, p-value, used lags, and number of observations. This process computes the PP metric for each feature and stores the results for future reference.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high P-value could imply that the series has a unit root and is therefore non-stationary.\n",
       "- Test statistic values that surpass critical values indicate additional evidence of non-stationarity.\n",
       "- A high 'usedlag' value for a series could point towards autocorrelation issues which could further impede the model's performance.\n",
       "\n",
       "**Strengths**:\n",
       "- Resilience against heteroskedasticity in the error term is a significant strength of the PP test.\n",
       "- Its capacity to handle long time series data.\n",
       "- Its ability to determine whether the time series is stationary or not, influencing the selection of suitable models for forecasting.\n",
       "\n",
       "**Limitations**:\n",
       "- The PP test can only be employed within a univariate time series framework.\n",
       "- The test relies on asymptotic theory, which means the test's power can significantly diminish for small sample sizes.\n",
       "- The need to convert non-stationary time series into stationary series through differencing might lead to loss of vital data points.</td>\n",
       "      <td id=\"T_1ac33_row45_col3\" class=\"data row45 col3\" >validmind.model_validation.statsmodels.PhillipsPerronArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row46_col0\" class=\"data row46 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row46_col1\" class=\"data row46 col1\" >Kolmogorov Smirnov</td>\n",
       "      <td id=\"T_1ac33_row46_col2\" class=\"data row46 col2\" >**Purpose**: This metric employs the Kolmogorov-Smirnov (KS) test to evaluate the distribution of a dataset's features. It specifically gauges whether the data from each feature aligns with a normal distribution, a common presumption in many statistical methods and machine learning models.\n",
       "\n",
       "**Test Mechanism**: This KS test calculates the KS statistic and the corresponding p-value for each column in a dataset. It achieves this by contrasting the cumulative distribution function of the dataset's feature with an ideal normal distribution. Subsequently, a feature-by-feature KS statistic and p-value are stored in a dictionary. The specific threshold p-value (the value below which we reject the hypothesis that the data is drawn from a normal distribution) is not firmly set within this implementation, allowing for definitional flexibility depending on the specific application.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Elevated KS statistic for a feature combined with a low p-value. This suggests a significant divergence between the feature's distribution and a normal one.\n",
       "- Features with notable deviations. These could create problems if the applicable model makes assumptions about normal data distribution, thereby representing a risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The KS test is acutely sensitive to differences in the location and shape of the empirical cumulative distribution functions of two samples.\n",
       "- It is non-parametric and does not presuppose any specific data distribution, making it adaptable to a range of datasets.\n",
       "- With its focus on individual features, it offers detailed insights into data distribution.\n",
       "\n",
       "**Limitations**:\n",
       "- The sensitivity of the KS test to disparities in data distribution tails can be excessive. Such sensitivity might prompt false alarms about non-normal distributions, particularly in situations where these tail tendencies are irrelevant to the model.\n",
       "- It could become less effective when applied to multivariate distributions, considering that it's primarily configured for univariate distributions.\n",
       "- As a goodness-of-fit test, the KS test does not identify specific types of non-normality, such as skewness or kurtosis, that could directly impact model fitting.</td>\n",
       "      <td id=\"T_1ac33_row46_col3\" class=\"data row46 col3\" >validmind.model_validation.statsmodels.KolmogorovSmirnov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row47_col0\" class=\"data row47 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row47_col1\" class=\"data row47 col1\" >Residuals Visual Inspection</td>\n",
       "      <td id=\"T_1ac33_row47_col2\" class=\"data row47 col2\" >**Purpose**: The main purpose of this metric is to visualize and analyze the residuals (the differences between the observed and predicted values) of a regression problem. It allows for a graphical exploration of the model's errors, helping to identify statistical patterns or anomalies that may indicate a systematic bias in the model's predictions. By inspecting the residuals, we can check how well the model fits the data and meets the assumptions of the model.\n",
       "\n",
       "**Test Mechanism**: The metric generates four common types of residual plots which are: a histogram with kernel density estimation, a quantile-quantile (Q-Q) plot, a residuals series dot plot, and an autocorrelation function (ACF) plot.\n",
       "\n",
       "- The residuals histogram with kernel density estimation visualizes the distribution of residuals and allows to check if they are normally distributed.\n",
       "- Q-Q plot compares the observed quantiles of the data to the quantiles of a standard normal distribution, helping to assess the normality of residuals.\n",
       "- A residuals dot plot indicates the variation in residuals over time, which helps in identifying any time-related pattern in residuals.\n",
       "- ACF plot visualizes the correlation of an observation with its previous observations, helping to pinpoint any seasonality effect within residuals.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Skewness or asymmetry in the histogram or a significant deviation from the straight line in the Q-Q plot, which indicates that the residuals aren't normally distributed.\n",
       "- Large spikes in the ACF plot, indicating that the residuals are correlated, in violation of the assumption that they are independent.\n",
       "- Non-random patterns in the dot plot of residuals, indicating potential model misspecification.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Visual analysis of residuals is a powerful yet simple way to understand a model’s behavior across the data set and to identify problems with the model's assumptions or its fit to the data.\n",
       "- The test is applicable to any regression model, irrespective of complexity.\n",
       "- By exploring residuals, we might uncover relationships that were not captured by the model, revealing opportunities for model improvement.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Visual tests are largely subjective and can be open to interpretation. Clear-cut decisions about the model based solely on these plots may not be possible.\n",
       "- The metrics from the test do not directly infer the action based on the results; domain-specific knowledge and expert judgement is often required to interpret the results.\n",
       "- These plots can indicate a problem with the model but they do not necessarily reveal the nature or cause of the problem.\n",
       "- The test assumes that the error terms are identically distributed, which might not always be the case in real-world scenarios.</td>\n",
       "      <td id=\"T_1ac33_row47_col3\" class=\"data row47 col3\" >validmind.model_validation.statsmodels.ResidualsVisualInspection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row48_col0\" class=\"data row48 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row48_col1\" class=\"data row48 col1\" >Shapiro Wilk</td>\n",
       "      <td id=\"T_1ac33_row48_col2\" class=\"data row48 col2\" >**Purpose**: The Shapiro-Wilk test is utilized to investigate whether a particular dataset conforms to the standard normal distribution. This analysis is crucial in machine learning modeling because the normality of the data can profoundly impact the performance of the model. This metric is especially useful in evaluating various features of the dataset in both classification and regression tasks.\n",
       "\n",
       "**Test Mechanism**: The Shapiro-Wilk test is conducted on each feature column of the training dataset to determine if the data contained fall within the normal distribution. The test presents a statistic and a p-value, with the p-value serving to validate or repudiate the null hypothesis, which is that the tested data is normally distributed.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A p-value that falls below 0.05 signifies a high risk as it discards the null hypothesis, indicating that the data does not adhere to the normal distribution.\n",
       "- For machine learning models built on the presumption of data normality, such an outcome could result in subpar performance or incorrect predictions.\n",
       "\n",
       "**Strengths**:\n",
       "- The Shapiro-Wilk test is esteemed for its level of accuracy, thereby making it particularly well-suited to datasets of small to moderate sizes.\n",
       "- It proves its versatility through its efficient functioning in both classification and regression tasks.\n",
       "- By separately testing each feature column, the Shapiro-Wilk test can raise an alarm if a specific feature does not comply with the normality.\n",
       "\n",
       "**Limitations**:\n",
       "- The Shapiro-Wilk test's sensitivity can be a disadvantage as it often rejects the null hypothesis (i.e., data is normally distributed), even for minor deviations, especially in large datasets. This may lead to unwarranted 'false alarms' of high risk by deeming the data as not normally distributed even if it approximates normal distribution.\n",
       "- Exceptional care must be taken in managing missing data or outliers prior to testing as these can greatly skew the results.\n",
       "- Lastly, the Shapiro-Wilk test is not optimally suited for processing data with pronounced skewness or kurtosis.</td>\n",
       "      <td id=\"T_1ac33_row48_col3\" class=\"data row48 col3\" >validmind.model_validation.statsmodels.ShapiroWilk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row49_col0\" class=\"data row49 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row49_col1\" class=\"data row49 col1\" >Scorecard Bucket Histogram</td>\n",
       "      <td id=\"T_1ac33_row49_col2\" class=\"data row49 col2\" >**Purpose**: The 'Scorecard Bucket Histogram' is employed as a metric to evaluate the performance of a classification model, specifically in credit risk assessment. It categorizes model scores into different rating classes, and visualizes the distribution of scores or probabilities within each class. It essentially measures how different risk categories (classes) are distributed in the model scores and provides insight into the model's classification ability. This makes it particularly useful in credit scoring and risk modeling where understanding the probability of default is critical.\n",
       "\n",
       "**Test Mechanism**: The test works by computing the probabilities for each record in the test and train dataset using the model's predict function. Subsequently, it calculates the scores using a formula incorporating target score, target odds, and points to double odds (PDO). The scores are then bucketed into predefined rating classes (such as 'A', 'B', 'C', 'D') and plotted in a histogram for both the train and test datasets. The target score, target odds, points to double the odds (PDO), and rating classes are customizable parameters, providing flexibility in test metrics based on differing model or industry norms.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Disproportionate scores within rating classes\n",
       "- Excessive overlap between classes\n",
       "- Inconsistent distribution of scores between the training and testing datasets\n",
       "\n",
       "If the model is accurately classifying and risk is being evenly distributed, we would anticipate smooth and relatively balanced histograms within classes.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Provides a quick visual snapshot of score distribution\n",
       "- Breaks down complex predictions into simple, understandable classes, making it easily interpretable for both technical and non-technical audiences\n",
       "- Caters to customization of parameters\n",
       "- Gives ownership of the class definitions to the user\n",
       "- Useful in the field of credit risk, providing a clear understanding of which class or 'bucket' a potential borrower belongs to\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Relies on manual setting of classes and other parameters (like target score, target odds, and PDO), potentially leading to arbitrary classifications and potential bias if not judiciously performed\n",
       "- Effectiveness can be limited with non-tabular data\n",
       "- Doesn't provide a numerical value easily compared across different models or runs as the output is primarily visual\n",
       "- Might not present a complete view of model performance and should be used in conjunction with other metrics</td>\n",
       "      <td id=\"T_1ac33_row49_col3\" class=\"data row49 col3\" >validmind.model_validation.statsmodels.ScorecardBucketHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row50_col0\" class=\"data row50 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row50_col1\" class=\"data row50 col1\" >Regression Model Insample Comparison</td>\n",
       "      <td id=\"T_1ac33_row50_col2\" class=\"data row50 col2\" >**Purpose**: The RegressionModelInsampleComparison test metric is utilized to evaluate and compare the performance of multiple regression models trained on the same dataset. Key performance indicators for this comparison include statistics related to the goodness of fit\n",
       "- R-Squared, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
       "\n",
       "**Test Mechanism**: The methodology behind this test is as follows\n",
       "- - Firstly, a verification that the list of models to be tested is indeed not empty occurs.\n",
       "- Once confirmed, the In-Sample performance of the models is calculated by a private function, `_in_sample_performance_ols`, that executes the following steps:\n",
       "- Iterates through each model in the supplied list.\n",
       "- For each model, the function extracts the features (`X`) and the target (`y_true`) from the training dataset and computes the predicted target values (`y_pred`).\n",
       "- The performance metrics for the model are calculated using formulas for R-Squared, Adjusted R-Squared, MSE, and RMSE.\n",
       "- The results, including the computed metrics, variables of the model, and the model's identifier, are stored in a dictionary that is appended to a list.\n",
       "- The collected results are finally returned as a pandas dataframe.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Significantly low values for R-Squared or Adjusted R-Squared.\n",
       "- Significantly high values for MSE and RMSE. Please note that what constitutes as \"low\" or \"high\" will vary based on the specific context or domain in which the model is being utilized.\n",
       "\n",
       "**Strengths**:\n",
       "- Enables comparison of in-sample performance across different models on the same dataset, providing insights into which model fits the data the best.\n",
       "- Utilizes multiple evaluation methods (R-Squared, Adjusted R-Squared, MSE, RMSE), offering a comprehensive review of a model's performance.\n",
       "\n",
       "**Limitations**:\n",
       "- The test measures only in-sample performance, i.e., how well a model fits the data it was trained on. However, it does not give any information on the performance of the model on new, unseen, or out-of-sample data.\n",
       "- Higher in-sample performance might be a result of overfitting, where the model is just memorizing the training data. This test is sensitive to such cases.\n",
       "- The test does not consider additional key factors such as the temporal dynamics of the data, that is, the pattern of changes in data over time.\n",
       "- The test does not provide an automated mechanism to determine if the reported metrics are within acceptable ranges, necessitating human judgment.</td>\n",
       "      <td id=\"T_1ac33_row50_col3\" class=\"data row50 col3\" >validmind.model_validation.statsmodels.RegressionModelInsampleComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row51_col0\" class=\"data row51 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row51_col1\" class=\"data row51 col1\" >Regression Feature Significance</td>\n",
       "      <td id=\"T_1ac33_row51_col2\" class=\"data row51 col2\" >**Purpose**: The Regression Feature Significance metric assesses the significance of each feature in a given set of regression models. It creates a visualization displaying p-values for every feature of each model, assisting model developers in understanding which features are most influential in their models.\n",
       "\n",
       "**Test Mechanism**: The test mechanism involves going through each fitted regression model in a given list, extracting the model coefficients and p-values for each feature, and then plotting these values. The x-axis on the plot contains the p-values while the y-axis denotes the coefficients of each feature. A vertical red line is drawn at the threshold for p-value significance, which is 0.05 by default. Any features with p-values to the left of this line are considered statistically significant at the chosen level.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Any feature with a high p-value (greater than the threshold) is considered a potential high risk, as it suggests the feature is not statistically significant and may not be reliably contributing to the model's predictions.\n",
       "- A high number of such features may indicate problems with the model validation, variable selection, and overall reliability of the model predictions.\n",
       "\n",
       "**Strengths**:\n",
       "- Helps identify the features that significantly contribute to a model's prediction, providing insights into the feature importance.\n",
       "- Provides tangible, easy-to-understand visualizations to interpret the feature significance.\n",
       "- Facilitates comparison of feature importance across multiple models.\n",
       "\n",
       "**Limitations**:\n",
       "- This metric assumes model features are independent, which may not always be the case. Multicollinearity (high correlation amongst predictors) can cause high variance and unreliable statistical tests of significance.\n",
       "- The p-value strategy for feature selection doesn't take into account the magnitude of the effect, focusing solely on whether the feature is likely non-zero.\n",
       "- This test is specific to regression models and wouldn't be suitable for other types of ML models.\n",
       "- P-value thresholds are somewhat arbitrary and do not always indicate practical significance, only statistical significance.</td>\n",
       "      <td id=\"T_1ac33_row51_col3\" class=\"data row51 col3\" >validmind.model_validation.statsmodels.RegressionFeatureSignificance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row52_col0\" class=\"data row52 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row52_col1\" class=\"data row52 col1\" >Regression Model Summary</td>\n",
       "      <td id=\"T_1ac33_row52_col2\" class=\"data row52 col2\" >**Purpose**: This metric test evaluates the performance of regression models by measuring their predictive ability with regards to dependent variables given changes in the independent variables. Its measurement tools include conventional regression metrics such as R-Squared, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
       "\n",
       "**Test Mechanism**: This test employs the 'train_ds' attribute of the model to gather and analyze the training data. Initially, it fetches the independent variables and uses the model to make predictions on these given features. Subsequently, it calculates several standard regression performance metrics including R-Square, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE), which quantify the approximation of the predicted responses to the actual responses.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low R-Squared and Adjusted R-Squared values. A poor fit between the model predictions and the true responses is indicated by low values of these metrics, suggesting the model explains a small fraction of the variance in the target variable.\n",
       "- High MSE and RMSE values represent a high prediction error and point to poor model performance.\n",
       "\n",
       "**Strengths**:\n",
       "- Offers an extensive evaluation of regression models by combining four key measures of model accuracy and fit.\n",
       "- Provides a comprehensive view of the model's performance.\n",
       "- Both the R-Squared and Adjusted R-Squared measures are readily interpretable. They represent the proportion of total variation in the dependent variable that can be explained by the independent variables.\n",
       "\n",
       "**Limitations**:\n",
       "- Applicable exclusively to regression models. It is not suited for evaluating binary classification models or time series models, thus limiting its scope.\n",
       "- Although RMSE and MSE are sound measures of prediction error, they might be sensitive to outliers, potentially leading to an overestimation of the model's prediction error.\n",
       "- A high R-squared or adjusted R-squared may not necessarily indicate a good model, especially in cases where the model is possibly overfitting the data.</td>\n",
       "      <td id=\"T_1ac33_row52_col3\" class=\"data row52 col3\" >validmind.model_validation.statsmodels.RegressionModelSummary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row53_col0\" class=\"data row53 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row53_col1\" class=\"data row53 col1\" >KPSS</td>\n",
       "      <td id=\"T_1ac33_row53_col2\" class=\"data row53 col2\" >**Purpose**: The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test is utilized to ensure the stationarity of data within the machine learning model. It specifically works on time-series data to establish the order of integration, which is a prime requirement for accurate forecasting, given the fundamental condition for any time series model is that the series should be stationary.\n",
       "\n",
       "**Test Mechanism**: This metric evaluates the KPSS score for every feature present in the dataset. Within the KPSS score, there are various components, namely: a statistic, a p-value, a used lag, and critical values. The core scheme behind the KPSS score is to test the hypothesis that an observable time series is stationary around a deterministic trend. If the computed statistic surpasses the critical value, the null hypothesis is dismissed, inferring the series is non-stationary.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High KPSS score represents a considerable risk, particularly if the calculated statistic is higher than the critical value.\n",
       "- If the null hypothesis is rejected and the series is recognized as non-stationary, it heavily influences the model's forecasting capability rendering it less effective.\n",
       "\n",
       "**Strengths**:\n",
       "- The KPSS test directly measures the stationarity of a series, allowing it to fulfill a key prerequisite for many time-series models, making it a valuable tool for model validation.\n",
       "- The logics underpinning the test are intuitive and simple, making it understandable and accessible for developers and risk management teams.\n",
       "\n",
       "**Limitations**:\n",
       "- The KPSS test presumes the absence of a unit root in the series and does not differentiate between series that are stationary and those border-lining stationarity.\n",
       "- The test might show restricted power against specific alternatives.\n",
       "- The reliability of the test is contingent on the number of lags selected, which introduces potential bias in the measurement.</td>\n",
       "      <td id=\"T_1ac33_row53_col3\" class=\"data row53 col3\" >validmind.model_validation.statsmodels.KPSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row54_col0\" class=\"data row54 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row54_col1\" class=\"data row54 col1\" >Lilliefors</td>\n",
       "      <td id=\"T_1ac33_row54_col2\" class=\"data row54 col2\" >**Purpose**: The purpose of this metric is to utilize the Lilliefors test, named in honor of the Swedish statistician Hubert Lilliefors, in order to assess whether the features of the machine learning model's training dataset conform to a normal distribution. This is done because the assumption of normal distribution plays a vital role in numerous statistical procedures as well as numerous machine learning models. Should the features fail to follow a normal distribution, some model types may not operate at optimal efficiency. This can potentially lead to inaccurate predictions.\n",
       "\n",
       "**Test Mechanism**: The application of this test happens across all feature columns within the training dataset. For each feature, the Lilliefors test returns a test statistic and p-value. The test statistic quantifies how far the feature's distribution is from an ideal normal distribution, whereas the p-value aids in determining the statistical relevance of this deviation. The final results are stored within a dictionary, the keys of which correspond to the name of the feature column, and the values being another dictionary which houses the test statistic and p-value.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- If the p-value corresponding to a specific feature sinks below a pre-established significance level, generally set at 0.05, then it can be deduced that the distribution of that feature significantly deviates from a normal distribution. This can present a high risk for models that assume normality, as these models may perform inaccurately or inefficiently in the presence of such a feature.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- One advantage of the Lilliefors test is its utility irrespective of whether the mean and variance of the normal distribution are known in advance. This makes it a more robust option in real-world situations where these values might not be known.\n",
       "- Second, the test has the ability to screen every feature column, offering a holistic view of the dataset.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Despite the practical applications of the Lilliefors test in validating normality, it does come with some limitations.\n",
       "- Firstly, it is only capable of testing unidimensional data, thus rendering it ineffective for datasets with interactions between features or multi-dimensional phenomena.\n",
       "- Additionally, the test might not be as sensitive as some other tests (like the Anderson-Darling test) in detecting deviations from a normal distribution.\n",
       "- Lastly, like any other statistical test, Lilliefors test may also produce false positives or negatives. Hence, banking solely on this test, without considering other characteristics of the data, may give rise to risks.</td>\n",
       "      <td id=\"T_1ac33_row54_col3\" class=\"data row54 col3\" >validmind.model_validation.statsmodels.Lilliefors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row55_col0\" class=\"data row55 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row55_col1\" class=\"data row55 col1\" >Logistic Reg Cumulative Prob</td>\n",
       "      <td id=\"T_1ac33_row55_col2\" class=\"data row55 col2\" >**Purpose**: This metric is utilized to evaluate the distribution of predicted probabilities for positive and negative classes in a logistic regression model. It's not solely intended to measure the model's performance but also provides a visual assessment of the model's behavior by plotting the cumulative probabilities for positive and negative classes across both the training and test datasets.\n",
       "\n",
       "**Test Mechanism**: The logistic regression model is evaluated by first computing the predicted probabilities for each instance in both the training and test datasets, which are then added as a new column in these sets. The cumulative probabilities for positive and negative classes are subsequently calculated and sorted in ascending order. Cumulative distributions of these probabilities are created for both positive and negative classes across both training and test datasets. These cumulative probabilities are represented visually in a plot, containing two subplots\n",
       "- one for the training data and the other for the test data, with lines representing cumulative distributions of positive and negative classes.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Imbalanced distribution of probabilities for either positive or negative classes.\n",
       "- Notable discrepancies or significant differences between the cumulative probability distributions for the training data versus the test data.\n",
       "- Marked discrepancies or large differences between the cumulative probability distributions for positive and negative classes.\n",
       "\n",
       "**Strengths**:\n",
       "- It offers not only numerical probabilities but also provides a visual illustration of data, which enhances the ease of understanding and interpreting the model's behavior.\n",
       "- Allows for the comparison of model's behavior across training and testing datasets, providing insights about how well the model is generalized.\n",
       "- It differentiates between positive and negative classes and their respective distribution patterns, which can aid in problem diagnosis.\n",
       "\n",
       "**Limitations**:\n",
       "- Exclusive to classification tasks and specifically to logistic regression models.\n",
       "- Graphical results necessitate human interpretation and may not be directly applicable for automated risk detection.\n",
       "- The method does not give a solitary quantifiable measure of model risk, rather it offers a visual representation and broad distributional information.\n",
       "- If the training and test datasets are not representative of the overall data distribution, the metric could provide misleading results.</td>\n",
       "      <td id=\"T_1ac33_row55_col3\" class=\"data row55 col3\" >validmind.model_validation.statsmodels.LogisticRegCumulativeProb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row56_col0\" class=\"data row56 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row56_col1\" class=\"data row56 col1\" >Runs Test</td>\n",
       "      <td id=\"T_1ac33_row56_col2\" class=\"data row56 col2\" >**Purpose**: The Runs Test is a statistical procedure used to determine whether the sequence of data extracted from the ML model behaves randomly or not. Specifically, it analyzes runs, sequences of consecutive positives or negatives, in the data to check if there are more or fewer runs than expected under the assumption of randomness. This can be an indication of some pattern, trend, or cycle in the model's output which may need attention.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism applies the Runs Test from the statsmodels module on each column of the training dataset. For every feature in the dataset, a Runs Test is executed, whose output includes a Runs Statistic and P-value. A low P-value suggests that data arrangement in the feature is not likely to be random. The results are stored in a dictionary where the keys are the feature names, and the values are another dictionary storing the test statistic and the P-value for each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk is indicated when the P-value is close to zero.\n",
       "- If the p-value is less than a predefined significance level (like 0.05), it suggests that the runs (series of positive or negative values) in the model's output are not random and are longer or shorter than what is expected under a random scenario.\n",
       "- This would mean there's a high risk of non-random distribution of errors or model outcomes, suggesting potential issues with the model.\n",
       "\n",
       "**Strengths**:\n",
       "- The strength of the Runs Test is that it's straightforward and fast for detecting non-random patterns in data sequence.\n",
       "- It can validate assumptions of randomness, which is particularly valuable for checking error distributions in regression models, trendless time series data, and making sure a classifier doesn't favour one class over another.\n",
       "- Moreover, it can be applied to both classification and regression tasks, making it versatile.\n",
       "\n",
       "**Limitations**:\n",
       "- The test assumes that the data is independently and identically distributed (i.i.d.), which might not be the case for many real-world datasets.\n",
       "- The conclusion drawn from the low p-value indicating non-randomness does not provide information about the type or the source of the detected pattern.\n",
       "- Also, it is sensitive to extreme values (outliers), and overly large or small run sequences can influence the results.\n",
       "- Furthermore, this test does not provide model performance evaluation; it is used to detect patterns in the sequence of outputs only.</td>\n",
       "      <td id=\"T_1ac33_row56_col3\" class=\"data row56 col3\" >validmind.model_validation.statsmodels.RunsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row57_col0\" class=\"data row57 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row57_col1\" class=\"data row57 col1\" >Scorecard Probabilities Histogram</td>\n",
       "      <td id=\"T_1ac33_row57_col2\" class=\"data row57 col2\" >**Purpose**: The Scorecard Probabilities Histogram, a specific metric used within the credit risk domain, is designed to evaluate and visualize risk classification of a model. It aims at examining the distribution of the probability of default across varied score buckets, with the score buckets being categories that entities (e.g., loan applicants) are classed under based on their predicted default risks. The key idea is to ensure that the model accurately classifies entities into appropriate risk categories (score buckets) and aptly represents their default probabilities.\n",
       "\n",
       "**Test Mechanism**: The mechanism behind the Scorecard Probabilities Histogram includes several steps. It starts with the calculation of default probabilities by the 'compute_probabilities' method, where the resulting probability is added as a fresh column to the input dataset. Following that, scores are computed using these probabilities, a target score, target odds, and a Points to Double the odds (pdo) factor by the 'compute_scores' method. These scores are then bucketed via the 'compute_buckets' method. A histogram is then plotted for each score bucket, with default probabilities as the x-axis and their frequency as the y-axis\n",
       "- implemented within the 'plot_probabilities_histogram' method. This entire process is executed distinctly for both training and testing datasets.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant overlap of different score buckets in the histogram indicates that the model is not efficiently distinguishing between various risk categories.\n",
       "- If very high or low probabilities are commonplace across all buckets, the model's predictions could be skewed.\n",
       "\n",
       "**Strengths**:\n",
       "- The Scorecard Probabilities Histogram allows for the visualization and analysis of the predicted default risk distribution across different risk classes, thereby facilitating a visual inspection of the model's performance and calibration for various risk categories.\n",
       "- It provides a means to visualize how these classifications are distributed on the training and testing datasets separately, contributing to a better comprehension of model generalization.\n",
       "\n",
       "**Limitations**:\n",
       "- The Scorecard Probabilities Histogram assumes linear and equally spaced risk categories, which might not always hold true.\n",
       "- If there are too few or too many score buckets, the visualization may not convey sufficient information.\n",
       "- While it effectively illustrates the distribution of probabilities, it does not provide adequate numerical metrics or threshold to definitively evaluate the model's performance. A more accurate evaluation necessitates its usage in conjunction with other metrics and tools including the confusion matrix, AUC-ROC, Precision, Recall, and so forth.</td>\n",
       "      <td id=\"T_1ac33_row57_col3\" class=\"data row57 col3\" >validmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row58_col0\" class=\"data row58 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row58_col1\" class=\"data row58 col1\" >DFGLS Arch</td>\n",
       "      <td id=\"T_1ac33_row58_col2\" class=\"data row58 col2\" >**Purpose**: The Dickey-Fuller GLS (DFGLS) Arch metric is utilized to determine the order of integration in time series data. For machine learning models dealing with time series and forecasting, this metric evaluates the existence of a unit root, thereby checking whether a time series is non-stationary. This analysis is a crucial initial step when dealing with time series data.\n",
       "\n",
       "**Test Mechanism**: This code implements the Dickey-Fuller GLS unit root test on each attribute of the dataset. This process involves iterating through every column of the dataset and applying the DFGLS test to assess the presence of a unit root. The resulting information, including the test statistic ('stat'), the p-value ('pvalue'), the quantity of lagged differences utilized in the regression ('usedlag'), and the number of observations ('nobs'), is subsequently stored.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high p-value for the DFGLS test represents a high risk. Specifically, a p-value above a typical threshold of 0.05 suggests that the time series data is quite likely to be non-stationary, thus presenting a high risk for generating unreliable forecasts.\n",
       "\n",
       "**Strengths**:\n",
       "- The Dickey-Fuller GLS test is a potent tool for checking the stationarity of time series data.\n",
       "- It helps to verify the assumptions of the models before the actual construction of the machine learning models proceeds.\n",
       "- The results produced by this metric offer a clear insight into whether the data is appropriate for specific machine learning models, especially those demanding the stationarity of time series data.\n",
       "\n",
       "**Limitations**:\n",
       "- Despite its benefits, the DFGLS test does present some drawbacks. It can potentially lead to inaccurate conclusions if the time series data incorporates a structural break.\n",
       "- If the time series tends to follow a trend while still being stationary, the test might misinterpret it, necessitating further detrending.\n",
       "- The test also presents challenges when dealing with shorter time series data or volatile data, not producing reliable results in these cases.</td>\n",
       "      <td id=\"T_1ac33_row58_col3\" class=\"data row58 col3\" >validmind.model_validation.statsmodels.DFGLSArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row59_col0\" class=\"data row59 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row59_col1\" class=\"data row59 col1\" >Auto ARIMA</td>\n",
       "      <td id=\"T_1ac33_row59_col2\" class=\"data row59 col2\" >**Purpose**: The AutoARIMA validation test is designed to evaluate and rank AutoRegressive Integrated Moving Average (ARIMA) models. These models are primarily used for forecasting time-series data. The validation test automatically fits multiple ARIMA models, with varying parameters, to every variable within the given dataset. The models are then ranked based on their Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) values, which provide a basis for the efficient model selection process.\n",
       "\n",
       "**Test Mechanism**: This metric proceeds by generating an array of feasible combinations of ARIMA model parameters which are within a prescribed limit. These limits include `max_p`, `max_d`, `max_q`; they represent the autoregressive, differencing, and moving average components respectively. Upon applying these sets of parameters, the validation test fits each ARIMA model to the time-series data provided. For each model, it subsequently proceeds to calculate and record both the BIC and AIC values, which serve as performance indicators for the model fit. Prior to this parameter fitting process, the Augmented Dickey-Fuller test for data stationarity is conducted on the data series. If a series is found to be non-stationary, a warning message is sent out, given that ARIMA models necessitate input series to be stationary.\n",
       "\n",
       "**Signs of High Risk**: * If the p-value of the Augmented Dickey-Fuller test for a variable exceeds 0.05, a warning is logged. This warning indicates that the series might not be stationary, leading to potentially inaccurate results. * Consistent failure in fitting ARIMA models (as made evident through logged errors) might disclose issues with either the data or model stability.\n",
       "\n",
       "**Strengths**: * The AutoARIMA validation test simplifies the often complex task of selecting the most suitable ARIMA model based on BIC and AIC criteria. * The mechanism incorporates a check for non-stationarity within the data, which is a critical prerequisite for ARIMA models. * The exhaustive search through all possible combinations of model parameters enhances the likelihood of identifying the best-fit model.\n",
       "\n",
       "**Limitations**: * This validation test can be computationally costly as it involves creating and fitting multiple ARIMA models for every variable. * Although the test checks for non-stationarity and logs warnings where present, it does not apply any transformations to the data to establish stationarity. * The selection of models leans solely on BIC and AIC criteria, which may not yield the best predictive model in all scenarios. * The test is only applicable to regression tasks involving time-series data, and may not work effectively for other types of machine learning tasks.</td>\n",
       "      <td id=\"T_1ac33_row59_col3\" class=\"data row59 col3\" >validmind.model_validation.statsmodels.AutoARIMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row60_col0\" class=\"data row60 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row60_col1\" class=\"data row60 col1\" >ADF Test</td>\n",
       "      <td id=\"T_1ac33_row60_col2\" class=\"data row60 col2\" >**Purpose**: The Augmented Dickey-Fuller (ADF) metric test is designed to evaluate the presence of a unit root in a time series. This essentially translates to assessing the stationarity of a time series dataset. This is vital in time series analysis, regression tasks, and forecasting, as these often need the data to be stationary.\n",
       "\n",
       "**Test Mechanism**: This test application utilizes the \"adfuller\" function from Python's “statsmodels” library. It applies this function to each column of the training dataset, subsequently calculating the ADF statistic, p-value, the number of lags used, and the number of observations in the sample for each column. If a column's p-value is lower than the predetermined threshold (usually 0.05), the series is considered stationary, and the test is deemed passed for that column.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A p-value that surpasses the threshold value indicates a high risk or potential model performance issue.\n",
       "- A high p-value suggests that the null hypothesis (of a unit root being present) cannot be rejected. This in turn suggests that the series is non-stationary which could potentially yield unreliable and falsified results for the model's performance and forecast.\n",
       "\n",
       "**Strengths**:\n",
       "- Archetypal Test for Stationarity: The ADF test is a comprehensive approach towards testing the stationarity of time series data. Such testing is vital for many machine learning and statistical models.\n",
       "- Detailed Output: The function generates detailed output, including the number of lags used and the number of observations, which adds to understanding a series’ behaviour.\n",
       "\n",
       "**Limitations**:\n",
       "- Dependence on Threshold: The result of this test freights heavily on the threshold chosen. Hence, an imprudent threshold value might lead to false acceptance or rejection of the null hypothesis.\n",
       "- Not Effective for Trending Data: The test suffers when it operates under the assumption that the data does not encapsulate any deterministic trend. In the presence of such a trend, it might falsely identify a series as non-stationary.\n",
       "- Potential for False Positives: The ADF test especially in the case of larger datasets, tends to reject the null hypothesis, escalating the chances of false positives.</td>\n",
       "      <td id=\"T_1ac33_row60_col3\" class=\"data row60 col3\" >validmind.model_validation.statsmodels.ADFTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row61_col0\" class=\"data row61 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row61_col1\" class=\"data row61 col1\" >GINI Table</td>\n",
       "      <td id=\"T_1ac33_row61_col2\" class=\"data row61 col2\" >**Purpose**: The 'GINITable' metric is designed to evaluate the performance of a classification model by emphasizing its discriminatory power. Specifically, it calculates and presents three important metrics\n",
       "- the Area under the ROC Curve (AUC), the GINI coefficient, and the Kolmogov-Smirnov (KS) statistic\n",
       "- for both training and test datasets.\n",
       "\n",
       "**Test Mechanism**: Using a dictionary for storing performance metrics for both the training and test datasets, the 'GINITable' metric calculates each of these metrics sequentially. The Area under the ROC Curve (AUC) is calculated via the `roc_auc_score` function from the Scikit-Learn library. The GINI coefficient, a measure of statistical dispersion, is then computed by doubling the AUC and subtracting 1. Finally, the Kolmogov-Smirnov (KS) statistic is calculated via the `roc_curve` function from Scikit-Learn, with the False Positive Rate (FPR) subtracted from the True Positive Rate (TPR) and the maximum value taken from the resulting data. These metrics are then stored in a pandas DataFrame for convenient visualization.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low values for performance metrics may suggest a reduction in model performance, particularly a low AUC which indicates poor classification performance, or a low GINI coefficient, which could suggest a decreased ability to discriminate different classes.\n",
       "- A high KS value may be an indicator of potential overfitting, as this generally signifies a substantial divergence between positive and negative distributions.\n",
       "- Significant discrepancies between the performance on the training dataset and the test dataset may present another signal of high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- Offers three key performance metrics (AUC, GINI, and KS) in one test, providing a more comprehensive evaluation of the model.\n",
       "- Provides a direct comparison between the model's performance on training and testing datasets, which aids in identifying potential underfitting or overfitting.\n",
       "- The applied metrics are class-distribution invariant, thereby remaining effective for evaluating model performance even when dealing with imbalanced datasets.\n",
       "- Presents the metrics in a user-friendly table format for easy comprehension and analysis.\n",
       "\n",
       "**Limitations**:\n",
       "- The GINI coefficient and KS statistic are both dependent on the AUC value. Therefore, any errors in the calculation of the latter will adversely impact the former metrics too.\n",
       "- Mainly suited for binary classification models and may require modifications for effective application in multi-class scenarios.\n",
       "- The metrics used are threshold-dependent and may exhibit high variability based on the chosen cut-off points.\n",
       "- The test does not incorporate a method to efficiently handle missing or inefficiently processed data, which could lead to inaccuracies in the metrics if the data is not appropriately preprocessed.</td>\n",
       "      <td id=\"T_1ac33_row61_col3\" class=\"data row61 col3\" >validmind.model_validation.statsmodels.GINITable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row62_col0\" class=\"data row62 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row62_col1\" class=\"data row62 col1\" >Regression Model Forecast Plot</td>\n",
       "      <td id=\"T_1ac33_row62_col2\" class=\"data row62 col2\" >**Purpose:** The \"regression_forecast_plot\" is intended to visually depict the performance of one or more regression models by comparing the model's forecasted outcomes against actual observed values within a specified date range. This metric is especially useful in time-series models or any model where the outcome changes over time, allowing direct comparison of predicted vs actual values.\n",
       "\n",
       "**Test Mechanism:** This test generates a plot for each fitted model in the list. The x-axis represents the date ranging from the specified \"start_date\" to the \"end_date\", while the y-axis shows the value of the outcome variable. Two lines are plotted: one representing the forecasted values and the other representing the observed values. The \"start_date\" and \"end_date\" can be parameters of this test; if these parameters are not provided, they are set to the minimum and maximum date available in the dataset. The test verifies that the provided date range is within the limits of the available data.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- High risk or failure signs could be deduced visually from the plots if the forecasted line significantly deviates from the observed line, indicating the model's predicted values are not matching actual outcomes.\n",
       "- A model that struggles to handle the edge conditions like maximum and minimum data points could also be considered a sign of risk.\n",
       "\n",
       "**Strengths:**\n",
       "- Visualization: The plot provides an intuitive and clear illustration of how well the forecast matches the actual values, making it straightforward even for non-technical stakeholders to interpret.\n",
       "- Flexibility: It allows comparison for multiple models and for specified time periods.\n",
       "- Model Evaluation: It can be useful in identifying overfitting or underfitting situations, as these will manifest as discrepancies between the forecasted and observed values.\n",
       "\n",
       "**Limitations:**\n",
       "- Interpretation Bias: Interpretation of the plot is subjective and can lead to different conclusions by different evaluators.\n",
       "- Lack of Precision: Visual representation might not provide precise values of the deviation.\n",
       "- Inapplicability: Limited to cases where the order of data points (time-series) matters, it might not be of much use in problems that are not related to time series prediction.</td>\n",
       "      <td id=\"T_1ac33_row62_col3\" class=\"data row62 col3\" >validmind.model_validation.statsmodels.RegressionModelForecastPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row63_col0\" class=\"data row63 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row63_col1\" class=\"data row63 col1\" >ADF</td>\n",
       "      <td id=\"T_1ac33_row63_col2\" class=\"data row63 col2\" >**Purpose**: The Augmented Dickey-Fuller (ADF) test metric is used here to determine the order of integration, i.e., the stationarity of a given time series data. The stationary property of data is pivotal in many machine learning models as it impacts the reliability and effectiveness of predictions and forecasts.\n",
       "\n",
       "**Test Mechanism**: The ADF test starts by executing the ADF function from the statsmodels library on every feature of the dataset. Multiple outputs are generated for each run, including the ADF test statistic and p-value, count of lags used, the number of observations factored into the test, critical values at various confidence levels, and the maximized information criterion. These results are stored for each feature for subsequent analysis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- An inflated ADF statistic and high p-value (generally above 0.05) insinuate a high risk to the model's performance due to the presence of a unit root indicating non-stationarity.\n",
       "- Such non-stationarity might result in untrustworthy or insufficient forecasts.\n",
       "\n",
       "**Strengths**:\n",
       "- The ADF test is robust to more sophisticated correlation within the data, which empowers it to be deployed in settings where data might display complex stochastic behavior.\n",
       "- The ADF test provides explicit outputs like test statistics, critical values, and information criterion, thereby enhancing our understanding and transparency of the model validation process.\n",
       "\n",
       "**Limitations**:\n",
       "- The ADF test might demonstrate low statistical power, making it challenging to differentiate between a unit root and near-unit-root processes causing false negatives.\n",
       "- The test assumes the data follows an autoregressive process, which might not be the case all the time.\n",
       "- The ADF test finds it demanding to manage time series data with structural breaks.</td>\n",
       "      <td id=\"T_1ac33_row63_col3\" class=\"data row63 col3\" >validmind.model_validation.statsmodels.ADF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row64_col0\" class=\"data row64 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row64_col1\" class=\"data row64 col1\" >Durbin Watson Test</td>\n",
       "      <td id=\"T_1ac33_row64_col2\" class=\"data row64 col2\" >**Purpose**: The Durbin-Watson Test metric detects autocorrelation in time series data (where a set of data values influences their predecessors). Autocorrelation is a crucial factor for regression tasks as these often assume the independence of residuals. A model with significant autocorrelation may give unreliable predictions.\n",
       "\n",
       "**Test Mechanism**: Utilizing the `durbin_watson` function in the `statsmodels` Python library, the Durbin-Watson (DW) Test metric generates a statistical value for each feature of the training dataset. The function is looped over all columns of the dataset, calculating and caching the DW value for each column for further analysis. A DW metric value nearing 2 indicates no autocorrelation. Conversely, values approaching 0 suggest positive autocorrelation, and those leaning towards 4 imply negative autocorrelation.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If a feature's DW value significantly deviates from 2, it could signal a high risk due to potential autocorrelation issues in the dataset.\n",
       "- A value closer to '0' could imply positive autocorrelation, while a value nearer to '4' could point to negative autocorrelation, both leading to potentially unreliable prediction models.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric specializes in identifying autocorrelation in prediction model residuals.\n",
       "- Autocorrelation detection assists in diagnosing violation of various modeling technique assumptions, particularly in regression analysis and time-series data modeling.\n",
       "\n",
       "**Limitations**:\n",
       "- The Durbin-Watson Test mainly detects linear autocorrelation and could overlook other types of relationships.\n",
       "- The metric is highly sensitive to data points order. Shuffling the order could lead to notably different results.\n",
       "- The test only checks for first-order autocorrelation (between a variable and its immediate predecessor) and fails to detect higher order autocorrelation.</td>\n",
       "      <td id=\"T_1ac33_row64_col3\" class=\"data row64 col3\" >validmind.model_validation.statsmodels.DurbinWatsonTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row65_col0\" class=\"data row65 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row65_col1\" class=\"data row65 col1\" >Missing Values Risk</td>\n",
       "      <td id=\"T_1ac33_row65_col2\" class=\"data row65 col2\" >**Purpose**: The Missing Values Risk metric is specifically designed to assess and quantify the risk associated with missing values in the dataset used for machine learning model training. It measures two specific risks: the percentage of total data that are missing, and the percentage of all variables (columns) that contain some missing values.\n",
       "\n",
       "**Test Mechanism**: Initially, the metric calculates the total number of data points in the dataset and the count of missing values. It then inspects each variable (column) to determine how many contain at least one missing datapoint. By methodically counting missing datapoints across the entire dataset and each variable (column), it identifies the percentage of missing values in the entire dataset and the percentage of variables (columns) with such values.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Record high percentages in either of the risk measures could suggest a high risk.\n",
       "- If the dataset indicates a high percentage of missing values, it might significantly undermine the model's performance and credibility.\n",
       "- If a significant portion of variables (columns) in the dataset are missing values, this could make the model susceptible to bias and overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The metric offers valuable insights into the readiness of a dataset for model training as missing values can heavily destabilize both the model's performance and predictive capabilities.\n",
       "- The metric's quantification of the risks caused by missing values allows for the use of targeted methods to manage these values correctly- either through removal, imputation, or alternative strategies.\n",
       "- The metric has the flexibility to be applied to both classification and regression assignments, maintaining its utility across a wide range of models and scenarios.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The metric primarily identifies and quantifies the risk associated with missing values without suggesting specific mitigation strategies.\n",
       "- The metric does not ascertain whether the missing values are random or associated with an underlying issue in the stages of data collection or preprocessing.\n",
       "- However, the identification of the presence and scale of missing data is the essential initial step towards improving data quality.</td>\n",
       "      <td id=\"T_1ac33_row65_col3\" class=\"data row65 col3\" >validmind.data_validation.MissingValuesRisk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row66_col0\" class=\"data row66 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row66_col1\" class=\"data row66 col1\" >IQR Outliers Table</td>\n",
       "      <td id=\"T_1ac33_row66_col2\" class=\"data row66 col2\" >**Purpose**: The \"Interquartile Range Outliers Table\" (IQROutliersTable) metric has been designed for identifying and summarizing outliers within numerical features of a dataset using the Interquartile Range (IQR) method. The purpose of this exercise is crucial in the pre-processing of data as outliers can substantially distort the statistical analysis and debilitate the performance of machine learning models.\n",
       "\n",
       "**Test Mechanism**: The IQR, which is the range separating the first quartile (25th percentile) from the third quartile (75th percentile), is calculated for each numerical feature within the dataset. An outlier is defined as a data point falling below the \"Q1\n",
       "- 1.5 * IQR\" or above \"Q3 + 1.5 * IQR\" range. The metric then computes the number of outliers along with their minimum, 25th percentile, median, 75th percentile, and maximum values for each numerical feature. If no specific features are chosen, the metric will apply to all numerical features in the dataset. The default outlier threshold is set to 1.5, following the standard definition of outliers in statistical analysis, although it can be customized by the user.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk is indicated by a large number of outliers in multiple features.\n",
       "- Outliers that are significantly distanced from the mean value of variables could potentially signal high risk.\n",
       "- Data entry errors or other data quality issues could be manifested through extremely high or low outlier values.\n",
       "\n",
       "**Strengths**:\n",
       "- It yields a comprehensive summary of outliers for each numerical feature within the dataset. This enables the user to pinpoint features with potential quality issues.\n",
       "- The IQR method is not overly affected by extremely high or low outlier values as it is based on quartile calculations.\n",
       "- The versatility of this metric grants the ability to customize the method to work on selected features and set a defined threshold for outliers.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric might cause false positives if the variable of interest veers away from a normal or near-normal distribution, notably in the case of skewed distributions.\n",
       "- It does not extend to provide interpretation or recommendations for tackling outliers and relies on the user or a data scientist to conduct further analysis of the results.\n",
       "- As it only functions on numerical features, it cannot be used for categorical data.\n",
       "- For data that has undergone heavy pre-processing, was manipulated, or inherently possesses a high kurtosis (heavy tails), the pre-set threshold may not be optimal for outlier detection.</td>\n",
       "      <td id=\"T_1ac33_row66_col3\" class=\"data row66 col3\" >validmind.data_validation.IQROutliersTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row67_col0\" class=\"data row67 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row67_col1\" class=\"data row67 col1\" >Bivariate Features Bar Plots</td>\n",
       "      <td id=\"T_1ac33_row67_col2\" class=\"data row67 col2\" >**Purpose**: The BivariateFeaturesBarPlots metric is intended to perform a visual analysis of categorical data within the model. The goal is to assess and understand the specific relationships between various feature pairs, while simultaneously highlighting the model's target variable. This form of bivariate plotting is immensely beneficial in uncovering trends, correlations, patterns, or inconsistencies that may not be readily apparent within raw tabular data.\n",
       "\n",
       "**Test Mechanism**: These tests establish bar plots for each pair of features defined within the parameters. The dataset is grouped by each feature pair and then calculates the mean of the target variable within each specific grouping. Each group is represented via a bar in the plot, and the height of this bar aligns with the calculated mean. The colors assigned to these bars are based on the categorical section to which they pertain: these colors can either come from a colormap or generated anew if the total number of categories exceeds the current colormap's scope.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If any values are found missing or inconsistent within the feature pairs.\n",
       "- If there exist large discrepancies or irregularities between the mean values of certain categories within feature pairs.\n",
       "- If the parameters for feature pairs have not been specified or if they were wrongly defined.\n",
       "\n",
       "**Strengths**:\n",
       "- The BivariateFeaturesBarPlots provides a clear, visual comprehension of the relationships between feature pairs and the target variable.\n",
       "- It allows an easy comparison between different categories within feature pairs.\n",
       "- The metric can handle a diverse array of categorical data, enhancing its universal applicability.\n",
       "- It is highly customizable due to its allowance for users to define feature pairs based on their specific requirements.\n",
       "\n",
       "**Limitations**:\n",
       "- It can only be used with categorical data, limiting its usability with numerical or textual data.\n",
       "- It relies on manual input for feature pairs, which could result in the overlooking of important feature pairs if not chosen judiciously.\n",
       "- The generated bar plots could become overly cluttered and difficult to decipher when dealing with feature pairs with a large number of categories.\n",
       "- This metric only provides a visual evaluation and fails to offer any numerical or statistical measures to quantify the relationship between feature pairs.</td>\n",
       "      <td id=\"T_1ac33_row67_col3\" class=\"data row67 col3\" >validmind.data_validation.BivariateFeaturesBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row68_col0\" class=\"data row68 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row68_col1\" class=\"data row68 col1\" >Skewness</td>\n",
       "      <td id=\"T_1ac33_row68_col2\" class=\"data row68 col2\" >**Purpose**: The purpose of the Skewness test is to measure the asymmetry in the distribution of data within a predictive machine learning model. Specifically, it evaluates the divergence of said distribution from a normal distribution. In understanding the level of skewness, we can potentially identify issues with data quality, an essential component for optimizing the performance of traditional machine learning models in both classification and regression settings.\n",
       "\n",
       "**Test Mechanism**: This test calculates skewness of numerical columns in a dataset, which is extracted from the DataFrame, specifically focusing on numerical data types. The skewness value is then contrasted against a predetermined maximum threshold, set by default to 1. The skewness value under review is deemed to have passed the test only if it is less than this maximum threshold; otherwise, the test is considered 'fail'. Subsequently, the test results of each column, together with the skewness value and column name, are cached.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- The presence of substantial skewness levels that significantly exceed the maximum threshold is an indication of skewed data distribution and subsequently high model risk.\n",
       "- Persistent skewness in data could signify that the foundational assumptions of the machine learning model may not be applicable, potentially leading to subpar model performance, erroneous predictions, or biased inferences.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Fast and efficient identification of unequal data\n",
       "- distributions within a machine learning model is enabled by the skewness test.\n",
       "- The maximum threshold parameter can be adjusted to meet the user's specific needs, enhancing the test's versatility.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The test only evaluates numeric columns, which means that data in non-numeric columns could still include bias or problematic skewness that this test does not capture.\n",
       "- The test inherently assumes that the data should follow a normal distribution, an expectation which may not always be met in real-world data.\n",
       "- The risk grading is largely dependent on a subjective threshold, which may result in excessive strictness or leniency depending upon selection. This factor might require expert input and recurrent iterations for refinement.</td>\n",
       "      <td id=\"T_1ac33_row68_col3\" class=\"data row68 col3\" >validmind.data_validation.Skewness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row69_col0\" class=\"data row69 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row69_col1\" class=\"data row69 col1\" >Duplicates</td>\n",
       "      <td id=\"T_1ac33_row69_col2\" class=\"data row69 col2\" >**Purpose**: The Duplicates test is designed to assess the data quality of an ML model by identifying any duplicate entries in the data set. It focuses on seeking out duplication in a specified text column or among the primary keys of the data set, which could have serious implications for the performance and integrity of the model. Duplicate entries could potentially skew the data distribution and influence model training inaccurately.\n",
       "\n",
       "**Test Mechanism**: This test operates by calculating the total number of duplicate entries in the data set. The algorithm will count duplicates within the 'text_column' if this property is specified. If primary keys are defined, the test will also be applied on them. The count of duplicates ('n_duplicates') is then compared to a predefined minimum threshold (the default 'min_threshold' is set at 1) to determine whether the test has passed or not. The results include the total number of duplicates as well as the percentage of duplicate rows in comparison to the overall dataset ('p_duplicates').\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large amount of duplicates, particularly those exceeding the predefined minimum threshold, point toward a high risk situation.\n",
       "- Overrepresentation of certain data which can lead to skewed results.\n",
       "- Indication of inefficient data collecting techniques leading to data redundancy.\n",
       "- Models that fail this test predominantly may necessitate a closer examination of their data preprocessing methods or source data.\n",
       "\n",
       "**Strengths**:\n",
       "- The Duplicates test is highly adaptable, being capable of being used with both text data and tabular data formats.\n",
       "- It is able to provide results both numerically and as a percentage of the total data set, allowing for a broader understanding of the extent of duplication.\n",
       "- Its utility lies in effectively flagging any data quality issues that could potentially upset model performance and generate erroneous predictions.\n",
       "\n",
       "**Limitations**:\n",
       "- The Duplicates test solely targets exact duplication in entries, meaning it may overlook near-duplicates or normalized forms of entries that might also affect data distribution and model integrity.\n",
       "- Data variations caused by errors, phrasing changes, or inconsistencies may not be detected.\n",
       "- A substantial number of duplicates in a datasets may not always denote poor data quality, as this can be dependent on the nature of the data and the problem being addressed.</td>\n",
       "      <td id=\"T_1ac33_row69_col3\" class=\"data row69 col3\" >validmind.data_validation.Duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row70_col0\" class=\"data row70 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row70_col1\" class=\"data row70 col1\" >Missing Values Bar Plot</td>\n",
       "      <td id=\"T_1ac33_row70_col2\" class=\"data row70 col2\" >**Purpose:** The 'MissingValuesBarPlot' metric provides a color-coded visual representation of the percentage of missing values for each column in an ML model's dataset. The primary purpose of this metric is to easily identify and quantify missing data, which are essential steps in data preprocessing. The presence of missing data can potentially skew the model's predictions and decrease its accuracy. Additionally, this metric uses a pre-set threshold to categorize various columns into ones that contain missing data above the threshold (high risk) and below the threshold (less risky).\n",
       "\n",
       "**Test Mechanism:** The test mechanism involves scanning each column in the input dataset and calculating the percentage of missing values. It then compares each column's missing data percentage with the predefined threshold, categorizing columns with missing data above the threshold as high-risk. The test generates a bar plot in which columns with missing data are represented on the y-axis and their corresponding missing data percentages are displayed on the x-axis. The color of each bar reflects the missing data percentage in relation to the threshold: grey for values below the threshold and light coral for those exceeding it. The user-defined threshold is represented by a red dashed line on the plot.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Columns with higher percentages of missing values beyond the threshold are high-risk. These are visually represented by light coral bars on the bar plot.\n",
       "\n",
       "**Strengths:**\n",
       "- Helps in quickly identifying and quantifying missing data across all columns of the dataset.\n",
       "- Facilitates pattern recognition through visual representation.\n",
       "- Enables customization of the level of risk tolerance via a user-defined threshold.\n",
       "- Supports both classification and regression tasks, sharing its versatility.\n",
       "\n",
       "**Limitations:**\n",
       "- It only considers the quantity of missing values, not differentiating between different types of missingness (Missing completely at random\n",
       "- MCAR, Missing at random\n",
       "- MAR, Not Missing at random\n",
       "- NMAR).\n",
       "- It doesn't offer insights into potential approaches for handling missing entries, such as various imputation strategies.\n",
       "- The metric does not consider possible impacts of the missing data on the model's accuracy or precision.\n",
       "- Interpretation of the findings and the next steps might require an expert understanding of the field.</td>\n",
       "      <td id=\"T_1ac33_row70_col3\" class=\"data row70 col3\" >validmind.data_validation.MissingValuesBarPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row71_col0\" class=\"data row71 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row71_col1\" class=\"data row71 col1\" >Dataset Description</td>\n",
       "      <td id=\"T_1ac33_row71_col2\" class=\"data row71 col2\" >**Purpose**: This metric, known as DatasetDescription, has been architected to furnish an extensive spectrum of descriptive statistics pertinent to the input data that fuels a machine learning model. The statistics generated by this metric include measures of central tendency, dispersion, in addition to frequency counts for each field, or variable, in the dataset. The purpose of these statistics lies in comprehending the nature of the data, identifying anomalies, and grasping the extent to which the data fulfills model assumptions.\n",
       "\n",
       "**Test Mechanism**: The testing function commences by morphing the input dataset back to its primary form by undoing any one-hot encoding, subsequently extracting each field from the dataset. Post this process, the script computes the descriptive statistics for each field, contingent on its data type (Numeric, Categorical, Boolean, Dummy, Text, Null). For numeric fields, the statistics include computations of mean, standard deviation, minimum, maximum, and varying percentiles. For categorical and boolean fields, the frequency counts of each category along with the most frequent category (or 'top') are computed. The script computes missing and distinct values for all fields, while also generating histograms for numeric and categorical types, as well as word count histograms for text\n",
       "- accomplished through counting the number of occurrences of each unique word.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant portion of missing or null values, which may hinder the model's performance.\n",
       "- Predominance of a single value (indicating low diversity) within a feature.\n",
       "- High variance in a numeric feature.\n",
       "- High number of distinct categories in categorical features.\n",
       "- High frequency of a specific word in text data as this might signal bias in the dataset.\n",
       "\n",
       "**Strengths**:\n",
       "- Capability to provide an all-inclusive, general summary of the dataset.\n",
       "- It helps to understand the distribution, dispersion, and central tendency of numeric features, moreover, the frequency distribution within categorical and text features.\n",
       "- It can efficaciously handle different data types, offering insights into the variety and distribution of field values.\n",
       "\n",
       "**Limitations**:\n",
       "- It does not provide insights regarding the relationships between different fields or features, as it focuses only on univariate analysis.\n",
       "- The statistical description for text data is severely limited, given it essentially forms a bag-of-words model.\n",
       "- The descriptive statistics generated, including mean and standard deviation, hold no meaningful value for ordinal categorical data.\n",
       "- This metric does not offer intelligence on how the derived data will influence the performance of the model.</td>\n",
       "      <td id=\"T_1ac33_row71_col3\" class=\"data row71 col3\" >validmind.data_validation.DatasetDescription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row72_col0\" class=\"data row72 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row72_col1\" class=\"data row72 col1\" >Scatter Plot</td>\n",
       "      <td id=\"T_1ac33_row72_col2\" class=\"data row72 col2\" >**Purpose**: The ScatterPlot metric is designed to offer a visual analysis of a given dataset by constructing a scatter plot matrix encapsulating all the dataset's features (or columns). Its primary function lies in unearthing relationships, patterns, or outliers across different features, thus providing both quantitative and qualitative insights into the multidimensional relationships within the dataset. This visual assessment aids in understanding the efficacy of the chosen features for model training and their overall suitability.\n",
       "\n",
       "**Test Mechanism**: Using the seaborn library, the ScatterPlot class creates the scatter plot matrix. The process includes retrieving all columns from the dataset, verifying their existence, and subsequently generating a pairplot for these columns. A kernel density estimate (kde) is utilized to present a smoother, univariate distribution along the grid's diagonal. The final plot is housed in an array of Figure objects, each wrapping a matplotlib figure instance for storage and future usage.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The emergence of non-linear or random patterns across different feature pairs. This may suggest intricate relationships unfit for linear presumptions.\n",
       "- A lack of clear patterns or clusters which might point to weak or non-existent correlations among features, thus creating a problem for certain model types.\n",
       "- The occurrence of outliers as visual outliers in your data can adversely influence the model's performance.\n",
       "\n",
       "**Strengths**:\n",
       "- It offers insight into the multidimensional relationships among multiple features.\n",
       "- It assists in identifying trends, correlations, and outliers which could potentially affect the model's performance.\n",
       "- As a diagnostic tool, it can validate whether certain assumptions made during the model-creation process, such as linearity, hold true.\n",
       "- The tool's versatility extends to its application for both regression and classification tasks.\n",
       "\n",
       "**Limitations**:\n",
       "- Scatter plot matrices may become cluttered and hard to decipher as the number of features escalates, resulting in complexity and confusion.\n",
       "- While extremely proficient in revealing pairwise relationships, these matrices may fail to illuminate complex interactions that involve three or more features.\n",
       "- These matrices are primarily visual tools, so the precision of quantitative analysis may be compromised.\n",
       "- If not clearly visible, outliers can be missed, which could negatively affect model performance.\n",
       "- It assumes that the dataset can fit into the computer's memory, which might not always be valid particularly for extremely large datasets.</td>\n",
       "      <td id=\"T_1ac33_row72_col3\" class=\"data row72 col3\" >validmind.data_validation.ScatterPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row73_col0\" class=\"data row73 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row73_col1\" class=\"data row73 col1\" >Time Series Outliers</td>\n",
       "      <td id=\"T_1ac33_row73_col2\" class=\"data row73 col2\" >**Purpose**: This test is designed to identify outliers in time-series data using the z-score method. It's vital for ensuring data quality before modeling, as outliers can skew predictive models and significantly impact their overall performance.\n",
       "\n",
       "**Test Mechanism**: The test processes a given dataset which must have datetime indexing, checks if a 'zscore_threshold' parameter has been supplied, and identifies columns with numeric data types. After finding numeric columns, the implementer then applies the z-score method to each numeric column, identifying outliers based on the threshold provided. Each outlier is listed together with their variable name, z-score, timestamp and relative threshold in a dictionary and converted to a DataFrame for convenient output. Additionally, it produces visual plots for each time series illustrating outliers in the context of the broader dataset. The 'zscore_threshold' parameter sets the limit beyond which a data point will be labeled as an outlier. The default threshold is set at 3, indicating that any data point that falls 3 standard deviations away from the mean will be marked as an outlier.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If many or substantial outliers are present within a dataset, this may be an indicator of high risk as it suggests that the dataset contains significant anomalies.\n",
       "- This could potentially affect the performance of the machine learning models, if not properly addressed.\n",
       "- Data points with z-scores higher than the set threshold would be flagged as outliers and could be considered as high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The z-score method is a popular and robust method for identifying outliers in a dataset.\n",
       "- Time series maintenance is simplified through requiring a datetime index.\n",
       "- Outliers are identified for each numeric feature individually.\n",
       "- Provides an elaborate report which shows variables, date, z-score and whether the test passed or failed.\n",
       "- Offers visual inspection for detected outliers in the respective time-series through plots.\n",
       "\n",
       "**Limitations**:\n",
       "- This test only identifies outliers in numeric columns, and won't identify outliers in categorical variables.\n",
       "- The utility and accuracy of z-scores can be limited if the data doesn't follow a normal distribution.\n",
       "- The method relies on a subjective z-score threshold for deciding what constitutes an outlier, which might not always be suitable depending on the dataset and the use case.\n",
       "- It does not address possible ways to handle identified outliers in the data.\n",
       "- The necessity for a datetime index could limit the extent of its application.</td>\n",
       "      <td id=\"T_1ac33_row73_col3\" class=\"data row73 col3\" >validmind.data_validation.TimeSeriesOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row74_col0\" class=\"data row74 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row74_col1\" class=\"data row74 col1\" >Tabular Categorical Bar Plots</td>\n",
       "      <td id=\"T_1ac33_row74_col2\" class=\"data row74 col2\" >**Purpose**: The purpose of this metric is to visually analyze categorical data using bar plots. It is intended to evaluate the dataset's composition by displaying the counts of each category in each categorical feature.\n",
       "\n",
       "**Test Mechanism**: The provided dataset is first checked to determine if it contains any categorical variables. If no categorical columns are found, the tool raises a ValueError. For each categorical variable in the dataset, a separate bar plot is generated. The number of occurrences for each category is calculated and displayed on the plot. If a dataset contains multiple categorical columns, multiple bar plots are produced.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- High risk could occur if the categorical variables exhibit an extreme imbalance, with categories having very few instances possibly being underrepresented in the model, which could affect the model's performance and its ability to generalize.\n",
       "- Another sign of risk is if there are too many categories in a single variable, which could lead to overfitting and make the model complex.\n",
       "\n",
       "**Strengths**: This metric provides a visual and intuitively understandable representation of categorical data, which aids in the analysis of variable distributions. By presenting model inputs in this way, we can easily identify imbalances or rare categories that could affect the model's performance.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- This method only works with categorical data, meaning it won't apply to numerical variables.\n",
       "- In addition, the method does not provide any informative value when there are too many categories, as the bar chart could become cluttered and hard to interpret.\n",
       "- It offers no insights into the model's performance or precision, but rather provides a descriptive analysis of the input.</td>\n",
       "      <td id=\"T_1ac33_row74_col3\" class=\"data row74 col3\" >validmind.data_validation.TabularCategoricalBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row75_col0\" class=\"data row75 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row75_col1\" class=\"data row75 col1\" >Auto Stationarity</td>\n",
       "      <td id=\"T_1ac33_row75_col2\" class=\"data row75 col2\" >**Purpose**: The AutoStationarity metric is intended to automatically detect and evaluate the stationary nature of each time series in a DataFrame. It incorporates the Augmented Dickey-Fuller (ADF) test, a statistical approach used to assess stationarity. Stationarity is a fundamental property suggesting that statistic features like mean and variance remain unchanged over time. This is necessary for many time-series models.\n",
       "\n",
       "**Test Mechanism**: The mechanism for the AutoStationarity test involves applying the Augmented Dicky-Fuller test to each time series within the given dataframe to assess if they are stationary. Every series in the dataframe is looped, using the ADF test up to a defined maximum order (configurable and by default set to 5). The p-value resulting from the ADF test is compared against a predetermined threshold (also configurable and by default set to 0.05). The time series is deemed stationary at its current differencing order if the p-value is less than the threshold.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant number of series not achieving stationarity even at the maximum order of differencing can indicate high risk or potential failure in the model.\n",
       "- This could suggest the series may not be appropriately modeled by a stationary process, hence other modeling approaches might be required.\n",
       "\n",
       " **Strengths**:\n",
       "- The key strength in this metric lies in the automation of the ADF test, enabling mass stationarity analysis across various time series and boosting the efficiency and credibility of the analysis.\n",
       "- The utilization of the ADF test, a widely accepted method for testing stationarity, lends authenticity to the results derived.\n",
       "- The introduction of the max order and threshold parameters give users the autonomy to determine their preferred levels of stringency in the tests.\n",
       "\n",
       "**Limitations**:\n",
       "- The Augumented Dicky-Fuller test and the stationarity test are not without their limitations. These tests are premised on the assumption that the series can be modeled by an autoregressive process, which may not always hold true.\n",
       "- The stationarity check is highly sensitive to the choice of threshold for the significance level; an extremely high or low threshold could lead to incorrect results regarding the stationarity properties.\n",
       "- There's also a risk of over-differencing if the maximum order is set too high, which could induce unnecessary cycles.</td>\n",
       "      <td id=\"T_1ac33_row75_col3\" class=\"data row75 col3\" >validmind.data_validation.AutoStationarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row76_col0\" class=\"data row76 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row76_col1\" class=\"data row76 col1\" >Descriptive Statistics</td>\n",
       "      <td id=\"T_1ac33_row76_col2\" class=\"data row76 col2\" >**Purpose**: The purpose of the Descriptive Statistics metric is to provide a comprehensive summary of both numerical and categorical data within a dataset. This involves statistics such as count, mean, standard deviation, minimum and maximum values for numerical data. For categorical data, it calculates the count, number of unique values, most common value and its frequency, and the proportion of the most frequent value relative to the total. The goal is to visualize the overall distribution of the variables in the dataset, aiding in understanding the model's behavior and predicting its performance.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism utilizes two in-built functions of pandas dataframes: describe() for numerical fields and value_counts() for categorical fields. The describe() function pulls out several summary statistics, while value_counts() accounts for unique values. The resulting data is formatted into two distinct tables, one for numerical and another for categorical variable summaries. These tables provide a clear summary of the main characteristics of the variables, which can be instrumental in assessing the model's performance.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Skewed data or significant outliers can represent high risk. For numerical data, this may be reflected via a significant difference between the mean and median (50% percentile).\n",
       "- For categorical data, a lack of diversity (low count of unique values), or overdominance of a single category (high frequency of the top value) can indicate high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- This metric provides a comprehensive summary of the dataset, shedding light on the distribution and characteristics of the variables under consideration.\n",
       "- It is a versatile and robust method, applicable to both numerical and categorical data.\n",
       "- It helps highlight crucial anomalies such as outliers, extreme skewness, or lack of diversity, which are vital in understanding model behavior during testing and validation.\n",
       "\n",
       "**Limitations**:\n",
       "- While this metric offers a high-level overview of the data, it may fail to detect subtle correlations or complex patterns.\n",
       "- It does not offer any insights on the relationship between variables.\n",
       "- Alone, descriptive statistics cannot be used to infer properties about future unseen data.\n",
       "- It should be used in conjunction with other statistical tests to provide a comprehensive understanding of the model's data.</td>\n",
       "      <td id=\"T_1ac33_row76_col3\" class=\"data row76 col3\" >validmind.data_validation.DescriptiveStatistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row77_col0\" class=\"data row77 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row77_col1\" class=\"data row77 col1\" >ANOVA One Way Table</td>\n",
       "      <td id=\"T_1ac33_row77_col2\" class=\"data row77 col2\" >**Purpose**: The ANOVA (Analysis of Variance) One-Way Table metric is utilized to determine whether the mean of numerical variables differs across different groups identified by target or categorical variables. Its primary purpose is to scrutinize the significant impact of categorical variables on numerical ones. This method proves essential in identifying statistically significant features corresponding to the target variable present in the dataset.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism involves the ANOVA F-test's performance on each numerical variable against the target. If no specific features are mentioned, all numerical features are tested. A p-value is produced for each test and compared against a certain threshold (default being 0.05 if not specified). If the p-value is less than or equal to this threshold, the feature is marked as 'Pass', indicating significant mean difference across the groups. Otherwise, it's marked as 'Fail'. The test produces a DataFrame that includes variable name, F statistic value, p-value, threshold, and pass/fail status for every numerical variable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of 'Fail' results in the ANOVA F-test could signify high risk or underperformance in the model. This issue may arise when multiple numerical variables in the dataset don't exhibit any significant difference across the target variable groups.\n",
       "- Features with high p-values also indicate a high risk as they imply a greater chance of obtaining observed data given that the null hypothesis is true.\n",
       "\n",
       "**Strengths**:\n",
       "- The ANOVA One Way Table is highly efficient in identifying statistically significant features by simultaneously comparing group means.\n",
       "- Its flexibility allows the testing of all numerical features in the dataset when no specific ones are mentioned.\n",
       "- This metric provides a convenient method to measure the statistical significance of numerical variables and assists in selecting those variables influencing the classifier's predictions considerably.\n",
       "\n",
       "**Limitations**:\n",
       "- This metric assumes that the data is normally distributed, which may not always be the case leading to erroneous test results.\n",
       "- The sensitivity of the F-test to variance changes may hinder this metric's effectiveness, especially for datasets with high variance.\n",
       "- The ANOVA One Way test does not specify which group means differ statistically from others; it strictly asserts the existence of a difference.\n",
       "- The metric fails to provide insights into variable interactions, and significant effects due to these interactions could easily be overlooked.</td>\n",
       "      <td id=\"T_1ac33_row77_col3\" class=\"data row77 col3\" >validmind.data_validation.ANOVAOneWayTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row78_col0\" class=\"data row78 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row78_col1\" class=\"data row78 col1\" >Target Rate Bar Plots</td>\n",
       "      <td id=\"T_1ac33_row78_col2\" class=\"data row78 col2\" >**Purpose**: This test, implemented as a metric, is designed to provide an intuitive, graphical summary of the decision-making patterns exhibited by a categorical classification machine learning model. The model's performance is evaluated using bar plots depicting the ratio of target rates—meaning the proportion of positive classes—for different categorical inputs. This allows for an easy, at-a-glance understanding of the model's accuracy.\n",
       "\n",
       "**Test Mechanism**: The test involves creating a pair of bar plots for each categorical feature in the dataset. The first plot depicts the frequency of each category in the dataset, with each category visually distinguished by its unique color. The second plot shows the mean target rate of each category (sourced from the \"default_column\"). Plotly, a Python library, is used to generate these plots, with distinct plots created for each feature. If no specific columns are selected, the test will generate plots for each categorical column in the dataset.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Inconsistent or non-binary values in the \"default_column\" could complicate or render impossible the calculation of average target rates.\n",
       "- Particularly low or high target rates for a specific category might suggest that the model is misclassifying instances of that category.\n",
       "\n",
       "**Strengths**:\n",
       "- This test offers a visually interpretable breakdown of the model's decisions, providing an easy way to spot irregularities, inconsistencies, or patterns.\n",
       "- Its flexibility allows for the inspection of one or multiple columns, as needed.\n",
       "\n",
       "**Limitations**:\n",
       "- The test is less useful when dealing with numeric or continuous data, as it's designed specifically for categorical features.\n",
       "- If the model in question is dealing with a multi-class problem rather than binary classification, the test's assumption of binary target values (0s and 1s) becomes a significant limitation.\n",
       "- The readability of the bar plots drops as the number of distinct categories increases in the dataset, which can make them harder to understand and less useful.</td>\n",
       "      <td id=\"T_1ac33_row78_col3\" class=\"data row78 col3\" >validmind.data_validation.TargetRateBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row79_col0\" class=\"data row79 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row79_col1\" class=\"data row79 col1\" >Pearson Correlation Matrix</td>\n",
       "      <td id=\"T_1ac33_row79_col2\" class=\"data row79 col2\" >**Purpose**: This test is intended to evaluate the extent of linear dependency between all pairs of numerical variables in the given dataset. It provides the Pearson Correlation coefficient, which reveals any high correlations present. The purpose of doing this is to identify potential redundancy, as variables that are highly correlated can often be removed to reduce the dimensionality of the dataset without significantly impacting the model's performance.\n",
       "\n",
       "**Test Mechanism**: This metric test generates a correlation matrix for all numerical variables in the dataset using the Pearson correlation formula. A heat map is subsequently created to visualize this matrix effectively. The color of each point on the heat map corresponds to the magnitude and direction (positive or negative) of the correlation, with a range from -1 (perfect negative correlation) to 1 (perfect positive correlation). Any correlation coefficients higher than 0.7 (in absolute terms) are indicated in white in the heat map, suggesting a high degree of correlation.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of variables in the dataset showing a high degree of correlation (coefficients approaching ±1). This indicates redundancy within the dataset, suggesting that some variables may not be contributing new information to the model.\n",
       "- This could potentially lead to overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- The primary strength of this metric test is its ability to detect and quantify the linearity of relationships between variables. This allows for the identification of redundant variables, which in turn can help in simplifying models and potentially improving their performance.\n",
       "- The visualization aspect (heatmap) is another strength as it offers an easy-to-understand overview of the correlations, beneficial for those not comfortable navigating numerical matrices.\n",
       "\n",
       "**Limitations**:\n",
       "- The primary limitation of Pearson Correlation is its inability to detect non-linear relationships between variables, which can lead to missed opportunities for dimensionality reduction.\n",
       "- It only measures the degree of linear relationship and not the strength of effect of one variable on the other.\n",
       "- The cutoff value of 0.7 for high correlation is a somewhat arbitrary choice and some valid dependencies might be missed if they have a correlation coefficient less than this value.</td>\n",
       "      <td id=\"T_1ac33_row79_col3\" class=\"data row79 col3\" >validmind.data_validation.PearsonCorrelationMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row80_col0\" class=\"data row80 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row80_col1\" class=\"data row80 col1\" >Feature Target Correlation Plot</td>\n",
       "      <td id=\"T_1ac33_row80_col2\" class=\"data row80 col2\" >**Purpose**: This test is designed to graphically illustrate the correlations between distinct input features and the target output of a Machine Learning model. Understanding how each feature influences the model's predictions is crucial\n",
       "- a higher correlation indicates stronger influence of the feature on the target variable. This correlation study is especially advantageous during feature selection and for comprehending the model's operation.\n",
       "\n",
       "**Test Mechanism**: This FeatureTargetCorrelationPlot test computes and presents the correlations between the features and the target variable using a specific dataset. These correlations are calculated, graphically represented in a horizontal bar plot, and color-coded based on the strength of the correlation. A hovering template can also be utilized for informative tooltips. It is possible to specify the features to be analyzed and adjust the graph's height according to need.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- There are no strong correlations (either positive or negative) between features and the target variable. This could suggest high risk as the supplied features do not appear to significantly impact the prediction output.\n",
       "- The presence of duplicated correlation values might hint at redundancy in the feature set.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides visual assistance to interpreting correlations more effectively.\n",
       "- Gives a clear and simple tour of how each feature affects the model's target variable.\n",
       "- Beneficial for feature selection and grasping the model's prediction nature.\n",
       "- Precise correlation values for each feature are offered by the hover template, contributing to a granular-level comprehension.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only accepts numerical data, meaning variables of other types need to be prepared beforehand.\n",
       "- The plot assumes all correlations to be linear, thus non-linear relationships might not be captured effectively.\n",
       "- Not apt for models that employ complex feature interactions, like Decision Trees or Neural Networks, as the test may not accurately reflect their importance.</td>\n",
       "      <td id=\"T_1ac33_row80_col3\" class=\"data row80 col3\" >validmind.data_validation.FeatureTargetCorrelationPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row81_col0\" class=\"data row81 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row81_col1\" class=\"data row81 col1\" >Tabular Numerical Histograms</td>\n",
       "      <td id=\"T_1ac33_row81_col2\" class=\"data row81 col2\" >**Purpose**: The purpose of this test is to provide visual analysis of numerical data through the generation of histograms for each numerical feature in the dataset. Histograms aid in the exploratory analysis of data, offering insight into the distribution of the data, skewness, presence of outliers, and central tendencies. It helps in understanding if the inputs to the model are normally distributed which is a common assumption in many machine learning algorithms.\n",
       "\n",
       "**Test Mechanism**: This test scans the provided dataset and extracts all the numerical columns. For each numerical column, it constructs a histogram using plotly, with 50 bins. The deployment of histograms offers a robust visual aid, ensuring unruffled identification and understanding of numerical data distribution patterns.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- A high degree of skewness\n",
       "- Unexpected data distributions\n",
       "- Existence of extreme outliers in the histograms These may indicate issues with the data that the model is receiving. If data for a numerical feature is expected to follow a certain distribution (like normal distribution) but does not, it could lead to sub-par performance by the model. As such these instances should be treated as high-risk indicators.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- This test provides a simple, easy-to-interpret visualization of how data for each numerical attribute is distributed.\n",
       "- It can help detect skewed values and outliers, that could potentially harm the AI model's performance.\n",
       "- It can be applied to large datasets and multiple numerical variables conveniently.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- This test only works with numerical data, thus ignoring non-numerical or categorical data.\n",
       "- It does not analyze relationships between different features, only the individual feature distributions.\n",
       "- It is a univariate analysis, and may miss patterns or anomalies that only appear when considering multiple variables together.\n",
       "- It does not provide any insight into how these features affect the output of the model; it is purely an input analysis tool.</td>\n",
       "      <td id=\"T_1ac33_row81_col3\" class=\"data row81 col3\" >validmind.data_validation.TabularNumericalHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row82_col0\" class=\"data row82 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row82_col1\" class=\"data row82 col1\" >Isolation Forest Outliers</td>\n",
       "      <td id=\"T_1ac33_row82_col2\" class=\"data row82 col2\" >**Purpose**: The `IsolationForestOutliers` test is designed to identify anomalies or outliers in the model's dataset using the isolation forest algorithm. This algorithm assumes that anomalous data points can be isolated more quickly due to their distinctive properties. By creating isolation trees and identifying instances with shorter average path lengths, the test is able to pick out data points that differ from the majority.\n",
       "\n",
       "**Test Mechanism**: The test uses the isolation forest algorithm, which builds an ensemble of isolation trees by randomly selecting features and splitting the data based on random thresholds. It isolates anomalies rather than focusing on normal data points. For each pair of variables, a scatter plot is generated which distinguishes the identified outliers from the inliers. The results of the test can be visualized using these scatter plots, illustrating the distinction between outliers and inliers.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The presence of high contamination, indicating a large number of anomalies\n",
       "- Inability to detect clusters of anomalies that are close in the feature space\n",
       "- Misclassifying normal instances as anomalies\n",
       "- Failure to detect actual anomalies\n",
       "\n",
       "**Strengths**:\n",
       "- Ability to handle large, high-dimensional datasets\n",
       "- Efficiency in isolating anomalies instead of normal instances\n",
       "- Insensitivity to the underlying distribution of data\n",
       "- Ability to recognize anomalies even when they are not separated from the main data cloud through identifying distinctive properties\n",
       "- Visually presents the test results for better understanding and interpretability\n",
       "\n",
       "**Limitations**:\n",
       "- Difficult to detect anomalies that are close to each other or prevalent in datasets\n",
       "- Dependency on the contamination parameter which may need fine-tuning to be effective\n",
       "- Potential failure in detecting collective anomalies if they behave similarly to normal data\n",
       "- Potential lack of precision in identifying which features contribute most to the anomalous behavior</td>\n",
       "      <td id=\"T_1ac33_row82_col3\" class=\"data row82 col3\" >validmind.data_validation.IsolationForestOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row83_col0\" class=\"data row83 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row83_col1\" class=\"data row83 col1\" >Chi Squared Features Table</td>\n",
       "      <td id=\"T_1ac33_row83_col2\" class=\"data row83 col2\" >**Purpose**: The `ChiSquaredFeaturesTable` metric is used to carry out a Chi-Squared test of independence for each categorical feature variable against a designated target column. The primary goal is to determine if a significant association exists between the categorical features and the target variable. This method typically finds its use in the context of Model Risk Management to understand feature relevance and detect potential bias in a classification model.\n",
       "\n",
       "**Test Mechanism**: The testing process generates a contingency table for each categorical variable and the target variable, after which a Chi-Squared test is performed. Using this approach, the Chi-Squared statistic and the p-value for each feature are calculated. The p-value threshold is a modifiable parameter, and a test will qualify as passed if the p-value is less than or equal to this threshold. If not, the test is labeled as failed. The outcome for each feature\n",
       "- comprising the variable name, Chi-squared statistic, p-value, threshold, and pass/fail status\n",
       "- is incorporated into a final summary table.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High p-values (greater than the set threshold) for specific variables could indicate a high risk.\n",
       "- These high p-values allude to the absence of a statistically significant relationship between the feature and the target variables, resulting in a 'Fail' status.\n",
       "- A categorical feature lacking a relevant association with the target variable could be a warning that the machine learning model might not be performing optimally.\n",
       "\n",
       "**Strengths**:\n",
       "- The test allows for a comprehensive understanding of the interaction between a model's input features and the target output, thus validating the relevance of categorical features.\n",
       "- It also produces an unambiguous 'Pass/Fail' output for each categorical feature.\n",
       "- The opportunity to adjust the p-value threshold contributes to flexibility in accommodating different statistical standards.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric presupposes that data is tabular and categorical, which may not always be the case with all datasets.\n",
       "- It is distinctively designed for classification tasks, hence unsuitable for regression scenarios.\n",
       "- The Chi-squared test, akin to any hypothesis testing-based test, cannot identify causal relationships, but only associations.\n",
       "- Furthermore, the test hinges on an adjustable p-value threshold, and varying threshold selections might lead to different conclusions regarding feature relevance.</td>\n",
       "      <td id=\"T_1ac33_row83_col3\" class=\"data row83 col3\" >validmind.data_validation.ChiSquaredFeaturesTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row84_col0\" class=\"data row84 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row84_col1\" class=\"data row84 col1\" >High Cardinality</td>\n",
       "      <td id=\"T_1ac33_row84_col2\" class=\"data row84 col2\" >**Purpose**: The “High Cardinality” test is used to evaluate the number of unique values present in the categorical columns of a dataset. In this context, high cardinality implies the presence of a large number of unique, non-repetitive values in the dataset.\n",
       "\n",
       "**Test Mechanism**: The test first infers the dataset's type and then calculates an initial numeric threshold based on the test parameters. It only considers columns classified as \"Categorical\". For each of these columns, the number of distinct values (n_distinct) and the percentage of distinct values (p_distinct) are calculated. The test will pass if n_distinct is less than the calculated numeric threshold. Lastly, the results, which include details such as column name, number of distinct values, and pass/fail status, are compiled into a table.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of distinct values (high cardinality) in one or more categorical columns implies a high risk.\n",
       "- A column failing the test (n_distinct >= num_threshold) is another indicator of high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The High Cardinality test is effective in early detection of potential overfitting and unwanted noise.\n",
       "- It aids in identifying potential outliers and inconsistencies, thereby improving data quality.\n",
       "- The test can be applied to both, classification and regression task types, demonstrating its versatility.\n",
       "\n",
       "**Limitations**:\n",
       "- The test is restricted to only \"Categorical\" data types and is thus not suitable for numerical or continuous features, limiting its scope.\n",
       "- The test does not consider the relevance or importance of unique values in categorical features, potentially causing it to overlook critical data points.\n",
       "- The threshold (both number and percent) used for the test is static and may not be optimal for diverse datasets and varied applications. Further mechanisms to adjust and refine this threshold could enhance its effectiveness.</td>\n",
       "      <td id=\"T_1ac33_row84_col3\" class=\"data row84 col3\" >validmind.data_validation.HighCardinality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row85_col0\" class=\"data row85 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row85_col1\" class=\"data row85 col1\" >Missing Values</td>\n",
       "      <td id=\"T_1ac33_row85_col2\" class=\"data row85 col2\" >**Purpose**: This test is designed to evaluate the quality of a dataset by measuring the number of missing values across all features. The objective is to ensure that the ratio of missing data to total data is less than a predefined threshold, defaulting to 1, to maintain the data quality necessary for reliable predictive strength in a machine learning model.\n",
       "\n",
       "**Test Mechanism**: The mechanism for this test involves iterating through each column of the dataset, counting missing values (represented as NaNs), and calculating the percentage they represent against the total number of rows. The test then checks if these missing value counts are less than the predefined `min_threshold`. The results are shown in a table summarizing each column, the number of missing values, the percentage of missing values in each column, and a Pass/Fail status based on the threshold comparison.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- When the number of missing values in any column exceeds the `min_threshold` value, it indicates a high risk.\n",
       "- A high risk is also flagged when missing values are present across many columns. In both instances, the test would return a \"Fail\" mark.\n",
       "\n",
       "**Strengths**:\n",
       "- The test offers a quick and granular identification of missing data across each feature in the dataset.\n",
       "- It provides an effective, straightforward means of maintaining data quality, which is vital for constructing efficient machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- Even though the test can efficiently identify missing values, it does not suggest the root causes of these missing values or recommend ways to impute or handle them.\n",
       "- The test might overlook features with a significant amount of missing data, but still less than the `min_threshold`. This could impact the model, especially if `min_threshold` is set too high.\n",
       "- The test does not account for data encoded as values (like \"-999\" or \"None\"), which might not technically classify as missing but could bear similar implications.</td>\n",
       "      <td id=\"T_1ac33_row85_col3\" class=\"data row85 col3\" >validmind.data_validation.MissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row86_col0\" class=\"data row86 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row86_col1\" class=\"data row86 col1\" >Default Ratesby Risk Band Plot</td>\n",
       "      <td id=\"T_1ac33_row86_col2\" class=\"data row86 col2\" >**Purpose**: The Default Rates by Risk Band Plot metric aims to quantify and visually represent default rates across varying risk bands within a specific dataset. This information is essential in evaluating the functionality of credit risk models, by providing a comprehensive view of default rates across a range of risk categories.\n",
       "\n",
       "**Test Mechanism**: The applied test approach involves a calculated bar plot. This plot is derived by initially determining the count of accounts in every risk band and then converting these count values into percentages by dividing by the total quantity of accounts. The percentages are then depicted as a bar plot, clearly showcasing the proportion of total accounts associated with each risk band. Hence, the plot delivers a summarized depiction of default risk across various bands. The 'Dark24' color sequence is used in the plot to ensure each risk band is easily distinguishable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk may be indicated by a significantly large percentage of accounts associated with high-risk bands.\n",
       "- High exposure to potential default risk in the dataset indicates potential weaknesses in the model's capability to effectively manage or predict credit risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric's primary strengths lie in its simplicity and visual impact.\n",
       "- The graphical display of default rates allows for a clear understanding of the spread of default risk across risk bands.\n",
       "- Using a bar chart simplifies the comparison between various risk bands and can highlight potential spots of high risk.\n",
       "- This approach assists in identifying any numerical imbalances or anomalies, thus facilitating the task of evaluating and contrasting performance across various credit risk models.\n",
       "\n",
       "**Limitations**:\n",
       "- The key constraint of this metric is that it cannot provide any insights as to why certain risk bands might have higher default rates than others.\n",
       "- If there is a large imbalance in the number of accounts across risk bands, the visual representation might not accurately depict the true distribution of risk.\n",
       "- Other factors contributing to credit risk beyond the risk bands are not considered.\n",
       "- The metric's reliance on a visual format might potentially lead to misinterpretation of results, as graphical depictions can sometimes be misleading.</td>\n",
       "      <td id=\"T_1ac33_row86_col3\" class=\"data row86 col3\" >validmind.data_validation.DefaultRatesbyRiskBandPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row87_col0\" class=\"data row87 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row87_col1\" class=\"data row87 col1\" >Rolling Stats Plot</td>\n",
       "      <td id=\"T_1ac33_row87_col2\" class=\"data row87 col2\" >**Purpose**: The `RollingStatsPlot` metric is employed to gauge the stationarity of time series data in a given dataset. This metric specifically evaluates the rolling mean and rolling standard deviation of the dataset over a pre-specified window size. The rolling mean provides an understanding of the average trend in the data, while the rolling standard deviation gauges the volatility of the data within the window. It is critical in preparing time series data for modeling as it reveals key insights into data behavior across time.\n",
       "\n",
       "**Test Mechanism**: This mechanism is comprised of two steps. Initially, the rolling mean and standard deviation for each of the dataset's columns are calculated over a window size, which can be user-specified or by default set to 12 data points. Then, the calculated rolling mean and standard deviation are visualized via separate plots, illustrating the trends and volatility in the dataset. A straightforward check is conducted to ensure the existence of columns in the dataset, and to verify that the given dataset has been indexed by its date and time—a necessary prerequisites for time series analysis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The presence of non-stationary patterns in either the rolling mean or the rolling standard deviation plots, which could indicate trends or seasonality in the data that may affect the performance of time series models.\n",
       "- Missing columns in the dataset, which would prevent the execution of this metric correctly.\n",
       "- The detection of NaN values in the dataset, which may need to be addressed before the metric can proceed successfully.\n",
       "\n",
       "**Strengths**:\n",
       "- Offers visualizations of trending behaviour and volatility within the data, facilitating a broader understanding of the dataset's inherent characteristics.\n",
       "- Checks of the dataset's integrity, such as existence of all required columns and the availability of a datetime index.\n",
       "- Adjusts to accommodate various window sizes, thus allowing accurate analysis of data with differing temporal granularities.\n",
       "- Considers each column of the data individually, thereby accommodating multi-feature datasets.\n",
       "\n",
       "**Limitations**:\n",
       "- For all columns, a fixed-size window is utilised. This may not accurately capture patterns in datasets where different features may require different optimal window sizes.\n",
       "- Requires the dataset to be indexed by date and time, hence it may not be useable for datasets without a timestamp index.\n",
       "- Primarily serves for data visualization as it does not facilitate any quantitative measures for stationarity, such as through statistical tests. Therefore, the interpretation is subjective and depends heavily on modeler discretion.</td>\n",
       "      <td id=\"T_1ac33_row87_col3\" class=\"data row87 col3\" >validmind.data_validation.RollingStatsPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row88_col0\" class=\"data row88 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row88_col1\" class=\"data row88 col1\" >Dataset Correlations</td>\n",
       "      <td id=\"T_1ac33_row88_col2\" class=\"data row88 col2\" >**Purpose**: The DatasetCorrelations metric is employed to examine the relationship between variables in a dataset, specifically designed for numerical and categorical data types. Using Pearson's R, Cramer's V, and Correlation ratios, it helps in understanding the linear relationship between numerical variables, association between categorical ones, and between numerical-categorical variables respectively. This allows for better awareness regarding dependency between features, which is crucial for optimizing model performance and understanding the model's behavior and predictors.\n",
       "\n",
       "**Test Mechanism**: During its execution, DatasetCorrelations initiates the calculation of the aforementioned correlation coefficients for the provided dataset. It leverages the built-in method 'get_correlations()', populating the 'correlations' attribute in the dataset object. It then invokes 'get_correlation_plots()' to generate graphical representations of these correlations. Finally, the correlation details and figures are cached for further study and analysis. The test does not dictate specific thresholds or grading scales.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- High correlation levels between input variables (multicollinearity), which can jeopardize the interpretability of the model and lead to overfitting.\n",
       "- The absence of any significant correlations, suggesting the variables may not have predictive power.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Comprehensive coverage of the correlation study of numerical, categorical, and numerical-categorical variables, negating the need for multiple individual tests.\n",
       "- Along with numerical correlation values, it provides visualization plots for a more intuitive understanding of relationships between variables.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Limitations of this metric include detecting only linear relationships and associations; nonlinear relationships may go unnoticed.\n",
       "- The absence of specified thresholds for determining correlation significance means the interpretation of the results is dependent on the user's expertise.\n",
       "- It doesn't manage missing values in the dataset, which need to be treated beforehand.</td>\n",
       "      <td id=\"T_1ac33_row88_col3\" class=\"data row88 col3\" >validmind.data_validation.DatasetCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row89_col0\" class=\"data row89 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row89_col1\" class=\"data row89 col1\" >Tabular Description Tables</td>\n",
       "      <td id=\"T_1ac33_row89_col2\" class=\"data row89 col2\" >**Purpose**: The main purpose of this metric is to gather and present the descriptive statistics of numerical, categorical, and datetime variables present in a dataset. The attributes it measures include the count, mean, minimum and maximum values, percentage of missing values, data types of fields, and unique values for categorical fields, among others.\n",
       "\n",
       "**Test Mechanism**: The test first segregates the variables in the dataset according to their data types (numerical, categorical, or datetime). Then, it compiles summary statistics for each type of variable. The specifics of these statistics vary depending on the type of variable:\n",
       "\n",
       "- For numerical variables, the metric extracts descriptors like count, mean, minimum and maximum values, count of missing values, and data types.\n",
       "- For categorical variables, it counts the number of unique values, displays unique values, counts missing values, and identifies data types.\n",
       "- For datetime variables, it counts the number of unique values, identifies the earliest and latest dates, counts missing values, and identifies data types.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Masses of missing values in the descriptive statistics results could hint at high risk or failure, indicating potential data collection, integrity, and quality issues.\n",
       "- Detection of inappropriate distributions for numerical variables, like having negative values for variables that are always supposed to be positive.\n",
       "- Identifying inappropriate data types, like a continuous variable being encoded as a categorical type.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a comprehensive overview of the dataset.\n",
       "- Gives a snapshot into the essence of the numerical, categorical, and datetime fields.\n",
       "- Identifies potential data quality issues such as missing values or inconsistencies crucial for building credible machine learning models.\n",
       "- The metadata, including the data type and missing value information, are vital for anyone including data scientists dealing with the dataset before the modeling process.\n",
       "\n",
       "**Limitations**:\n",
       "- It does not perform any deeper statistical analysis or tests on the data.\n",
       "- It does not handle issues such as outliers, or relationships between variables.\n",
       "- It offers no insights into potential correlations or possible interactions between variables.\n",
       "- It does not investigate the potential impact of missing values on the performance of the machine learning models.\n",
       "- It does not explore potential transformation requirements that may be necessary to enhance the performance of the chosen algorithm.</td>\n",
       "      <td id=\"T_1ac33_row89_col3\" class=\"data row89 col3\" >validmind.data_validation.TabularDescriptionTables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row90_col0\" class=\"data row90 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row90_col1\" class=\"data row90 col1\" >Auto MA</td>\n",
       "      <td id=\"T_1ac33_row90_col2\" class=\"data row90 col2\" >**Purpose**: The `AutoMA` metric serves an essential role of automated decision-making for selecting the optimal Moving Average (MA) order for every variable in a given time series dataset. The selection is dependent on the minimalization of BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion); these are established statistical tools used for model selection. Furthermore, prior to the commencement of the model fitting process, the algorithm conducts a stationarity test (Augmented Dickey-Fuller test) on each series.\n",
       "\n",
       "**Test Mechanism**: Starting off, the `AutoMA` algorithm checks whether the `max_ma_order` parameter has been provided. It consequently loops through all variables in the dataset, carrying out the Dickey-Fuller test for stationarity. For each stationary variable, it fits an ARIMA model for orders running from 0 to `max_ma_order`. The result is a list showcasing the BIC and AIC values of the ARIMA models based on different orders. The MA order, which yields the smallest BIC, is chosen as the 'best MA order' for every single variable. The final results include a table summarizing the auto MA analysis and another table listing the best MA order for each variable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- When a series is non-stationary (p-value>0.05 in the Dickey-Fuller test), the produced result could be inaccurate.\n",
       "- Any error that arises in the process of fitting the ARIMA models, especially with a higher MA order, can potentially indicate risks and might need further investigation.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric facilitates automation in the process of selecting the MA order for time series forecasting. This significantly saves time and reduces efforts conventionally necessary for manual hyperparameter tuning.\n",
       "- The use of both BIC and AIC enhances the likelihood of selecting the most suitable model.\n",
       "- The metric ascertains the stationarity of the series prior to model fitting, thus ensuring that the underlying assumptions of the MA model are fulfilled.\n",
       "\n",
       "**Limitations**:\n",
       "- If the time series fails to be stationary, the metric may yield inaccurate results. Consequently, it necessitates pre-processing steps to stabilize the series before fitting the ARIMA model.\n",
       "- The metric adopts a rudimentary model selection process based on BIC and doesn't consider other potential model selection strategies. Depending on the specific dataset, other strategies could be more appropriate.\n",
       "- The 'max_ma_order' parameter must be manually input which doesn't always guarantee optimal performance, especially when configured too low.\n",
       "- The computation time increases with the rise in `max_ma_order`, hence, the metric may become computationally costly for larger values.</td>\n",
       "      <td id=\"T_1ac33_row90_col3\" class=\"data row90 col3\" >validmind.data_validation.AutoMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row91_col0\" class=\"data row91 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row91_col1\" class=\"data row91 col1\" >Unique Rows</td>\n",
       "      <td id=\"T_1ac33_row91_col2\" class=\"data row91 col2\" >**Purpose**: The UniqueRows test is designed to gauge the quality of the data supplied to the machine learning model by verifying that the count of distinct rows in the dataset exceeds a specific threshold, thereby ensuring a varied collection of data. Diversity in data is essential for training an unbiased and robust model that excels when faced with novel data.\n",
       "\n",
       "**Test Mechanism**: The testing process starts with calculating the total number of rows in the dataset. Subsequently, the count of unique rows is determined for each column in the dataset. If the percentage of unique rows (calculated as the ratio of unique rows to the overall row count) is less than the prescribed minimum percentage threshold given as a function parameter, the test is passed. The results are cached and a final pass or fail verdict is given based on whether all columns have successfully passed the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A lack of diversity in data columns, demonstrated by a count of unique rows that falls short of the preset minimum percentage threshold, is indicative of high risk.\n",
       "- This lack of variety in the data signals potential issues with data quality, possibly leading to overfitting in the model and issues with generalization, thus posing a significant risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The UniqueRows test is efficient in evaluating the data's diversity across each information column in the dataset.\n",
       "- This test provides a quick, systematic method to assess data quality based on uniqueness, which can be pivotal in developing effective and unbiased machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- A limitation of the UniqueRows test is its assumption that the data's quality is directly proportionate to its uniqueness, which may not always hold true. There might be contexts where certain non-unique rows are essential and should not be overlooked.\n",
       "- The test does not consider the relative 'importance' of each column in predicting the output, treating all columns equally.\n",
       "- This test may not be suitable or useful for categorical variables, where the count of unique categories is inherently limited.</td>\n",
       "      <td id=\"T_1ac33_row91_col3\" class=\"data row91 col3\" >validmind.data_validation.UniqueRows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row92_col0\" class=\"data row92 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row92_col1\" class=\"data row92 col1\" >Too Many Zero Values</td>\n",
       "      <td id=\"T_1ac33_row92_col2\" class=\"data row92 col2\" >**Purpose**: The 'TooManyZeroValues' test is utilized to identify numerical columns in the dataset that may present a quantity of zero values considered excessive. The aim is to detect situations where these may implicate data sparsity or a lack of variation, limiting their effectiveness within a machine learning model. The definition of 'too many' is quantified as a percentage of total values, with a default set to 3%.\n",
       "\n",
       "**Test Mechanism**: This test is conducted by looping through each column in the dataset and categorizing those that pertain to numerical data. On identifying a numerical column, the function computes the total quantity of zero values and their ratio to the total row count. Should the proportion exceed a pre-set threshold parameter, set by default at 0.03 or 3%, the column is considered to have failed the test. The results for each column are summarised and reported, indicating the count and percentage of zero values for each numerical column, alongside a status indicating whether the column has passed or failed the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Indicators evidencing a high risk connected with this test would include numerical columns showing a high ratio of zero values when compared to the total count of rows (exceeding a pre-determined threshold).\n",
       "- Columns characterized by zero values across the board suggest a complete lack of data variation, signifying high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- Assists in highlighting columns featuring an excess of zero values that could otherwise go unnoticed within a large dataset.\n",
       "- Provides the flexibility to alter the threshold that determines when the quantity of zero values becomes 'too many', thus catering to specific needs of a particular analysis or model.\n",
       "- Offers feedback in the form of both counts and percentages of zero values, which allows a closer inspection of the distribution and proportion of zeros within a column.\n",
       "- Targets specifically numerical data, thereby avoiding inappropriate application to non-numerical columns and mitigating the risk of false test failures.\n",
       "\n",
       "**Limitations**:\n",
       "- Is exclusively designed to check for zero values, and doesn’t assesses the potential impact of other values that could affect the dataset, such as extremely high or low figures, missing values or outliers.\n",
       "- Lacks the ability to detect a repetitive pattern of zeros, which could be significant in time-series or longitudinal data.\n",
       "- Zero values can actually be meaningful in some contexts, therefore tagging them as 'too many' could potentially misinterpret the data to some extent.\n",
       "- This test does not take into consideration the context of the dataset, and fails to recognize that within certain columns, a high number of zero values could be quite normal and not necessarily an indicator of poor data quality.\n",
       "- Cannot evaluate non-numerical or categorical columns, which might bring with them different types of concerns or issues.</td>\n",
       "      <td id=\"T_1ac33_row92_col3\" class=\"data row92 col3\" >validmind.data_validation.TooManyZeroValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row93_col0\" class=\"data row93 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row93_col1\" class=\"data row93 col1\" >High Pearson Correlation</td>\n",
       "      <td id=\"T_1ac33_row93_col2\" class=\"data row93 col2\" >**Purpose**: The High Pearson Correlation test measures the linear relationship between features in a dataset, with the main goal of identifying high correlations that might indicate feature redundancy or multicollinearity. Identification of such issue allows developers and risk management teams to properly deal with potential impacts on the machine learning model's performance and interpretability.\n",
       "\n",
       "**Test Mechanism**: The test works by generating pairwise Pearson correlations for all features in the dataset, then sorting and eliminating duplicate and self-correlations. It assigns a Pass or Fail based on whether the absolute value of the correlation coefficient surpasses a pre-set threshold (defaulted at 0.3). It lastly returns the top ten strongest correlations regardless of passing or failing status.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk indication would be the presence of correlation coefficients exceeding the threshold.\n",
       "- If the features share a strong linear relationship, this could lead to potential multicollinearity and model overfitting.\n",
       "- Redundancy of variables can undermine the interpretability of the model due to uncertainty over the authenticity of individual variable's predictive power.\n",
       "\n",
       "**Strengths**:\n",
       "- The High Pearson Correlation test provides a quick and simple means of identifying relationships between feature pairs.\n",
       "- It generates a transparent output which not only displays pairs of correlated variables but also delivers the Pearson correlation coefficient and a Pass or Fail status for each.\n",
       "- It aids early identification of potential multicollinearity issues that may disrupt model training.\n",
       "\n",
       "**Limitations**:\n",
       "- The Pearson correlation test can only delineate linear relationships. It fails to shed light on nonlinear relationships or dependencies.\n",
       "- It is sensitive to outliers where a few outliers could notably affect the correlation coefficient.\n",
       "- It is limited to identifying redundancy only within feature pairs. When three or more variables are linearly dependent, it may fail to spot this complex relationship.\n",
       "- The top 10 result filter might not fully capture the richness of the data; an option to configure the number of retained results could be helpful.</td>\n",
       "      <td id=\"T_1ac33_row93_col3\" class=\"data row93 col3\" >validmind.data_validation.HighPearsonCorrelation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row94_col0\" class=\"data row94 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row94_col1\" class=\"data row94 col1\" >AC Fand PACF Plot</td>\n",
       "      <td id=\"T_1ac33_row94_col2\" class=\"data row94 col2\" >**Purpose**: The ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) plot test is employed to analyze time series data in machine learning models. It illuminates the correlation of the data over time by plotting the correlation of the series with its own lags (ACF), and the correlations after removing effects already accounted for by earlier lags (PACF). This information can identify trends, such as seasonality, degrees of autocorrelation, and inform the selection of order parameters for AutoRegressive Integrated Moving Average (ARIMA) models.\n",
       "\n",
       "**Test Mechanism**: The `ACFandPACFPlot` test accepts a dataset with a time-based index. It first confirms the index is of a datetime type, then handles any NaN values. The test subsequently generates ACF and PACF plots for each column in the dataset, producing a subplot for each. If the dataset doesn't include key columns, an error is returned.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Sudden drops in the correlation at a specific lag might signal a model at high risk.\n",
       "- Consistent high correlation across multiple lags could also indicate non-stationarity in the data, which may suggest that a model estimated on this data won't generalize well to future, unknown data.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- ACF and PACF plots offer clear graphical representations of the correlations in time series data.\n",
       "- These plots are effective at revealing important data characteristics such as seasonality, trends, and correlation patterns.\n",
       "- The insights from these plots aid in better model configuration, particularly in the selection of ARIMA model parameters.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- ACF and PACF plots are exclusively for time series data and hence, can't be applied to all ML models.\n",
       "- These plots require large, consistent datasets as gaps could lead to misleading results.\n",
       "- The plots can only represent linear correlations and fail to capture any non-linear relationships within the data.\n",
       "- The plots might be difficult for non-experts to interpret and should not replace more advanced analyses.</td>\n",
       "      <td id=\"T_1ac33_row94_col3\" class=\"data row94 col3\" >validmind.data_validation.ACFandPACFPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row95_col0\" class=\"data row95 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row95_col1\" class=\"data row95 col1\" >Bivariate Histograms</td>\n",
       "      <td id=\"T_1ac33_row95_col2\" class=\"data row95 col2\" >**Purpose**: This metric, dubbed BivariateHistograms, is primarily used for visual data analysis via the inspection of variable distribution, specifically categorical variables. Its main objective is to ascertain any potential correlations between these variables and distributions within each defined target class. This is achieved by offering an intuitive avenue into gaining insights into the characteristics of the data and any plausible patterns therein.\n",
       "\n",
       "**Test Mechanism**: The working mechanism of the BivariateHistograms module revolves around an input dataset and a series of feature pairs. It uses seaborn's histogram plotting function and matplotlib techniques to create bivariate histograms for each feature pair in the dataset. Two histograms, stratified by the target column status, are produced for every pair of features. This enables the telling apart of different target statuses through color differentiation. The module also offers optional functionality for restricting the data by a specific status through the target_filter parameter.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Irregular or unexpected distributions of data across the different categories.\n",
       "- Highly skewed data distributions.\n",
       "- Significant deviations from the perceived 'normal' or anticipated distributions.\n",
       "- Large discrepancies in distribution patterns between various target statuses.\n",
       "\n",
       "**Strengths**:\n",
       "- Owing to its simplicity, the histogram-based approach is easy to implement and interpret which translates to quick insights.\n",
       "- The metrics provides a consolidated view of the distribution of data across different target conditions for each variable pair, thereby assisting in highlighting potential correlations and patterns.\n",
       "- It proves advantageous in spotting anomalies, comprehending interactions among features, and facilitating exploratory data analysis.\n",
       "\n",
       "**Limitations**:\n",
       "- Its simplicity may be a drawback when it comes to spotting intricate or complex patterns in data.\n",
       "- Overplotting might occur when working with larger datasets.\n",
       "- The metric is only applicable to categorical data, and offers limited insights for numerical or continuous variables.\n",
       "- The interpretation of visual results hinges heavily on the expertise of the observer, possibly leading to subjective analysis.</td>\n",
       "      <td id=\"T_1ac33_row95_col3\" class=\"data row95 col3\" >validmind.data_validation.BivariateHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row96_col0\" class=\"data row96 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row96_col1\" class=\"data row96 col1\" >WOE Bin Table</td>\n",
       "      <td id=\"T_1ac33_row96_col2\" class=\"data row96 col2\" >**Purpose**: The Weight of Evidence (WoE) and Information Value (IV) test is intended to evaluate the predictive power of each feature in the machine learning model. The test generates binned groups of values from each feature in a dataset, computes the WoE value and the IV value for each bin. These values provide insights on the relationship between each feature and the target variable and their contribution towards the predictive output of the model.\n",
       "\n",
       "**Test Mechanism**: The metric leverages the `scorecardpy.woebin` method to perform WoE-based automatic binning on the dataset. Depending on the parameter `breaks_adj`, the method adjusts the cut-off points for binning numeric variables. The bins are then used to calculate the WoE and IV. The metric requires a dataset with the target variable defined. The metric outputs a dataframe that comprises the bin boundaries, WoE, and IV values for each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High IV values, which denote variables with too much predictive power which might lead to overfitting\n",
       "- Errors during the binning process, which might be due to inappropriate data types or poorly defined bins\n",
       "\n",
       "**Strengths**:\n",
       "- The WoE and IV test is highly effective for feature selection in binary classification problems, as it quantifies how much predictive information is packed within each feature regarding the binary outcome\n",
       "- The WoE transformation creates a monotonic relationship between the target and independent variables\n",
       "\n",
       "**Limitations**:\n",
       "- Mainly designed for binary classification tasks, therefore it might not be applicable or reliable for multi-class classification or regression tasks\n",
       "- If the dataset has many features or the features are not binnable or they are non-numeric, this process might encounter difficulties\n",
       "- This metric doesn't help in identifying if the predictive factor being observed is a coincidence or a real phenomenon due to data randomness</td>\n",
       "      <td id=\"T_1ac33_row96_col3\" class=\"data row96 col3\" >validmind.data_validation.WOEBinTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row97_col0\" class=\"data row97 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row97_col1\" class=\"data row97 col1\" >Heatmap Feature Correlations</td>\n",
       "      <td id=\"T_1ac33_row97_col2\" class=\"data row97 col2\" >**Purpose:** The HeatmapFeatureCorrelations metric is utilized to evaluate the degree of interrelationships between pairs of input features within a dataset. This metric allows us to visually comprehend the correlation patterns through a heatmap, which can be essential in understanding which features may contribute most significantly to the performance of the model. Features that have high intercorrelation can potentially reduce the model's ability to learn, thus impacting the overall performance and stability of the machine learning model.\n",
       "\n",
       "**Test Mechanism:** The metric executes the correlation test by computing the Pearson correlations for all pairs of numerical features. It then generates a heatmap plot using seaborn, a Python data visualization library. The colormap ranges from -1 to 1, indicating perfect negative correlation and perfect positive correlation respectively. A 'declutter' option is provided which, if set to true, removes variable names and numerical correlations from the plot to provide a more streamlined view. The size of feature names and correlation coefficients can be controlled through 'fontsize' parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Indicators of potential risk include features with high absolute correlation values.\n",
       "- A significant degree of multicollinearity might lead to instabilities in the trained model and can also result in overfitting.\n",
       "- The presence of multiple homogeneous blocks of high positive or negative correlation within the plot might indicate redundant or irrelevant features included within the dataset.\n",
       "\n",
       "**Strengths:**\n",
       "- The strength of this metric lies in its ability to visually represent the extent and direction of correlation between any two numeric features, which aids in the interpretation and understanding of complex data relationships.\n",
       "- The heatmap provides an immediate and intuitively understandable representation, hence, it is extremely useful for high-dimensional datasets where extracting meaningful relationships might be challenging.\n",
       "\n",
       "**Limitations:**\n",
       "- The central limitation might be that it can only calculate correlation between numeric features, making it unsuitable for categorical variables unless they are already numerically encoded in a meaningful manner.\n",
       "- It uses Pearson's correlation, which only measures linear relationships between features. It may perform poorly in cases where the relationship is non-linear.\n",
       "- Large feature sets might result in cluttered and difficult-to-read correlation heatmaps, especially when the 'declutter' option is set to false.</td>\n",
       "      <td id=\"T_1ac33_row97_col3\" class=\"data row97 col3\" >validmind.data_validation.HeatmapFeatureCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row98_col0\" class=\"data row98 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row98_col1\" class=\"data row98 col1\" >Time Series Frequency</td>\n",
       "      <td id=\"T_1ac33_row98_col2\" class=\"data row98 col2\" >**Purpose**: The purpose of the TimeSeriesFrequency test is to evaluate the consistency in the frequency of data points in a time-series dataset. This test inspects the intervals or duration between each data point to determine if a fixed pattern (such as daily, weekly, or monthly) exists. The identification of such patterns is crucial to time-series analysis as any irregularities could lead to erroneous results and hinder the model's capacity for identifying trends and patterns.\n",
       "\n",
       "**Test Mechanism**: Initially, the test checks if the dataframe index is in datetime format. Subsequently, it utilizes pandas' `infer_freq` method to identify the frequency of each data series within the dataframe. The `infer_freq` method attempts to establish the frequency of a time series and returns both the frequency string and a dictionary relating these strings to their respective labels. The test compares the frequencies of all datasets. If they share a common frequency, the test passes, but it fails if they do not. Additionally, Plotly is used to create a frequency plot, offering a visual depiction of the time differences between consecutive entries in the dataframe index.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The test fails, indicating multiple unique frequencies within the dataset. This failure could suggest irregular intervals between observations, potentially interrupting pattern recognition or trend analysis.\n",
       "- The presence of missing or null frequencies could be an indication of inconsistencies in data or gaps within the data collection process.\n",
       "\n",
       "**Strengths**:\n",
       "- This test uses a systematic approach to checking the consistency of data frequency within a time-series dataset.\n",
       "- It increases the model's reliability by asserting the consistency of observations over time, an essential factor in time-series analysis.\n",
       "- The test generates a visual plot, providing an intuitive representation of the dataset's frequency distribution, which caters to visual learners and aids in interpretation and explanation.\n",
       "\n",
       "**Limitations**:\n",
       "- This test is only applicable to time-series datasets and hence not suitable for other types of datasets.\n",
       "- The `infer_freq` method might not always correctly infer frequency when faced with missing or irregular data points.\n",
       "- Depending on context or the model under development, mixed frequencies might sometimes be acceptable, but this test considers them a failing condition.</td>\n",
       "      <td id=\"T_1ac33_row98_col3\" class=\"data row98 col3\" >validmind.data_validation.TimeSeriesFrequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row99_col0\" class=\"data row99 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row99_col1\" class=\"data row99 col1\" >Dataset Split</td>\n",
       "      <td id=\"T_1ac33_row99_col2\" class=\"data row99 col2\" >**Purpose:** The DatasetSplit test is designed to evaluate and visualize the distribution of data among training, testing, and validation datasets, if available, within a given machine learning model. The main purpose is to assess whether the model's datasets are split appropriately, as an imbalanced split might affect the model's ability to learn from the data and generalize to unseen data.\n",
       "\n",
       "**Test Mechanism:** The DatasetSplit test first calculates the total size of all available datasets in the model. Then, for each individual dataset, the methodology involves determining the size of the dataset and its proportion relative to the total size. The results are then conveniently summarized in a table that shows dataset names, sizes, and proportions. Absolute size and proportion of the total dataset size are displayed for each individual dataset.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- A very small training dataset, which may result in the model not learning enough from the data.\n",
       "- A very large training dataset and a small test dataset, which may lead to model overfitting and poor generalization to unseen data.\n",
       "- A small or non-existent validation dataset, which might complicate the model's performance assessment.\n",
       "\n",
       "**Strengths:**\n",
       "- The DatasetSplit test provides a clear, understandable visualization of dataset split proportions, which can highlight any potential imbalance in dataset splits quickly.\n",
       "- It covers a wide range of task types including classification, regression, and text-related tasks.\n",
       "- The metric is not tied to any specific data type and is applicable to tabular data, time series data, or text data.\n",
       "\n",
       "**Limitations:**\n",
       "- The DatasetSplit test does not provide any insight into the quality or diversity of the data within each split, just the size and proportion.\n",
       "- The test does not give any recommendations or adjustments for imbalanced datasets.\n",
       "- Potential lack of compatibility with more complex modes of data splitting (for example, stratified or time-based splits) could limit the applicability of this test.</td>\n",
       "      <td id=\"T_1ac33_row99_col3\" class=\"data row99 col3\" >validmind.data_validation.DatasetSplit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row100_col0\" class=\"data row100 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row100_col1\" class=\"data row100 col1\" >Spread Plot</td>\n",
       "      <td id=\"T_1ac33_row100_col2\" class=\"data row100 col2\" >**Purpose**: The SpreadPlot metric is intended to graphically illustrate and analyse the relationships between pairs of time series variables within a given dataset. This facilitated understanding helps in identifying and assessing potential time series correlations, like cointegration, between the variables.\n",
       "\n",
       "**Test Mechanism**: The SpreadPlot metric operates by computing and representing the spread between each pair of time series variables in the dataset. In particular, the difference between two variables is calculated and presented as a line graph. This method is iterated for each unique pair of variables in the dataset.\n",
       "\n",
       "**Signs of High Risk**: Potential indicators of high risk related to the SpreadPlot metric might include:\n",
       "\n",
       "- Large fluctuations in the spread over a given timespan\n",
       "- Unexpected patterns or trends that may signal a potential risk in the underlying correlations between the variables\n",
       "- Presence of significant missing data or extreme outlier values, which could potentially skew the spread and indicate high risk\n",
       "\n",
       "**Strengths**: The SpreadPlot metric provides several key advantages:\n",
       "\n",
       "- It allows for thorough visual examination and interpretation of the correlations between time-series pairs\n",
       "- It aids in revealing complex relationships like cointegration\n",
       "- It enhances interpretability by visualising the relationships, thereby helping in spotting outliers and trends\n",
       "- It is capable of handling numerous variable pairs from the dataset through a versatile and adaptable process\n",
       "\n",
       "**Limitations**: Despite its advantages, the SpreadPlot metric does have certain drawbacks:\n",
       "\n",
       "- It primarily serves as a visualisation tool and does not offer quantitative measurements or statistics to objectively determine relationships\n",
       "- It heavily relies on the quality and granularity of the data\n",
       "- missing data or outliers can notably disturb the interpretation of the relationships\n",
       "- It can become inefficient or difficult to interpret with a high number of variables due to the profuse number of plots\n",
       "- It might not completely capture intricate non-linear relationships between the variables</td>\n",
       "      <td id=\"T_1ac33_row100_col3\" class=\"data row100 col3\" >validmind.data_validation.SpreadPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row101_col0\" class=\"data row101 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row101_col1\" class=\"data row101 col1\" >Time Series Line Plot</td>\n",
       "      <td id=\"T_1ac33_row101_col2\" class=\"data row101 col2\" >**Purpose**: The TimeSeriesLinePlot metric is designed to generate and analyze time series data through the creation of line plots. This assists in the initial inspection of the data by providing a visual representation of patterns, trends, seasonality, irregularity, and anomalies that may be present in the dataset over a period of time.\n",
       "\n",
       "**Test Mechanism**: The mechanism for this Python class involves extracting the column names from the provided dataset and subsequently generates line plots for each column using the Plotly Python library. For every column in the dataset, a time-series line plot is created where the values are plotted against the dataset's datetime index. It is important to note that indexes that are not of datetime type will result in a ValueError.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Presence of time-series data that does not have datetime indices.\n",
       "- Provided columns do not exist in the provided dataset.\n",
       "- The detection of anomalous patterns or irregularities in the time-series plots, indicating potential high model instability or probable predictive error.\n",
       "\n",
       "**Strengths**:\n",
       "- The visual representation of complex time series data, which simplifies understanding and helps in recognizing temporal trends, patterns, and anomalies.\n",
       "- The adaptability of the metric, which allows it to effectively work with multiple time series within the same dataset.\n",
       "- Enables the identification of anomalies and irregular patterns through visual inspection, assisting in spotting potential data or model performance problems.\n",
       "\n",
       "**Limitations**:\n",
       "- The effectiveness of the metric is heavily reliant on the quality and patterns of the provided time series data.\n",
       "- Exclusively a visual tool, it lacks the capability to provide quantitative measurements, making it less effective for comparing and ranking multiple models or when specific numerical diagnostics are needed.\n",
       "- The metric necessitates that the time-specific data has been transformed into a datetime index, with the data formatted correctly.\n",
       "- The metric has an inherent limitation in that it cannot extract deeper statistical insights from the time series data, which can limit its efficacy with complex data structures and phenomena.</td>\n",
       "      <td id=\"T_1ac33_row101_col3\" class=\"data row101 col3\" >validmind.data_validation.TimeSeriesLinePlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row102_col0\" class=\"data row102 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row102_col1\" class=\"data row102 col1\" >Pi T Credit Scores Histogram</td>\n",
       "      <td id=\"T_1ac33_row102_col2\" class=\"data row102 col2\" >**Purpose**: The PiT (Point in Time) Credit Scores Histogram metric is used to evaluate the predictive performance of a credit risk assessment model. This metric provides a visual representation of observed versus predicted default scores and enables quick and intuitive comparison for model assessment.\n",
       "\n",
       "**Test Mechanism**: This metric generates histograms for both observed and predicted score distributions of defaults and non-defaults. The simultaneous representation of both the observed and predicted scores sheds light on the model's ability to accurately predict credit risk.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Significant discrepancies between the observed and predicted histograms, suggesting that the model may not be adequately addressing certain risk factors.\n",
       "- Concentration of predicted defaults towards one end of the graph, or uneven distribution in comparison to observed scores, indicating potential issues in the model's interpretation of the data or outcome prediction.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides an intuitive visual representation of model performance that's easy to comprehend, even for individuals without a technical background.\n",
       "- Useful for understanding the model's ability to distinguish between defaulting and non-defaulting entities.\n",
       "- Specifically tailored for assessing credit risk models. The Point in Time (PiT) factor considers the evolution of credit risk over time.\n",
       "\n",
       "**Limitations**:\n",
       "- As the information is visual, precise and quantitative results for detailed statistical analyses may not be obtained.\n",
       "- The method relies on manual inspection and comparison, introducing subjectivity and potential bias.\n",
       "- Subtle discrepancies might go unnoticed and it could be less reliable for identifying such cues.\n",
       "- Performance may degrade when score distributions overlap significantly or when too many scores are plotted, resulting in cluttered or hard-to-decipher graphs.</td>\n",
       "      <td id=\"T_1ac33_row102_col3\" class=\"data row102 col3\" >validmind.data_validation.PiTCreditScoresHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row103_col0\" class=\"data row103 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row103_col1\" class=\"data row103 col1\" >Auto Seasonality</td>\n",
       "      <td id=\"T_1ac33_row103_col2\" class=\"data row103 col2\" >**Purpose:** The AutoSeasonality metric's purpose is to automatically detect and identify the best seasonal order or period for each variable in a time series dataset. This detection helps to quantify periodic patterns and seasonality that reoccur at fixed intervals of time in the data. This is especially significant for forecasting-based models, where understanding the seasonality component can drastically improve prediction accuracy.\n",
       "\n",
       "**Test Mechanism:** This metric uses the seasonal decomposition method from the Statsmodels Python library. The function takes the 'additive' model type for each variable and applies it within the prescribed range of 'min_period' and 'max_period'. The function decomposes the seasonality for each period in the range and calculates the mean residual error for each period. The seasonal period that results in the minimum residuals is marked as the 'Best Period'. The test results include the 'Best Period', the calculated residual errors, and a determination of 'Seasonality' or 'No Seasonality'.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- If the optimal seasonal period (or 'Best Period') is consistently at the maximum or minimum limit of the offered range for a majority of variables, it may suggest that the range set does not adequately capture the true seasonal pattern in the series.\n",
       "- A high average 'Residual Error' for the selected 'Best Period' could indicate issues with the model's performance.\n",
       "\n",
       "**Strengths:**\n",
       "- The metric offers an automatic approach to identifying and quantifying the optimal seasonality, providing a robust method for analyzing time series datasets.\n",
       "- It is applicable to multiple variables in a dataset, providing a comprehensive evaluation of each variable's seasonality.\n",
       "- The use of concrete and measurable statistical methods improves the objectivity and reproducibility of the model.\n",
       "\n",
       "**Limitations:**\n",
       "- This AutoSeasonality metric may not be suitable if the time series data exhibits random walk behaviour or lacks clear seasonality, as the seasonal decomposition model may not be appropriate.\n",
       "- The defined range for the seasonal period (min_period and max_period) can influence the outcomes. If the actual seasonality period lies outside this range, this method will not be able to identify the true seasonal order.\n",
       "- This metric may not be able to fully interpret complex patterns that go beyond the simple additive model for seasonal decomposition.\n",
       "- The tool may incorrectly infer seasonality if random fluctuations in the data match the predefined seasonal period range.</td>\n",
       "      <td id=\"T_1ac33_row103_col3\" class=\"data row103 col3\" >validmind.data_validation.AutoSeasonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row104_col0\" class=\"data row104 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row104_col1\" class=\"data row104 col1\" >Bivariate Scatter Plots</td>\n",
       "      <td id=\"T_1ac33_row104_col2\" class=\"data row104 col2\" >**Purpose**: This metric is intended for visual inspection and monitoring of relationships between pairs of variables in a machine learning model targeting classification tasks. It is especially useful for understanding how predictor variables (features) behave in relation to each other and how they are distributed for different classes of the target variable, which could inform feature selection, model-building strategies, and even alert to possible biases and irregularities in the data.\n",
       "\n",
       "**Test Mechanism**: This metric operates by creating a scatter plot for each pair of the selected features in the dataset. If the parameters \"features_pairs\" are not specified, an error will be thrown. The metric offers flexibility by allowing the user to filter on a specific target class\n",
       "- specified by the \"target_filter\" parameter\n",
       "- for more granified insights. Each scatterplot is then color-coded based on the category of the target variable for better visual differentiation. The seaborn scatterplot library is used for generating the plots.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Visual patterns which might suggest non-linear relationships, substantial skewness, multicollinearity, clustering, or isolated outlier points in the scatter plot.\n",
       "- Such issues could affect the assumptions and performance of some models, especially the ones assuming linearity like linear regression or logistic regression.\n",
       "\n",
       "**Strengths**:\n",
       "- Scatterplots are simple and intuitive for users to understand, providing a visual tool to pinpoint complex relationships between two variables.\n",
       "- They are useful for outlier detection, identification of variable associations and trends, including non-linear patterns which can be overlooked by other linear-focused metrics or tests.\n",
       "- The implementation also supports visualizing binary or multi-class classification datasets.\n",
       "\n",
       "**Limitations**:\n",
       "- Scatterplots are limited to bivariate analysis\n",
       "- the relationship of two variables at a time\n",
       "- and might not reveal the full picture in higher dimensions or where interactions are present.\n",
       "- They are not ideal for very large datasets as points will overlap and render the visualization less informative.\n",
       "- Scatterplots are more of an exploratory tool rather than a formal statistical test, so they don't provide any quantitative measure of model quality or performance.\n",
       "- Interpretation of scatterplots relies heavily on the domain knowledge and judgment of the viewer, which can introduce subjective bias.</td>\n",
       "      <td id=\"T_1ac33_row104_col3\" class=\"data row104 col3\" >validmind.data_validation.BivariateScatterPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row105_col0\" class=\"data row105 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row105_col1\" class=\"data row105 col1\" >Engle Granger Coint</td>\n",
       "      <td id=\"T_1ac33_row105_col2\" class=\"data row105 col2\" >**Purpose**: The intent of this Engle-Granger cointegration test is to explore and quantify the degree of co-movement between pairs of time series variables in a dataset. This is particularly useful in enhancing the accuracy of predictive regressions whenever the underlying variables are co-integrated, i.e., they move together over time.\n",
       "\n",
       "**Test Mechanism**: The test first drops any non-applicable values from the input dataset and then iterates over each pair of variables to apply the Engle-Granger cointegration test. The test generates a 'p' value, which is then compared against a pre-specified threshold (0.05 by default). The pair is labeled as 'Cointegrated' if the 'p' value is less than or equal to the threshold or 'Not cointegrated' otherwise. A summary table is returned by the metric showing cointegration results for each variable pair.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk might be indicated if a significant number of variables that were hypothesized to be cointegrated do not pass the test.\n",
       "- Another sign of high risk is if a considerable number of 'p' values are close to the threshold. This is a risk because minor fluctuations in the data can switch the decision between 'Cointegrated' and 'Not cointegrated'.\n",
       "\n",
       "**Strengths**:\n",
       "- The Engle-Granger cointegration test provides an effective way to analyze relationships between time series, particularly in contexts where it's essential to check if variables are moving together in a statistically significant manner.\n",
       "- It is useful in various domains, especially finance or economics. Here, predictive models often hinge on understanding how different variables move together over time.\n",
       "\n",
       "**Limitations**:\n",
       "- The Engle-Granger cointegration test assumes that the time series are integrated of the same order, which isn't always true in multivariate time series datasets.\n",
       "- The presence of non-stationary characteristics in the series or structural breaks can result in falsely positive or negative cointegration results.\n",
       "- The test may not perform well for small sample sizes due to lack of statistical power. Therefore, it should be used with caution, and whenever possible, supplemented with other predictive indicators for a more robust model evaluation.</td>\n",
       "      <td id=\"T_1ac33_row105_col3\" class=\"data row105 col3\" >validmind.data_validation.EngleGrangerCoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row106_col0\" class=\"data row106 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row106_col1\" class=\"data row106 col1\" >Time Series Missing Values</td>\n",
       "      <td id=\"T_1ac33_row106_col2\" class=\"data row106 col2\" >**Purpose**: This test is designed to validate the quality of a historical time-series dataset by verifying that the number of missing values is below a specified threshold. As time-series models greatly depend on the continuity and temporality of data points, missing values could compromise the model's performance. Consequently, this test aims to ensure data quality and readiness for the machine learning model, safeguarding its predictive capacity.\n",
       "\n",
       "**Test Mechanism**: The test method commences by validating if the dataset has a datetime index, if not, an error is raised. It establishes a lower limit threshold for missing values and performs a missing values check on each column of the dataset. An object for the test result is created stating whether the number of missing values is within the specified threshold. Additionally, the test calculates the percentage of missing values alongside the raw count.\n",
       "\n",
       "To aid in data visualization, the test generates two plots\n",
       "- a bar plot and a heatmap, to better illustrate the distribution and quantity of missing values per variable. The test results, a count of missing values, the percentage of missing values, and a pass/fail status are returned in a results table.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The number of missing values in any column of the dataset surpasses the threshold, marking a failure and a high-risk scenario. The reasons could range from incomplete data collection, faulty sensors to data preprocessing errors.\n",
       "- A continuous visual 'streak' in the heatmap may indicate a systematic error during data collection, pointing towards another potential risk source.\n",
       "\n",
       "**Strengths**:\n",
       "- Effectively identifies missing values which could adversely affect the model's performance.\n",
       "- Applicable and customizable through the threshold parameter across different data sets.\n",
       "- Goes beyond raw numbers by calculating the percentage of missing values, offering a more relative understanding of data scarcity.\n",
       "- Includes a robust visualization mechanism for easy and fast understanding of data quality.\n",
       "\n",
       "**Limitations**:\n",
       "- Although it identifies missing values, the test does not provide solutions to handle them.\n",
       "- The test demands that the dataset should have a datetime index, hence limiting its use only to time series analysis.\n",
       "- The test's sensitivity to the 'min_threshold' parameter may raise false alarms if set too strictly or may overlook problematic data if set too loosely.\n",
       "- Solely focuses on the 'missingness' of the data and might fall short in addressing other aspects of data quality.</td>\n",
       "      <td id=\"T_1ac33_row106_col3\" class=\"data row106 col3\" >validmind.data_validation.TimeSeriesMissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row107_col0\" class=\"data row107 col0\" >DatasetMetadata</td>\n",
       "      <td id=\"T_1ac33_row107_col1\" class=\"data row107 col1\" >Dataset Metadata</td>\n",
       "      <td id=\"T_1ac33_row107_col2\" class=\"data row107 col2\" >**Purpose**: The `DatasetMetadata` test is primarily aimed at collecting and logging essential descriptive statistics related to the training datasets. This test generates essential metadata such as the types of tasks (classification, regression, text_classification, text_summarization) and tags (tabular_data, time_series_data, text_data) associated with the datasets. This transparency facilitates model validation by linking different metrics and test results to the originating dataset.\n",
       "\n",
       "**Test Mechanism**: Rather than conducting a test or implementing a grading scale, this class collects and logs dataset metadata. During post-initialization, the metadata is linked to the dataset object. The `run` method produces a `TestSuiteDatasetResult` object, which is assigned a unique ID and is bound to a dataset. The dataset metadata is associated with this ID for use in future, more focused, validation procedures.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The metadata is incomplete or incorrect which can lead to inaccuracies in model risk assessment.\n",
       "- Dataset labels or types are missing, leading to issues in further model validation or mispresentations.\n",
       "\n",
       "**Strengths**:\n",
       "- The class brings transparency to model validation exercises by providing detailed information about the dataset.\n",
       "- It assists in error diagnosis and behaviors correlation to the model.\n",
       "- Ensures the correctness of tasks and data types associations and allows superior model explanations.\n",
       "- Supports dataset versioning by logging each dataset's metadata, maintaining a trackable history of alterations.\n",
       "\n",
       "**Limitations**:\n",
       "- The `DatasetMetadata` class's completeness and accuracy might be questionable, especially if metadata isn't appropriately added or is inaccurate.\n",
       "- It doesn't involve the evaluation of the dataset's quality or the direct validation of model predictions, hence it should be combined with other tests for a more comprehensive assessment.\n",
       "- The class cannot detect potential bias in the dataset. For bias detection, separate tests specifically tailored towards fairness and bias detection would be necessary.</td>\n",
       "      <td id=\"T_1ac33_row107_col3\" class=\"data row107 col3\" >validmind.data_validation.DatasetMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row108_col0\" class=\"data row108 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row108_col1\" class=\"data row108 col1\" >Time Series Histogram</td>\n",
       "      <td id=\"T_1ac33_row108_col2\" class=\"data row108 col2\" >**Purpose**: The purpose of this metric is to perform a histogram analysis on time-series data. It primarily assesses the distribution of values within a dataset over a period of time, typically used for regression tasks. The types of data that this metric can be applicable to are diverse, ranging from internet traffic and stock prices to weather data. This analysis provides valuable insights into the probability distribution, skewness, and peakness (kurtosis) underlying the data.\n",
       "\n",
       "**Test Mechanism**: This test operates on a specific column within the dataset that is required to have a datetime type index. It goes through each column in the given dataset, creating a histogram with Seaborn's histplot function. In cases where the dataset includes more than one time-series (i.e., more than one column with a datetime type index), a distinct histogram is plotted for each series. Additionally, a kernel density estimate (KDE) line is drawn for each histogram, providing a visualization of the data's underlying probability distribution. The x and y-axis labels are purposely hidden to concentrate solely on the data distribution.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The dataset lacks a column with a datetime type index.\n",
       "- The specified columns do not exist within the dataset.\n",
       "- The data distribution within the histogram demonstrates high degrees of skewness or kurtosis, which could bias the model.\n",
       "- Outliers that differ significantly from the primary data distribution are present.\n",
       "\n",
       "**Strengths**:\n",
       "- It serves as a visual diagnostic tool, offering an ideal starting point for understanding the overall behavior and distribution trends within the dataset.\n",
       "- It is effective for both single and multiple time-series data analysis.\n",
       "- The Kernel Density Estimation (KDE) line provides a smooth estimate of the overall trend in data distribution.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric only presents a high-level view of data distribution and does not offer specific numeric measures such as skewness or kurtosis.\n",
       "- The histogram does not display precise data values; due to the data grouping into bins, some detail is inevitably lost, marking a trade-off between precision and general overview.\n",
       "- The histogram cannot handle non-numeric data columns.\n",
       "- The histogram's shape may be sensitive to the number of bins used.</td>\n",
       "      <td id=\"T_1ac33_row108_col3\" class=\"data row108 col3\" >validmind.data_validation.TimeSeriesHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row109_col0\" class=\"data row109 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row109_col1\" class=\"data row109 col1\" >Lagged Correlation Heatmap</td>\n",
       "      <td id=\"T_1ac33_row109_col2\" class=\"data row109 col2\" >**Purpose**: The LaggedCorrelationHeatmap metric is utilized to appraise and illustrate the correlation between the target variable and delayed copies (lags) of independent variables in a time-series dataset. It assists in revealing relationships in time-series data where the influence of an independent variable on the dependent variable is not immediate but occurs after a period (lags).\n",
       "\n",
       "**Test Mechanism**: To execute this test, Python's Pandas library pairs with Plotly to perform computations and present the visualization in the form of a heatmap. The test begins by extracting the target variable and corresponding independent variables from the dataset. Then, generation of lags of independent variables takes place, followed by the calculation of correlation between these lagged variables and the target variable. The outcome is a correlation matrix that gets recorded and illustrated as a heatmap, where different color intensities represent the strength of the correlation, making patterns easier to identify.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Insignificant correlations across the heatmap, indicating a lack of noteworthy relationships between variables.\n",
       "- Correlations that break intuition or previous understanding, suggesting potential issues with the dataset or the model.\n",
       "\n",
       "**Strengths**:\n",
       "- This metric serves as an exceptional tool for exploring and visualizing time-dependent relationships between features and the target variable in a time-series dataset.\n",
       "- It aids in identifying delayed effects that might go unnoticed with other correlation measures.\n",
       "- The heatmap offers an intuitive visual representation of time-dependent correlations and influences.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric presumes linear relationships between variables, potentially ignoring non-linear relationships.\n",
       "- The correlation considered is linear; therefore, intricate non-linear interactions might be overlooked.\n",
       "- The metric is only applicable for time-series data, limiting its utility outside of this context.\n",
       "- The number of lags chosen can significantly influence the results; too many lags can render the heatmap difficult to interpret, while too few might overlook delayed effects.\n",
       "- This metric does not take into account any causal relationships, but merely demonstrates correlation.</td>\n",
       "      <td id=\"T_1ac33_row109_col3\" class=\"data row109 col3\" >validmind.data_validation.LaggedCorrelationHeatmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row110_col0\" class=\"data row110 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row110_col1\" class=\"data row110 col1\" >Seasonal Decompose</td>\n",
       "      <td id=\"T_1ac33_row110_col2\" class=\"data row110 col2\" >**Purpose**: This test utilizes the Seasonal Decomposition of Time Series by Loess (STL) method to decompose a dataset into its fundamental components: observed, trend, seasonal, and residuals. The purpose is to identify implicit patterns, majorly any seasonality, in the dataset's features which aid in developing a more comprehensive understanding and effectively validating the dataset.\n",
       "\n",
       "**Test Mechanism**: The testing process exploits the `seasonal_decompose` function from the `statsmodels.tsa.seasonal` library to evaluate each feature in the dataset. It isolates each feature into four components: observed, trend, seasonal, and residuals, and generates essentially six subplot graphs per feature for visual interpretation of the results. Prior to the seasonal decomposition, non-finite values are scrutinized and removed thus, ensuring reliability in the analysis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- **Non-Finiteness**: If a dataset carries too many non-finite values it might flag high risk as these values are omitted before conducting the seasonal decomposition.\n",
       "- **Frequent Warnings**: The test could be at risk if it chronically fails to infer frequency for a scrutinized feature.\n",
       "- **High Seasonality**: A high seasonal component could potentially render forecasts unreliable due to overwhelming seasonal variation.\n",
       "\n",
       "**Strengths**:\n",
       "- **Seasonality Detection**: The code aptly discerns hidden seasonality patterns in the features of datasets.\n",
       "- **Visualization**: The test facilitates interpretation and comprehension via graphical representations.\n",
       "- **Unrestricted Usage**: The code is not confined to any specific regression model, thereby promoting wide-ranging applicability.\n",
       "\n",
       "**Limitations**:\n",
       "- **Dependence on Assumptions**: The test presumes that features in the dataset are periodically distributed. If no frequency could be inferred for a variable, that feature is excluded from the test.\n",
       "- **Handling Non-finite Values**: The test disregards non-finite values during the analysis which could potentially result in incomplete understanding of the dataset.\n",
       "- **Unreliability with Noisy Datasets**: The test tends to produce unreliable results when used with heavy noise present in the dataset.</td>\n",
       "      <td id=\"T_1ac33_row110_col3\" class=\"data row110 col3\" >validmind.data_validation.SeasonalDecompose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row111_col0\" class=\"data row111 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row111_col1\" class=\"data row111 col1\" >WOE Bin Plots</td>\n",
       "      <td id=\"T_1ac33_row111_col2\" class=\"data row111 col2\" >**Purpose**: This test is designed to visualize the Weight of Evidence (WoE) and Information Value (IV) for categorical variables in a provided dataset. By showcasing the data distribution across different categories of each feature, it aids in understanding each variable's predictive power in the context of a classification-based machine learning model. Commonly used in credit scoring models, WoE and IV are robust statistical methods for evaluating a variable's predictive power.\n",
       "\n",
       "**Test Mechanism**: The test implementation follows defined steps. Initially, it selects non-numeric columns from the dataset and changes them to string type, paving the way for accurate binning. It then performs an automated WoE binning operation on these selected features, effectively categorizing the potential values of a variable into distinct bins. After the binning process, the function generates two separate visualizations (a scatter chart for WoE values and a bar chart for IV) for each variable. These visual presentations are formed according to the spread of each metric across various categories of each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Errors occurring during the binning process.\n",
       "- Challenges in converting non-numeric columns into string data type.\n",
       "- Misbalance in the distribution of WoE and IV, with certain bins overtaking others conspicuously. This could denote that the model is disproportionately dependent on certain variables or categories for predictions, an indication of potential risks to its robustness and generalizability.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a detailed visual representation of the relationship between feature categories and the target variable. This grants an intuitive understanding of each feature's contribution to the model.\n",
       "- Allows for easy identification of features with high impact, facilitating feature selection and enhancing comprehension of the model's decision logic.\n",
       "- WoE conversions are monotonic, upholding the rank ordering of the original data points, which simplifies analysis.\n",
       "\n",
       "**Limitations**:\n",
       "- The method is largely reliant on the binning process, and an inappropriate binning threshold or bin number choice might result in a misrepresentation of the variable's distribution.\n",
       "- While excellent for categorical data, the encoding of continuous variables into categorical can sometimes lead to information loss.\n",
       "- Extreme or outlier values can dramatically affect the computation of WoE and IV, skewing results.\n",
       "- The method requires a sufficient number of events per bin to generate a reliable information value and weight of evidence.</td>\n",
       "      <td id=\"T_1ac33_row111_col3\" class=\"data row111 col3\" >validmind.data_validation.WOEBinPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row112_col0\" class=\"data row112 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row112_col1\" class=\"data row112 col1\" >Class Imbalance</td>\n",
       "      <td id=\"T_1ac33_row112_col2\" class=\"data row112 col2\" >**Purpose**: The ClassImbalance test is designed to evaluate the distribution of target classes in a dataset that's utilized by a machine learning model. Specifically, it aims to ensure that the classes aren't overly skewed, which could lead to bias in the model's predictions. It's crucial to have a balanced training dataset to avoid creating a model that's biased with high accuracy for the majority class and low accuracy for the minority class.\n",
       "\n",
       "**Test Mechanism**: This ClassImbalance test operates by calculating the frequency (expressed as a percentage) of each class in the target column of the dataset. It then checks whether each class appears in at least a set minimum percentage of the total records. This minimum percentage is a modifiable parameter, but the default value is set to 10%.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Any class that represents less than the pre-set minimum percentage threshold is marked as high risk, implying a potential class imbalance.\n",
       "- The function provides a pass/fail outcome for each class based on this criterion.\n",
       "- Fundamentally, if any class fails this test, it's highly likely that the dataset possesses imbalanced class distribution.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The test can spot under-represented classes that could affect the efficiency of a machine learning model.\n",
       "- The calculation is straightforward and swift.\n",
       "- The test is highly informative because it not only spots imbalance, but it also quantifies the degree of imbalance.\n",
       "- The adjustable threshold enables flexibility and adaptation to differing use-cases or domain-specific needs.\n",
       "- The test creates a visually insightful plot showing the classes and their corresponding proportions, enhancing interpretability and comprehension of the data.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The test might struggle to perform well or provide vital insights for datasets with a high number of classes. In such cases, the imbalance could be inevitable due to the inherent class distribution.\n",
       "- Sensitivity to the threshold value might result in faulty detection of imbalance if the threshold is set excessively high.\n",
       "- Regardless of the percentage threshold, it doesn't account for varying costs or impacts of misclassifying different classes, which might fluctuate based on specific applications or domains.\n",
       "- While it can identify imbalances in class distribution, it doesn't provide direct methods to address or correct these imbalances.\n",
       "- The test is only applicable for classification opearations and unsuitable for regression or clustering tasks.</td>\n",
       "      <td id=\"T_1ac33_row112_col3\" class=\"data row112 col3\" >validmind.data_validation.ClassImbalance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row113_col0\" class=\"data row113 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row113_col1\" class=\"data row113 col1\" >IQR Outliers Bar Plot</td>\n",
       "      <td id=\"T_1ac33_row113_col2\" class=\"data row113 col2\" >**Purpose**: The InterQuartile Range Outliers Bar Plot (IQROutliersBarPlot) metric aims to visually analyze and evaluate the extent of outliers in numeric variables based on percentiles. Its primary purpose is to clarify the dataset's distribution, flag possible abnormalities in it and gauge potential risks associated with processing potentially skewed data, which can affect the machine learning model's predictive prowess.\n",
       "\n",
       "**Test Mechanism**: The examination invokes a series of steps:\n",
       "\n",
       "1. For every numeric feature in the dataset, the 25th percentile (Q1) and 75th percentile (Q3) are calculated before deriving the Interquartile Range (IQR), the difference between Q1 and Q3. 2. Subsequently, the metric calculates the lower and upper thresholds by subtracting Q1 from the `threshold` times IQR and adding Q3 to `threshold` times IQR, respectively. The default `threshold` is set at 1.5. 3. Any value in the feature that falls below the lower threshold or exceeds the upper threshold is labeled as an outlier. 4. The number of outliers are tallied for different percentiles, such as [0-25], [25-50], [50-75], and [75-100]. 5. These counts are employed to construct a bar plot for the feature, showcasing the distribution of outliers across different percentiles.\n",
       "\n",
       "**Signs of High Risk**: High risk or a potential lapse in the model's performance could be unveiled by the following signs:\n",
       "\n",
       "- A prevalence of outliers in the data, potentially skewing its distribution.\n",
       "- Outliers dominating higher percentiles (75-100) which implies the presence of extreme values, capable of severely influencing the model's performance.\n",
       "- Certain features harboring most of their values as outliers, which signifies that these features might not contribute positively to the model's forecasting ability.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Effectively identifies outliers in the data through visual means, facilitating easier comprehension and offering insights into the outliers' possible impact on the model.\n",
       "- Provides flexibility by accommodating all numeric features or a chosen subset.\n",
       "- Task-agnostic in nature; it is viable for both classification and regression tasks.\n",
       "- Can handle large datasets as its operation does not hinge on computationally heavy operations.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Its application is limited to numerical variables and does not extend to categorical ones.\n",
       "- Relies on a predefined threshold (default being 1.5) for outlier identification, which may not be suitable for all cases.\n",
       "- Only reveals the presence and distribution of outliers and does not provide insights into how these outliers might affect the model's predictive performance.\n",
       "- The assumption that data is unimodal and symmetric may not always hold true. In cases with non-normal distributions, the results can be misleading.</td>\n",
       "      <td id=\"T_1ac33_row113_col3\" class=\"data row113 col3\" >validmind.data_validation.IQROutliersBarPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row114_col0\" class=\"data row114 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row114_col1\" class=\"data row114 col1\" >Pi TPD Histogram</td>\n",
       "      <td id=\"T_1ac33_row114_col2\" class=\"data row114 col2\" >**Purpose**: The PiTPDHistogram metric uses Probability of Default (PD) calculations for individual instances within both training and test data sets in order to assess a model's proficiency in predicting credit risk. A distinctive point in time (PiT) is chosen for these PD calculations, and the results for both actual and predicted defaults are presented in histogram form. This visualization is aimed at simplifying the understanding of model prediction accuracy.\n",
       "\n",
       "**Test Mechanism**: Instances are categorized into two groups\n",
       "- those for actual defaults and those for predicted defaults, with '1' indicating a default and '0' indicating non-default. PD is calculated for each instance, and based on these calculations, two histograms are created, one for actual defaults and one for predicted defaults. If the predicted default frequency matches that of the actual defaults, the model's performance is deemed effective.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Discrepancies between the actual and predicted default histograms may suggest model inefficiency.\n",
       "- Variations in histogram shapes or divergences in default probability distributions could be concerning.\n",
       "- Significant mismatches in peak default probabilities could also be red flags.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a visual comparison between actual and predicted defaults, aiding in the understanding of model performance.\n",
       "- Helps reveal model bias and areas where the model's performance could be improved.\n",
       "- Easier to understand than purely numerical evaluations or other complicated visualization measures.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric remains largely interpretive and subjective, as the extent and relevance of visual discrepancies often need to be evaluated manually, leading to potentially inconsistent results across different analyses.\n",
       "- This metric alone may not capture all the complexities and nuances of model performance.\n",
       "- The information provided is limited to a specific point in time, potentially neglecting the model's performance under various circumstances or different time periods.</td>\n",
       "      <td id=\"T_1ac33_row114_col3\" class=\"data row114 col3\" >validmind.data_validation.PiTPDHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row115_col0\" class=\"data row115 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row115_col1\" class=\"data row115 col1\" >Auto AR</td>\n",
       "      <td id=\"T_1ac33_row115_col2\" class=\"data row115 col2\" >**Purpose**:\n",
       "\n",
       "The AutoAR test is intended to automatically identify the Autoregressive (AR) order of a time series by utilizing the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC). AR order is crucial in forecasting tasks as it dictates the quantity of prior terms in the sequence to use for predicting the current term. The objective is to select the most fitting AR model that encapsulates the trend and seasonality in the time series data.\n",
       "\n",
       "**Test Mechanism**:\n",
       "\n",
       "The test mechanism operates by iterating through a possible range of AR orders up to a defined maximum. An AR model is fitted for each order, and the corresponding BIC and AIC are computed. BIC and AIC statistical measures are designed to penalize models for complexity, preferring simpler models that fit the data proficiently. To verify the stationarity of the time series, the Augmented Dickey-Fuller test is executed. The AR order, BIC, and AIC findings, are compiled into a dataframe for effortless comparison. Then, the AR order with the smallest BIC is established as the desirable order for each variable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- An augmented Dickey Fuller test p-value > 0.05, indicating the time series isn't stationary, may lead to inaccurate results.\n",
       "- Problems with the model fitting procedure, such as computational or convergence issues.\n",
       "- Continuous selection of the maximum specified AR order may suggest insufficient set limit.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The test independently pinpoints the optimal AR order, thereby reducing potential human bias.\n",
       "- It strikes a balance between model simplicity and goodness-of-fit to avoid overfitting.\n",
       "- Has the capability to account for stationarity in a time series, an essential aspect for dependable AR modelling.\n",
       "- The results are aggregated into an comprehensive table, enabling an easy interpretation.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The tests need a stationary time series input.\n",
       "- They presume a linear relationship between the series and its lags.\n",
       "- The search for the best model is constrained by the maximum AR order supplied in the parameters. Therefore, a low max_ar_order could result in subpar outcomes.\n",
       "- AIC and BIC may not always agree on the selection of the best model. This potentially requires the user to juggle interpretational choices.</td>\n",
       "      <td id=\"T_1ac33_row115_col3\" class=\"data row115 col3\" >validmind.data_validation.AutoAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row116_col0\" class=\"data row116 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row116_col1\" class=\"data row116 col1\" >Tabular Date Time Histograms</td>\n",
       "      <td id=\"T_1ac33_row116_col2\" class=\"data row116 col2\" >**Purpose**: The `TabularDateTimeHistograms` metric is designed to provide graphical insight into the distribution of time intervals in a machine learning model's datetime data. By plotting histograms of differences between consecutive date entries in all datetime variables, it enables an examination of the underlying pattern of time series data and identification of anomalies.\n",
       "\n",
       "**Test Mechanism**: This test operates by first identifying all datetime columns and extracting them from the dataset. For each datetime column, it next computes the differences (in days) between consecutive dates, excluding zero values, and visualizes these differences in a histogram. The seaborn library's histplot function is used to generate histograms, which are labeled appropriately and provide a graphical representation of the frequency of different day intervals in the dataset.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If no datetime columns are detected in the dataset, this would lead to a ValueError. Hence, the absence of datetime columns signifies a high risk.\n",
       "- A severely skewed or irregular distribution depicted in the histogram may indicate possible complications with the data, such as faulty timestamps or abnormalities.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric offers a visual overview of time interval frequencies within the dataset, supporting the recognition of inherent patterns.\n",
       "- Histogram plots can aid in the detection of potential outliers and data anomalies, contributing to an assessment of data quality.\n",
       "- The metric is versatile, compatible with a range of task types, including classification and regression, and can work with multiple datetime variables if present.\n",
       "\n",
       "**Limitations**:\n",
       "- A major weakness of this metric is its dependence on the visual examination of data, as it does not provide a measurable evaluation of the model.\n",
       "- The metric might overlook complex or multi-dimensional trends in the data.\n",
       "- The test is only applicable to datasets containing datetime columns and will fail if such columns are unavailable.\n",
       "- The interpretation of the histograms relies heavily on the domain expertise and experience of the reviewer.</td>\n",
       "      <td id=\"T_1ac33_row116_col3\" class=\"data row116 col3\" >validmind.data_validation.TabularDateTimeHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row117_col0\" class=\"data row117 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row117_col1\" class=\"data row117 col1\" >Punctuations</td>\n",
       "      <td id=\"T_1ac33_row117_col2\" class=\"data row117 col2\" >**1. Purpose:** The Punctuations Metric's primary purpose is to analyze the frequency of punctuation usage within a given text dataset. This is often used in Natural Language Processing tasks, such as text classification and text summarization.\n",
       "\n",
       "**2. Test Mechanism:** The test begins by verifying that the input \"dataset\" is of the type VMDataset. Following that, a corpus is created from the dataset by splitting its text on spaces. Each unique punctuation character in the text corpus is then tallied. Then, the frequency distribution of each punctuation symbol is visualized as a bar graph, with these results being stored as Figures and associated with the main Punctuations object.\n",
       "\n",
       "**3. Signs of High Risk:**\n",
       "- High risk can be indicated by the excessive or unusual frequency of specific punctuation marks, potentially denoting dubious quality, data corruption, or skewed data.\n",
       "\n",
       "**4. Strengths:**\n",
       "- The Punctuations Metric provides valuable insights into the distribution of punctuation usage in a text dataset.\n",
       "- This insight can be important in validating the quality, consistency, and nature of the data.\n",
       "- It can provide hints about the style or tonality of the text corpus. For example, frequent usage of exclamation marks may suggest a more informal and emotional context.\n",
       "\n",
       "**5. Limitations:**\n",
       "- The metric focuses solely on punctuation usage and can miss other important textual characteristics.\n",
       "- It's important not to make general cultural or tonality assumptions based solely on punctuation distribution, since these can vary greatly across different languages and contexts.\n",
       "- The metric may be less effective with languages that use non-standard or different punctuation.\n",
       "- The visualization may lack interpretability when there are many unique punctuation marks in the dataset.</td>\n",
       "      <td id=\"T_1ac33_row117_col3\" class=\"data row117 col3\" >validmind.data_validation.nlp.Punctuations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row118_col0\" class=\"data row118 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row118_col1\" class=\"data row118 col1\" >Common Words</td>\n",
       "      <td id=\"T_1ac33_row118_col2\" class=\"data row118 col2\" >**Purpose**: The CommonWords metric is used to identify and visualize the most prevalent words within a specified text column of a dataset. This provides insights into the prevalent language patterns and vocabulary, especially useful in Natural Language Processing (NLP) tasks such as text classification and text summarization.\n",
       "\n",
       "**Test Mechanism**: The test methodology involves splitting the specified text column's entries into words, collating them into a corpus, and then counting the frequency of each word using the Counter. The forty most frequently occurring non-stopwords are then visualized in a bar chart, where the x-axis represents the words, and the y-axis indicates their frequency of occurrence.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A lack of distinct words within the list, or the most common words being stopwords.\n",
       "- Frequent occurrence of irrelevant or inappropriate words could point out a poorly curated or noisy dataset.\n",
       "- An error returned due to the absence of a valid Dataset object indicates high risk as the metric cannot be effectively implemented without it.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric provides clear insights into the language features – specifically word frequency – of unstructured text data.\n",
       "- It can reveal prominent vocabulary and language patterns, which prove vital for feature extraction in NLP tasks.\n",
       "- The visualization helps in quickly capturing the patterns and understanding the data intuitively.\n",
       "\n",
       "**Limitations**:\n",
       "- The test disregards semantic or context-related information as it solely focuses on word frequency.\n",
       "- It intentionally ignores stopwords which might carry necessary significance in certain scenarios.\n",
       "- The applicability is limited to English language text data as English stopwords are used for filtering, hence cannot account for data in other languages.\n",
       "- The metric requires a valid Dataset object, indicating a dependency condition that limits its broader applicability.</td>\n",
       "      <td id=\"T_1ac33_row118_col3\" class=\"data row118 col3\" >validmind.data_validation.nlp.CommonWords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row119_col0\" class=\"data row119 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row119_col1\" class=\"data row119 col1\" >Hashtags</td>\n",
       "      <td id=\"T_1ac33_row119_col2\" class=\"data row119 col2\" >**Purpose**: The Hashtags test is designed to measure the frequency of hashtags used within a given text column in a dataset. It is particularly useful for natural language processing tasks such as text classification and text summarization. The goal is to identify common trends and patterns in the use of hashtags, which can serve as critical indicators or features within a machine learning model.\n",
       "\n",
       "**Test Mechanism**: The test implements a regular expression (regex) to extract all hashtags from the specified text column. For each hashtag found, it makes a tally of its occurrences. It then outputs a list of the top N hashtags (default is 25, but customizable), sorted by their counts in descending order. The results are also visualized in a bar plot, with frequency counts on the y-axis and the corresponding hashtags on the x-axis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A low diversity in the usage of hashtags, as indicated by a few hashtags being used disproportionately more than others.\n",
       "- Repeated usage of one or few hashtags can be indicative of spam or a biased dataset.\n",
       "- If there are no or extremely few hashtags found in the dataset, it perhaps signifies that the text data does not contain structured social media data.\n",
       "\n",
       "**Strengths**:\n",
       "- It provides a concise visual representation of the frequency of hashtags, which can be critical for understanding trends about a particular topic in text data.\n",
       "- It is instrumental in tasks specifically related to social media text analytics, such as opinion analysis and trend discovery.\n",
       "- The test is adaptable, allowing the flexibility to determine the number of top hashtags to be analyzed.\n",
       "\n",
       "**Limitations**:\n",
       "- The test assumes the presence of hashtags and therefore may not be applicable for text datasets that do not contain hashtags (e.g., formal documents, scientific literature).\n",
       "- Language-specific limitations of hashtag formulations are not taken into account.\n",
       "- It does not account for typographical errors, variations, or synonyms in hashtags.\n",
       "- This test does not provide context or sentiment associated with the hashtags, so the information provided may have limited utility on its own.</td>\n",
       "      <td id=\"T_1ac33_row119_col3\" class=\"data row119 col3\" >validmind.data_validation.nlp.Hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row120_col0\" class=\"data row120 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row120_col1\" class=\"data row120 col1\" >Mentions</td>\n",
       "      <td id=\"T_1ac33_row120_col2\" class=\"data row120 col2\" >**Purpose**: This test, termed \"Mentions\", is designed to gauge the quality of data in a Natural Language Processing (NLP) or text-focused Machine Learning model. The primary objective is to identify and calculate the frequency of 'mentions' within a chosen text column of a dataset. A 'mention' in this context refers to individual text elements that are prefixed by '@'. The output of this test reveals the most frequently mentioned entities or usernames, which can be integral for applications such as social media analyses, customer sentiment analyses, and so on.\n",
       "\n",
       "**Test Mechanism**: The test first verifies the existence of a text column in the provided dataset. It then employs a regular expression pattern to extract mentions from the text. Subsequently, the frequency of each unique mention is calculated. The test selects the most frequent mentions based on default or user-defined parameters, the default being the top 25, for representation. This process of thresholding forms the core of the test. A treemap plot visualizes the test results, where the size of each rectangle corresponds to the frequency of a particular mention.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The lack of a valid text column in the dataset, which would result in the failure of the test execution.\n",
       "- The absence of any mentions within the text data, indicating that there might not be any text associated with '@'. This situation could point towards sparse or poor-quality data, thereby hampering the model's generalization or learning capabilities.\n",
       "\n",
       "**Strengths**:\n",
       "- The test is specifically optimized for text-based datasets which gives it distinct power in the context of NLP.\n",
       "- It enables quick identification and visually appealing representation of the predominant elements or mentions.\n",
       "- It can provide crucial insights about the most frequently mentioned entities or usernames.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only recognizes mentions that are prefixed by '@', hence useful textual aspects not preceded by '@' might be ignored.\n",
       "- This test isn't suited for datasets devoid of textual data.\n",
       "- It does not provide insights on less frequently occurring data or outliers, which means potentially significant patterns could be overlooked.</td>\n",
       "      <td id=\"T_1ac33_row120_col3\" class=\"data row120 col3\" >validmind.data_validation.nlp.Mentions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row121_col0\" class=\"data row121 col0\" >Metric</td>\n",
       "      <td id=\"T_1ac33_row121_col1\" class=\"data row121 col1\" >Text Description</td>\n",
       "      <td id=\"T_1ac33_row121_col2\" class=\"data row121 col2\" >**Purpose**: This test uses the TextDescription metric to conduct a comprehensive textual analysis of a given dataset. Various parameters such as total words, total sentences, average sentence length, total paragraphs, total unique words, most common words, total punctuations, and lexical diversity are evaluated. This metric aids in comprehending the nature of the text and evaluating the potential challenges that machine learning algorithms deployed for textual analysis, language processing, or summarization might face.\n",
       "\n",
       "**Test Mechanism**: The test works by parsing the given dataset and utilizes the NLTK (Natural Language Toolkit) library for tokenizing the text into words, sentences, and paragraphs. Subsequently, it processes the text further by eliminating stopwords declared in 'unwanted_tokens' and punctuations. Next, it determines parameters like the total count of words, sentences, paragraphs, punctuations alongside the average sentence length and lexical diversity. Lastly, the result from these calculations is condensed and scatter plots for certain variable combinations (e.g. Total Words vs Total Sentences, Total Words vs Total Unique Words) are produced, providing a visual representation of the text's structure.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Anomalies or an increase in complexity within the lexical diversity results.\n",
       "- Longer sentences and paragraphs.\n",
       "- High uniqueness of words.\n",
       "- Presence of a significant amount of unwanted tokens.\n",
       "- Missing or erroneous visualizations. These signs suggest potential risk in text processing ML models, indicating that the ability of the model to absorb and process text could be compromised.\n",
       "\n",
       "**Strengths**:\n",
       "- An essential pre-processing tool, specifically for textual analysis in machine learning model data.\n",
       "- Provides a comprehensive breakdown of a text dataset, which aids in understanding both structural and vocabulary complexity.\n",
       "- Generates visualizations of correlations between chosen variables to further comprehend the text's structure and complexity.\n",
       "\n",
       "**Limitations**:\n",
       "- Heavy reliance on the NLTK library, restricting its use to only the languages that NLTK supports.\n",
       "- Limited customization capacity as the undesirable tokens and stop words are predefined.\n",
       "- Lacks the ability to consider semantics or grammatical complexities, which could be crucial aspects in language processing.\n",
       "- Assumes that the document is well-structured (includes sentences and paragraphs); therefore, unstructured or poorly formatted text may distort the results.</td>\n",
       "      <td id=\"T_1ac33_row121_col3\" class=\"data row121 col3\" >validmind.data_validation.nlp.TextDescription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1ac33_row122_col0\" class=\"data row122 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_1ac33_row122_col1\" class=\"data row122 col1\" >Stop Words</td>\n",
       "      <td id=\"T_1ac33_row122_col2\" class=\"data row122 col2\" >**Purpose**: The StopWords threshold test is a tool designed for assessing the quality of text data in an ML model. It focuses on the identification and analysis of \"stop words\" in a given dataset. Stop words are frequent, common, yet semantically insignificant words (for example: \"the\", \"and\", \"is\") in a language. This test evaluates the proportion of stop words to the total word count in the dataset, in essence, scrutinizing the frequency of stop word usage. The core objective is to highlight the prevalent stop words based on their usage frequency, which can be instrumental in cleaning the data from noise and improving ML model performance.\n",
       "\n",
       "**Test Mechanism**: The StopWords test initiates on receiving an input of a 'VMDataset' object. Absence of such an object will trigger an error. The methodology involves inspection of the text column of the VMDataset to create a 'corpus' (a collection of written texts). Leveraging the Natural Language Toolkit's (NLTK) stop word repository, the test screens the corpus for any stop words and documents their frequency. It further calculates the percentage usage of each stop word compared to the total word count in the corpus. This percentage is evaluated against a predefined 'min_percent_threshold'. If this threshold is breached, the test returns a failed output. Top prevailing stop words along with their usage percentages are returned, facilitated by a bar chart visualization of these stop words and their frequency.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A percentage of any stop words exceeding the predefined 'min_percent_threshold'.\n",
       "- High frequency of stop words in the dataset which may adversely affect the application's analytical performance due to noise creation.\n",
       "\n",
       "**Strengths**:\n",
       "- The ability to scrutinize and quantify the usage of stop words.\n",
       "- Provides insights into potential noise in the text data due to stop words. This can directly aid in enhancing model training efficiency.\n",
       "- The test includes a bar chart visualization feature to easily interpret and action upon the stop words frequency information.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only supports English stop words, making it less effective with datasets of other languages.\n",
       "- The 'min_percent_threshold' parameter may require fine-tuning for different datasets, impacting the overall effectiveness of the test.\n",
       "- Contextual use of the stop words within the dataset is not considered which may lead to overlooking their significance in certain contexts.\n",
       "- The test focuses specifically on the frequency of stop words, not providing direct measures of model performance or predictive accuracy.</td>\n",
       "      <td id=\"T_1ac33_row122_col3\" class=\"data row122 col3\" >validmind.data_validation.nlp.StopWords</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d5919e70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.list_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tags and Task Types\n",
    "\n",
    "Effectively using ValidMind's tests involves a deep understanding of its 'tags' and 'task types'. Here's a breakdown:\n",
    "\n",
    "- **Task Types**: Represent the kind of modeling task associated with a test. For instance:\n",
    "  - **classification:** Classifying data into specific categories.\n",
    "  - **regression:** Predicting a continuous outcome variable.\n",
    "  - **text classification:** Classifying text into specific categories.\n",
    "  - **text summarization:** Producing a concise summary for a text.\n",
    "\n",
    "- **Tags**: Free-form descriptors providing detailed insights about a test. Some examples include:\n",
    "  - **nlp:** Tests relevant for natural language processing.\n",
    "  - **binary_classification:** Tests for binary classification tasks.\n",
    "  - **forecasting:** Tests for forecasting and time-series analysis.\n",
    "  - **tabular_data:** Tests for tabular data like CSVs and Excel spreadsheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Specific Tests using `tags` and `task_types`\n",
    "\n",
    "While listing all tests is valuable, there are times when you need to narrow down your search. The `list_tests` function offers `filter`, `task`, and `tags` parameters to assist in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fccf5 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fccf5_row0_col0, #T_fccf5_row0_col1, #T_fccf5_row0_col2, #T_fccf5_row0_col3, #T_fccf5_row1_col0, #T_fccf5_row1_col1, #T_fccf5_row1_col2, #T_fccf5_row1_col3, #T_fccf5_row2_col0, #T_fccf5_row2_col1, #T_fccf5_row2_col2, #T_fccf5_row2_col3, #T_fccf5_row3_col0, #T_fccf5_row3_col1, #T_fccf5_row3_col2, #T_fccf5_row3_col3, #T_fccf5_row4_col0, #T_fccf5_row4_col1, #T_fccf5_row4_col2, #T_fccf5_row4_col3, #T_fccf5_row5_col0, #T_fccf5_row5_col1, #T_fccf5_row5_col2, #T_fccf5_row5_col3, #T_fccf5_row6_col0, #T_fccf5_row6_col1, #T_fccf5_row6_col2, #T_fccf5_row6_col3, #T_fccf5_row7_col0, #T_fccf5_row7_col1, #T_fccf5_row7_col2, #T_fccf5_row7_col3, #T_fccf5_row8_col0, #T_fccf5_row8_col1, #T_fccf5_row8_col2, #T_fccf5_row8_col3, #T_fccf5_row9_col0, #T_fccf5_row9_col1, #T_fccf5_row9_col2, #T_fccf5_row9_col3, #T_fccf5_row10_col0, #T_fccf5_row10_col1, #T_fccf5_row10_col2, #T_fccf5_row10_col3, #T_fccf5_row11_col0, #T_fccf5_row11_col1, #T_fccf5_row11_col2, #T_fccf5_row11_col3, #T_fccf5_row12_col0, #T_fccf5_row12_col1, #T_fccf5_row12_col2, #T_fccf5_row12_col3, #T_fccf5_row13_col0, #T_fccf5_row13_col1, #T_fccf5_row13_col2, #T_fccf5_row13_col3, #T_fccf5_row14_col0, #T_fccf5_row14_col1, #T_fccf5_row14_col2, #T_fccf5_row14_col3, #T_fccf5_row15_col0, #T_fccf5_row15_col1, #T_fccf5_row15_col2, #T_fccf5_row15_col3, #T_fccf5_row16_col0, #T_fccf5_row16_col1, #T_fccf5_row16_col2, #T_fccf5_row16_col3, #T_fccf5_row17_col0, #T_fccf5_row17_col1, #T_fccf5_row17_col2, #T_fccf5_row17_col3, #T_fccf5_row18_col0, #T_fccf5_row18_col1, #T_fccf5_row18_col2, #T_fccf5_row18_col3, #T_fccf5_row19_col0, #T_fccf5_row19_col1, #T_fccf5_row19_col2, #T_fccf5_row19_col3, #T_fccf5_row20_col0, #T_fccf5_row20_col1, #T_fccf5_row20_col2, #T_fccf5_row20_col3, #T_fccf5_row21_col0, #T_fccf5_row21_col1, #T_fccf5_row21_col2, #T_fccf5_row21_col3, #T_fccf5_row22_col0, #T_fccf5_row22_col1, #T_fccf5_row22_col2, #T_fccf5_row22_col3, #T_fccf5_row23_col0, #T_fccf5_row23_col1, #T_fccf5_row23_col2, #T_fccf5_row23_col3, #T_fccf5_row24_col0, #T_fccf5_row24_col1, #T_fccf5_row24_col2, #T_fccf5_row24_col3, #T_fccf5_row25_col0, #T_fccf5_row25_col1, #T_fccf5_row25_col2, #T_fccf5_row25_col3, #T_fccf5_row26_col0, #T_fccf5_row26_col1, #T_fccf5_row26_col2, #T_fccf5_row26_col3, #T_fccf5_row27_col0, #T_fccf5_row27_col1, #T_fccf5_row27_col2, #T_fccf5_row27_col3, #T_fccf5_row28_col0, #T_fccf5_row28_col1, #T_fccf5_row28_col2, #T_fccf5_row28_col3, #T_fccf5_row29_col0, #T_fccf5_row29_col1, #T_fccf5_row29_col2, #T_fccf5_row29_col3, #T_fccf5_row30_col0, #T_fccf5_row30_col1, #T_fccf5_row30_col2, #T_fccf5_row30_col3, #T_fccf5_row31_col0, #T_fccf5_row31_col1, #T_fccf5_row31_col2, #T_fccf5_row31_col3, #T_fccf5_row32_col0, #T_fccf5_row32_col1, #T_fccf5_row32_col2, #T_fccf5_row32_col3, #T_fccf5_row33_col0, #T_fccf5_row33_col1, #T_fccf5_row33_col2, #T_fccf5_row33_col3, #T_fccf5_row34_col0, #T_fccf5_row34_col1, #T_fccf5_row34_col2, #T_fccf5_row34_col3, #T_fccf5_row35_col0, #T_fccf5_row35_col1, #T_fccf5_row35_col2, #T_fccf5_row35_col3, #T_fccf5_row36_col0, #T_fccf5_row36_col1, #T_fccf5_row36_col2, #T_fccf5_row36_col3, #T_fccf5_row37_col0, #T_fccf5_row37_col1, #T_fccf5_row37_col2, #T_fccf5_row37_col3, #T_fccf5_row38_col0, #T_fccf5_row38_col1, #T_fccf5_row38_col2, #T_fccf5_row38_col3, #T_fccf5_row39_col0, #T_fccf5_row39_col1, #T_fccf5_row39_col2, #T_fccf5_row39_col3, #T_fccf5_row40_col0, #T_fccf5_row40_col1, #T_fccf5_row40_col2, #T_fccf5_row40_col3, #T_fccf5_row41_col0, #T_fccf5_row41_col1, #T_fccf5_row41_col2, #T_fccf5_row41_col3, #T_fccf5_row42_col0, #T_fccf5_row42_col1, #T_fccf5_row42_col2, #T_fccf5_row42_col3, #T_fccf5_row43_col0, #T_fccf5_row43_col1, #T_fccf5_row43_col2, #T_fccf5_row43_col3, #T_fccf5_row44_col0, #T_fccf5_row44_col1, #T_fccf5_row44_col2, #T_fccf5_row44_col3, #T_fccf5_row45_col0, #T_fccf5_row45_col1, #T_fccf5_row45_col2, #T_fccf5_row45_col3, #T_fccf5_row46_col0, #T_fccf5_row46_col1, #T_fccf5_row46_col2, #T_fccf5_row46_col3, #T_fccf5_row47_col0, #T_fccf5_row47_col1, #T_fccf5_row47_col2, #T_fccf5_row47_col3, #T_fccf5_row48_col0, #T_fccf5_row48_col1, #T_fccf5_row48_col2, #T_fccf5_row48_col3, #T_fccf5_row49_col0, #T_fccf5_row49_col1, #T_fccf5_row49_col2, #T_fccf5_row49_col3, #T_fccf5_row50_col0, #T_fccf5_row50_col1, #T_fccf5_row50_col2, #T_fccf5_row50_col3, #T_fccf5_row51_col0, #T_fccf5_row51_col1, #T_fccf5_row51_col2, #T_fccf5_row51_col3, #T_fccf5_row52_col0, #T_fccf5_row52_col1, #T_fccf5_row52_col2, #T_fccf5_row52_col3, #T_fccf5_row53_col0, #T_fccf5_row53_col1, #T_fccf5_row53_col2, #T_fccf5_row53_col3, #T_fccf5_row54_col0, #T_fccf5_row54_col1, #T_fccf5_row54_col2, #T_fccf5_row54_col3, #T_fccf5_row55_col0, #T_fccf5_row55_col1, #T_fccf5_row55_col2, #T_fccf5_row55_col3, #T_fccf5_row56_col0, #T_fccf5_row56_col1, #T_fccf5_row56_col2, #T_fccf5_row56_col3, #T_fccf5_row57_col0, #T_fccf5_row57_col1, #T_fccf5_row57_col2, #T_fccf5_row57_col3, #T_fccf5_row58_col0, #T_fccf5_row58_col1, #T_fccf5_row58_col2, #T_fccf5_row58_col3, #T_fccf5_row59_col0, #T_fccf5_row59_col1, #T_fccf5_row59_col2, #T_fccf5_row59_col3, #T_fccf5_row60_col0, #T_fccf5_row60_col1, #T_fccf5_row60_col2, #T_fccf5_row60_col3, #T_fccf5_row61_col0, #T_fccf5_row61_col1, #T_fccf5_row61_col2, #T_fccf5_row61_col3, #T_fccf5_row62_col0, #T_fccf5_row62_col1, #T_fccf5_row62_col2, #T_fccf5_row62_col3, #T_fccf5_row63_col0, #T_fccf5_row63_col1, #T_fccf5_row63_col2, #T_fccf5_row63_col3, #T_fccf5_row64_col0, #T_fccf5_row64_col1, #T_fccf5_row64_col2, #T_fccf5_row64_col3, #T_fccf5_row65_col0, #T_fccf5_row65_col1, #T_fccf5_row65_col2, #T_fccf5_row65_col3, #T_fccf5_row66_col0, #T_fccf5_row66_col1, #T_fccf5_row66_col2, #T_fccf5_row66_col3, #T_fccf5_row67_col0, #T_fccf5_row67_col1, #T_fccf5_row67_col2, #T_fccf5_row67_col3, #T_fccf5_row68_col0, #T_fccf5_row68_col1, #T_fccf5_row68_col2, #T_fccf5_row68_col3, #T_fccf5_row69_col0, #T_fccf5_row69_col1, #T_fccf5_row69_col2, #T_fccf5_row69_col3, #T_fccf5_row70_col0, #T_fccf5_row70_col1, #T_fccf5_row70_col2, #T_fccf5_row70_col3, #T_fccf5_row71_col0, #T_fccf5_row71_col1, #T_fccf5_row71_col2, #T_fccf5_row71_col3, #T_fccf5_row72_col0, #T_fccf5_row72_col1, #T_fccf5_row72_col2, #T_fccf5_row72_col3, #T_fccf5_row73_col0, #T_fccf5_row73_col1, #T_fccf5_row73_col2, #T_fccf5_row73_col3, #T_fccf5_row74_col0, #T_fccf5_row74_col1, #T_fccf5_row74_col2, #T_fccf5_row74_col3, #T_fccf5_row75_col0, #T_fccf5_row75_col1, #T_fccf5_row75_col2, #T_fccf5_row75_col3, #T_fccf5_row76_col0, #T_fccf5_row76_col1, #T_fccf5_row76_col2, #T_fccf5_row76_col3, #T_fccf5_row77_col0, #T_fccf5_row77_col1, #T_fccf5_row77_col2, #T_fccf5_row77_col3, #T_fccf5_row78_col0, #T_fccf5_row78_col1, #T_fccf5_row78_col2, #T_fccf5_row78_col3, #T_fccf5_row79_col0, #T_fccf5_row79_col1, #T_fccf5_row79_col2, #T_fccf5_row79_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fccf5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_fccf5_level0_col0\" class=\"col_heading level0 col0\" >Test Type</th>\n",
       "      <th id=\"T_fccf5_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_fccf5_level0_col2\" class=\"col_heading level0 col2\" >Description</th>\n",
       "      <th id=\"T_fccf5_level0_col3\" class=\"col_heading level0 col3\" >ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row0_col0\" class=\"data row0 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row0_col1\" class=\"data row0 col1\" >High Cardinality</td>\n",
       "      <td id=\"T_fccf5_row0_col2\" class=\"data row0 col2\" >**Purpose**: The “High Cardinality” test is used to evaluate the number of unique values present in the categorical columns of a dataset. In this context, high cardinality implies the presence of a large number of unique, non-repetitive values in the dataset.\n",
       "\n",
       "**Test Mechanism**: The test first infers the dataset's type and then calculates an initial numeric threshold based on the test parameters. It only considers columns classified as \"Categorical\". For each of these columns, the number of distinct values (n_distinct) and the percentage of distinct values (p_distinct) are calculated. The test will pass if n_distinct is less than the calculated numeric threshold. Lastly, the results, which include details such as column name, number of distinct values, and pass/fail status, are compiled into a table.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of distinct values (high cardinality) in one or more categorical columns implies a high risk.\n",
       "- A column failing the test (n_distinct >= num_threshold) is another indicator of high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The High Cardinality test is effective in early detection of potential overfitting and unwanted noise.\n",
       "- It aids in identifying potential outliers and inconsistencies, thereby improving data quality.\n",
       "- The test can be applied to both, classification and regression task types, demonstrating its versatility.\n",
       "\n",
       "**Limitations**:\n",
       "- The test is restricted to only \"Categorical\" data types and is thus not suitable for numerical or continuous features, limiting its scope.\n",
       "- The test does not consider the relevance or importance of unique values in categorical features, potentially causing it to overlook critical data points.\n",
       "- The threshold (both number and percent) used for the test is static and may not be optimal for diverse datasets and varied applications. Further mechanisms to adjust and refine this threshold could enhance its effectiveness.</td>\n",
       "      <td id=\"T_fccf5_row0_col3\" class=\"data row0 col3\" >validmind.data_validation.HighCardinality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row1_col0\" class=\"data row1 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row1_col1\" class=\"data row1 col1\" >Specificity</td>\n",
       "      <td id=\"T_fccf5_row1_col2\" class=\"data row1 col2\" >**Purpose:** The Specificity Test evaluates the clarity, precision, and effectiveness of the prompts provided to a Language Learning Model (LLM). It aims to ensure that the instructions embedded in a prompt are indisputably clear and relevant, thereby helping to yank out ambiguity and steer the LLM towards desired outputs. This level of specificity significantly affects the accuracy and relevance of LLM outputs.\n",
       "\n",
       "**Test Mechanism:** The Specificity Test employs an LLM to grade each prompt based on clarity, detail, and relevance parameters within a specificity scale that extends from 1 to 10. On this scale, prompts scoring equal to or more than a predefined threshold (set to 7 by default) pass the evaluation, while those scoring below this threshold fail it. Users can adjust this threshold as per their requirements.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Prompts scoring consistently below the established threshold\n",
       "- Vague or ambiguous prompts that do not provide clear direction to the LLM\n",
       "- Overly verbose prompts that may confuse the LLM instead of providing clear guidance\n",
       "\n",
       "**Strengths:**\n",
       "- Enables precise and clear communication with the LLM to achieve desired outputs\n",
       "- Serves as a crucial means to measure the effectiveness of prompts\n",
       "- Highly customizable, allowing users to set their threshold based on specific use cases\n",
       "\n",
       "**Limitations:**\n",
       "- This test doesn't consider the content comprehension capability of the LLM\n",
       "- High specificity score doesn't guarantee a high-quality response from the LLM, as the model's performance is also dependent on various other factors\n",
       "- Striking a balance between specificity and verbosity can be challenging, as overly detailed prompts might confuse or mislead the model.</td>\n",
       "      <td id=\"T_fccf5_row1_col3\" class=\"data row1 col3\" >validmind.prompt_validation.Specificity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row2_col0\" class=\"data row2 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row2_col1\" class=\"data row2 col1\" >Feature Target Correlation Plot</td>\n",
       "      <td id=\"T_fccf5_row2_col2\" class=\"data row2 col2\" >**Purpose**: This test is designed to graphically illustrate the correlations between distinct input features and the target output of a Machine Learning model. Understanding how each feature influences the model's predictions is crucial\n",
       "- a higher correlation indicates stronger influence of the feature on the target variable. This correlation study is especially advantageous during feature selection and for comprehending the model's operation.\n",
       "\n",
       "**Test Mechanism**: This FeatureTargetCorrelationPlot test computes and presents the correlations between the features and the target variable using a specific dataset. These correlations are calculated, graphically represented in a horizontal bar plot, and color-coded based on the strength of the correlation. A hovering template can also be utilized for informative tooltips. It is possible to specify the features to be analyzed and adjust the graph's height according to need.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- There are no strong correlations (either positive or negative) between features and the target variable. This could suggest high risk as the supplied features do not appear to significantly impact the prediction output.\n",
       "- The presence of duplicated correlation values might hint at redundancy in the feature set.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides visual assistance to interpreting correlations more effectively.\n",
       "- Gives a clear and simple tour of how each feature affects the model's target variable.\n",
       "- Beneficial for feature selection and grasping the model's prediction nature.\n",
       "- Precise correlation values for each feature are offered by the hover template, contributing to a granular-level comprehension.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only accepts numerical data, meaning variables of other types need to be prepared beforehand.\n",
       "- The plot assumes all correlations to be linear, thus non-linear relationships might not be captured effectively.\n",
       "- Not apt for models that employ complex feature interactions, like Decision Trees or Neural Networks, as the test may not accurately reflect their importance.</td>\n",
       "      <td id=\"T_fccf5_row2_col3\" class=\"data row2 col3\" >validmind.data_validation.FeatureTargetCorrelationPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row3_col0\" class=\"data row3 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row3_col1\" class=\"data row3 col1\" >Pearson Correlation Matrix</td>\n",
       "      <td id=\"T_fccf5_row3_col2\" class=\"data row3 col2\" >**Purpose**: This test is intended to evaluate the extent of linear dependency between all pairs of numerical variables in the given dataset. It provides the Pearson Correlation coefficient, which reveals any high correlations present. The purpose of doing this is to identify potential redundancy, as variables that are highly correlated can often be removed to reduce the dimensionality of the dataset without significantly impacting the model's performance.\n",
       "\n",
       "**Test Mechanism**: This metric test generates a correlation matrix for all numerical variables in the dataset using the Pearson correlation formula. A heat map is subsequently created to visualize this matrix effectively. The color of each point on the heat map corresponds to the magnitude and direction (positive or negative) of the correlation, with a range from -1 (perfect negative correlation) to 1 (perfect positive correlation). Any correlation coefficients higher than 0.7 (in absolute terms) are indicated in white in the heat map, suggesting a high degree of correlation.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of variables in the dataset showing a high degree of correlation (coefficients approaching ±1). This indicates redundancy within the dataset, suggesting that some variables may not be contributing new information to the model.\n",
       "- This could potentially lead to overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- The primary strength of this metric test is its ability to detect and quantify the linearity of relationships between variables. This allows for the identification of redundant variables, which in turn can help in simplifying models and potentially improving their performance.\n",
       "- The visualization aspect (heatmap) is another strength as it offers an easy-to-understand overview of the correlations, beneficial for those not comfortable navigating numerical matrices.\n",
       "\n",
       "**Limitations**:\n",
       "- The primary limitation of Pearson Correlation is its inability to detect non-linear relationships between variables, which can lead to missed opportunities for dimensionality reduction.\n",
       "- It only measures the degree of linear relationship and not the strength of effect of one variable on the other.\n",
       "- The cutoff value of 0.7 for high correlation is a somewhat arbitrary choice and some valid dependencies might be missed if they have a correlation coefficient less than this value.</td>\n",
       "      <td id=\"T_fccf5_row3_col3\" class=\"data row3 col3\" >validmind.data_validation.PearsonCorrelationMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row4_col0\" class=\"data row4 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row4_col1\" class=\"data row4 col1\" >ANOVA One Way Table</td>\n",
       "      <td id=\"T_fccf5_row4_col2\" class=\"data row4 col2\" >**Purpose**: The ANOVA (Analysis of Variance) One-Way Table metric is utilized to determine whether the mean of numerical variables differs across different groups identified by target or categorical variables. Its primary purpose is to scrutinize the significant impact of categorical variables on numerical ones. This method proves essential in identifying statistically significant features corresponding to the target variable present in the dataset.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism involves the ANOVA F-test's performance on each numerical variable against the target. If no specific features are mentioned, all numerical features are tested. A p-value is produced for each test and compared against a certain threshold (default being 0.05 if not specified). If the p-value is less than or equal to this threshold, the feature is marked as 'Pass', indicating significant mean difference across the groups. Otherwise, it's marked as 'Fail'. The test produces a DataFrame that includes variable name, F statistic value, p-value, threshold, and pass/fail status for every numerical variable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of 'Fail' results in the ANOVA F-test could signify high risk or underperformance in the model. This issue may arise when multiple numerical variables in the dataset don't exhibit any significant difference across the target variable groups.\n",
       "- Features with high p-values also indicate a high risk as they imply a greater chance of obtaining observed data given that the null hypothesis is true.\n",
       "\n",
       "**Strengths**:\n",
       "- The ANOVA One Way Table is highly efficient in identifying statistically significant features by simultaneously comparing group means.\n",
       "- Its flexibility allows the testing of all numerical features in the dataset when no specific ones are mentioned.\n",
       "- This metric provides a convenient method to measure the statistical significance of numerical variables and assists in selecting those variables influencing the classifier's predictions considerably.\n",
       "\n",
       "**Limitations**:\n",
       "- This metric assumes that the data is normally distributed, which may not always be the case leading to erroneous test results.\n",
       "- The sensitivity of the F-test to variance changes may hinder this metric's effectiveness, especially for datasets with high variance.\n",
       "- The ANOVA One Way test does not specify which group means differ statistically from others; it strictly asserts the existence of a difference.\n",
       "- The metric fails to provide insights into variable interactions, and significant effects due to these interactions could easily be overlooked.</td>\n",
       "      <td id=\"T_fccf5_row4_col3\" class=\"data row4 col3\" >validmind.data_validation.ANOVAOneWayTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row5_col0\" class=\"data row5 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row5_col1\" class=\"data row5 col1\" >Delimitation</td>\n",
       "      <td id=\"T_fccf5_row5_col2\" class=\"data row5 col2\" >**Purpose:** This test, dubbed the \"Delimitation Test\", is engineered to assess whether prompts provided to the Language Learning Model (LLM) correctly use delimiters to mark different sections of the input. Well-delimited prompts simplify the interpretation process for LLM, ensuring responses are precise and accurate.\n",
       "\n",
       "**Test Mechanism:** The test employs an LLM to examine prompts for appropriate use of delimiters such as triple quotation marks, XML tags, and section titles. Each prompt is assigned a score from 1 to 10 based on its delimitation integrity. Those with scores equal to or above the preset threshold (which is 7 by default, although it can be adjusted as necessary) pass the test.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- The test identifies prompts where a delimiter is missing, improperly placed, or incorrect, which can lead to misinterpretation by the LLM.\n",
       "- A high-risk scenario may involve complex prompts with multiple tasks or diverse data where correct delimitation is integral to understanding.\n",
       "- Low scores (below the threshold) are a clear indicator of high risk.\n",
       "\n",
       "**Strengths:**\n",
       "- This test ensures clarity in the demarcation of different components of given prompts.\n",
       "- It helps reduce ambiguity in understanding prompts, particularly for complex tasks.\n",
       "- Scoring allows for quantified insight into the appropriateness of delimiter usage, aiding continuous improvement.\n",
       "\n",
       "**Limitations:**\n",
       "- The test only checks for the presence and placement of delimiter, not whether the correct delimiter type is used for the specific data or task.\n",
       "- It may not fully reveal the impacts of poor delimitation on LLM's final performance.\n",
       "- Depending on the complexity of the tasks and prompts, the preset score threshold may not be refined enough, requiring regular manual adjustment.</td>\n",
       "      <td id=\"T_fccf5_row5_col3\" class=\"data row5 col3\" >validmind.prompt_validation.Delimitation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row6_col0\" class=\"data row6 col0\" >DatasetMetadata</td>\n",
       "      <td id=\"T_fccf5_row6_col1\" class=\"data row6 col1\" >Dataset Metadata</td>\n",
       "      <td id=\"T_fccf5_row6_col2\" class=\"data row6 col2\" >**Purpose**: The `DatasetMetadata` test is primarily aimed at collecting and logging essential descriptive statistics related to the training datasets. This test generates essential metadata such as the types of tasks (classification, regression, text_classification, text_summarization) and tags (tabular_data, time_series_data, text_data) associated with the datasets. This transparency facilitates model validation by linking different metrics and test results to the originating dataset.\n",
       "\n",
       "**Test Mechanism**: Rather than conducting a test or implementing a grading scale, this class collects and logs dataset metadata. During post-initialization, the metadata is linked to the dataset object. The `run` method produces a `TestSuiteDatasetResult` object, which is assigned a unique ID and is bound to a dataset. The dataset metadata is associated with this ID for use in future, more focused, validation procedures.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The metadata is incomplete or incorrect which can lead to inaccuracies in model risk assessment.\n",
       "- Dataset labels or types are missing, leading to issues in further model validation or mispresentations.\n",
       "\n",
       "**Strengths**:\n",
       "- The class brings transparency to model validation exercises by providing detailed information about the dataset.\n",
       "- It assists in error diagnosis and behaviors correlation to the model.\n",
       "- Ensures the correctness of tasks and data types associations and allows superior model explanations.\n",
       "- Supports dataset versioning by logging each dataset's metadata, maintaining a trackable history of alterations.\n",
       "\n",
       "**Limitations**:\n",
       "- The `DatasetMetadata` class's completeness and accuracy might be questionable, especially if metadata isn't appropriately added or is inaccurate.\n",
       "- It doesn't involve the evaluation of the dataset's quality or the direct validation of model predictions, hence it should be combined with other tests for a more comprehensive assessment.\n",
       "- The class cannot detect potential bias in the dataset. For bias detection, separate tests specifically tailored towards fairness and bias detection would be necessary.</td>\n",
       "      <td id=\"T_fccf5_row6_col3\" class=\"data row6 col3\" >validmind.data_validation.DatasetMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row7_col0\" class=\"data row7 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row7_col1\" class=\"data row7 col1\" >Stop Words</td>\n",
       "      <td id=\"T_fccf5_row7_col2\" class=\"data row7 col2\" >**Purpose**: The StopWords threshold test is a tool designed for assessing the quality of text data in an ML model. It focuses on the identification and analysis of \"stop words\" in a given dataset. Stop words are frequent, common, yet semantically insignificant words (for example: \"the\", \"and\", \"is\") in a language. This test evaluates the proportion of stop words to the total word count in the dataset, in essence, scrutinizing the frequency of stop word usage. The core objective is to highlight the prevalent stop words based on their usage frequency, which can be instrumental in cleaning the data from noise and improving ML model performance.\n",
       "\n",
       "**Test Mechanism**: The StopWords test initiates on receiving an input of a 'VMDataset' object. Absence of such an object will trigger an error. The methodology involves inspection of the text column of the VMDataset to create a 'corpus' (a collection of written texts). Leveraging the Natural Language Toolkit's (NLTK) stop word repository, the test screens the corpus for any stop words and documents their frequency. It further calculates the percentage usage of each stop word compared to the total word count in the corpus. This percentage is evaluated against a predefined 'min_percent_threshold'. If this threshold is breached, the test returns a failed output. Top prevailing stop words along with their usage percentages are returned, facilitated by a bar chart visualization of these stop words and their frequency.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A percentage of any stop words exceeding the predefined 'min_percent_threshold'.\n",
       "- High frequency of stop words in the dataset which may adversely affect the application's analytical performance due to noise creation.\n",
       "\n",
       "**Strengths**:\n",
       "- The ability to scrutinize and quantify the usage of stop words.\n",
       "- Provides insights into potential noise in the text data due to stop words. This can directly aid in enhancing model training efficiency.\n",
       "- The test includes a bar chart visualization feature to easily interpret and action upon the stop words frequency information.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only supports English stop words, making it less effective with datasets of other languages.\n",
       "- The 'min_percent_threshold' parameter may require fine-tuning for different datasets, impacting the overall effectiveness of the test.\n",
       "- Contextual use of the stop words within the dataset is not considered which may lead to overlooking their significance in certain contexts.\n",
       "- The test focuses specifically on the frequency of stop words, not providing direct measures of model performance or predictive accuracy.</td>\n",
       "      <td id=\"T_fccf5_row7_col3\" class=\"data row7 col3\" >validmind.data_validation.nlp.StopWords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row8_col0\" class=\"data row8 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row8_col1\" class=\"data row8 col1\" >IQR Outliers Table</td>\n",
       "      <td id=\"T_fccf5_row8_col2\" class=\"data row8 col2\" >**Purpose**: The \"Interquartile Range Outliers Table\" (IQROutliersTable) metric has been designed for identifying and summarizing outliers within numerical features of a dataset using the Interquartile Range (IQR) method. The purpose of this exercise is crucial in the pre-processing of data as outliers can substantially distort the statistical analysis and debilitate the performance of machine learning models.\n",
       "\n",
       "**Test Mechanism**: The IQR, which is the range separating the first quartile (25th percentile) from the third quartile (75th percentile), is calculated for each numerical feature within the dataset. An outlier is defined as a data point falling below the \"Q1\n",
       "- 1.5 * IQR\" or above \"Q3 + 1.5 * IQR\" range. The metric then computes the number of outliers along with their minimum, 25th percentile, median, 75th percentile, and maximum values for each numerical feature. If no specific features are chosen, the metric will apply to all numerical features in the dataset. The default outlier threshold is set to 1.5, following the standard definition of outliers in statistical analysis, although it can be customized by the user.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk is indicated by a large number of outliers in multiple features.\n",
       "- Outliers that are significantly distanced from the mean value of variables could potentially signal high risk.\n",
       "- Data entry errors or other data quality issues could be manifested through extremely high or low outlier values.\n",
       "\n",
       "**Strengths**:\n",
       "- It yields a comprehensive summary of outliers for each numerical feature within the dataset. This enables the user to pinpoint features with potential quality issues.\n",
       "- The IQR method is not overly affected by extremely high or low outlier values as it is based on quartile calculations.\n",
       "- The versatility of this metric grants the ability to customize the method to work on selected features and set a defined threshold for outliers.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric might cause false positives if the variable of interest veers away from a normal or near-normal distribution, notably in the case of skewed distributions.\n",
       "- It does not extend to provide interpretation or recommendations for tackling outliers and relies on the user or a data scientist to conduct further analysis of the results.\n",
       "- As it only functions on numerical features, it cannot be used for categorical data.\n",
       "- For data that has undergone heavy pre-processing, was manipulated, or inherently possesses a high kurtosis (heavy tails), the pre-set threshold may not be optimal for outlier detection.</td>\n",
       "      <td id=\"T_fccf5_row8_col3\" class=\"data row8 col3\" >validmind.data_validation.IQROutliersTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row9_col0\" class=\"data row9 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row9_col1\" class=\"data row9 col1\" >Training Test Degradation</td>\n",
       "      <td id=\"T_fccf5_row9_col2\" class=\"data row9 col2\" >**Purpose**: The 'TrainingTestDegradation' class serves as a test to verify that the degradation in performance between the training and test datasets does not exceed a predefined threshold. This test serves as a measure to check the model's ability to generalize from its training data to unseen test data. It assesses key classification metric scores such as accuracy, precision, recall and f1 score, to verify the model's robustness and reliability.\n",
       "\n",
       "**Test Mechanism**: The code applies several predefined metrics including accuracy, precision, recall and f1 scores to the model's predictions for both the training and test datasets. It calculates the degradation as the difference between the training score and test score divided by the training score. The test is considered successful if the degradation for each metric is less than the preset maximum threshold of 10%. The results are summarized in a table showing each metric's train score, test score, degradation percentage, and pass/fail status.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A degradation percentage that exceeds the maximum allowed threshold of 10% for any of the evaluated metrics.\n",
       "- A high difference or gap between the metric scores on the training and the test datasets.\n",
       "- The 'Pass/Fail' column displaying 'Fail' for any of the evaluated metrics.\n",
       "\n",
       "**Strengths**:\n",
       "- This test provides a quantitative measure of the model's ability to generalize to unseen data, which is key for predicting its practical real-world performance.\n",
       "- By evaluating multiple metrics, it takes into account different facets of model performance and enables a more holistic evaluation.\n",
       "- The use of a variable predefined threshold allows the flexibility to adjust the acceptability criteria for different scenarios.\n",
       "\n",
       "**Limitations**:\n",
       "- The test compares raw performance on training and test data, but does not factor in the nature of the data. Areas with less representation in the training set, for instance, might still perform poorly on unseen data.\n",
       "- It requires good coverage and balance in the test and training datasets to produce reliable results, which may not always be available.\n",
       "- The test is currently only designed for classification tasks.</td>\n",
       "      <td id=\"T_fccf5_row9_col3\" class=\"data row9 col3\" >validmind.model_validation.sklearn.TrainingTestDegradation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row10_col0\" class=\"data row10 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row10_col1\" class=\"data row10 col1\" >Model Metadata</td>\n",
       "      <td id=\"T_fccf5_row10_col2\" class=\"data row10 col2\" >**Purpose:** This test is designed to collect and summarize important metadata related to a particular machine learning model. Such metadata includes the model's architecture (modeling technique), the version and type of modeling framework used, and the programming language the model is written in.\n",
       "\n",
       "**Test Mechanism:** The mechanism of this test consists of extracting information from the model instance. It tries to extract the model information such as the modeling technique used, the modeling framework version, and the programming language. It decorates this information into a data frame and returns a summary of the results.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- High risk could be determined by a lack of documentation or inscrutable metadata for the model.\n",
       "- Unidentifiable language, outdated or unsupported versions of modeling frameworks, or undisclosed model architectures reflect risky situations, as they could hinder future reproducibility, support, and debugging of the model.\n",
       "\n",
       "**Strengths:**\n",
       "- The strengths of this test lie in the increased transparency and understanding it brings regarding the model's setup.\n",
       "- Knowing the model's architecture, the specific modeling framework version used, and the language involved, provides multiple benefits: supports better error understanding and debugging, facilitates model reuse, aids compliance of software policies, and assists in planning for model obsolescence due to evolving or discontinuing software and dependencies.\n",
       "\n",
       "**Limitations:**\n",
       "- Notably, this test is largely dependent on the compliance and correctness of information provided by the model or the model developer.\n",
       "- If the model's built-in methods for describing its architecture, framework or language are incorrect or lack necessary information, this test will hold limitations.\n",
       "- Moreover, it is not designed to directly evaluate the performance or accuracy of the model, rather it provides supplementary information which aids in comprehensive analysis.</td>\n",
       "      <td id=\"T_fccf5_row10_col3\" class=\"data row10 col3\" >validmind.model_validation.ModelMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row11_col0\" class=\"data row11 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row11_col1\" class=\"data row11 col1\" >Tabular Date Time Histograms</td>\n",
       "      <td id=\"T_fccf5_row11_col2\" class=\"data row11 col2\" >**Purpose**: The `TabularDateTimeHistograms` metric is designed to provide graphical insight into the distribution of time intervals in a machine learning model's datetime data. By plotting histograms of differences between consecutive date entries in all datetime variables, it enables an examination of the underlying pattern of time series data and identification of anomalies.\n",
       "\n",
       "**Test Mechanism**: This test operates by first identifying all datetime columns and extracting them from the dataset. For each datetime column, it next computes the differences (in days) between consecutive dates, excluding zero values, and visualizes these differences in a histogram. The seaborn library's histplot function is used to generate histograms, which are labeled appropriately and provide a graphical representation of the frequency of different day intervals in the dataset.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If no datetime columns are detected in the dataset, this would lead to a ValueError. Hence, the absence of datetime columns signifies a high risk.\n",
       "- A severely skewed or irregular distribution depicted in the histogram may indicate possible complications with the data, such as faulty timestamps or abnormalities.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric offers a visual overview of time interval frequencies within the dataset, supporting the recognition of inherent patterns.\n",
       "- Histogram plots can aid in the detection of potential outliers and data anomalies, contributing to an assessment of data quality.\n",
       "- The metric is versatile, compatible with a range of task types, including classification and regression, and can work with multiple datetime variables if present.\n",
       "\n",
       "**Limitations**:\n",
       "- A major weakness of this metric is its dependence on the visual examination of data, as it does not provide a measurable evaluation of the model.\n",
       "- The metric might overlook complex or multi-dimensional trends in the data.\n",
       "- The test is only applicable to datasets containing datetime columns and will fail if such columns are unavailable.\n",
       "- The interpretation of the histograms relies heavily on the domain expertise and experience of the reviewer.</td>\n",
       "      <td id=\"T_fccf5_row11_col3\" class=\"data row11 col3\" >validmind.data_validation.TabularDateTimeHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row12_col0\" class=\"data row12 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row12_col1\" class=\"data row12 col1\" >Too Many Zero Values</td>\n",
       "      <td id=\"T_fccf5_row12_col2\" class=\"data row12 col2\" >**Purpose**: The 'TooManyZeroValues' test is utilized to identify numerical columns in the dataset that may present a quantity of zero values considered excessive. The aim is to detect situations where these may implicate data sparsity or a lack of variation, limiting their effectiveness within a machine learning model. The definition of 'too many' is quantified as a percentage of total values, with a default set to 3%.\n",
       "\n",
       "**Test Mechanism**: This test is conducted by looping through each column in the dataset and categorizing those that pertain to numerical data. On identifying a numerical column, the function computes the total quantity of zero values and their ratio to the total row count. Should the proportion exceed a pre-set threshold parameter, set by default at 0.03 or 3%, the column is considered to have failed the test. The results for each column are summarised and reported, indicating the count and percentage of zero values for each numerical column, alongside a status indicating whether the column has passed or failed the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Indicators evidencing a high risk connected with this test would include numerical columns showing a high ratio of zero values when compared to the total count of rows (exceeding a pre-determined threshold).\n",
       "- Columns characterized by zero values across the board suggest a complete lack of data variation, signifying high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- Assists in highlighting columns featuring an excess of zero values that could otherwise go unnoticed within a large dataset.\n",
       "- Provides the flexibility to alter the threshold that determines when the quantity of zero values becomes 'too many', thus catering to specific needs of a particular analysis or model.\n",
       "- Offers feedback in the form of both counts and percentages of zero values, which allows a closer inspection of the distribution and proportion of zeros within a column.\n",
       "- Targets specifically numerical data, thereby avoiding inappropriate application to non-numerical columns and mitigating the risk of false test failures.\n",
       "\n",
       "**Limitations**:\n",
       "- Is exclusively designed to check for zero values, and doesn’t assesses the potential impact of other values that could affect the dataset, such as extremely high or low figures, missing values or outliers.\n",
       "- Lacks the ability to detect a repetitive pattern of zeros, which could be significant in time-series or longitudinal data.\n",
       "- Zero values can actually be meaningful in some contexts, therefore tagging them as 'too many' could potentially misinterpret the data to some extent.\n",
       "- This test does not take into consideration the context of the dataset, and fails to recognize that within certain columns, a high number of zero values could be quite normal and not necessarily an indicator of poor data quality.\n",
       "- Cannot evaluate non-numerical or categorical columns, which might bring with them different types of concerns or issues.</td>\n",
       "      <td id=\"T_fccf5_row12_col3\" class=\"data row12 col3\" >validmind.data_validation.TooManyZeroValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row13_col0\" class=\"data row13 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row13_col1\" class=\"data row13 col1\" >Pi T Credit Scores Histogram</td>\n",
       "      <td id=\"T_fccf5_row13_col2\" class=\"data row13 col2\" >**Purpose**: The PiT (Point in Time) Credit Scores Histogram metric is used to evaluate the predictive performance of a credit risk assessment model. This metric provides a visual representation of observed versus predicted default scores and enables quick and intuitive comparison for model assessment.\n",
       "\n",
       "**Test Mechanism**: This metric generates histograms for both observed and predicted score distributions of defaults and non-defaults. The simultaneous representation of both the observed and predicted scores sheds light on the model's ability to accurately predict credit risk.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Significant discrepancies between the observed and predicted histograms, suggesting that the model may not be adequately addressing certain risk factors.\n",
       "- Concentration of predicted defaults towards one end of the graph, or uneven distribution in comparison to observed scores, indicating potential issues in the model's interpretation of the data or outcome prediction.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides an intuitive visual representation of model performance that's easy to comprehend, even for individuals without a technical background.\n",
       "- Useful for understanding the model's ability to distinguish between defaulting and non-defaulting entities.\n",
       "- Specifically tailored for assessing credit risk models. The Point in Time (PiT) factor considers the evolution of credit risk over time.\n",
       "\n",
       "**Limitations**:\n",
       "- As the information is visual, precise and quantitative results for detailed statistical analyses may not be obtained.\n",
       "- The method relies on manual inspection and comparison, introducing subjectivity and potential bias.\n",
       "- Subtle discrepancies might go unnoticed and it could be less reliable for identifying such cues.\n",
       "- Performance may degrade when score distributions overlap significantly or when too many scores are plotted, resulting in cluttered or hard-to-decipher graphs.</td>\n",
       "      <td id=\"T_fccf5_row13_col3\" class=\"data row13 col3\" >validmind.data_validation.PiTCreditScoresHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row14_col0\" class=\"data row14 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row14_col1\" class=\"data row14 col1\" >Hashtags</td>\n",
       "      <td id=\"T_fccf5_row14_col2\" class=\"data row14 col2\" >**Purpose**: The Hashtags test is designed to measure the frequency of hashtags used within a given text column in a dataset. It is particularly useful for natural language processing tasks such as text classification and text summarization. The goal is to identify common trends and patterns in the use of hashtags, which can serve as critical indicators or features within a machine learning model.\n",
       "\n",
       "**Test Mechanism**: The test implements a regular expression (regex) to extract all hashtags from the specified text column. For each hashtag found, it makes a tally of its occurrences. It then outputs a list of the top N hashtags (default is 25, but customizable), sorted by their counts in descending order. The results are also visualized in a bar plot, with frequency counts on the y-axis and the corresponding hashtags on the x-axis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A low diversity in the usage of hashtags, as indicated by a few hashtags being used disproportionately more than others.\n",
       "- Repeated usage of one or few hashtags can be indicative of spam or a biased dataset.\n",
       "- If there are no or extremely few hashtags found in the dataset, it perhaps signifies that the text data does not contain structured social media data.\n",
       "\n",
       "**Strengths**:\n",
       "- It provides a concise visual representation of the frequency of hashtags, which can be critical for understanding trends about a particular topic in text data.\n",
       "- It is instrumental in tasks specifically related to social media text analytics, such as opinion analysis and trend discovery.\n",
       "- The test is adaptable, allowing the flexibility to determine the number of top hashtags to be analyzed.\n",
       "\n",
       "**Limitations**:\n",
       "- The test assumes the presence of hashtags and therefore may not be applicable for text datasets that do not contain hashtags (e.g., formal documents, scientific literature).\n",
       "- Language-specific limitations of hashtag formulations are not taken into account.\n",
       "- It does not account for typographical errors, variations, or synonyms in hashtags.\n",
       "- This test does not provide context or sentiment associated with the hashtags, so the information provided may have limited utility on its own.</td>\n",
       "      <td id=\"T_fccf5_row14_col3\" class=\"data row14 col3\" >validmind.data_validation.nlp.Hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row15_col0\" class=\"data row15 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row15_col1\" class=\"data row15 col1\" >Dataset Correlations</td>\n",
       "      <td id=\"T_fccf5_row15_col2\" class=\"data row15 col2\" >**Purpose**: The DatasetCorrelations metric is employed to examine the relationship between variables in a dataset, specifically designed for numerical and categorical data types. Using Pearson's R, Cramer's V, and Correlation ratios, it helps in understanding the linear relationship between numerical variables, association between categorical ones, and between numerical-categorical variables respectively. This allows for better awareness regarding dependency between features, which is crucial for optimizing model performance and understanding the model's behavior and predictors.\n",
       "\n",
       "**Test Mechanism**: During its execution, DatasetCorrelations initiates the calculation of the aforementioned correlation coefficients for the provided dataset. It leverages the built-in method 'get_correlations()', populating the 'correlations' attribute in the dataset object. It then invokes 'get_correlation_plots()' to generate graphical representations of these correlations. Finally, the correlation details and figures are cached for further study and analysis. The test does not dictate specific thresholds or grading scales.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- High correlation levels between input variables (multicollinearity), which can jeopardize the interpretability of the model and lead to overfitting.\n",
       "- The absence of any significant correlations, suggesting the variables may not have predictive power.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Comprehensive coverage of the correlation study of numerical, categorical, and numerical-categorical variables, negating the need for multiple individual tests.\n",
       "- Along with numerical correlation values, it provides visualization plots for a more intuitive understanding of relationships between variables.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Limitations of this metric include detecting only linear relationships and associations; nonlinear relationships may go unnoticed.\n",
       "- The absence of specified thresholds for determining correlation significance means the interpretation of the results is dependent on the user's expertise.\n",
       "- It doesn't manage missing values in the dataset, which need to be treated beforehand.</td>\n",
       "      <td id=\"T_fccf5_row15_col3\" class=\"data row15 col3\" >validmind.data_validation.DatasetCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row16_col0\" class=\"data row16 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row16_col1\" class=\"data row16 col1\" >Logistic Reg Cumulative Prob</td>\n",
       "      <td id=\"T_fccf5_row16_col2\" class=\"data row16 col2\" >**Purpose**: This metric is utilized to evaluate the distribution of predicted probabilities for positive and negative classes in a logistic regression model. It's not solely intended to measure the model's performance but also provides a visual assessment of the model's behavior by plotting the cumulative probabilities for positive and negative classes across both the training and test datasets.\n",
       "\n",
       "**Test Mechanism**: The logistic regression model is evaluated by first computing the predicted probabilities for each instance in both the training and test datasets, which are then added as a new column in these sets. The cumulative probabilities for positive and negative classes are subsequently calculated and sorted in ascending order. Cumulative distributions of these probabilities are created for both positive and negative classes across both training and test datasets. These cumulative probabilities are represented visually in a plot, containing two subplots\n",
       "- one for the training data and the other for the test data, with lines representing cumulative distributions of positive and negative classes.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Imbalanced distribution of probabilities for either positive or negative classes.\n",
       "- Notable discrepancies or significant differences between the cumulative probability distributions for the training data versus the test data.\n",
       "- Marked discrepancies or large differences between the cumulative probability distributions for positive and negative classes.\n",
       "\n",
       "**Strengths**:\n",
       "- It offers not only numerical probabilities but also provides a visual illustration of data, which enhances the ease of understanding and interpreting the model's behavior.\n",
       "- Allows for the comparison of model's behavior across training and testing datasets, providing insights about how well the model is generalized.\n",
       "- It differentiates between positive and negative classes and their respective distribution patterns, which can aid in problem diagnosis.\n",
       "\n",
       "**Limitations**:\n",
       "- Exclusive to classification tasks and specifically to logistic regression models.\n",
       "- Graphical results necessitate human interpretation and may not be directly applicable for automated risk detection.\n",
       "- The method does not give a solitary quantifiable measure of model risk, rather it offers a visual representation and broad distributional information.\n",
       "- If the training and test datasets are not representative of the overall data distribution, the metric could provide misleading results.</td>\n",
       "      <td id=\"T_fccf5_row16_col3\" class=\"data row16 col3\" >validmind.model_validation.statsmodels.LogisticRegCumulativeProb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row17_col0\" class=\"data row17 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row17_col1\" class=\"data row17 col1\" >Chi Squared Features Table</td>\n",
       "      <td id=\"T_fccf5_row17_col2\" class=\"data row17 col2\" >**Purpose**: The `ChiSquaredFeaturesTable` metric is used to carry out a Chi-Squared test of independence for each categorical feature variable against a designated target column. The primary goal is to determine if a significant association exists between the categorical features and the target variable. This method typically finds its use in the context of Model Risk Management to understand feature relevance and detect potential bias in a classification model.\n",
       "\n",
       "**Test Mechanism**: The testing process generates a contingency table for each categorical variable and the target variable, after which a Chi-Squared test is performed. Using this approach, the Chi-Squared statistic and the p-value for each feature are calculated. The p-value threshold is a modifiable parameter, and a test will qualify as passed if the p-value is less than or equal to this threshold. If not, the test is labeled as failed. The outcome for each feature\n",
       "- comprising the variable name, Chi-squared statistic, p-value, threshold, and pass/fail status\n",
       "- is incorporated into a final summary table.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High p-values (greater than the set threshold) for specific variables could indicate a high risk.\n",
       "- These high p-values allude to the absence of a statistically significant relationship between the feature and the target variables, resulting in a 'Fail' status.\n",
       "- A categorical feature lacking a relevant association with the target variable could be a warning that the machine learning model might not be performing optimally.\n",
       "\n",
       "**Strengths**:\n",
       "- The test allows for a comprehensive understanding of the interaction between a model's input features and the target output, thus validating the relevance of categorical features.\n",
       "- It also produces an unambiguous 'Pass/Fail' output for each categorical feature.\n",
       "- The opportunity to adjust the p-value threshold contributes to flexibility in accommodating different statistical standards.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric presupposes that data is tabular and categorical, which may not always be the case with all datasets.\n",
       "- It is distinctively designed for classification tasks, hence unsuitable for regression scenarios.\n",
       "- The Chi-squared test, akin to any hypothesis testing-based test, cannot identify causal relationships, but only associations.\n",
       "- Furthermore, the test hinges on an adjustable p-value threshold, and varying threshold selections might lead to different conclusions regarding feature relevance.</td>\n",
       "      <td id=\"T_fccf5_row17_col3\" class=\"data row17 col3\" >validmind.data_validation.ChiSquaredFeaturesTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row18_col0\" class=\"data row18 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row18_col1\" class=\"data row18 col1\" >Bias</td>\n",
       "      <td id=\"T_fccf5_row18_col2\" class=\"data row18 col2\" >**Purpose:** The Bias Evaluation test calculates if and how the order and distribution of exemplars (examples) in a few-shot learning prompt affect the output of a Language Learning Model (LLM). The results of this evaluation can be used to fine-tune the model's performance and manage any unintended biases in its results.\n",
       "\n",
       "**Test Mechanism:** This test uses two checks:\n",
       "\n",
       "1. *Distribution of Exemplars:* The number of positive vs. negative examples in a prompt is varied. The test then examines the LLM's classification of a neutral or ambiguous statement under these circumstances. 2. *Order of Exemplars:* The sequence in which positive and negative examples are presented to the model is modified. Their resultant effect on the LLM's response is studied.\n",
       "\n",
       "For each test case, the LLM grades the input prompt on a scale of 1 to 10. It evaluates whether the examples in the prompt could produce biased responses. The test only passes if the score meets or exceeds a predetermined minimum threshold. This threshold is set at 7 by default, but it can be modified as per the requirements via the test parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "\n",
       "- A skewed result favoring either positive or negative responses may suggest potential bias in the model. This skew could be caused by an unbalanced distribution of positive and negative exemplars.\n",
       "- If the score given by the model is less than the set minimum threshold, it might indicate a risk of high bias and hence poor performance.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "- This test provides a quantitative measure of potential bias, providing clear guidelines for developers about whether their Language Learning Model (LLM) contains significant bias.\n",
       "- It's useful in evaluating the impartiality of the model based on the distribution and sequence of examples.\n",
       "- The flexibility to adjust the minimum required threshold allows tailoring this test to stricter or more lenient bias standards.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "- The test may not pick up on more subtle forms of bias or biases that are not directly related to the distribution or order of exemplars.\n",
       "- The test's effectiveness will decrease if the quality or balance of positive and negative exemplars is not representative of the problem space the model is intended to solve.\n",
       "- The use of a grading mechanism to gauge bias may not be entirely accurate in every case, particularly when the difference between threshold and score is narrow.</td>\n",
       "      <td id=\"T_fccf5_row18_col3\" class=\"data row18 col3\" >validmind.prompt_validation.Bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row19_col0\" class=\"data row19 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row19_col1\" class=\"data row19 col1\" >Minimum ROCAUC Score</td>\n",
       "      <td id=\"T_fccf5_row19_col2\" class=\"data row19 col2\" >**Purpose**: This test metric, Minimum ROC AUC Score, is used to determine the model's performance by ensuring that the Receiver Operating Characteristic Area Under the Curve (ROC AUC) score on the validation dataset meets or exceeds a predefined threshold. The ROC AUC score is an indicator of how well the model is capable of distinguishing between different classes, making it a crucial measure in binary and multiclass classification tasks.\n",
       "\n",
       "**Test Mechanism**: This test implementation calculates the multiclass ROC AUC score on the true target values and the model's prediction. The test converts the multi-class target variables into binary format using `LabelBinarizer` before computing the score. If this ROC AUC score is higher than the predefined threshold (defaulted to 0.5), the test passes; otherwise, it fails. The results, including the ROC AUC score, the threshold, and whether the test passed or failed, are then stored in a `ThresholdTestResult` object.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk or failure in the model's performance as related to this metric would be represented by a low ROC AUC score, specifically any score lower than the predefined minimum threshold. This suggests that the model is struggling to distinguish between different classes effectively.\n",
       "\n",
       "**Strengths**:\n",
       "- The test considers both the true positive rate and false positive rate, providing a comprehensive performance measure.\n",
       "- ROC AUC score is threshold-independent meaning it measures the model's quality across various classification thresholds.\n",
       "- Works robustly with binary as well as multi-class classification problems.\n",
       "\n",
       "**Limitations**:\n",
       "- ROC AUC may not be useful if the class distribution is highly imbalanced; it could perform well in terms of AUC but still fail to predict the minority class.\n",
       "- The test does not provide insight into what specific aspects of the model are causing poor performance if the ROC AUC score is unsatisfactory.\n",
       "- The use of macro average for multiclass ROC AUC score implies equal weightage to each class, which might not be appropriate if the classes are imbalanced.</td>\n",
       "      <td id=\"T_fccf5_row19_col3\" class=\"data row19 col3\" >validmind.model_validation.sklearn.MinimumROCAUCScore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row20_col0\" class=\"data row20 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row20_col1\" class=\"data row20 col1\" >Tabular Numerical Histograms</td>\n",
       "      <td id=\"T_fccf5_row20_col2\" class=\"data row20 col2\" >**Purpose**: The purpose of this test is to provide visual analysis of numerical data through the generation of histograms for each numerical feature in the dataset. Histograms aid in the exploratory analysis of data, offering insight into the distribution of the data, skewness, presence of outliers, and central tendencies. It helps in understanding if the inputs to the model are normally distributed which is a common assumption in many machine learning algorithms.\n",
       "\n",
       "**Test Mechanism**: This test scans the provided dataset and extracts all the numerical columns. For each numerical column, it constructs a histogram using plotly, with 50 bins. The deployment of histograms offers a robust visual aid, ensuring unruffled identification and understanding of numerical data distribution patterns.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- A high degree of skewness\n",
       "- Unexpected data distributions\n",
       "- Existence of extreme outliers in the histograms These may indicate issues with the data that the model is receiving. If data for a numerical feature is expected to follow a certain distribution (like normal distribution) but does not, it could lead to sub-par performance by the model. As such these instances should be treated as high-risk indicators.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- This test provides a simple, easy-to-interpret visualization of how data for each numerical attribute is distributed.\n",
       "- It can help detect skewed values and outliers, that could potentially harm the AI model's performance.\n",
       "- It can be applied to large datasets and multiple numerical variables conveniently.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- This test only works with numerical data, thus ignoring non-numerical or categorical data.\n",
       "- It does not analyze relationships between different features, only the individual feature distributions.\n",
       "- It is a univariate analysis, and may miss patterns or anomalies that only appear when considering multiple variables together.\n",
       "- It does not provide any insight into how these features affect the output of the model; it is purely an input analysis tool.</td>\n",
       "      <td id=\"T_fccf5_row20_col3\" class=\"data row20 col3\" >validmind.data_validation.TabularNumericalHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row21_col0\" class=\"data row21 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row21_col1\" class=\"data row21 col1\" >Common Words</td>\n",
       "      <td id=\"T_fccf5_row21_col2\" class=\"data row21 col2\" >**Purpose**: The CommonWords metric is used to identify and visualize the most prevalent words within a specified text column of a dataset. This provides insights into the prevalent language patterns and vocabulary, especially useful in Natural Language Processing (NLP) tasks such as text classification and text summarization.\n",
       "\n",
       "**Test Mechanism**: The test methodology involves splitting the specified text column's entries into words, collating them into a corpus, and then counting the frequency of each word using the Counter. The forty most frequently occurring non-stopwords are then visualized in a bar chart, where the x-axis represents the words, and the y-axis indicates their frequency of occurrence.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A lack of distinct words within the list, or the most common words being stopwords.\n",
       "- Frequent occurrence of irrelevant or inappropriate words could point out a poorly curated or noisy dataset.\n",
       "- An error returned due to the absence of a valid Dataset object indicates high risk as the metric cannot be effectively implemented without it.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric provides clear insights into the language features – specifically word frequency – of unstructured text data.\n",
       "- It can reveal prominent vocabulary and language patterns, which prove vital for feature extraction in NLP tasks.\n",
       "- The visualization helps in quickly capturing the patterns and understanding the data intuitively.\n",
       "\n",
       "**Limitations**:\n",
       "- The test disregards semantic or context-related information as it solely focuses on word frequency.\n",
       "- It intentionally ignores stopwords which might carry necessary significance in certain scenarios.\n",
       "- The applicability is limited to English language text data as English stopwords are used for filtering, hence cannot account for data in other languages.\n",
       "- The metric requires a valid Dataset object, indicating a dependency condition that limits its broader applicability.</td>\n",
       "      <td id=\"T_fccf5_row21_col3\" class=\"data row21 col3\" >validmind.data_validation.nlp.CommonWords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row22_col0\" class=\"data row22 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row22_col1\" class=\"data row22 col1\" >Missing Values Bar Plot</td>\n",
       "      <td id=\"T_fccf5_row22_col2\" class=\"data row22 col2\" >**Purpose:** The 'MissingValuesBarPlot' metric provides a color-coded visual representation of the percentage of missing values for each column in an ML model's dataset. The primary purpose of this metric is to easily identify and quantify missing data, which are essential steps in data preprocessing. The presence of missing data can potentially skew the model's predictions and decrease its accuracy. Additionally, this metric uses a pre-set threshold to categorize various columns into ones that contain missing data above the threshold (high risk) and below the threshold (less risky).\n",
       "\n",
       "**Test Mechanism:** The test mechanism involves scanning each column in the input dataset and calculating the percentage of missing values. It then compares each column's missing data percentage with the predefined threshold, categorizing columns with missing data above the threshold as high-risk. The test generates a bar plot in which columns with missing data are represented on the y-axis and their corresponding missing data percentages are displayed on the x-axis. The color of each bar reflects the missing data percentage in relation to the threshold: grey for values below the threshold and light coral for those exceeding it. The user-defined threshold is represented by a red dashed line on the plot.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Columns with higher percentages of missing values beyond the threshold are high-risk. These are visually represented by light coral bars on the bar plot.\n",
       "\n",
       "**Strengths:**\n",
       "- Helps in quickly identifying and quantifying missing data across all columns of the dataset.\n",
       "- Facilitates pattern recognition through visual representation.\n",
       "- Enables customization of the level of risk tolerance via a user-defined threshold.\n",
       "- Supports both classification and regression tasks, sharing its versatility.\n",
       "\n",
       "**Limitations:**\n",
       "- It only considers the quantity of missing values, not differentiating between different types of missingness (Missing completely at random\n",
       "- MCAR, Missing at random\n",
       "- MAR, Not Missing at random\n",
       "- NMAR).\n",
       "- It doesn't offer insights into potential approaches for handling missing entries, such as various imputation strategies.\n",
       "- The metric does not consider possible impacts of the missing data on the model's accuracy or precision.\n",
       "- Interpretation of the findings and the next steps might require an expert understanding of the field.</td>\n",
       "      <td id=\"T_fccf5_row22_col3\" class=\"data row22 col3\" >validmind.data_validation.MissingValuesBarPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row23_col0\" class=\"data row23 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row23_col1\" class=\"data row23 col1\" >Dataset Split</td>\n",
       "      <td id=\"T_fccf5_row23_col2\" class=\"data row23 col2\" >**Purpose:** The DatasetSplit test is designed to evaluate and visualize the distribution of data among training, testing, and validation datasets, if available, within a given machine learning model. The main purpose is to assess whether the model's datasets are split appropriately, as an imbalanced split might affect the model's ability to learn from the data and generalize to unseen data.\n",
       "\n",
       "**Test Mechanism:** The DatasetSplit test first calculates the total size of all available datasets in the model. Then, for each individual dataset, the methodology involves determining the size of the dataset and its proportion relative to the total size. The results are then conveniently summarized in a table that shows dataset names, sizes, and proportions. Absolute size and proportion of the total dataset size are displayed for each individual dataset.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- A very small training dataset, which may result in the model not learning enough from the data.\n",
       "- A very large training dataset and a small test dataset, which may lead to model overfitting and poor generalization to unseen data.\n",
       "- A small or non-existent validation dataset, which might complicate the model's performance assessment.\n",
       "\n",
       "**Strengths:**\n",
       "- The DatasetSplit test provides a clear, understandable visualization of dataset split proportions, which can highlight any potential imbalance in dataset splits quickly.\n",
       "- It covers a wide range of task types including classification, regression, and text-related tasks.\n",
       "- The metric is not tied to any specific data type and is applicable to tabular data, time series data, or text data.\n",
       "\n",
       "**Limitations:**\n",
       "- The DatasetSplit test does not provide any insight into the quality or diversity of the data within each split, just the size and proportion.\n",
       "- The test does not give any recommendations or adjustments for imbalanced datasets.\n",
       "- Potential lack of compatibility with more complex modes of data splitting (for example, stratified or time-based splits) could limit the applicability of this test.</td>\n",
       "      <td id=\"T_fccf5_row23_col3\" class=\"data row23 col3\" >validmind.data_validation.DatasetSplit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row24_col0\" class=\"data row24 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row24_col1\" class=\"data row24 col1\" >Robustness Diagnosis</td>\n",
       "      <td id=\"T_fccf5_row24_col2\" class=\"data row24 col2\" >**Purpose**:\n",
       "\n",
       "The purpose of this test code is to evaluate the robustness of a machine learning model. Robustness refers to a model's ability to maintain a high level of performance in the face of perturbations or changes—particularly noise—added to its input data. This test is designed to help gauge how well the model can handle potential real-world scenarios where the input data might be incomplete or corrupted.\n",
       "\n",
       "**Test Mechanism**:\n",
       "\n",
       "This test is conducted by adding Gaussian noise, proportional to a particular standard deviation scale, to numeric input features of both the training and testing datasets. The model performance in the face of these perturbed features is then evaluated using metrics (default: 'accuracy'). This process is iterated over a range of scale factors. The resulting accuracy trend against the amount of noise introduced is illustrated with a line chart. A predetermined threshold determines what level of accuracy decay due to perturbation is considered acceptable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Substantial decreases in accuracy when noise is introduced to feature inputs.\n",
       "- The decay in accuracy surpasses the configured threshold, indicating that the model is not robust against input noise.\n",
       "- Instances where one or more elements provided in the features list don't match with the training dataset's numerical feature columns.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides an empirical measure of the model's performance in tackling noise or data perturbations, revealing insights into the model's stability.\n",
       "- Offers flexibility with the ability to choose specific features to perturb and control the level of noise applied.\n",
       "- Detailed results visualization helps in interpreting the outcome of robustness testing.\n",
       "\n",
       "**Limitations**:\n",
       "- Only numerical features are perturbed, leaving out non-numerical features, which can lead to an incomplete analysis of robustness.\n",
       "- The default metric used is accuracy, which might not always give the best measure of a model's success, particularly for imbalanced datasets.\n",
       "- The test is contingent on the assumption that the added Gaussian noise sufficiently represents potential data corruption or incompleteness in real-world scenarios.\n",
       "- There might be a requirement to fine-tune the set decay threshold for accuracy with the help of domain knowledge or specific project requisites.\n",
       "- The robustness test might not deliver the expected results for datasets with a text column.</td>\n",
       "      <td id=\"T_fccf5_row24_col3\" class=\"data row24 col3\" >validmind.model_validation.sklearn.RobustnessDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row25_col0\" class=\"data row25 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row25_col1\" class=\"data row25 col1\" >SHAP Global Importance</td>\n",
       "      <td id=\"T_fccf5_row25_col2\" class=\"data row25 col2\" >**Purpose:** The SHAP (SHapley Additive exPlanations) Global Importance metric aims to elucidate model outcomes by attributing them to the contributing features. It assigns a quantifiable global importance to each feature via their respective absolute Shapley values, thereby making it suitable for tasks like classification (both binary and multiclass). This metric forms an essential part of model risk management.\n",
       "\n",
       "**Test Mechanism:** The exam begins with the selection of a suitable explainer which aligns with the model's type. For tree-based models like XGBClassifier, RandomForestClassifier, CatBoostClassifier, TreeExplainer is used whereas for linear models like LogisticRegression, XGBRegressor, LinearRegression, it is the LinearExplainer. Once the explainer calculates the Shapley values, these values are visualized using two specific graphical representations:\n",
       "\n",
       "1. Mean Importance Plot: This graph portrays the significance of individual features based on their absolute Shapley values. It calculates the average of these absolute Shapley values across all instances to highlight the global importance of features.\n",
       "\n",
       "2. Summary Plot: This visual tool combines the feature importance with their effects. Every dot on this chart represents a Shapley value for a certain feature in a specific case. The vertical axis is denoted by the feature whereas the horizontal one corresponds to the Shapley value. A color gradient indicates the value of the feature, gradually changing from low to high. Features are systematically organized in accordance with their importance. These plots are generated by the function `_generate_shap_plot()`.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Overemphasis on certain features in SHAP importance plots, thus hinting at the possibility of model overfitting\n",
       "- Anomalies such as unexpected or illogical features showing high importance, which might suggest that the model's decisions are rooted in incorrect or undesirable reasoning\n",
       "- A SHAP summary plot filled with high variability or scattered data points, indicating a cause for concern\n",
       "\n",
       "**Strengths:**\n",
       "- SHAP does more than just illustrating global feature significance, it offers a detailed perspective on how different features shape the model's decision-making logic for each instance.\n",
       "- It provides clear insights into model behavior.\n",
       "- It demonstrates flexibility by supporting a wide array of model types, thereby promising uniform interpretations across different models.\n",
       "\n",
       "**Limitations:**\n",
       "- SHAP might demand considerable time and resources for large datasets or intricate models.\n",
       "- Its compatibility doesn't cover all model classes. Models from libraries like \"statsmodels\", \"pytorch\", \"catboost\", \"transformers\", \"FoundationModel\", and \"R\" can't be handled.\n",
       "- High-dimensional data can convolute interpretations.\n",
       "- Associating importance with tangible real-world impact still involves a certain degree of subjectivity.</td>\n",
       "      <td id=\"T_fccf5_row25_col3\" class=\"data row25 col3\" >validmind.model_validation.sklearn.SHAPGlobalImportance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row26_col0\" class=\"data row26 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row26_col1\" class=\"data row26 col1\" >Missing Values Risk</td>\n",
       "      <td id=\"T_fccf5_row26_col2\" class=\"data row26 col2\" >**Purpose**: The Missing Values Risk metric is specifically designed to assess and quantify the risk associated with missing values in the dataset used for machine learning model training. It measures two specific risks: the percentage of total data that are missing, and the percentage of all variables (columns) that contain some missing values.\n",
       "\n",
       "**Test Mechanism**: Initially, the metric calculates the total number of data points in the dataset and the count of missing values. It then inspects each variable (column) to determine how many contain at least one missing datapoint. By methodically counting missing datapoints across the entire dataset and each variable (column), it identifies the percentage of missing values in the entire dataset and the percentage of variables (columns) with such values.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Record high percentages in either of the risk measures could suggest a high risk.\n",
       "- If the dataset indicates a high percentage of missing values, it might significantly undermine the model's performance and credibility.\n",
       "- If a significant portion of variables (columns) in the dataset are missing values, this could make the model susceptible to bias and overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The metric offers valuable insights into the readiness of a dataset for model training as missing values can heavily destabilize both the model's performance and predictive capabilities.\n",
       "- The metric's quantification of the risks caused by missing values allows for the use of targeted methods to manage these values correctly- either through removal, imputation, or alternative strategies.\n",
       "- The metric has the flexibility to be applied to both classification and regression assignments, maintaining its utility across a wide range of models and scenarios.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The metric primarily identifies and quantifies the risk associated with missing values without suggesting specific mitigation strategies.\n",
       "- The metric does not ascertain whether the missing values are random or associated with an underlying issue in the stages of data collection or preprocessing.\n",
       "- However, the identification of the presence and scale of missing data is the essential initial step towards improving data quality.</td>\n",
       "      <td id=\"T_fccf5_row26_col3\" class=\"data row26 col3\" >validmind.data_validation.MissingValuesRisk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row27_col0\" class=\"data row27 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row27_col1\" class=\"data row27 col1\" >Descriptive Statistics</td>\n",
       "      <td id=\"T_fccf5_row27_col2\" class=\"data row27 col2\" >**Purpose**: The purpose of the Descriptive Statistics metric is to provide a comprehensive summary of both numerical and categorical data within a dataset. This involves statistics such as count, mean, standard deviation, minimum and maximum values for numerical data. For categorical data, it calculates the count, number of unique values, most common value and its frequency, and the proportion of the most frequent value relative to the total. The goal is to visualize the overall distribution of the variables in the dataset, aiding in understanding the model's behavior and predicting its performance.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism utilizes two in-built functions of pandas dataframes: describe() for numerical fields and value_counts() for categorical fields. The describe() function pulls out several summary statistics, while value_counts() accounts for unique values. The resulting data is formatted into two distinct tables, one for numerical and another for categorical variable summaries. These tables provide a clear summary of the main characteristics of the variables, which can be instrumental in assessing the model's performance.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Skewed data or significant outliers can represent high risk. For numerical data, this may be reflected via a significant difference between the mean and median (50% percentile).\n",
       "- For categorical data, a lack of diversity (low count of unique values), or overdominance of a single category (high frequency of the top value) can indicate high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- This metric provides a comprehensive summary of the dataset, shedding light on the distribution and characteristics of the variables under consideration.\n",
       "- It is a versatile and robust method, applicable to both numerical and categorical data.\n",
       "- It helps highlight crucial anomalies such as outliers, extreme skewness, or lack of diversity, which are vital in understanding model behavior during testing and validation.\n",
       "\n",
       "**Limitations**:\n",
       "- While this metric offers a high-level overview of the data, it may fail to detect subtle correlations or complex patterns.\n",
       "- It does not offer any insights on the relationship between variables.\n",
       "- Alone, descriptive statistics cannot be used to infer properties about future unseen data.\n",
       "- It should be used in conjunction with other statistical tests to provide a comprehensive understanding of the model's data.</td>\n",
       "      <td id=\"T_fccf5_row27_col3\" class=\"data row27 col3\" >validmind.data_validation.DescriptiveStatistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row28_col0\" class=\"data row28 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row28_col1\" class=\"data row28 col1\" >Shapiro Wilk</td>\n",
       "      <td id=\"T_fccf5_row28_col2\" class=\"data row28 col2\" >**Purpose**: The Shapiro-Wilk test is utilized to investigate whether a particular dataset conforms to the standard normal distribution. This analysis is crucial in machine learning modeling because the normality of the data can profoundly impact the performance of the model. This metric is especially useful in evaluating various features of the dataset in both classification and regression tasks.\n",
       "\n",
       "**Test Mechanism**: The Shapiro-Wilk test is conducted on each feature column of the training dataset to determine if the data contained fall within the normal distribution. The test presents a statistic and a p-value, with the p-value serving to validate or repudiate the null hypothesis, which is that the tested data is normally distributed.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A p-value that falls below 0.05 signifies a high risk as it discards the null hypothesis, indicating that the data does not adhere to the normal distribution.\n",
       "- For machine learning models built on the presumption of data normality, such an outcome could result in subpar performance or incorrect predictions.\n",
       "\n",
       "**Strengths**:\n",
       "- The Shapiro-Wilk test is esteemed for its level of accuracy, thereby making it particularly well-suited to datasets of small to moderate sizes.\n",
       "- It proves its versatility through its efficient functioning in both classification and regression tasks.\n",
       "- By separately testing each feature column, the Shapiro-Wilk test can raise an alarm if a specific feature does not comply with the normality.\n",
       "\n",
       "**Limitations**:\n",
       "- The Shapiro-Wilk test's sensitivity can be a disadvantage as it often rejects the null hypothesis (i.e., data is normally distributed), even for minor deviations, especially in large datasets. This may lead to unwarranted 'false alarms' of high risk by deeming the data as not normally distributed even if it approximates normal distribution.\n",
       "- Exceptional care must be taken in managing missing data or outliers prior to testing as these can greatly skew the results.\n",
       "- Lastly, the Shapiro-Wilk test is not optimally suited for processing data with pronounced skewness or kurtosis.</td>\n",
       "      <td id=\"T_fccf5_row28_col3\" class=\"data row28 col3\" >validmind.model_validation.statsmodels.ShapiroWilk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row29_col0\" class=\"data row29 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row29_col1\" class=\"data row29 col1\" >Dataset Description</td>\n",
       "      <td id=\"T_fccf5_row29_col2\" class=\"data row29 col2\" >**Purpose**: This metric, known as DatasetDescription, has been architected to furnish an extensive spectrum of descriptive statistics pertinent to the input data that fuels a machine learning model. The statistics generated by this metric include measures of central tendency, dispersion, in addition to frequency counts for each field, or variable, in the dataset. The purpose of these statistics lies in comprehending the nature of the data, identifying anomalies, and grasping the extent to which the data fulfills model assumptions.\n",
       "\n",
       "**Test Mechanism**: The testing function commences by morphing the input dataset back to its primary form by undoing any one-hot encoding, subsequently extracting each field from the dataset. Post this process, the script computes the descriptive statistics for each field, contingent on its data type (Numeric, Categorical, Boolean, Dummy, Text, Null). For numeric fields, the statistics include computations of mean, standard deviation, minimum, maximum, and varying percentiles. For categorical and boolean fields, the frequency counts of each category along with the most frequent category (or 'top') are computed. The script computes missing and distinct values for all fields, while also generating histograms for numeric and categorical types, as well as word count histograms for text\n",
       "- accomplished through counting the number of occurrences of each unique word.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant portion of missing or null values, which may hinder the model's performance.\n",
       "- Predominance of a single value (indicating low diversity) within a feature.\n",
       "- High variance in a numeric feature.\n",
       "- High number of distinct categories in categorical features.\n",
       "- High frequency of a specific word in text data as this might signal bias in the dataset.\n",
       "\n",
       "**Strengths**:\n",
       "- Capability to provide an all-inclusive, general summary of the dataset.\n",
       "- It helps to understand the distribution, dispersion, and central tendency of numeric features, moreover, the frequency distribution within categorical and text features.\n",
       "- It can efficaciously handle different data types, offering insights into the variety and distribution of field values.\n",
       "\n",
       "**Limitations**:\n",
       "- It does not provide insights regarding the relationships between different fields or features, as it focuses only on univariate analysis.\n",
       "- The statistical description for text data is severely limited, given it essentially forms a bag-of-words model.\n",
       "- The descriptive statistics generated, including mean and standard deviation, hold no meaningful value for ordinal categorical data.\n",
       "- This metric does not offer intelligence on how the derived data will influence the performance of the model.</td>\n",
       "      <td id=\"T_fccf5_row29_col3\" class=\"data row29 col3\" >validmind.data_validation.DatasetDescription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row30_col0\" class=\"data row30 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row30_col1\" class=\"data row30 col1\" >Negative Instruction</td>\n",
       "      <td id=\"T_fccf5_row30_col2\" class=\"data row30 col2\" >**Purpose:** The Negative Instruction test is utilized to scrutinize the prompts given to a Language Learning Model (LLM). The objective is to ensure these prompts are expressed using proactive, affirmative language. The focus is on instructions indicating what needs to be done rather than what needs to be avoided, thereby guiding the LLM more efficiently towards the desired output.\n",
       "\n",
       "**Test Mechanism:** An LLM is employed to evaluate each prompt. The prompt is graded based on its use of positive instructions with scores ranging between 1-10. This grade reflects how effectively the prompt leverages affirmative language while shying away from negative or restrictive instructions. A prompt that attains a grade equal to or above a predetermined threshold (7 by default) is regarded as adhering effectively to the best practices of positive instruction. This threshold can be custom-tailored through the test parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Low score obtained from the LLM analysis, indicating heavy reliance on negative instructions in the prompts.\n",
       "- Failure to surpass the preset minimum threshold.\n",
       "- The LLM generates ambiguous or undesirable outputs as a consequence of the negative instructions used in the prompt.\n",
       "\n",
       "**Strengths:**\n",
       "- Encourages the usage of affirmative, proactive language in prompts, aiding in more accurate and advantageous model responses.\n",
       "- The test result provides a comprehensible score, helping to understand how well a prompt follows the positive instruction best practices.\n",
       "\n",
       "**Limitations:**\n",
       "- Despite an adequate score, a prompt could still be misleading or could lead to undesired responses due to factors not covered by this test.\n",
       "- The test necessitates an LLM for evaluation, which might not be available or feasible in certain scenarios.\n",
       "- A numeric scoring system, while straightforward, may oversimplify complex issues related to prompt designing and instruction clarity.\n",
       "- The effectiveness of the test hinges significantly on the predetermined threshold level, which can be subjective and may need to be adjusted according to specific use-cases.</td>\n",
       "      <td id=\"T_fccf5_row30_col3\" class=\"data row30 col3\" >validmind.prompt_validation.NegativeInstruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row31_col0\" class=\"data row31 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row31_col1\" class=\"data row31 col1\" >PD Rating Class Plot</td>\n",
       "      <td id=\"T_fccf5_row31_col2\" class=\"data row31 col2\" >**Purpose**: The purpose of the Probability of Default (PD) Rating Class Plot test is to measure and evaluate the distribution of calculated default probabilities across different rating classes. This is critical for understanding and inferring credit risk and can provide insights into how effectively the model is differentiating between different risk levels in a credit dataset.\n",
       "\n",
       "**Test Mechanism**: This metric is implemented via a visualization mechanism. It sorts the predicted probabilities of defaults into user-defined rating classes defined in \"rating_classes\" in default parameters. When it has classified the probabilities, it then calculates the average default rates within each rating class. Subsequently, it produces bar plots for each of these rating classes, illustrating the average likelihood of a default within each class. This process is executed separately for both the training and testing data sets. The classification of predicted probabilities utilizes the pandas \"cut\" function, sorting and sectioning the data values into bins.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- If lower rating classes present higher average likelihoods of default than higher rating classes\n",
       "- If there is poor differentiation between the averages across the different rating classes\n",
       "- If the model generates a significant contrast between the likelihoods for the training set and the testing set, suggestive of model overfitting\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Presents a clear visual representation of how efficient the model is at predicting credit risk across different risk levels\n",
       "- Allows for rapid identification and understanding of model performance per rating class\n",
       "- Highlights potential overfitting issues by including both training and testing datasets in the analysis\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Making an incorrect choice for the number of rating classes, either oversimplifying or overcomplicating the distribution of default rates\n",
       "- Relying on the assumption that the rating classes are effective at differentiating risk levels and that the boundaries between classes truly represent the risk distribution\n",
       "- Not accounting for data set class imbalance, which could cause skewed average probabilities\n",
       "- Inability to gauge the overall performance of the model only based on this metric, emphasizing the requirement of combining it with other evaluation metrics</td>\n",
       "      <td id=\"T_fccf5_row31_col3\" class=\"data row31 col3\" >validmind.model_validation.statsmodels.PDRatingClassPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row32_col0\" class=\"data row32 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row32_col1\" class=\"data row32 col1\" >Overfit Diagnosis</td>\n",
       "      <td id=\"T_fccf5_row32_col2\" class=\"data row32 col2\" >**Purpose**: The OverfitDiagnosis test is devised to detect areas within a Machine Learning model that might be prone to overfitting. It achieves this by comparing the model's performance on both the training and testing datasets. These datasets are broken down into distinct sections defined by a Feature Space. Areas, where the model underperforms by displaying high residual values or a significant amount of overfitting, are highlighted, prompting actions for mitigation using regularization techniques such as L1 or L2 regularization, Dropout, Early Stopping or data augmentation.\n",
       "\n",
       "**Test Mechanism**: The metric conducts the test by executing the method 'run' on the default parameters and metrics with 'accuracy' as the specified metric. It segments the feature space by binning crucial feature columns from both the training and testing datasets. Then, the method computes the prediction results for each defined region. Subsequently, the prediction's efficacy is evaluated, i.e., the model's performance gap (defined as the discrepancy between the actual and the model's predictions) for both datasets is calculated and compared with a preset cut-off value for the overfitting condition. A test failure presents an overfit model, whereas a pass signifies a fit model. Meanwhile, the function also prepares figures further illustrating the regions with overfitting.\n",
       "\n",
       "**Signs of High Risk**: Indicators of a high-risk model are:\n",
       "- A high 'gap' value indicating discrepancies in the training and testing data accuracy signals an overfit model.\n",
       "- Multiple or vast overfitting zones within the feature space suggest overcomplication of the model.\n",
       "\n",
       "**Strengths**:\n",
       "- Presents a visual perspective by plotting regions with overfit issues, simplifying understanding of the model structure.\n",
       "- Permits a feature-focused assessment, which promotes specific, targeted modifications to the model.\n",
       "- Caters to modifications of the testing parameters such as 'cut_off_percentage' and 'features_column' enabling a personalized analysis.\n",
       "- Handles both numerical and categorical features.\n",
       "\n",
       "**Limitations**:\n",
       "- Does not currently support regression tasks and is limited to classification tasks only.\n",
       "- Ineffectual for text-based features, which in turn restricts its usage for Natural Language Processing models.\n",
       "- Primarily depends on the bins setting, responsible for segmenting the feature space. Different bin configurations might yield varying results.\n",
       "- Utilization of a fixed cut-off percentage for making overfitting decisions, set arbitrarily, leading to a possible risk of inaccuracy.\n",
       "- Limitation of performance metrics to accuracy alone might prove inadequate for detailed examination, especially for imbalanced datasets.</td>\n",
       "      <td id=\"T_fccf5_row32_col3\" class=\"data row32 col3\" >validmind.model_validation.sklearn.OverfitDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row33_col0\" class=\"data row33 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row33_col1\" class=\"data row33 col1\" >Runs Test</td>\n",
       "      <td id=\"T_fccf5_row33_col2\" class=\"data row33 col2\" >**Purpose**: The Runs Test is a statistical procedure used to determine whether the sequence of data extracted from the ML model behaves randomly or not. Specifically, it analyzes runs, sequences of consecutive positives or negatives, in the data to check if there are more or fewer runs than expected under the assumption of randomness. This can be an indication of some pattern, trend, or cycle in the model's output which may need attention.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism applies the Runs Test from the statsmodels module on each column of the training dataset. For every feature in the dataset, a Runs Test is executed, whose output includes a Runs Statistic and P-value. A low P-value suggests that data arrangement in the feature is not likely to be random. The results are stored in a dictionary where the keys are the feature names, and the values are another dictionary storing the test statistic and the P-value for each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk is indicated when the P-value is close to zero.\n",
       "- If the p-value is less than a predefined significance level (like 0.05), it suggests that the runs (series of positive or negative values) in the model's output are not random and are longer or shorter than what is expected under a random scenario.\n",
       "- This would mean there's a high risk of non-random distribution of errors or model outcomes, suggesting potential issues with the model.\n",
       "\n",
       "**Strengths**:\n",
       "- The strength of the Runs Test is that it's straightforward and fast for detecting non-random patterns in data sequence.\n",
       "- It can validate assumptions of randomness, which is particularly valuable for checking error distributions in regression models, trendless time series data, and making sure a classifier doesn't favour one class over another.\n",
       "- Moreover, it can be applied to both classification and regression tasks, making it versatile.\n",
       "\n",
       "**Limitations**:\n",
       "- The test assumes that the data is independently and identically distributed (i.i.d.), which might not be the case for many real-world datasets.\n",
       "- The conclusion drawn from the low p-value indicating non-randomness does not provide information about the type or the source of the detected pattern.\n",
       "- Also, it is sensitive to extreme values (outliers), and overly large or small run sequences can influence the results.\n",
       "- Furthermore, this test does not provide model performance evaluation; it is used to detect patterns in the sequence of outputs only.</td>\n",
       "      <td id=\"T_fccf5_row33_col3\" class=\"data row33 col3\" >validmind.model_validation.statsmodels.RunsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row34_col0\" class=\"data row34 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row34_col1\" class=\"data row34 col1\" >WOE Bin Plots</td>\n",
       "      <td id=\"T_fccf5_row34_col2\" class=\"data row34 col2\" >**Purpose**: This test is designed to visualize the Weight of Evidence (WoE) and Information Value (IV) for categorical variables in a provided dataset. By showcasing the data distribution across different categories of each feature, it aids in understanding each variable's predictive power in the context of a classification-based machine learning model. Commonly used in credit scoring models, WoE and IV are robust statistical methods for evaluating a variable's predictive power.\n",
       "\n",
       "**Test Mechanism**: The test implementation follows defined steps. Initially, it selects non-numeric columns from the dataset and changes them to string type, paving the way for accurate binning. It then performs an automated WoE binning operation on these selected features, effectively categorizing the potential values of a variable into distinct bins. After the binning process, the function generates two separate visualizations (a scatter chart for WoE values and a bar chart for IV) for each variable. These visual presentations are formed according to the spread of each metric across various categories of each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Errors occurring during the binning process.\n",
       "- Challenges in converting non-numeric columns into string data type.\n",
       "- Misbalance in the distribution of WoE and IV, with certain bins overtaking others conspicuously. This could denote that the model is disproportionately dependent on certain variables or categories for predictions, an indication of potential risks to its robustness and generalizability.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a detailed visual representation of the relationship between feature categories and the target variable. This grants an intuitive understanding of each feature's contribution to the model.\n",
       "- Allows for easy identification of features with high impact, facilitating feature selection and enhancing comprehension of the model's decision logic.\n",
       "- WoE conversions are monotonic, upholding the rank ordering of the original data points, which simplifies analysis.\n",
       "\n",
       "**Limitations**:\n",
       "- The method is largely reliant on the binning process, and an inappropriate binning threshold or bin number choice might result in a misrepresentation of the variable's distribution.\n",
       "- While excellent for categorical data, the encoding of continuous variables into categorical can sometimes lead to information loss.\n",
       "- Extreme or outlier values can dramatically affect the computation of WoE and IV, skewing results.\n",
       "- The method requires a sufficient number of events per bin to generate a reliable information value and weight of evidence.</td>\n",
       "      <td id=\"T_fccf5_row34_col3\" class=\"data row34 col3\" >validmind.data_validation.WOEBinPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row35_col0\" class=\"data row35 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row35_col1\" class=\"data row35 col1\" >Mentions</td>\n",
       "      <td id=\"T_fccf5_row35_col2\" class=\"data row35 col2\" >**Purpose**: This test, termed \"Mentions\", is designed to gauge the quality of data in a Natural Language Processing (NLP) or text-focused Machine Learning model. The primary objective is to identify and calculate the frequency of 'mentions' within a chosen text column of a dataset. A 'mention' in this context refers to individual text elements that are prefixed by '@'. The output of this test reveals the most frequently mentioned entities or usernames, which can be integral for applications such as social media analyses, customer sentiment analyses, and so on.\n",
       "\n",
       "**Test Mechanism**: The test first verifies the existence of a text column in the provided dataset. It then employs a regular expression pattern to extract mentions from the text. Subsequently, the frequency of each unique mention is calculated. The test selects the most frequent mentions based on default or user-defined parameters, the default being the top 25, for representation. This process of thresholding forms the core of the test. A treemap plot visualizes the test results, where the size of each rectangle corresponds to the frequency of a particular mention.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The lack of a valid text column in the dataset, which would result in the failure of the test execution.\n",
       "- The absence of any mentions within the text data, indicating that there might not be any text associated with '@'. This situation could point towards sparse or poor-quality data, thereby hampering the model's generalization or learning capabilities.\n",
       "\n",
       "**Strengths**:\n",
       "- The test is specifically optimized for text-based datasets which gives it distinct power in the context of NLP.\n",
       "- It enables quick identification and visually appealing representation of the predominant elements or mentions.\n",
       "- It can provide crucial insights about the most frequently mentioned entities or usernames.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only recognizes mentions that are prefixed by '@', hence useful textual aspects not preceded by '@' might be ignored.\n",
       "- This test isn't suited for datasets devoid of textual data.\n",
       "- It does not provide insights on less frequently occurring data or outliers, which means potentially significant patterns could be overlooked.</td>\n",
       "      <td id=\"T_fccf5_row35_col3\" class=\"data row35 col3\" >validmind.data_validation.nlp.Mentions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row36_col0\" class=\"data row36 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row36_col1\" class=\"data row36 col1\" >GINI Table</td>\n",
       "      <td id=\"T_fccf5_row36_col2\" class=\"data row36 col2\" >**Purpose**: The 'GINITable' metric is designed to evaluate the performance of a classification model by emphasizing its discriminatory power. Specifically, it calculates and presents three important metrics\n",
       "- the Area under the ROC Curve (AUC), the GINI coefficient, and the Kolmogov-Smirnov (KS) statistic\n",
       "- for both training and test datasets.\n",
       "\n",
       "**Test Mechanism**: Using a dictionary for storing performance metrics for both the training and test datasets, the 'GINITable' metric calculates each of these metrics sequentially. The Area under the ROC Curve (AUC) is calculated via the `roc_auc_score` function from the Scikit-Learn library. The GINI coefficient, a measure of statistical dispersion, is then computed by doubling the AUC and subtracting 1. Finally, the Kolmogov-Smirnov (KS) statistic is calculated via the `roc_curve` function from Scikit-Learn, with the False Positive Rate (FPR) subtracted from the True Positive Rate (TPR) and the maximum value taken from the resulting data. These metrics are then stored in a pandas DataFrame for convenient visualization.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low values for performance metrics may suggest a reduction in model performance, particularly a low AUC which indicates poor classification performance, or a low GINI coefficient, which could suggest a decreased ability to discriminate different classes.\n",
       "- A high KS value may be an indicator of potential overfitting, as this generally signifies a substantial divergence between positive and negative distributions.\n",
       "- Significant discrepancies between the performance on the training dataset and the test dataset may present another signal of high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- Offers three key performance metrics (AUC, GINI, and KS) in one test, providing a more comprehensive evaluation of the model.\n",
       "- Provides a direct comparison between the model's performance on training and testing datasets, which aids in identifying potential underfitting or overfitting.\n",
       "- The applied metrics are class-distribution invariant, thereby remaining effective for evaluating model performance even when dealing with imbalanced datasets.\n",
       "- Presents the metrics in a user-friendly table format for easy comprehension and analysis.\n",
       "\n",
       "**Limitations**:\n",
       "- The GINI coefficient and KS statistic are both dependent on the AUC value. Therefore, any errors in the calculation of the latter will adversely impact the former metrics too.\n",
       "- Mainly suited for binary classification models and may require modifications for effective application in multi-class scenarios.\n",
       "- The metrics used are threshold-dependent and may exhibit high variability based on the chosen cut-off points.\n",
       "- The test does not incorporate a method to efficiently handle missing or inefficiently processed data, which could lead to inaccuracies in the metrics if the data is not appropriately preprocessed.</td>\n",
       "      <td id=\"T_fccf5_row36_col3\" class=\"data row36 col3\" >validmind.model_validation.statsmodels.GINITable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row37_col0\" class=\"data row37 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row37_col1\" class=\"data row37 col1\" >Default Ratesby Risk Band Plot</td>\n",
       "      <td id=\"T_fccf5_row37_col2\" class=\"data row37 col2\" >**Purpose**: The Default Rates by Risk Band Plot metric aims to quantify and visually represent default rates across varying risk bands within a specific dataset. This information is essential in evaluating the functionality of credit risk models, by providing a comprehensive view of default rates across a range of risk categories.\n",
       "\n",
       "**Test Mechanism**: The applied test approach involves a calculated bar plot. This plot is derived by initially determining the count of accounts in every risk band and then converting these count values into percentages by dividing by the total quantity of accounts. The percentages are then depicted as a bar plot, clearly showcasing the proportion of total accounts associated with each risk band. Hence, the plot delivers a summarized depiction of default risk across various bands. The 'Dark24' color sequence is used in the plot to ensure each risk band is easily distinguishable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk may be indicated by a significantly large percentage of accounts associated with high-risk bands.\n",
       "- High exposure to potential default risk in the dataset indicates potential weaknesses in the model's capability to effectively manage or predict credit risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric's primary strengths lie in its simplicity and visual impact.\n",
       "- The graphical display of default rates allows for a clear understanding of the spread of default risk across risk bands.\n",
       "- Using a bar chart simplifies the comparison between various risk bands and can highlight potential spots of high risk.\n",
       "- This approach assists in identifying any numerical imbalances or anomalies, thus facilitating the task of evaluating and contrasting performance across various credit risk models.\n",
       "\n",
       "**Limitations**:\n",
       "- The key constraint of this metric is that it cannot provide any insights as to why certain risk bands might have higher default rates than others.\n",
       "- If there is a large imbalance in the number of accounts across risk bands, the visual representation might not accurately depict the true distribution of risk.\n",
       "- Other factors contributing to credit risk beyond the risk bands are not considered.\n",
       "- The metric's reliance on a visual format might potentially lead to misinterpretation of results, as graphical depictions can sometimes be misleading.</td>\n",
       "      <td id=\"T_fccf5_row37_col3\" class=\"data row37 col3\" >validmind.data_validation.DefaultRatesbyRiskBandPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row38_col0\" class=\"data row38 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row38_col1\" class=\"data row38 col1\" >Log Regression Confusion Matrix</td>\n",
       "      <td id=\"T_fccf5_row38_col2\" class=\"data row38 col2\" >**Purpose**: The Logistic Regression Confusion Matrix is a metric used to measure the performance of a logistic regression classification model. This metric is particularly useful for scenarios where a model's predictions are formulated by thresholding probabilities. The main advantage of this approach is that it includes true positives, true negatives, false positives, and false negatives in its assessment, providing a more comprehensive overview of the model's effectiveness in distinguishing between correct and incorrect classifications.\n",
       "\n",
       "**Test Mechanism**: The methodology behind the Logistic Regression Confusion Matrix uses the `sklearn.metrics.confusion_matrix` function from the Python library to generate a matrix. This matrix is created by comparing the model's predicted probabilities, which are initially converted to binary predictions using a predetermined cut-off threshold (default is 0.5), against the actual classes. The matrix's design consists of the predicted class labels forming the x-axis, and the actual class labels forming the y-axis, with each cell containing the record of true positives, true negatives, false positives, and false negatives respectively.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant number of false positives and false negatives, indicating that the model is incorrectly classifying instances.\n",
       "- The counts of true positives and true negatives being substantially lower than projected, positioning this as a potential high-risk indicator.\n",
       "\n",
       "**Strengths**:\n",
       "- Simple, intuitive, and provides a comprehensive understanding of the model's performance.\n",
       "- Provides a detailed breakdown of error types, improving transparency.\n",
       "- Offers flexible adaptation for diverse prediction scenarios by allowing adjustments to the cut-off threshold, and enabling exploration of trade-offs between precision (minimizing false positives) and recall (minimizing false negatives).\n",
       "\n",
       "**Limitations**:\n",
       "- Acceptable performance on majority classes but potential poor performance on minority classes in imbalanced datasets, as the confusion matrix may supply misleading results.\n",
       "- Lack of insight into the severity of the mistakes and the cost trade-off between different types of misclassification.\n",
       "- Selection of the cut-off threshold can significantly alter the interpretation, and a poorly chosen threshold may lead to erroneous conclusions.</td>\n",
       "      <td id=\"T_fccf5_row38_col3\" class=\"data row38 col3\" >validmind.model_validation.statsmodels.LogRegressionConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row39_col0\" class=\"data row39 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row39_col1\" class=\"data row39 col1\" >Confusion Matrix</td>\n",
       "      <td id=\"T_fccf5_row39_col2\" class=\"data row39 col2\" >**Purpose**: The Confusion Matrix tester is designed to assess the performance of a classification Machine Learning model. This performance is evaluated based on how well the model is able to correctly classify True Positives, True Negatives, False Positives, and False Negatives\n",
       "- fundamental aspects of model accuracy.\n",
       "\n",
       "**Test Mechanism**: The mechanism used involves taking the predicted results (`y_test_predict`) from the classification model and comparing them against the actual values (`y_test_true`). A confusion matrix is built using the unique labels extracted from `y_test_true`, employing scikit-learn's metrics. The matrix is then visually rendered with the help of Plotly's `create_annotated_heatmap` function. A heatmap is created which provides a two-dimensional graphical representation of the model's performance, showcasing distributions of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
       "\n",
       "**Signs of High Risk**: Indicators of high risk related to the model include:\n",
       "- High numbers of False Positives (FP) and False Negatives (FN), depicting that the model is not effectively classifying the values.\n",
       "- Low numbers of True Positives (TP) and True Negatives (TN), implying that the model is struggling with correctly identifying class labels.\n",
       "\n",
       "**Strengths**: The Confusion Matrix tester brings numerous strengths:\n",
       "- It provides a simplified yet comprehensive visual snapshot of the classification model's predictive performance.\n",
       "- It distinctly brings out True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), thus, making it easier to focus on potential areas of improvement.\n",
       "- The matrix is beneficial in dealing with multi-class classification problems as it can provide a simple view of complex model performances.\n",
       "- It aids in understanding the different types of errors that the model could potentially make, as it provides in-depth insights into Type-I and Type-II errors.\n",
       "\n",
       "**Limitations**: Despite its various strengths, the Confusion Matrix tester does exhibit some limitations:\n",
       "- In cases of unbalanced classes, the effectiveness of the confusion matrix might be lessened. It may wrongly interpret the accuracy of a model that is essentially just predicting the majority class.\n",
       "- It does not provide a single unified statistic that could evaluate the overall performance of the model. Different aspects of the model's performance are evaluated separately instead.\n",
       "- It mainly serves as a descriptive tool and does not offer the capability for statistical hypothesis testing.\n",
       "- Risks of misinterpretation exist because the matrix doesn't directly provide precision, recall, or F1-score data. These metrics have to be computed separately.</td>\n",
       "      <td id=\"T_fccf5_row39_col3\" class=\"data row39 col3\" >validmind.model_validation.sklearn.ConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row40_col0\" class=\"data row40 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row40_col1\" class=\"data row40 col1\" >Classifier Out Of Sample Performance</td>\n",
       "      <td id=\"T_fccf5_row40_col2\" class=\"data row40 col2\" >**Purpose**: This test is designed to assess the performance of a Machine Learning model on out-of-sample data, specifically data not utilized during the training phase. The performance metrics used in the test (accuracy, precision, recall, and F1 score) serve to measure the model's generalization capability towards unseen data. The primary goal is to ensure that the model has not overfitted to the training data and retains the ability to make accurate predictions on novel data.\n",
       "\n",
       "**Test Mechanism**: The mechanism for this test involves applying the performance metrics to the predictions made by the model on the test dataset. These are then compared with the actual outcomes. It is assumed that the test dataset remains unutilized during the model training phase, therefore providing an unbiased and fair evaluation of the model's generalization capabilities. The various metrics used include:\n",
       "- Accuracy: The ratio of correct predictions\n",
       "- Precision: The ratio of correct positive predictions\n",
       "- Recall: The ratio of actual positives that were correctly predicted\n",
       "- F1 Score: Harmonic mean of precision and recall, effectively balancing both.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low accuracy rate.\n",
       "- Low precision and recall rates.\n",
       "- Low F1 score.\n",
       "- Significant discrepancies between the model's performance on training data and testing data, indicating overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- The test provides a realistic assessment of a model's predictive performance on unseen data, thereby estimating its generalizability.\n",
       "- It incorporates several performance metrics into the evaluation, offering a comprehensive look at performance.\n",
       "- The test aids in the detection of overfitting, a crucial factor for all machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- The effectiveness of this test is significantly dependent on the quality and the representativeness of the test dataset. Performance metrics may not accurately reflect the true performance of the model if the test database is not a good representative of the real-world data the model will be working on.\n",
       "- The metrics used (accuracy, precision, recall and F1 score) make the assumption that all errors and misclassifications bear equal importance. This, however, may not align with certain real-world scenarios where some types of errors might have more significant implications than others.</td>\n",
       "      <td id=\"T_fccf5_row40_col3\" class=\"data row40 col3\" >validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row41_col0\" class=\"data row41 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row41_col1\" class=\"data row41 col1\" >Heatmap Feature Correlations</td>\n",
       "      <td id=\"T_fccf5_row41_col2\" class=\"data row41 col2\" >**Purpose:** The HeatmapFeatureCorrelations metric is utilized to evaluate the degree of interrelationships between pairs of input features within a dataset. This metric allows us to visually comprehend the correlation patterns through a heatmap, which can be essential in understanding which features may contribute most significantly to the performance of the model. Features that have high intercorrelation can potentially reduce the model's ability to learn, thus impacting the overall performance and stability of the machine learning model.\n",
       "\n",
       "**Test Mechanism:** The metric executes the correlation test by computing the Pearson correlations for all pairs of numerical features. It then generates a heatmap plot using seaborn, a Python data visualization library. The colormap ranges from -1 to 1, indicating perfect negative correlation and perfect positive correlation respectively. A 'declutter' option is provided which, if set to true, removes variable names and numerical correlations from the plot to provide a more streamlined view. The size of feature names and correlation coefficients can be controlled through 'fontsize' parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Indicators of potential risk include features with high absolute correlation values.\n",
       "- A significant degree of multicollinearity might lead to instabilities in the trained model and can also result in overfitting.\n",
       "- The presence of multiple homogeneous blocks of high positive or negative correlation within the plot might indicate redundant or irrelevant features included within the dataset.\n",
       "\n",
       "**Strengths:**\n",
       "- The strength of this metric lies in its ability to visually represent the extent and direction of correlation between any two numeric features, which aids in the interpretation and understanding of complex data relationships.\n",
       "- The heatmap provides an immediate and intuitively understandable representation, hence, it is extremely useful for high-dimensional datasets where extracting meaningful relationships might be challenging.\n",
       "\n",
       "**Limitations:**\n",
       "- The central limitation might be that it can only calculate correlation between numeric features, making it unsuitable for categorical variables unless they are already numerically encoded in a meaningful manner.\n",
       "- It uses Pearson's correlation, which only measures linear relationships between features. It may perform poorly in cases where the relationship is non-linear.\n",
       "- Large feature sets might result in cluttered and difficult-to-read correlation heatmaps, especially when the 'declutter' option is set to false.</td>\n",
       "      <td id=\"T_fccf5_row41_col3\" class=\"data row41 col3\" >validmind.data_validation.HeatmapFeatureCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row42_col0\" class=\"data row42 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row42_col1\" class=\"data row42 col1\" >Classifier In Sample Performance</td>\n",
       "      <td id=\"T_fccf5_row42_col2\" class=\"data row42 col2\" >**Purpose**: The purpose of this metric is to evaluate the performance of the machine learning model on the training data. This test gauges the model's ability to generalize its predictions to new, unseen data and assesses the level of the model's overfitting on the training set by measuring commonly-used metrics such as accuracy, precision, recall, and F1 score.\n",
       "\n",
       "**Test Mechanism**: The implementation of this test incorporates various metrics including accuracy, precision, recall, and F1 score. These metrics are applied on the model's predictions of the training set and compared with the true output. The accuracy evaluates the proportion of correct predictions out of the total predictions. Meanwhile, precision measures the accurate positive predictions relative to the total number of positive predictions. The recall metric indicates the proportion of true positive predictions in relation to the overall number of actual positives in the dataset. Lastly, the F1 score represents the harmonic mean of precision and recall, thus providing a comprehensive appraisal of the model's performance.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A near-perfect performance on all metrics on the training data, coupled with inferior performance on unseen data, may be indicative of overfitting. This constitutes a high-risk scenario.\n",
       "- Low values on any of these metrics may signal an underperforming model, posing a potential risk for production-grade applications.\n",
       "\n",
       "**Strengths**:\n",
       "- Using conventional metrics such as accuracy, precision, recall, and F1 score allows for an all-encompassing evaluation of the model's performance.\n",
       "- The results are interpretable due to the widespread use and understanding of these metrics in the machine learning field.\n",
       "- Being applied to the training set, this test can detect overfitting early in the model's development stage.\n",
       "\n",
       "**Limitations**:\n",
       "- Although these metrics yield valuable insights, they are susceptible to biases inherent in the training data.\n",
       "- There's always a chance for disparity between the model's performance in the training set and performance with new, unseen data.\n",
       "- Therefore, this test should be supplemented with additional validation tactics, such as k-fold cross-validation or out-of-sample testing, to provide a more unbiased evaluation of the model's performance.</td>\n",
       "      <td id=\"T_fccf5_row42_col3\" class=\"data row42 col3\" >validmind.model_validation.sklearn.ClassifierInSamplePerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row43_col0\" class=\"data row43 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row43_col1\" class=\"data row43 col1\" >Conciseness</td>\n",
       "      <td id=\"T_fccf5_row43_col2\" class=\"data row43 col2\" >**Purpose:** The Conciseness Assessment is designed to evaluate the brevity and succinctness of prompts provided to a Language Learning Model (LLM). A concise prompt strikes a balance between offering clear instructions and eliminating redundant or unnecessary information, ensuring that the LLM receives relevant input without being overwhelmed.\n",
       "\n",
       "**Test Mechanism:** Using an LLM, this test conducts a conciseness analysis on input prompts. The analysis grades the prompt on a scale from 1 to 10, where the grade reflects how well the prompt delivers clear instructions without being verbose. Prompts that score equal to or above a predefined threshold (default set to 7) are deemed successfully concise. This threshold can be adjusted to meet specific requirements.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Prompts that consistently score below the predefined threshold.\n",
       "- Prompts that are overly wordy or contain unnecessary information.\n",
       "- Prompts that create confusion or ambiguity due to excess or unnecessary information.\n",
       "\n",
       "**Strengths:**\n",
       "- Ensures clarity and effectiveness of the prompts.\n",
       "- Promotes brevity and preciseness in prompts without sacrificing essential information.\n",
       "- Useful for models like LLMs, where input prompt length and clarity greatly influence model performance.\n",
       "- Provides a quantifiable measure of prompt conciseness.\n",
       "\n",
       "**Limitations:**\n",
       "- The conciseness score is based on an AI's assessment, which might not fully capture human interpretation of conciseness.\n",
       "- The predefined threshold for conciseness could be subjective and might need adjustment based on application.\n",
       "- The test is dependent on the LLM’s understanding of conciseness, which might vary from model to model.</td>\n",
       "      <td id=\"T_fccf5_row43_col3\" class=\"data row43 col3\" >validmind.prompt_validation.Conciseness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row44_col0\" class=\"data row44 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row44_col1\" class=\"data row44 col1\" >Population Stability Index</td>\n",
       "      <td id=\"T_fccf5_row44_col2\" class=\"data row44 col2\" >**Purpose:** The Population Stability Index (PSI) serves as a quantitative assessment for evaluating the stability of a machine learning model's output distributions when comparing two different datasets. Typically, these would be a development and a validation dataset or two datasets collected at different periods. The PSI provides a measurable indication of any significant shift in the model's performance over time or noticeable changes in the characteristics of the population the model is making predictions for.\n",
       "\n",
       "**Test Mechanism:** The implementation of the PSI in this script involves calculating the PSI for each feature between the training and test datasets. Data from both datasets is sorted and placed into either a predetermined number of bins or quantiles. The boundaries for these bins are initially determined based on the distribution of the training data. The contents of each bin are calculated and their respective proportions determined. Subsequently, the PSI is derived for each bin through a logarithmic transformation of the ratio of the proportions of data for each feature in the training and test datasets. The PSI, along with the proportions of data in each bin for both datasets, are displayed in a summary table, a grouped bar chart, and a scatter plot.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- A high PSI value is a clear indicator of high risk. Such a value suggests a significant shift in the model predictions or severe changes in the characteristics of the underlying population.\n",
       "- This ultimately suggests that the model may not be performing as well as expected and that it may be less reliable for making future predictions.\n",
       "\n",
       "**Strengths:**\n",
       "- The PSI provides a quantitative measure of the stability of a model over time or across different samples, making it an invaluable tool for evaluating changes in a model's performance.\n",
       "- It allows for direct comparisons across different features based on the PSI value.\n",
       "- The calculation and interpretation of the PSI are straightforward, facilitating its use in model risk management.\n",
       "- The use of visual aids such as tables and charts further simplifies the comprehension and interpretation of the PSI.\n",
       "\n",
       "**Limitations:**\n",
       "- The PSI test does not account for the interdependence between features: features that are dependent on one another may show similar shifts in their distributions, which in turn may result in similar PSI values.\n",
       "- The PSI test does not inherently provide insights into why there are differences in distributions or why the PSI values may have changed.\n",
       "- The test may not handle features with significant outliers adequately.\n",
       "- Additionally, the PSI test is performed on model predictions, not on the underlying data distributions which can lead to misinterpretations. Any changes in PSI could be due to shifts in the model (model drift), changes in the relationships between features and the target variable (concept drift), or both. However, distinguishing between these causes is non-trivial.</td>\n",
       "      <td id=\"T_fccf5_row44_col3\" class=\"data row44 col3\" >validmind.model_validation.sklearn.PopulationStabilityIndex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row45_col0\" class=\"data row45 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row45_col1\" class=\"data row45 col1\" >Scorecard Histogram</td>\n",
       "      <td id=\"T_fccf5_row45_col2\" class=\"data row45 col2\" >**Purpose**: The Scorecard Histogram test metric provides a visual interpretation of the credit scores generated by a machine learning model for credit-risk classification tasks. It aims to compare the alignment of the model's scoring decisions with the actual outcomes of credit loan applications. It helps in identifying potential discrepancies between the model's predictions and real-world risk levels.\n",
       "\n",
       "**Test Mechanism**: This metric uses logistic regression to generate a histogram of credit scores for both default (negative class) and non-default (positive class) instances. Using both training and test datasets, the metric calculates the credit score of each instance with a scorecard method, considering the impact of different features on the likelihood of default. İncludes the default point to odds (PDO) scaling factor and predefined target score and odds settings. Histograms for training and test sets are computed and plotted separately to offer insights into the model's generalizability to unseen data.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Discrepancies between the distributions of training and testing data, indicating a model's poor generalisation ability\n",
       "- Skewed distributions favouring specific scores or classes, representing potential bias\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a visual interpretation of the model's credit scoring system, enhancing comprehension of model behavior\n",
       "- Enables a direct comparison between actual and predicted scores for both training and testing data\n",
       "- Its intuitive visualization helps understand the model's ability to differentiate between positive and negative classes\n",
       "- Can unveil patterns or anomalies not easily discerned through numerical metrics alone\n",
       "\n",
       "**Limitations**:\n",
       "- Despite its value for visual interpretation, it doesn't quantify the performance of the model, and therefore may lack precision for thorough model evaluation\n",
       "- The quality of input data can strongly influence the metric, as bias or noise in the data will affect both the score calculation and resultant histogram\n",
       "- Its specificity to credit scoring models limits its applicability across a wider variety of machine learning tasks and models\n",
       "- The metric's effectiveness is somewhat tied to the subjective interpretation of the analyst, since it relies on the analyst's judgment of the characteristics and implications of the plot.</td>\n",
       "      <td id=\"T_fccf5_row45_col3\" class=\"data row45 col3\" >validmind.model_validation.statsmodels.ScorecardHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row46_col0\" class=\"data row46 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row46_col1\" class=\"data row46 col1\" >Scorecard Probabilities Histogram</td>\n",
       "      <td id=\"T_fccf5_row46_col2\" class=\"data row46 col2\" >**Purpose**: The Scorecard Probabilities Histogram, a specific metric used within the credit risk domain, is designed to evaluate and visualize risk classification of a model. It aims at examining the distribution of the probability of default across varied score buckets, with the score buckets being categories that entities (e.g., loan applicants) are classed under based on their predicted default risks. The key idea is to ensure that the model accurately classifies entities into appropriate risk categories (score buckets) and aptly represents their default probabilities.\n",
       "\n",
       "**Test Mechanism**: The mechanism behind the Scorecard Probabilities Histogram includes several steps. It starts with the calculation of default probabilities by the 'compute_probabilities' method, where the resulting probability is added as a fresh column to the input dataset. Following that, scores are computed using these probabilities, a target score, target odds, and a Points to Double the odds (pdo) factor by the 'compute_scores' method. These scores are then bucketed via the 'compute_buckets' method. A histogram is then plotted for each score bucket, with default probabilities as the x-axis and their frequency as the y-axis\n",
       "- implemented within the 'plot_probabilities_histogram' method. This entire process is executed distinctly for both training and testing datasets.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant overlap of different score buckets in the histogram indicates that the model is not efficiently distinguishing between various risk categories.\n",
       "- If very high or low probabilities are commonplace across all buckets, the model's predictions could be skewed.\n",
       "\n",
       "**Strengths**:\n",
       "- The Scorecard Probabilities Histogram allows for the visualization and analysis of the predicted default risk distribution across different risk classes, thereby facilitating a visual inspection of the model's performance and calibration for various risk categories.\n",
       "- It provides a means to visualize how these classifications are distributed on the training and testing datasets separately, contributing to a better comprehension of model generalization.\n",
       "\n",
       "**Limitations**:\n",
       "- The Scorecard Probabilities Histogram assumes linear and equally spaced risk categories, which might not always hold true.\n",
       "- If there are too few or too many score buckets, the visualization may not convey sufficient information.\n",
       "- While it effectively illustrates the distribution of probabilities, it does not provide adequate numerical metrics or threshold to definitively evaluate the model's performance. A more accurate evaluation necessitates its usage in conjunction with other metrics and tools including the confusion matrix, AUC-ROC, Precision, Recall, and so forth.</td>\n",
       "      <td id=\"T_fccf5_row46_col3\" class=\"data row46 col3\" >validmind.model_validation.statsmodels.ScorecardProbabilitiesHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row47_col0\" class=\"data row47 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row47_col1\" class=\"data row47 col1\" >Pi TPD Histogram</td>\n",
       "      <td id=\"T_fccf5_row47_col2\" class=\"data row47 col2\" >**Purpose**: The PiTPDHistogram metric uses Probability of Default (PD) calculations for individual instances within both training and test data sets in order to assess a model's proficiency in predicting credit risk. A distinctive point in time (PiT) is chosen for these PD calculations, and the results for both actual and predicted defaults are presented in histogram form. This visualization is aimed at simplifying the understanding of model prediction accuracy.\n",
       "\n",
       "**Test Mechanism**: Instances are categorized into two groups\n",
       "- those for actual defaults and those for predicted defaults, with '1' indicating a default and '0' indicating non-default. PD is calculated for each instance, and based on these calculations, two histograms are created, one for actual defaults and one for predicted defaults. If the predicted default frequency matches that of the actual defaults, the model's performance is deemed effective.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Discrepancies between the actual and predicted default histograms may suggest model inefficiency.\n",
       "- Variations in histogram shapes or divergences in default probability distributions could be concerning.\n",
       "- Significant mismatches in peak default probabilities could also be red flags.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a visual comparison between actual and predicted defaults, aiding in the understanding of model performance.\n",
       "- Helps reveal model bias and areas where the model's performance could be improved.\n",
       "- Easier to understand than purely numerical evaluations or other complicated visualization measures.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric remains largely interpretive and subjective, as the extent and relevance of visual discrepancies often need to be evaluated manually, leading to potentially inconsistent results across different analyses.\n",
       "- This metric alone may not capture all the complexities and nuances of model performance.\n",
       "- The information provided is limited to a specific point in time, potentially neglecting the model's performance under various circumstances or different time periods.</td>\n",
       "      <td id=\"T_fccf5_row47_col3\" class=\"data row47 col3\" >validmind.data_validation.PiTPDHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row48_col0\" class=\"data row48 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row48_col1\" class=\"data row48 col1\" >Logistic Reg Prediction Histogram</td>\n",
       "      <td id=\"T_fccf5_row48_col2\" class=\"data row48 col2\" >**Purpose**: This code is designed to generate histograms that display the Probability of Default (PD) predictions for positive and negative classes in both the training and testing datasets. By doing so, it evaluates the performance of a logistic regression model, particularly in the context of credit risk prediction.\n",
       "\n",
       "**Test Mechanism**: The metric executes these steps to run the test:\n",
       "- Firstly, it extracts the target column from both the train and test datasets.\n",
       "- The model's predict function is then used to calculate probabilities.\n",
       "- These probabilities are added as a new column to the training and testing dataframes.\n",
       "- Histograms are generated for each class (0 or 1 in binary classification scenarios) within the training and testing datasets.\n",
       "- To enhance visualization, the histograms are set to have different opacities.\n",
       "- The four histograms (two for training data and two for testing) are overlaid on two different subplot frames (one for training and one for testing data).\n",
       "- The test returns a plotly graph object displaying the visualization.\n",
       "\n",
       "**Signs of High Risk**: Several indicators could suggest a high risk or failure in the model's performance. These include:\n",
       "- Significant discrepancies observed between the histograms of training and testing data.\n",
       "- Large disparities between the histograms for the positive and negative classes.\n",
       "- These issues could signal potential overfitting or bias in the model.\n",
       "- Unevenly distributed probabilities may also indicate that the model does not accurately predict outcomes.\n",
       "\n",
       "**Strengths**: This metric and test offer several benefits, including:\n",
       "- The visual representation of the PD predictions made by the model, which aids in understanding the model's behaviour.\n",
       "- The ability to assess both the training and testing datasets, adding depth to the validation of the model.\n",
       "- Highlighting disparities between multiple classes, providing potential insights into class imbalance or data skewness issues.\n",
       "- Particularly beneficial for credit risk prediction, it effectively visualizes the spread of risk across different classes.\n",
       "\n",
       "**Limitations**: Despite its strengths, the test has several limitations:\n",
       "- It is specifically tailored for binary classification scenarios, where the target variable only has two classes; as such, it isn't suited for multi-class classification tasks.\n",
       "- This metric is mainly applicable for logistic regression models. It might not be effective or accurate when used on other model types.\n",
       "- While the test provides a robust visual representation of the model's PD predictions, it does not provide a quantifiable measure or score to assess model performance.</td>\n",
       "      <td id=\"T_fccf5_row48_col3\" class=\"data row48 col3\" >validmind.model_validation.statsmodels.LogisticRegPredictionHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row49_col0\" class=\"data row49 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row49_col1\" class=\"data row49 col1\" >Unique Rows</td>\n",
       "      <td id=\"T_fccf5_row49_col2\" class=\"data row49 col2\" >**Purpose**: The UniqueRows test is designed to gauge the quality of the data supplied to the machine learning model by verifying that the count of distinct rows in the dataset exceeds a specific threshold, thereby ensuring a varied collection of data. Diversity in data is essential for training an unbiased and robust model that excels when faced with novel data.\n",
       "\n",
       "**Test Mechanism**: The testing process starts with calculating the total number of rows in the dataset. Subsequently, the count of unique rows is determined for each column in the dataset. If the percentage of unique rows (calculated as the ratio of unique rows to the overall row count) is less than the prescribed minimum percentage threshold given as a function parameter, the test is passed. The results are cached and a final pass or fail verdict is given based on whether all columns have successfully passed the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A lack of diversity in data columns, demonstrated by a count of unique rows that falls short of the preset minimum percentage threshold, is indicative of high risk.\n",
       "- This lack of variety in the data signals potential issues with data quality, possibly leading to overfitting in the model and issues with generalization, thus posing a significant risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The UniqueRows test is efficient in evaluating the data's diversity across each information column in the dataset.\n",
       "- This test provides a quick, systematic method to assess data quality based on uniqueness, which can be pivotal in developing effective and unbiased machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- A limitation of the UniqueRows test is its assumption that the data's quality is directly proportionate to its uniqueness, which may not always hold true. There might be contexts where certain non-unique rows are essential and should not be overlooked.\n",
       "- The test does not consider the relative 'importance' of each column in predicting the output, treating all columns equally.\n",
       "- This test may not be suitable or useful for categorical variables, where the count of unique categories is inherently limited.</td>\n",
       "      <td id=\"T_fccf5_row49_col3\" class=\"data row49 col3\" >validmind.data_validation.UniqueRows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row50_col0\" class=\"data row50 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row50_col1\" class=\"data row50 col1\" >IQR Outliers Bar Plot</td>\n",
       "      <td id=\"T_fccf5_row50_col2\" class=\"data row50 col2\" >**Purpose**: The InterQuartile Range Outliers Bar Plot (IQROutliersBarPlot) metric aims to visually analyze and evaluate the extent of outliers in numeric variables based on percentiles. Its primary purpose is to clarify the dataset's distribution, flag possible abnormalities in it and gauge potential risks associated with processing potentially skewed data, which can affect the machine learning model's predictive prowess.\n",
       "\n",
       "**Test Mechanism**: The examination invokes a series of steps:\n",
       "\n",
       "1. For every numeric feature in the dataset, the 25th percentile (Q1) and 75th percentile (Q3) are calculated before deriving the Interquartile Range (IQR), the difference between Q1 and Q3. 2. Subsequently, the metric calculates the lower and upper thresholds by subtracting Q1 from the `threshold` times IQR and adding Q3 to `threshold` times IQR, respectively. The default `threshold` is set at 1.5. 3. Any value in the feature that falls below the lower threshold or exceeds the upper threshold is labeled as an outlier. 4. The number of outliers are tallied for different percentiles, such as [0-25], [25-50], [50-75], and [75-100]. 5. These counts are employed to construct a bar plot for the feature, showcasing the distribution of outliers across different percentiles.\n",
       "\n",
       "**Signs of High Risk**: High risk or a potential lapse in the model's performance could be unveiled by the following signs:\n",
       "\n",
       "- A prevalence of outliers in the data, potentially skewing its distribution.\n",
       "- Outliers dominating higher percentiles (75-100) which implies the presence of extreme values, capable of severely influencing the model's performance.\n",
       "- Certain features harboring most of their values as outliers, which signifies that these features might not contribute positively to the model's forecasting ability.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Effectively identifies outliers in the data through visual means, facilitating easier comprehension and offering insights into the outliers' possible impact on the model.\n",
       "- Provides flexibility by accommodating all numeric features or a chosen subset.\n",
       "- Task-agnostic in nature; it is viable for both classification and regression tasks.\n",
       "- Can handle large datasets as its operation does not hinge on computationally heavy operations.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Its application is limited to numerical variables and does not extend to categorical ones.\n",
       "- Relies on a predefined threshold (default being 1.5) for outlier identification, which may not be suitable for all cases.\n",
       "- Only reveals the presence and distribution of outliers and does not provide insights into how these outliers might affect the model's predictive performance.\n",
       "- The assumption that data is unimodal and symmetric may not always hold true. In cases with non-normal distributions, the results can be misleading.</td>\n",
       "      <td id=\"T_fccf5_row50_col3\" class=\"data row50 col3\" >validmind.data_validation.IQROutliersBarPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row51_col0\" class=\"data row51 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row51_col1\" class=\"data row51 col1\" >Bivariate Histograms</td>\n",
       "      <td id=\"T_fccf5_row51_col2\" class=\"data row51 col2\" >**Purpose**: This metric, dubbed BivariateHistograms, is primarily used for visual data analysis via the inspection of variable distribution, specifically categorical variables. Its main objective is to ascertain any potential correlations between these variables and distributions within each defined target class. This is achieved by offering an intuitive avenue into gaining insights into the characteristics of the data and any plausible patterns therein.\n",
       "\n",
       "**Test Mechanism**: The working mechanism of the BivariateHistograms module revolves around an input dataset and a series of feature pairs. It uses seaborn's histogram plotting function and matplotlib techniques to create bivariate histograms for each feature pair in the dataset. Two histograms, stratified by the target column status, are produced for every pair of features. This enables the telling apart of different target statuses through color differentiation. The module also offers optional functionality for restricting the data by a specific status through the target_filter parameter.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Irregular or unexpected distributions of data across the different categories.\n",
       "- Highly skewed data distributions.\n",
       "- Significant deviations from the perceived 'normal' or anticipated distributions.\n",
       "- Large discrepancies in distribution patterns between various target statuses.\n",
       "\n",
       "**Strengths**:\n",
       "- Owing to its simplicity, the histogram-based approach is easy to implement and interpret which translates to quick insights.\n",
       "- The metrics provides a consolidated view of the distribution of data across different target conditions for each variable pair, thereby assisting in highlighting potential correlations and patterns.\n",
       "- It proves advantageous in spotting anomalies, comprehending interactions among features, and facilitating exploratory data analysis.\n",
       "\n",
       "**Limitations**:\n",
       "- Its simplicity may be a drawback when it comes to spotting intricate or complex patterns in data.\n",
       "- Overplotting might occur when working with larger datasets.\n",
       "- The metric is only applicable to categorical data, and offers limited insights for numerical or continuous variables.\n",
       "- The interpretation of visual results hinges heavily on the expertise of the observer, possibly leading to subjective analysis.</td>\n",
       "      <td id=\"T_fccf5_row51_col3\" class=\"data row51 col3\" >validmind.data_validation.BivariateHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row52_col0\" class=\"data row52 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row52_col1\" class=\"data row52 col1\" >Clarity</td>\n",
       "      <td id=\"T_fccf5_row52_col2\" class=\"data row52 col2\" >**Purpose:** The Clarity evaluation metric is used to assess how clear the prompts of a Language Learning Model (LLM) are. This assessment is particularly important because clear prompts assist the LLM in more accurately interpreting and responding to instructions.\n",
       "\n",
       "**Test Mechanism:** The evaluation uses an LLM to scrutinize the clarity of prompts, factoring in considerations such as the inclusion of relevant details, persona adoption, step-by-step instructions, usage of examples and specification of desired output length. Each prompt is rated on a clarity scale of 1 to 10, and any prompt scoring at or above the preset threshold (default of 7) will be marked as clear. It is important to note that this threshold can be adjusted via test parameters, providing flexibility in the evaluation process.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "\n",
       "- Prompts that consistently score below the clarity threshold\n",
       "- Repeated failure of prompts to adhere to guidelines for clarity. These guidelines could include detail inclusion, persona adoption, explicit step-by-step instructions, use of examples, and specification of output length.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "- Encourages the development of more effective prompts that aid the LLM in interpreting instructions accurately.\n",
       "- Applies a quantifiable measure (a score from 1 to 10) to evaluate the clarity of prompts.\n",
       "- Threshold for clarity is adjustable, allowing for flexible evaluation depending on the context.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "- Scoring system is subjective and relies on the AI’s interpretation of 'clarity'.\n",
       "- The test assumes that all required factors (detail inclusion, persona adoption, step-by-step instructions, use of examples, and specification of output length) contribute equally to clarity, which might not always be the case.\n",
       "- The evaluation may not be as effective if used on non-textual models.</td>\n",
       "      <td id=\"T_fccf5_row52_col3\" class=\"data row52 col3\" >validmind.prompt_validation.Clarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row53_col0\" class=\"data row53 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row53_col1\" class=\"data row53 col1\" >Robustness</td>\n",
       "      <td id=\"T_fccf5_row53_col2\" class=\"data row53 col2\" >**Purpose:** The Robustness test is meant to evaluate the resilience and reliability of prompts provided to a Language Learning Model (LLM). The aim of this test is to guarantee that the prompts consistently generate accurate and the expected outputs, despite being in diverse or challenging scenarios.\n",
       "\n",
       "**Test Mechanism:** The Robustness test appraises prompts under various conditions, alterations, and contexts to ascertain their stability in producing consistent responses from the LLM. Factors evaluated range from different phrasings, inclusion of potential distracting elements, and various input complexities. By default, the test generates 10 inputs for a prompt but can be adjusted according to test parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- If the output from the tests diverges extensively from the expected results, this indicates high risk.\n",
       "- When the prompt doesn't give a consistent performance across various tests.\n",
       "- A high risk is indicated when the prompt is susceptible to breaking, especially when the output is expected to be of a specific type.\n",
       "\n",
       "**Strengths:**\n",
       "- The robustness test helps to ensure stable performance of the LLM prompts and lowers the chances of generating unexpected or off-target outputs.\n",
       "- This test is vital for applications where predictability and reliability of the LLM’s output are crucial.\n",
       "\n",
       "**Limitations:**\n",
       "- Currently, the test only supports single-variable prompts, which restricts its application to more complex models.\n",
       "- When there are too many target classes (over 10), the test is skipped, which can leave potential vulnerabilities unchecked in complex multi-class models.\n",
       "- The test may not account for all potential conditions or alterations that could show up in practical use scenarios.</td>\n",
       "      <td id=\"T_fccf5_row53_col3\" class=\"data row53 col3\" >validmind.prompt_validation.Robustness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row54_col0\" class=\"data row54 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row54_col1\" class=\"data row54 col1\" >Isolation Forest Outliers</td>\n",
       "      <td id=\"T_fccf5_row54_col2\" class=\"data row54 col2\" >**Purpose**: The `IsolationForestOutliers` test is designed to identify anomalies or outliers in the model's dataset using the isolation forest algorithm. This algorithm assumes that anomalous data points can be isolated more quickly due to their distinctive properties. By creating isolation trees and identifying instances with shorter average path lengths, the test is able to pick out data points that differ from the majority.\n",
       "\n",
       "**Test Mechanism**: The test uses the isolation forest algorithm, which builds an ensemble of isolation trees by randomly selecting features and splitting the data based on random thresholds. It isolates anomalies rather than focusing on normal data points. For each pair of variables, a scatter plot is generated which distinguishes the identified outliers from the inliers. The results of the test can be visualized using these scatter plots, illustrating the distinction between outliers and inliers.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The presence of high contamination, indicating a large number of anomalies\n",
       "- Inability to detect clusters of anomalies that are close in the feature space\n",
       "- Misclassifying normal instances as anomalies\n",
       "- Failure to detect actual anomalies\n",
       "\n",
       "**Strengths**:\n",
       "- Ability to handle large, high-dimensional datasets\n",
       "- Efficiency in isolating anomalies instead of normal instances\n",
       "- Insensitivity to the underlying distribution of data\n",
       "- Ability to recognize anomalies even when they are not separated from the main data cloud through identifying distinctive properties\n",
       "- Visually presents the test results for better understanding and interpretability\n",
       "\n",
       "**Limitations**:\n",
       "- Difficult to detect anomalies that are close to each other or prevalent in datasets\n",
       "- Dependency on the contamination parameter which may need fine-tuning to be effective\n",
       "- Potential failure in detecting collective anomalies if they behave similarly to normal data\n",
       "- Potential lack of precision in identifying which features contribute most to the anomalous behavior</td>\n",
       "      <td id=\"T_fccf5_row54_col3\" class=\"data row54 col3\" >validmind.data_validation.IsolationForestOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row55_col0\" class=\"data row55 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row55_col1\" class=\"data row55 col1\" >Weakspots Diagnosis</td>\n",
       "      <td id=\"T_fccf5_row55_col2\" class=\"data row55 col2\" >**Purpose:** The weak spots test is applied to evaluate the performance of a machine learning model within specific regions of its feature space. This test slices the feature space into various sections, evaluating the model's outputs within each section against specific performance metrics (e.g., accuracy, precision, recall, and F1 scores). The ultimate aim is to identify areas where the model's performance falls below the set thresholds, thereby exposing its possible weaknesses and limitations.\n",
       "\n",
       "**Test Mechanism:** The test mechanism adopts an approach of dividing the feature space of the training dataset into numerous bins. The model's performance metrics (accuracy, precision, recall, F1 scores) are then computed for each bin on both the training and test datasets. A \"weak spot\" is identified if any of the performance metrics fall below a predetermined threshold for a particular bin on the test dataset. The test results are visually plotted as bar charts for each performance metric, indicating the bins which fail to meet the established threshold.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Any performance metric of the model dropping below the set thresholds.\n",
       "- Significant disparity in performance between the training and test datasets within a bin could be an indication of overfitting.\n",
       "- Regions or slices with consistently low performance metrics. Such instances could mean that the model struggles to handle specific types of input data adequately, resulting in potentially inaccurate predictions.\n",
       "\n",
       "**Strengths:**\n",
       "- The test helps pinpoint precise regions of the feature space where the model's performance is below par, allowing for more targeted improvements to the model.\n",
       "- The graphical presentation of the performance metrics offers an intuitive way to understand the model's performance across different feature areas.\n",
       "- The test exhibits flexibility, letting users set different thresholds for various performance metrics according to the specific requirements of the application.\n",
       "\n",
       "**Limitations:**\n",
       "- The binning system utilized for the feature space in the test could over-simplify the model's behavior within each bin. The granularity of this slicing depends on the chosen 'bins' parameter and can sometimes be arbitrary.\n",
       "- The effectiveness of this test largely hinges on the selection of thresholds for the performance metrics, which may not hold universally applicable and could be subjected to the specifications of a particular model and application.\n",
       "- The test is unable to handle datasets with a text column, limiting its application to numerical or categorical data types only.\n",
       "- Despite its usefulness in highlighting problematic regions, the test does not offer direct suggestions for model improvement.</td>\n",
       "      <td id=\"T_fccf5_row55_col3\" class=\"data row55 col3\" >validmind.model_validation.sklearn.WeakspotsDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row56_col0\" class=\"data row56 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row56_col1\" class=\"data row56 col1\" >Minimum F1 Score</td>\n",
       "      <td id=\"T_fccf5_row56_col2\" class=\"data row56 col2\" >**Purpose:** The main objective of this test is to ensure that the F1 score, a balanced measure of precision and recall, of the model meets or surpasses a predefined threshold on the validation dataset. The F1 score is highly useful for gauging model performance in classification tasks, especially in cases where the distribution of positive and negative classes is skewed.\n",
       "\n",
       "**Test Mechanism:** The F1 score for the validation dataset is computed through the scikit-learn's metrics in Python. The scoring mechanism differs based on the classification problem: for multi-class problems, macro averaging is used (metrics are calculated separately and their unweighted mean is found), and for binary classification, the built-in f1_score calculation is used. The obtained F1 score is then assessed against the predefined minimum F1 score that is expected from the model.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "\n",
       "- If a model returns an F1 score that is less than the established threshold, it is regarded as high risk.\n",
       "- A low F1 score might suggest that the model is not finding an optimal balance between precision and recall, see: it isn't successfully identifying positive classes while minimizing false positives.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "- This metric gives a balanced measure of a model's performance by accounting for both false positives and false negatives.\n",
       "- It has a particular advantage in scenarios with imbalanced class distribution, where an accuracy measure can be misleading.\n",
       "- The flexibility of setting the threshold value allows for tailoring the minimum acceptable performance.\n",
       "\n",
       "**Limitations:**\n",
       "\n",
       "- The testing method may not be suitable for all types of models and machine learning tasks.\n",
       "- Although the F1 score gives a balanced view of a model's performance, it presupposes an equal cost for false positives and false negatives, which may not always be true in certain real-world scenarios. As a consequence, practitioners might have to rely on other metrics such as precision, recall, or the ROC-AUC score that align more closely with their specific requirements.</td>\n",
       "      <td id=\"T_fccf5_row56_col3\" class=\"data row56 col3\" >validmind.model_validation.sklearn.MinimumF1Score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row57_col0\" class=\"data row57 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row57_col1\" class=\"data row57 col1\" >Lilliefors</td>\n",
       "      <td id=\"T_fccf5_row57_col2\" class=\"data row57 col2\" >**Purpose**: The purpose of this metric is to utilize the Lilliefors test, named in honor of the Swedish statistician Hubert Lilliefors, in order to assess whether the features of the machine learning model's training dataset conform to a normal distribution. This is done because the assumption of normal distribution plays a vital role in numerous statistical procedures as well as numerous machine learning models. Should the features fail to follow a normal distribution, some model types may not operate at optimal efficiency. This can potentially lead to inaccurate predictions.\n",
       "\n",
       "**Test Mechanism**: The application of this test happens across all feature columns within the training dataset. For each feature, the Lilliefors test returns a test statistic and p-value. The test statistic quantifies how far the feature's distribution is from an ideal normal distribution, whereas the p-value aids in determining the statistical relevance of this deviation. The final results are stored within a dictionary, the keys of which correspond to the name of the feature column, and the values being another dictionary which houses the test statistic and p-value.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- If the p-value corresponding to a specific feature sinks below a pre-established significance level, generally set at 0.05, then it can be deduced that the distribution of that feature significantly deviates from a normal distribution. This can present a high risk for models that assume normality, as these models may perform inaccurately or inefficiently in the presence of such a feature.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- One advantage of the Lilliefors test is its utility irrespective of whether the mean and variance of the normal distribution are known in advance. This makes it a more robust option in real-world situations where these values might not be known.\n",
       "- Second, the test has the ability to screen every feature column, offering a holistic view of the dataset.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Despite the practical applications of the Lilliefors test in validating normality, it does come with some limitations.\n",
       "- Firstly, it is only capable of testing unidimensional data, thus rendering it ineffective for datasets with interactions between features or multi-dimensional phenomena.\n",
       "- Additionally, the test might not be as sensitive as some other tests (like the Anderson-Darling test) in detecting deviations from a normal distribution.\n",
       "- Lastly, like any other statistical test, Lilliefors test may also produce false positives or negatives. Hence, banking solely on this test, without considering other characteristics of the data, may give rise to risks.</td>\n",
       "      <td id=\"T_fccf5_row57_col3\" class=\"data row57 col3\" >validmind.model_validation.statsmodels.Lilliefors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row58_col0\" class=\"data row58 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row58_col1\" class=\"data row58 col1\" >Punctuations</td>\n",
       "      <td id=\"T_fccf5_row58_col2\" class=\"data row58 col2\" >**1. Purpose:** The Punctuations Metric's primary purpose is to analyze the frequency of punctuation usage within a given text dataset. This is often used in Natural Language Processing tasks, such as text classification and text summarization.\n",
       "\n",
       "**2. Test Mechanism:** The test begins by verifying that the input \"dataset\" is of the type VMDataset. Following that, a corpus is created from the dataset by splitting its text on spaces. Each unique punctuation character in the text corpus is then tallied. Then, the frequency distribution of each punctuation symbol is visualized as a bar graph, with these results being stored as Figures and associated with the main Punctuations object.\n",
       "\n",
       "**3. Signs of High Risk:**\n",
       "- High risk can be indicated by the excessive or unusual frequency of specific punctuation marks, potentially denoting dubious quality, data corruption, or skewed data.\n",
       "\n",
       "**4. Strengths:**\n",
       "- The Punctuations Metric provides valuable insights into the distribution of punctuation usage in a text dataset.\n",
       "- This insight can be important in validating the quality, consistency, and nature of the data.\n",
       "- It can provide hints about the style or tonality of the text corpus. For example, frequent usage of exclamation marks may suggest a more informal and emotional context.\n",
       "\n",
       "**5. Limitations:**\n",
       "- The metric focuses solely on punctuation usage and can miss other important textual characteristics.\n",
       "- It's important not to make general cultural or tonality assumptions based solely on punctuation distribution, since these can vary greatly across different languages and contexts.\n",
       "- The metric may be less effective with languages that use non-standard or different punctuation.\n",
       "- The visualization may lack interpretability when there are many unique punctuation marks in the dataset.</td>\n",
       "      <td id=\"T_fccf5_row58_col3\" class=\"data row58 col3\" >validmind.data_validation.nlp.Punctuations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row59_col0\" class=\"data row59 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row59_col1\" class=\"data row59 col1\" >Jarque Bera</td>\n",
       "      <td id=\"T_fccf5_row59_col2\" class=\"data row59 col2\" >**Purpose**: The purpose of the Jarque-Bera test as implemented in this metric is to determine if the features in the dataset of a given Machine Learning model follows a normal distribution. This is crucial for understanding the distribution and behavior of the model's features, as numerous statistical methods assume normal distribution of the data.\n",
       "\n",
       "**Test Mechanism**: The test mechanism involves computing the Jarque-Bera statistic, p-value, skew, and kurtosis for each feature in the dataset. It utilizes the 'jarque_bera' function from the 'statsmodels' library in Python, storing the results in a dictionary. The test evaluates the skewness and kurtosis to ascertain whether the dataset follows a normal distribution. A significant p-value (typically less than 0.05) implies that the data does not possess normal distribution.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high Jarque-Bera statistic and a low p-value (usually less than 0.05) indicates high-risk conditions.\n",
       "- Such results suggest the data significantly deviates from a normal distribution. If a machine learning model expects feature data to be normally distributed, these findings imply that it may not function as intended.\n",
       "\n",
       "**Strengths**:\n",
       "- This test provides insights into the shape of the data distribution, helping determine whether a given set of data follows a normal distribution.\n",
       "- This is particularly useful for risk assessment for models that assume a normal distribution of data.\n",
       "- By measuring skewness and kurtosis, it provides additional insights into the nature and magnitude of a distribution's deviation.\n",
       "\n",
       "**Limitations**:\n",
       "- The Jarque-Bera test only checks for normality in the data distribution. It cannot provide insights into other types of distributions.\n",
       "- Datasets that aren't normally distributed but follow some other distribution might lead to inaccurate risk assessments.\n",
       "- The test is highly sensitive to large sample sizes, often rejecting the null hypothesis (that data is normally distributed) even for minor deviations in larger datasets.</td>\n",
       "      <td id=\"T_fccf5_row59_col3\" class=\"data row59 col3\" >validmind.model_validation.statsmodels.JarqueBera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row60_col0\" class=\"data row60 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row60_col1\" class=\"data row60 col1\" >Precision Recall Curve</td>\n",
       "      <td id=\"T_fccf5_row60_col2\" class=\"data row60 col2\" >**Purpose**: The Precision Recall Curve metric is intended to evaluate the trade-off between precision and recall in classification models, particularly binary classification models. It assesses the model's capacity to produce accurate results (high precision), as well as its ability to capture a majority of all positive instances (high recall).\n",
       "\n",
       "**Test Mechanism**: The test extracts ground truth labels and prediction probabilities from the model's test dataset. It applies the precision_recall_curve method from the sklearn metrics module to these extracted labels and predictions, which computes a precision-recall pair for each possible threshold. This calculation results in an array of precision and recall scores that can be plotted against each other to form the Precision-Recall Curve. This curve is then visually represented by using Plotly's scatter plot.\n",
       "\n",
       "**Signs of High Risk**: * A lower area under the Precision-Recall Curve signifies high risk. * This corresponds to a model yielding a high amount of false positives (low precision) and/or false negatives (low recall). * If the curve is closer to the bottom left of the plot, rather than being closer to the top right corner, it can be a sign of high risk.\n",
       "\n",
       "**Strengths**: * This metric aptly represents the balance between precision (minimizing false positives) and recall (minimizing false negatives), which is especially critical in scenarios where both values are significant. * Through the graphic representation, it enables an intuitive understanding of the model's performance across different threshold levels.\n",
       "\n",
       "**Limitations**: * This metric is only applicable to binary classification models – it raises errors for multiclass classification models or Foundation models. * It may not fully represent the overall accuracy of the model if the cost of false positives and false negatives are extremely different, or if the dataset is heavily imbalanced.</td>\n",
       "      <td id=\"T_fccf5_row60_col3\" class=\"data row60 col3\" >validmind.model_validation.sklearn.PrecisionRecallCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row61_col0\" class=\"data row61 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row61_col1\" class=\"data row61 col1\" >Bivariate Features Bar Plots</td>\n",
       "      <td id=\"T_fccf5_row61_col2\" class=\"data row61 col2\" >**Purpose**: The BivariateFeaturesBarPlots metric is intended to perform a visual analysis of categorical data within the model. The goal is to assess and understand the specific relationships between various feature pairs, while simultaneously highlighting the model's target variable. This form of bivariate plotting is immensely beneficial in uncovering trends, correlations, patterns, or inconsistencies that may not be readily apparent within raw tabular data.\n",
       "\n",
       "**Test Mechanism**: These tests establish bar plots for each pair of features defined within the parameters. The dataset is grouped by each feature pair and then calculates the mean of the target variable within each specific grouping. Each group is represented via a bar in the plot, and the height of this bar aligns with the calculated mean. The colors assigned to these bars are based on the categorical section to which they pertain: these colors can either come from a colormap or generated anew if the total number of categories exceeds the current colormap's scope.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If any values are found missing or inconsistent within the feature pairs.\n",
       "- If there exist large discrepancies or irregularities between the mean values of certain categories within feature pairs.\n",
       "- If the parameters for feature pairs have not been specified or if they were wrongly defined.\n",
       "\n",
       "**Strengths**:\n",
       "- The BivariateFeaturesBarPlots provides a clear, visual comprehension of the relationships between feature pairs and the target variable.\n",
       "- It allows an easy comparison between different categories within feature pairs.\n",
       "- The metric can handle a diverse array of categorical data, enhancing its universal applicability.\n",
       "- It is highly customizable due to its allowance for users to define feature pairs based on their specific requirements.\n",
       "\n",
       "**Limitations**:\n",
       "- It can only be used with categorical data, limiting its usability with numerical or textual data.\n",
       "- It relies on manual input for feature pairs, which could result in the overlooking of important feature pairs if not chosen judiciously.\n",
       "- The generated bar plots could become overly cluttered and difficult to decipher when dealing with feature pairs with a large number of categories.\n",
       "- This metric only provides a visual evaluation and fails to offer any numerical or statistical measures to quantify the relationship between feature pairs.</td>\n",
       "      <td id=\"T_fccf5_row61_col3\" class=\"data row61 col3\" >validmind.data_validation.BivariateFeaturesBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row62_col0\" class=\"data row62 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row62_col1\" class=\"data row62 col1\" >Tabular Categorical Bar Plots</td>\n",
       "      <td id=\"T_fccf5_row62_col2\" class=\"data row62 col2\" >**Purpose**: The purpose of this metric is to visually analyze categorical data using bar plots. It is intended to evaluate the dataset's composition by displaying the counts of each category in each categorical feature.\n",
       "\n",
       "**Test Mechanism**: The provided dataset is first checked to determine if it contains any categorical variables. If no categorical columns are found, the tool raises a ValueError. For each categorical variable in the dataset, a separate bar plot is generated. The number of occurrences for each category is calculated and displayed on the plot. If a dataset contains multiple categorical columns, multiple bar plots are produced.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- High risk could occur if the categorical variables exhibit an extreme imbalance, with categories having very few instances possibly being underrepresented in the model, which could affect the model's performance and its ability to generalize.\n",
       "- Another sign of risk is if there are too many categories in a single variable, which could lead to overfitting and make the model complex.\n",
       "\n",
       "**Strengths**: This metric provides a visual and intuitively understandable representation of categorical data, which aids in the analysis of variable distributions. By presenting model inputs in this way, we can easily identify imbalances or rare categories that could affect the model's performance.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- This method only works with categorical data, meaning it won't apply to numerical variables.\n",
       "- In addition, the method does not provide any informative value when there are too many categories, as the bar chart could become cluttered and hard to interpret.\n",
       "- It offers no insights into the model's performance or precision, but rather provides a descriptive analysis of the input.</td>\n",
       "      <td id=\"T_fccf5_row62_col3\" class=\"data row62 col3\" >validmind.data_validation.TabularCategoricalBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row63_col0\" class=\"data row63 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row63_col1\" class=\"data row63 col1\" >Duplicates</td>\n",
       "      <td id=\"T_fccf5_row63_col2\" class=\"data row63 col2\" >**Purpose**: The Duplicates test is designed to assess the data quality of an ML model by identifying any duplicate entries in the data set. It focuses on seeking out duplication in a specified text column or among the primary keys of the data set, which could have serious implications for the performance and integrity of the model. Duplicate entries could potentially skew the data distribution and influence model training inaccurately.\n",
       "\n",
       "**Test Mechanism**: This test operates by calculating the total number of duplicate entries in the data set. The algorithm will count duplicates within the 'text_column' if this property is specified. If primary keys are defined, the test will also be applied on them. The count of duplicates ('n_duplicates') is then compared to a predefined minimum threshold (the default 'min_threshold' is set at 1) to determine whether the test has passed or not. The results include the total number of duplicates as well as the percentage of duplicate rows in comparison to the overall dataset ('p_duplicates').\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large amount of duplicates, particularly those exceeding the predefined minimum threshold, point toward a high risk situation.\n",
       "- Overrepresentation of certain data which can lead to skewed results.\n",
       "- Indication of inefficient data collecting techniques leading to data redundancy.\n",
       "- Models that fail this test predominantly may necessitate a closer examination of their data preprocessing methods or source data.\n",
       "\n",
       "**Strengths**:\n",
       "- The Duplicates test is highly adaptable, being capable of being used with both text data and tabular data formats.\n",
       "- It is able to provide results both numerically and as a percentage of the total data set, allowing for a broader understanding of the extent of duplication.\n",
       "- Its utility lies in effectively flagging any data quality issues that could potentially upset model performance and generate erroneous predictions.\n",
       "\n",
       "**Limitations**:\n",
       "- The Duplicates test solely targets exact duplication in entries, meaning it may overlook near-duplicates or normalized forms of entries that might also affect data distribution and model integrity.\n",
       "- Data variations caused by errors, phrasing changes, or inconsistencies may not be detected.\n",
       "- A substantial number of duplicates in a datasets may not always denote poor data quality, as this can be dependent on the nature of the data and the problem being addressed.</td>\n",
       "      <td id=\"T_fccf5_row63_col3\" class=\"data row63 col3\" >validmind.data_validation.Duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row64_col0\" class=\"data row64 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row64_col1\" class=\"data row64 col1\" >Scatter Plot</td>\n",
       "      <td id=\"T_fccf5_row64_col2\" class=\"data row64 col2\" >**Purpose**: The ScatterPlot metric is designed to offer a visual analysis of a given dataset by constructing a scatter plot matrix encapsulating all the dataset's features (or columns). Its primary function lies in unearthing relationships, patterns, or outliers across different features, thus providing both quantitative and qualitative insights into the multidimensional relationships within the dataset. This visual assessment aids in understanding the efficacy of the chosen features for model training and their overall suitability.\n",
       "\n",
       "**Test Mechanism**: Using the seaborn library, the ScatterPlot class creates the scatter plot matrix. The process includes retrieving all columns from the dataset, verifying their existence, and subsequently generating a pairplot for these columns. A kernel density estimate (kde) is utilized to present a smoother, univariate distribution along the grid's diagonal. The final plot is housed in an array of Figure objects, each wrapping a matplotlib figure instance for storage and future usage.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The emergence of non-linear or random patterns across different feature pairs. This may suggest intricate relationships unfit for linear presumptions.\n",
       "- A lack of clear patterns or clusters which might point to weak or non-existent correlations among features, thus creating a problem for certain model types.\n",
       "- The occurrence of outliers as visual outliers in your data can adversely influence the model's performance.\n",
       "\n",
       "**Strengths**:\n",
       "- It offers insight into the multidimensional relationships among multiple features.\n",
       "- It assists in identifying trends, correlations, and outliers which could potentially affect the model's performance.\n",
       "- As a diagnostic tool, it can validate whether certain assumptions made during the model-creation process, such as linearity, hold true.\n",
       "- The tool's versatility extends to its application for both regression and classification tasks.\n",
       "\n",
       "**Limitations**:\n",
       "- Scatter plot matrices may become cluttered and hard to decipher as the number of features escalates, resulting in complexity and confusion.\n",
       "- While extremely proficient in revealing pairwise relationships, these matrices may fail to illuminate complex interactions that involve three or more features.\n",
       "- These matrices are primarily visual tools, so the precision of quantitative analysis may be compromised.\n",
       "- If not clearly visible, outliers can be missed, which could negatively affect model performance.\n",
       "- It assumes that the dataset can fit into the computer's memory, which might not always be valid particularly for extremely large datasets.</td>\n",
       "      <td id=\"T_fccf5_row64_col3\" class=\"data row64 col3\" >validmind.data_validation.ScatterPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row65_col0\" class=\"data row65 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row65_col1\" class=\"data row65 col1\" >ROC Curve</td>\n",
       "      <td id=\"T_fccf5_row65_col2\" class=\"data row65 col2\" >**Purpose**: The Receiver Operating Characteristic (ROC) curve is designed to evaluate the performance of binary classification models. This curve illustrates the balance between the True Positive Rate (TPR) and False Positive Rate (FPR) across various threshold levels. In combination with the Area Under the Curve (AUC), the ROC curve aims to measure the model's discrimination ability between the two defined classes in a binary classification problem (e.g., default vs non-default). Ideally, a higher AUC score signifies superior model performance in accurately distinguishing between the positive and negative classes.\n",
       "\n",
       "**Test Mechanism**: First, this script selects the target model and datasets that require binary classification. It then calculates the predicted probabilities for the test set, and uses this data, along with the true outcomes, to generate and plot the ROC curve. Additionally, it concludes a line signifying randomness (AUC of 0.5). The AUC score for the model's ROC curve is also computed, presenting a numerical estimation of the model's performance. If any Infinite values are detected in the ROC threshold, these are effectively eliminated. The resulting ROC curve, AUC score, and thresholds are consequently saved for future reference.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk is potentially linked to the model's performance if the AUC score drops below or nears 0.5.\n",
       "- Another warning sign would be the ROC curve lying closer to the line of randomness, indicating no discriminative ability.\n",
       "- For the model to be deemed competent at its classification tasks, it is crucial that the AUC score is significantly above 0.5.\n",
       "\n",
       "**Strengths**:\n",
       "- This ROC Curve offers an inclusive visual depiction of a model's discriminative power throughout all conceivable classification thresholds, unlike other metrics that solely disclose model performance at one fixed threshold.\n",
       "- Despite the proportions of the dataset, the AUC Score, which represents the entire ROC curve as a single data point, continues to be consistent, proving to be the ideal choice for such situations.\n",
       "\n",
       "**Limitations**:\n",
       "- The primary limitation is that this test is exclusively structured for binary classification tasks, thus limiting its application towards other model types.\n",
       "- Furthermore, its performance might be subpar with models that output probabilities highly skewed towards 0 or 1.\n",
       "- At the extreme, the ROC curve could reflect high performance even when the majority of classifications are incorrect, provided that the model's ranking format is retained. This phenomenon is commonly termed the \"Class Imbalance Problem\".</td>\n",
       "      <td id=\"T_fccf5_row65_col3\" class=\"data row65 col3\" >validmind.model_validation.sklearn.ROCCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row66_col0\" class=\"data row66 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row66_col1\" class=\"data row66 col1\" >Minimum Accuracy</td>\n",
       "      <td id=\"T_fccf5_row66_col2\" class=\"data row66 col2\" >**Purpose**: The Minimum Accuracy test’s objective is to verify whether the model's prediction accuracy on a specific dataset meets or surpasses a predetermined minimum threshold. Accuracy, which is simply the ratio of right predictions to total predictions, is a key metric for evaluating the model's performance. Considering binary as well as multiclass classifications, accurate labeling becomes indispensable.\n",
       "\n",
       "**Test Mechanism**: The test mechanism involves contrasting the model's accuracy score with a pre-set minimum threshold value, default value being 0.7. The accuracy score is computed utilizing sklearn’s `accuracy_score` method, where the true label `y_true` and predicted label `class_pred` are compared. If the accuracy score is above the threshold, the test gets a passing mark. The test returns the result along with the accuracy score and threshold used for the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The risk level for this test surges considerably when the model is unable to achieve or surpass the predefined score threshold.\n",
       "- When the model persistently scores below the threshold, it suggests a high risk of inaccurate predictions, which in turn affects the model’s efficiency and reliability.\n",
       "\n",
       "**Strengths**:\n",
       "- One of the key strengths of this test is its simplicity, presenting a straightforward measure of the holistic model performance across all classes.\n",
       "- This test is particularly advantageous when classes are balanced.\n",
       "- Another advantage of this test is its versatility as it can be implemented on both binary and multiclass classification tasks.\n",
       "\n",
       "**Limitations**:\n",
       "- When analyzing imbalanced datasets, certain limitations of this test emerge. The accuracy score can be misleading when classes in the dataset are skewed considerably.\n",
       "- This can result in favoritism towards the majority class, consequently giving an inaccurate perception of the model performance.\n",
       "- Another limitation is its inability to measure the model's precision, recall, or capacity to manage false positives or false negatives.\n",
       "- The test majorly focuses on overall correctness and may not be sufficient for all types of model analytics.</td>\n",
       "      <td id=\"T_fccf5_row66_col3\" class=\"data row66 col3\" >validmind.model_validation.sklearn.MinimumAccuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row67_col0\" class=\"data row67 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row67_col1\" class=\"data row67 col1\" >Scorecard Bucket Histogram</td>\n",
       "      <td id=\"T_fccf5_row67_col2\" class=\"data row67 col2\" >**Purpose**: The 'Scorecard Bucket Histogram' is employed as a metric to evaluate the performance of a classification model, specifically in credit risk assessment. It categorizes model scores into different rating classes, and visualizes the distribution of scores or probabilities within each class. It essentially measures how different risk categories (classes) are distributed in the model scores and provides insight into the model's classification ability. This makes it particularly useful in credit scoring and risk modeling where understanding the probability of default is critical.\n",
       "\n",
       "**Test Mechanism**: The test works by computing the probabilities for each record in the test and train dataset using the model's predict function. Subsequently, it calculates the scores using a formula incorporating target score, target odds, and points to double odds (PDO). The scores are then bucketed into predefined rating classes (such as 'A', 'B', 'C', 'D') and plotted in a histogram for both the train and test datasets. The target score, target odds, points to double the odds (PDO), and rating classes are customizable parameters, providing flexibility in test metrics based on differing model or industry norms.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Disproportionate scores within rating classes\n",
       "- Excessive overlap between classes\n",
       "- Inconsistent distribution of scores between the training and testing datasets\n",
       "\n",
       "If the model is accurately classifying and risk is being evenly distributed, we would anticipate smooth and relatively balanced histograms within classes.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Provides a quick visual snapshot of score distribution\n",
       "- Breaks down complex predictions into simple, understandable classes, making it easily interpretable for both technical and non-technical audiences\n",
       "- Caters to customization of parameters\n",
       "- Gives ownership of the class definitions to the user\n",
       "- Useful in the field of credit risk, providing a clear understanding of which class or 'bucket' a potential borrower belongs to\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Relies on manual setting of classes and other parameters (like target score, target odds, and PDO), potentially leading to arbitrary classifications and potential bias if not judiciously performed\n",
       "- Effectiveness can be limited with non-tabular data\n",
       "- Doesn't provide a numerical value easily compared across different models or runs as the output is primarily visual\n",
       "- Might not present a complete view of model performance and should be used in conjunction with other metrics</td>\n",
       "      <td id=\"T_fccf5_row67_col3\" class=\"data row67 col3\" >validmind.model_validation.statsmodels.ScorecardBucketHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row68_col0\" class=\"data row68 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row68_col1\" class=\"data row68 col1\" >Target Rate Bar Plots</td>\n",
       "      <td id=\"T_fccf5_row68_col2\" class=\"data row68 col2\" >**Purpose**: This test, implemented as a metric, is designed to provide an intuitive, graphical summary of the decision-making patterns exhibited by a categorical classification machine learning model. The model's performance is evaluated using bar plots depicting the ratio of target rates—meaning the proportion of positive classes—for different categorical inputs. This allows for an easy, at-a-glance understanding of the model's accuracy.\n",
       "\n",
       "**Test Mechanism**: The test involves creating a pair of bar plots for each categorical feature in the dataset. The first plot depicts the frequency of each category in the dataset, with each category visually distinguished by its unique color. The second plot shows the mean target rate of each category (sourced from the \"default_column\"). Plotly, a Python library, is used to generate these plots, with distinct plots created for each feature. If no specific columns are selected, the test will generate plots for each categorical column in the dataset.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Inconsistent or non-binary values in the \"default_column\" could complicate or render impossible the calculation of average target rates.\n",
       "- Particularly low or high target rates for a specific category might suggest that the model is misclassifying instances of that category.\n",
       "\n",
       "**Strengths**:\n",
       "- This test offers a visually interpretable breakdown of the model's decisions, providing an easy way to spot irregularities, inconsistencies, or patterns.\n",
       "- Its flexibility allows for the inspection of one or multiple columns, as needed.\n",
       "\n",
       "**Limitations**:\n",
       "- The test is less useful when dealing with numeric or continuous data, as it's designed specifically for categorical features.\n",
       "- If the model in question is dealing with a multi-class problem rather than binary classification, the test's assumption of binary target values (0s and 1s) becomes a significant limitation.\n",
       "- The readability of the bar plots drops as the number of distinct categories increases in the dataset, which can make them harder to understand and less useful.</td>\n",
       "      <td id=\"T_fccf5_row68_col3\" class=\"data row68 col3\" >validmind.data_validation.TargetRateBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row69_col0\" class=\"data row69 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row69_col1\" class=\"data row69 col1\" >Class Imbalance</td>\n",
       "      <td id=\"T_fccf5_row69_col2\" class=\"data row69 col2\" >**Purpose**: The ClassImbalance test is designed to evaluate the distribution of target classes in a dataset that's utilized by a machine learning model. Specifically, it aims to ensure that the classes aren't overly skewed, which could lead to bias in the model's predictions. It's crucial to have a balanced training dataset to avoid creating a model that's biased with high accuracy for the majority class and low accuracy for the minority class.\n",
       "\n",
       "**Test Mechanism**: This ClassImbalance test operates by calculating the frequency (expressed as a percentage) of each class in the target column of the dataset. It then checks whether each class appears in at least a set minimum percentage of the total records. This minimum percentage is a modifiable parameter, but the default value is set to 10%.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Any class that represents less than the pre-set minimum percentage threshold is marked as high risk, implying a potential class imbalance.\n",
       "- The function provides a pass/fail outcome for each class based on this criterion.\n",
       "- Fundamentally, if any class fails this test, it's highly likely that the dataset possesses imbalanced class distribution.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The test can spot under-represented classes that could affect the efficiency of a machine learning model.\n",
       "- The calculation is straightforward and swift.\n",
       "- The test is highly informative because it not only spots imbalance, but it also quantifies the degree of imbalance.\n",
       "- The adjustable threshold enables flexibility and adaptation to differing use-cases or domain-specific needs.\n",
       "- The test creates a visually insightful plot showing the classes and their corresponding proportions, enhancing interpretability and comprehension of the data.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The test might struggle to perform well or provide vital insights for datasets with a high number of classes. In such cases, the imbalance could be inevitable due to the inherent class distribution.\n",
       "- Sensitivity to the threshold value might result in faulty detection of imbalance if the threshold is set excessively high.\n",
       "- Regardless of the percentage threshold, it doesn't account for varying costs or impacts of misclassifying different classes, which might fluctuate based on specific applications or domains.\n",
       "- While it can identify imbalances in class distribution, it doesn't provide direct methods to address or correct these imbalances.\n",
       "- The test is only applicable for classification opearations and unsuitable for regression or clustering tasks.</td>\n",
       "      <td id=\"T_fccf5_row69_col3\" class=\"data row69 col3\" >validmind.data_validation.ClassImbalance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row70_col0\" class=\"data row70 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row70_col1\" class=\"data row70 col1\" >Kolmogorov Smirnov</td>\n",
       "      <td id=\"T_fccf5_row70_col2\" class=\"data row70 col2\" >**Purpose**: This metric employs the Kolmogorov-Smirnov (KS) test to evaluate the distribution of a dataset's features. It specifically gauges whether the data from each feature aligns with a normal distribution, a common presumption in many statistical methods and machine learning models.\n",
       "\n",
       "**Test Mechanism**: This KS test calculates the KS statistic and the corresponding p-value for each column in a dataset. It achieves this by contrasting the cumulative distribution function of the dataset's feature with an ideal normal distribution. Subsequently, a feature-by-feature KS statistic and p-value are stored in a dictionary. The specific threshold p-value (the value below which we reject the hypothesis that the data is drawn from a normal distribution) is not firmly set within this implementation, allowing for definitional flexibility depending on the specific application.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Elevated KS statistic for a feature combined with a low p-value. This suggests a significant divergence between the feature's distribution and a normal one.\n",
       "- Features with notable deviations. These could create problems if the applicable model makes assumptions about normal data distribution, thereby representing a risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The KS test is acutely sensitive to differences in the location and shape of the empirical cumulative distribution functions of two samples.\n",
       "- It is non-parametric and does not presuppose any specific data distribution, making it adaptable to a range of datasets.\n",
       "- With its focus on individual features, it offers detailed insights into data distribution.\n",
       "\n",
       "**Limitations**:\n",
       "- The sensitivity of the KS test to disparities in data distribution tails can be excessive. Such sensitivity might prompt false alarms about non-normal distributions, particularly in situations where these tail tendencies are irrelevant to the model.\n",
       "- It could become less effective when applied to multivariate distributions, considering that it's primarily configured for univariate distributions.\n",
       "- As a goodness-of-fit test, the KS test does not identify specific types of non-normality, such as skewness or kurtosis, that could directly impact model fitting.</td>\n",
       "      <td id=\"T_fccf5_row70_col3\" class=\"data row70 col3\" >validmind.model_validation.statsmodels.KolmogorovSmirnov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row71_col0\" class=\"data row71 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row71_col1\" class=\"data row71 col1\" >Classifier Performance</td>\n",
       "      <td id=\"T_fccf5_row71_col2\" class=\"data row71 col2\" >**Purpose**: The supplied script is designed to evaluate the performance of Machine Learning classification models. It accomplishes this by computing precision, recall, F1-Score, and accuracy, as well as the ROC AUC (Receiver operating characteristic\n",
       "- Area under the curve) scores, thereby providing a comprehensive analytic view of the models' performance. The test is adaptable, handling binary and multiclass models equally effectively.\n",
       "\n",
       "**Test Mechanism**: The script produces a report that includes precision, recall, F1-Score, and accuracy, by leveraging the `classification_report` from the scikit-learn's metrics module. For multiclass models, macro and weighted averages for these scores are also calculated. Additionally, the ROC AUC scores are calculated and included in the report using the script's unique `multiclass_roc_auc_score` function. The outcome of the test (report format) differs based on whether the model is binary or multiclass.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low values for precision, recall, F1-Score, accuracy, and ROC AUC, indicating poor performance.\n",
       "- Imbalance in precision and recall scores. Precision highlights correct positive class predictions, while recall indicates the accurate identification of actual positive cases. Imbalance may indicate flawed model performance.\n",
       "- A low ROC AUC score, especially scores close to 0.5 or lower, strongly suggests a failing model.\n",
       "\n",
       "**Strengths**:\n",
       "- The script is versatile, capable of assessing both binary and multiclass models.\n",
       "- It uses a variety of commonly employed performance metrics, offering a comprehensive view of a model's performance.\n",
       "- The use of ROC-AUC as a metric aids in determining the most optimal threshold for classification, especially beneficial when evaluation datasets are unbalanced.\n",
       "\n",
       "**Limitations**:\n",
       "- The test assumes correctly identified labels for binary classification models and raises an exception if the positive class is not labeled as \"1\". However, this setup may not align with all practical applications.\n",
       "- This script is specifically designed for classification models and is not suited to evaluate regression models.\n",
       "- The metrics computed may provide limited insights in cases where the test dataset does not adequately represent the data the model will encounter in real-world scenarios.</td>\n",
       "      <td id=\"T_fccf5_row71_col3\" class=\"data row71 col3\" >validmind.model_validation.sklearn.ClassifierPerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row72_col0\" class=\"data row72 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row72_col1\" class=\"data row72 col1\" >Tabular Description Tables</td>\n",
       "      <td id=\"T_fccf5_row72_col2\" class=\"data row72 col2\" >**Purpose**: The main purpose of this metric is to gather and present the descriptive statistics of numerical, categorical, and datetime variables present in a dataset. The attributes it measures include the count, mean, minimum and maximum values, percentage of missing values, data types of fields, and unique values for categorical fields, among others.\n",
       "\n",
       "**Test Mechanism**: The test first segregates the variables in the dataset according to their data types (numerical, categorical, or datetime). Then, it compiles summary statistics for each type of variable. The specifics of these statistics vary depending on the type of variable:\n",
       "\n",
       "- For numerical variables, the metric extracts descriptors like count, mean, minimum and maximum values, count of missing values, and data types.\n",
       "- For categorical variables, it counts the number of unique values, displays unique values, counts missing values, and identifies data types.\n",
       "- For datetime variables, it counts the number of unique values, identifies the earliest and latest dates, counts missing values, and identifies data types.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Masses of missing values in the descriptive statistics results could hint at high risk or failure, indicating potential data collection, integrity, and quality issues.\n",
       "- Detection of inappropriate distributions for numerical variables, like having negative values for variables that are always supposed to be positive.\n",
       "- Identifying inappropriate data types, like a continuous variable being encoded as a categorical type.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a comprehensive overview of the dataset.\n",
       "- Gives a snapshot into the essence of the numerical, categorical, and datetime fields.\n",
       "- Identifies potential data quality issues such as missing values or inconsistencies crucial for building credible machine learning models.\n",
       "- The metadata, including the data type and missing value information, are vital for anyone including data scientists dealing with the dataset before the modeling process.\n",
       "\n",
       "**Limitations**:\n",
       "- It does not perform any deeper statistical analysis or tests on the data.\n",
       "- It does not handle issues such as outliers, or relationships between variables.\n",
       "- It offers no insights into potential correlations or possible interactions between variables.\n",
       "- It does not investigate the potential impact of missing values on the performance of the machine learning models.\n",
       "- It does not explore potential transformation requirements that may be necessary to enhance the performance of the chosen algorithm.</td>\n",
       "      <td id=\"T_fccf5_row72_col3\" class=\"data row72 col3\" >validmind.data_validation.TabularDescriptionTables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row73_col0\" class=\"data row73 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row73_col1\" class=\"data row73 col1\" >Bivariate Scatter Plots</td>\n",
       "      <td id=\"T_fccf5_row73_col2\" class=\"data row73 col2\" >**Purpose**: This metric is intended for visual inspection and monitoring of relationships between pairs of variables in a machine learning model targeting classification tasks. It is especially useful for understanding how predictor variables (features) behave in relation to each other and how they are distributed for different classes of the target variable, which could inform feature selection, model-building strategies, and even alert to possible biases and irregularities in the data.\n",
       "\n",
       "**Test Mechanism**: This metric operates by creating a scatter plot for each pair of the selected features in the dataset. If the parameters \"features_pairs\" are not specified, an error will be thrown. The metric offers flexibility by allowing the user to filter on a specific target class\n",
       "- specified by the \"target_filter\" parameter\n",
       "- for more granified insights. Each scatterplot is then color-coded based on the category of the target variable for better visual differentiation. The seaborn scatterplot library is used for generating the plots.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Visual patterns which might suggest non-linear relationships, substantial skewness, multicollinearity, clustering, or isolated outlier points in the scatter plot.\n",
       "- Such issues could affect the assumptions and performance of some models, especially the ones assuming linearity like linear regression or logistic regression.\n",
       "\n",
       "**Strengths**:\n",
       "- Scatterplots are simple and intuitive for users to understand, providing a visual tool to pinpoint complex relationships between two variables.\n",
       "- They are useful for outlier detection, identification of variable associations and trends, including non-linear patterns which can be overlooked by other linear-focused metrics or tests.\n",
       "- The implementation also supports visualizing binary or multi-class classification datasets.\n",
       "\n",
       "**Limitations**:\n",
       "- Scatterplots are limited to bivariate analysis\n",
       "- the relationship of two variables at a time\n",
       "- and might not reveal the full picture in higher dimensions or where interactions are present.\n",
       "- They are not ideal for very large datasets as points will overlap and render the visualization less informative.\n",
       "- Scatterplots are more of an exploratory tool rather than a formal statistical test, so they don't provide any quantitative measure of model quality or performance.\n",
       "- Interpretation of scatterplots relies heavily on the domain knowledge and judgment of the viewer, which can introduce subjective bias.</td>\n",
       "      <td id=\"T_fccf5_row73_col3\" class=\"data row73 col3\" >validmind.data_validation.BivariateScatterPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row74_col0\" class=\"data row74 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row74_col1\" class=\"data row74 col1\" >Models Performance Comparison</td>\n",
       "      <td id=\"T_fccf5_row74_col2\" class=\"data row74 col2\" >**Purpose**: This metric test aims to evaluate and compare the performance of various Machine Learning models using test data. It employs multiple metrics such as accuracy, precision, recall, and the F1 score, among others, to assess model performance and assist in selecting the most effective model for the designated task.\n",
       "\n",
       "**Test Mechanism**: The test employs Scikit-learn’s performance metrics to evaluate each model's performance for both binary and multiclass classification tasks. To compare performances, the test runs each model against the test dataset, then produces a comprehensive classification report. This report includes metrics such as accuracy, precision, recall, and the F1 score. Based on whether the task at hand is binary or multiclass classification, it calculates metrics globally for the \"positive\" class or, alternatively, their weighted averages, macro averages, and per class metrics. The test will be skipped if no models are supplied.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low scores in accuracy, precision, recall, and F1 metrics indicate a potentially high risk.\n",
       "- A low area under the Receiver Operating Characteristic (ROC) curve (roc_auc score) is another possible indicator of high risk.\n",
       "- If the metrics scores are significantly lower than alternative models, this might suggest a high risk of failure.\n",
       "\n",
       "**Strengths**:\n",
       "- The test provides a simple way to compare the performance of multiple models, accommodating both binary and multiclass classification tasks.\n",
       "- It provides a holistic view of model performance through a comprehensive report of key performance metrics.\n",
       "- The inclusion of the ROC AUC score is advantageous, as this robust performance metric can effectively handle class imbalance issues.\n",
       "\n",
       "**Limitations**:\n",
       "- This test may not be suitable for more complex performance evaluations that consider factors such as prediction speed, computational cost, or business-specific constraints.\n",
       "- The test's reliability depends on the provided test dataset; hence, the selected models' performance could vary with unseen data or changes in the data distribution.\n",
       "- The ROC AUC score might not be as meaningful or easily interpretable for multilabel/multiclass tasks.</td>\n",
       "      <td id=\"T_fccf5_row74_col3\" class=\"data row74 col3\" >validmind.model_validation.sklearn.ModelsPerformanceComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row75_col0\" class=\"data row75 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row75_col1\" class=\"data row75 col1\" >High Pearson Correlation</td>\n",
       "      <td id=\"T_fccf5_row75_col2\" class=\"data row75 col2\" >**Purpose**: The High Pearson Correlation test measures the linear relationship between features in a dataset, with the main goal of identifying high correlations that might indicate feature redundancy or multicollinearity. Identification of such issue allows developers and risk management teams to properly deal with potential impacts on the machine learning model's performance and interpretability.\n",
       "\n",
       "**Test Mechanism**: The test works by generating pairwise Pearson correlations for all features in the dataset, then sorting and eliminating duplicate and self-correlations. It assigns a Pass or Fail based on whether the absolute value of the correlation coefficient surpasses a pre-set threshold (defaulted at 0.3). It lastly returns the top ten strongest correlations regardless of passing or failing status.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk indication would be the presence of correlation coefficients exceeding the threshold.\n",
       "- If the features share a strong linear relationship, this could lead to potential multicollinearity and model overfitting.\n",
       "- Redundancy of variables can undermine the interpretability of the model due to uncertainty over the authenticity of individual variable's predictive power.\n",
       "\n",
       "**Strengths**:\n",
       "- The High Pearson Correlation test provides a quick and simple means of identifying relationships between feature pairs.\n",
       "- It generates a transparent output which not only displays pairs of correlated variables but also delivers the Pearson correlation coefficient and a Pass or Fail status for each.\n",
       "- It aids early identification of potential multicollinearity issues that may disrupt model training.\n",
       "\n",
       "**Limitations**:\n",
       "- The Pearson correlation test can only delineate linear relationships. It fails to shed light on nonlinear relationships or dependencies.\n",
       "- It is sensitive to outliers where a few outliers could notably affect the correlation coefficient.\n",
       "- It is limited to identifying redundancy only within feature pairs. When three or more variables are linearly dependent, it may fail to spot this complex relationship.\n",
       "- The top 10 result filter might not fully capture the richness of the data; an option to configure the number of retained results could be helpful.</td>\n",
       "      <td id=\"T_fccf5_row75_col3\" class=\"data row75 col3\" >validmind.data_validation.HighPearsonCorrelation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row76_col0\" class=\"data row76 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row76_col1\" class=\"data row76 col1\" >Missing Values</td>\n",
       "      <td id=\"T_fccf5_row76_col2\" class=\"data row76 col2\" >**Purpose**: This test is designed to evaluate the quality of a dataset by measuring the number of missing values across all features. The objective is to ensure that the ratio of missing data to total data is less than a predefined threshold, defaulting to 1, to maintain the data quality necessary for reliable predictive strength in a machine learning model.\n",
       "\n",
       "**Test Mechanism**: The mechanism for this test involves iterating through each column of the dataset, counting missing values (represented as NaNs), and calculating the percentage they represent against the total number of rows. The test then checks if these missing value counts are less than the predefined `min_threshold`. The results are shown in a table summarizing each column, the number of missing values, the percentage of missing values in each column, and a Pass/Fail status based on the threshold comparison.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- When the number of missing values in any column exceeds the `min_threshold` value, it indicates a high risk.\n",
       "- A high risk is also flagged when missing values are present across many columns. In both instances, the test would return a \"Fail\" mark.\n",
       "\n",
       "**Strengths**:\n",
       "- The test offers a quick and granular identification of missing data across each feature in the dataset.\n",
       "- It provides an effective, straightforward means of maintaining data quality, which is vital for constructing efficient machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- Even though the test can efficiently identify missing values, it does not suggest the root causes of these missing values or recommend ways to impute or handle them.\n",
       "- The test might overlook features with a significant amount of missing data, but still less than the `min_threshold`. This could impact the model, especially if `min_threshold` is set too high.\n",
       "- The test does not account for data encoded as values (like \"-999\" or \"None\"), which might not technically classify as missing but could bear similar implications.</td>\n",
       "      <td id=\"T_fccf5_row76_col3\" class=\"data row76 col3\" >validmind.data_validation.MissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row77_col0\" class=\"data row77 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row77_col1\" class=\"data row77 col1\" >WOE Bin Table</td>\n",
       "      <td id=\"T_fccf5_row77_col2\" class=\"data row77 col2\" >**Purpose**: The Weight of Evidence (WoE) and Information Value (IV) test is intended to evaluate the predictive power of each feature in the machine learning model. The test generates binned groups of values from each feature in a dataset, computes the WoE value and the IV value for each bin. These values provide insights on the relationship between each feature and the target variable and their contribution towards the predictive output of the model.\n",
       "\n",
       "**Test Mechanism**: The metric leverages the `scorecardpy.woebin` method to perform WoE-based automatic binning on the dataset. Depending on the parameter `breaks_adj`, the method adjusts the cut-off points for binning numeric variables. The bins are then used to calculate the WoE and IV. The metric requires a dataset with the target variable defined. The metric outputs a dataframe that comprises the bin boundaries, WoE, and IV values for each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High IV values, which denote variables with too much predictive power which might lead to overfitting\n",
       "- Errors during the binning process, which might be due to inappropriate data types or poorly defined bins\n",
       "\n",
       "**Strengths**:\n",
       "- The WoE and IV test is highly effective for feature selection in binary classification problems, as it quantifies how much predictive information is packed within each feature regarding the binary outcome\n",
       "- The WoE transformation creates a monotonic relationship between the target and independent variables\n",
       "\n",
       "**Limitations**:\n",
       "- Mainly designed for binary classification tasks, therefore it might not be applicable or reliable for multi-class classification or regression tasks\n",
       "- If the dataset has many features or the features are not binnable or they are non-numeric, this process might encounter difficulties\n",
       "- This metric doesn't help in identifying if the predictive factor being observed is a coincidence or a real phenomenon due to data randomness</td>\n",
       "      <td id=\"T_fccf5_row77_col3\" class=\"data row77 col3\" >validmind.data_validation.WOEBinTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row78_col0\" class=\"data row78 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_fccf5_row78_col1\" class=\"data row78 col1\" >Skewness</td>\n",
       "      <td id=\"T_fccf5_row78_col2\" class=\"data row78 col2\" >**Purpose**: The purpose of the Skewness test is to measure the asymmetry in the distribution of data within a predictive machine learning model. Specifically, it evaluates the divergence of said distribution from a normal distribution. In understanding the level of skewness, we can potentially identify issues with data quality, an essential component for optimizing the performance of traditional machine learning models in both classification and regression settings.\n",
       "\n",
       "**Test Mechanism**: This test calculates skewness of numerical columns in a dataset, which is extracted from the DataFrame, specifically focusing on numerical data types. The skewness value is then contrasted against a predetermined maximum threshold, set by default to 1. The skewness value under review is deemed to have passed the test only if it is less than this maximum threshold; otherwise, the test is considered 'fail'. Subsequently, the test results of each column, together with the skewness value and column name, are cached.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- The presence of substantial skewness levels that significantly exceed the maximum threshold is an indication of skewed data distribution and subsequently high model risk.\n",
       "- Persistent skewness in data could signify that the foundational assumptions of the machine learning model may not be applicable, potentially leading to subpar model performance, erroneous predictions, or biased inferences.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Fast and efficient identification of unequal data\n",
       "- distributions within a machine learning model is enabled by the skewness test.\n",
       "- The maximum threshold parameter can be adjusted to meet the user's specific needs, enhancing the test's versatility.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The test only evaluates numeric columns, which means that data in non-numeric columns could still include bias or problematic skewness that this test does not capture.\n",
       "- The test inherently assumes that the data should follow a normal distribution, an expectation which may not always be met in real-world data.\n",
       "- The risk grading is largely dependent on a subjective threshold, which may result in excessive strictness or leniency depending upon selection. This factor might require expert input and recurrent iterations for refinement.</td>\n",
       "      <td id=\"T_fccf5_row78_col3\" class=\"data row78 col3\" >validmind.data_validation.Skewness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fccf5_row79_col0\" class=\"data row79 col0\" >Metric</td>\n",
       "      <td id=\"T_fccf5_row79_col1\" class=\"data row79 col1\" >Permutation Feature Importance</td>\n",
       "      <td id=\"T_fccf5_row79_col2\" class=\"data row79 col2\" >**Purpose**: The purpose of the Permutation Feature Importance (PFI) metric is to assess the importance of each feature used by the Machine Learning model. The significance is measured by evaluating the decrease in the model's performance when the feature's values are randomly arranged.\n",
       "\n",
       "**Test Mechanism**: PFI is calculated via the `permutation_importance` method from the `sklearn.inspection` module. This method shuffles the columns of the feature dataset and measures the impact on the model's performance. A significant decrease in performance after permutating a feature's values deems the feature as important. On the other hand, if performance remains the same, the feature is likely not important. The output of the PFI metric is a figure illustrating the importance of each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The model heavily relies on a feature with highly variable or easily permutable values, indicating instability.\n",
       "- A feature, deemed unimportant by the model but based on domain knowledge should have a significant effect on the outcome, is not influencing the model's predictions.\n",
       "\n",
       "**Strengths**:\n",
       "- PFI provides insights into the importance of different features and may reveal underlying data structure.\n",
       "- It can indicate overfitting if a particular feature or set of features overly impacts the model's predictions.\n",
       "- The metric is model-agnostic and can be used with any classifier that provides a measure of prediction accuracy before and after feature permutation.\n",
       "\n",
       "**Limitations**:\n",
       "- The feature importance calculated does not imply causality, it only presents the amount of information that a feature provides for the prediction task.\n",
       "- The metric does not account for interactions between features. If features are correlated, the permutation importance may allocate importance to one and not the other.\n",
       "- PFI cannot interact with certain libraries like statsmodels, pytorch, catboost, etc, thus limiting its applicability.</td>\n",
       "      <td id=\"T_fccf5_row79_col3\" class=\"data row79 col3\" >validmind.model_validation.sklearn.PermutationFeatureImportance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d59458d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.list_tests(filter=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're targeting a specific test or tests that match a particular task type, the `filter` parameter comes in handy. For example, to list tests for 'classification':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cf15d th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_cf15d_row0_col0, #T_cf15d_row0_col1, #T_cf15d_row0_col2, #T_cf15d_row0_col3, #T_cf15d_row1_col0, #T_cf15d_row1_col1, #T_cf15d_row1_col2, #T_cf15d_row1_col3, #T_cf15d_row2_col0, #T_cf15d_row2_col1, #T_cf15d_row2_col2, #T_cf15d_row2_col3, #T_cf15d_row3_col0, #T_cf15d_row3_col1, #T_cf15d_row3_col2, #T_cf15d_row3_col3, #T_cf15d_row4_col0, #T_cf15d_row4_col1, #T_cf15d_row4_col2, #T_cf15d_row4_col3, #T_cf15d_row5_col0, #T_cf15d_row5_col1, #T_cf15d_row5_col2, #T_cf15d_row5_col3, #T_cf15d_row6_col0, #T_cf15d_row6_col1, #T_cf15d_row6_col2, #T_cf15d_row6_col3, #T_cf15d_row7_col0, #T_cf15d_row7_col1, #T_cf15d_row7_col2, #T_cf15d_row7_col3, #T_cf15d_row8_col0, #T_cf15d_row8_col1, #T_cf15d_row8_col2, #T_cf15d_row8_col3, #T_cf15d_row9_col0, #T_cf15d_row9_col1, #T_cf15d_row9_col2, #T_cf15d_row9_col3, #T_cf15d_row10_col0, #T_cf15d_row10_col1, #T_cf15d_row10_col2, #T_cf15d_row10_col3, #T_cf15d_row11_col0, #T_cf15d_row11_col1, #T_cf15d_row11_col2, #T_cf15d_row11_col3, #T_cf15d_row12_col0, #T_cf15d_row12_col1, #T_cf15d_row12_col2, #T_cf15d_row12_col3, #T_cf15d_row13_col0, #T_cf15d_row13_col1, #T_cf15d_row13_col2, #T_cf15d_row13_col3, #T_cf15d_row14_col0, #T_cf15d_row14_col1, #T_cf15d_row14_col2, #T_cf15d_row14_col3, #T_cf15d_row15_col0, #T_cf15d_row15_col1, #T_cf15d_row15_col2, #T_cf15d_row15_col3, #T_cf15d_row16_col0, #T_cf15d_row16_col1, #T_cf15d_row16_col2, #T_cf15d_row16_col3, #T_cf15d_row17_col0, #T_cf15d_row17_col1, #T_cf15d_row17_col2, #T_cf15d_row17_col3, #T_cf15d_row18_col0, #T_cf15d_row18_col1, #T_cf15d_row18_col2, #T_cf15d_row18_col3, #T_cf15d_row19_col0, #T_cf15d_row19_col1, #T_cf15d_row19_col2, #T_cf15d_row19_col3, #T_cf15d_row20_col0, #T_cf15d_row20_col1, #T_cf15d_row20_col2, #T_cf15d_row20_col3, #T_cf15d_row21_col0, #T_cf15d_row21_col1, #T_cf15d_row21_col2, #T_cf15d_row21_col3, #T_cf15d_row22_col0, #T_cf15d_row22_col1, #T_cf15d_row22_col2, #T_cf15d_row22_col3, #T_cf15d_row23_col0, #T_cf15d_row23_col1, #T_cf15d_row23_col2, #T_cf15d_row23_col3, #T_cf15d_row24_col0, #T_cf15d_row24_col1, #T_cf15d_row24_col2, #T_cf15d_row24_col3, #T_cf15d_row25_col0, #T_cf15d_row25_col1, #T_cf15d_row25_col2, #T_cf15d_row25_col3, #T_cf15d_row26_col0, #T_cf15d_row26_col1, #T_cf15d_row26_col2, #T_cf15d_row26_col3, #T_cf15d_row27_col0, #T_cf15d_row27_col1, #T_cf15d_row27_col2, #T_cf15d_row27_col3, #T_cf15d_row28_col0, #T_cf15d_row28_col1, #T_cf15d_row28_col2, #T_cf15d_row28_col3, #T_cf15d_row29_col0, #T_cf15d_row29_col1, #T_cf15d_row29_col2, #T_cf15d_row29_col3, #T_cf15d_row30_col0, #T_cf15d_row30_col1, #T_cf15d_row30_col2, #T_cf15d_row30_col3, #T_cf15d_row31_col0, #T_cf15d_row31_col1, #T_cf15d_row31_col2, #T_cf15d_row31_col3, #T_cf15d_row32_col0, #T_cf15d_row32_col1, #T_cf15d_row32_col2, #T_cf15d_row32_col3, #T_cf15d_row33_col0, #T_cf15d_row33_col1, #T_cf15d_row33_col2, #T_cf15d_row33_col3, #T_cf15d_row34_col0, #T_cf15d_row34_col1, #T_cf15d_row34_col2, #T_cf15d_row34_col3, #T_cf15d_row35_col0, #T_cf15d_row35_col1, #T_cf15d_row35_col2, #T_cf15d_row35_col3, #T_cf15d_row36_col0, #T_cf15d_row36_col1, #T_cf15d_row36_col2, #T_cf15d_row36_col3, #T_cf15d_row37_col0, #T_cf15d_row37_col1, #T_cf15d_row37_col2, #T_cf15d_row37_col3, #T_cf15d_row38_col0, #T_cf15d_row38_col1, #T_cf15d_row38_col2, #T_cf15d_row38_col3, #T_cf15d_row39_col0, #T_cf15d_row39_col1, #T_cf15d_row39_col2, #T_cf15d_row39_col3, #T_cf15d_row40_col0, #T_cf15d_row40_col1, #T_cf15d_row40_col2, #T_cf15d_row40_col3, #T_cf15d_row41_col0, #T_cf15d_row41_col1, #T_cf15d_row41_col2, #T_cf15d_row41_col3, #T_cf15d_row42_col0, #T_cf15d_row42_col1, #T_cf15d_row42_col2, #T_cf15d_row42_col3, #T_cf15d_row43_col0, #T_cf15d_row43_col1, #T_cf15d_row43_col2, #T_cf15d_row43_col3, #T_cf15d_row44_col0, #T_cf15d_row44_col1, #T_cf15d_row44_col2, #T_cf15d_row44_col3, #T_cf15d_row45_col0, #T_cf15d_row45_col1, #T_cf15d_row45_col2, #T_cf15d_row45_col3, #T_cf15d_row46_col0, #T_cf15d_row46_col1, #T_cf15d_row46_col2, #T_cf15d_row46_col3, #T_cf15d_row47_col0, #T_cf15d_row47_col1, #T_cf15d_row47_col2, #T_cf15d_row47_col3, #T_cf15d_row48_col0, #T_cf15d_row48_col1, #T_cf15d_row48_col2, #T_cf15d_row48_col3, #T_cf15d_row49_col0, #T_cf15d_row49_col1, #T_cf15d_row49_col2, #T_cf15d_row49_col3, #T_cf15d_row50_col0, #T_cf15d_row50_col1, #T_cf15d_row50_col2, #T_cf15d_row50_col3, #T_cf15d_row51_col0, #T_cf15d_row51_col1, #T_cf15d_row51_col2, #T_cf15d_row51_col3, #T_cf15d_row52_col0, #T_cf15d_row52_col1, #T_cf15d_row52_col2, #T_cf15d_row52_col3, #T_cf15d_row53_col0, #T_cf15d_row53_col1, #T_cf15d_row53_col2, #T_cf15d_row53_col3, #T_cf15d_row54_col0, #T_cf15d_row54_col1, #T_cf15d_row54_col2, #T_cf15d_row54_col3, #T_cf15d_row55_col0, #T_cf15d_row55_col1, #T_cf15d_row55_col2, #T_cf15d_row55_col3, #T_cf15d_row56_col0, #T_cf15d_row56_col1, #T_cf15d_row56_col2, #T_cf15d_row56_col3, #T_cf15d_row57_col0, #T_cf15d_row57_col1, #T_cf15d_row57_col2, #T_cf15d_row57_col3, #T_cf15d_row58_col0, #T_cf15d_row58_col1, #T_cf15d_row58_col2, #T_cf15d_row58_col3, #T_cf15d_row59_col0, #T_cf15d_row59_col1, #T_cf15d_row59_col2, #T_cf15d_row59_col3, #T_cf15d_row60_col0, #T_cf15d_row60_col1, #T_cf15d_row60_col2, #T_cf15d_row60_col3, #T_cf15d_row61_col0, #T_cf15d_row61_col1, #T_cf15d_row61_col2, #T_cf15d_row61_col3, #T_cf15d_row62_col0, #T_cf15d_row62_col1, #T_cf15d_row62_col2, #T_cf15d_row62_col3, #T_cf15d_row63_col0, #T_cf15d_row63_col1, #T_cf15d_row63_col2, #T_cf15d_row63_col3, #T_cf15d_row64_col0, #T_cf15d_row64_col1, #T_cf15d_row64_col2, #T_cf15d_row64_col3, #T_cf15d_row65_col0, #T_cf15d_row65_col1, #T_cf15d_row65_col2, #T_cf15d_row65_col3, #T_cf15d_row66_col0, #T_cf15d_row66_col1, #T_cf15d_row66_col2, #T_cf15d_row66_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cf15d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_cf15d_level0_col0\" class=\"col_heading level0 col0\" >Test Type</th>\n",
       "      <th id=\"T_cf15d_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_cf15d_level0_col2\" class=\"col_heading level0 col2\" >Description</th>\n",
       "      <th id=\"T_cf15d_level0_col3\" class=\"col_heading level0 col3\" >ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row0_col0\" class=\"data row0 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row0_col1\" class=\"data row0 col1\" >Model Metadata</td>\n",
       "      <td id=\"T_cf15d_row0_col2\" class=\"data row0 col2\" >**Purpose:** This test is designed to collect and summarize important metadata related to a particular machine learning model. Such metadata includes the model's architecture (modeling technique), the version and type of modeling framework used, and the programming language the model is written in.\n",
       "\n",
       "**Test Mechanism:** The mechanism of this test consists of extracting information from the model instance. It tries to extract the model information such as the modeling technique used, the modeling framework version, and the programming language. It decorates this information into a data frame and returns a summary of the results.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- High risk could be determined by a lack of documentation or inscrutable metadata for the model.\n",
       "- Unidentifiable language, outdated or unsupported versions of modeling frameworks, or undisclosed model architectures reflect risky situations, as they could hinder future reproducibility, support, and debugging of the model.\n",
       "\n",
       "**Strengths:**\n",
       "- The strengths of this test lie in the increased transparency and understanding it brings regarding the model's setup.\n",
       "- Knowing the model's architecture, the specific modeling framework version used, and the language involved, provides multiple benefits: supports better error understanding and debugging, facilitates model reuse, aids compliance of software policies, and assists in planning for model obsolescence due to evolving or discontinuing software and dependencies.\n",
       "\n",
       "**Limitations:**\n",
       "- Notably, this test is largely dependent on the compliance and correctness of information provided by the model or the model developer.\n",
       "- If the model's built-in methods for describing its architecture, framework or language are incorrect or lack necessary information, this test will hold limitations.\n",
       "- Moreover, it is not designed to directly evaluate the performance or accuracy of the model, rather it provides supplementary information which aids in comprehensive analysis.</td>\n",
       "      <td id=\"T_cf15d_row0_col3\" class=\"data row0 col3\" >validmind.model_validation.ModelMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row1_col0\" class=\"data row1 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row1_col1\" class=\"data row1 col1\" >Regression Models Coeffs</td>\n",
       "      <td id=\"T_cf15d_row1_col2\" class=\"data row1 col2\" >**Purpose**: The 'RegressionModelsCoeffs' metric is utilized to evaluate and compare coefficients of different regression models trained on the same dataset. By examining how each model weighted the importance of different features during training, this metric provides key insights into which factors have the most impact on the model's predictions and how these patterns differ across models.\n",
       "\n",
       "**Test Mechanism**: The test operates by extracting the coefficients of each regression model using the 'regression_coefficients()' method. These coefficients are then consolidated into a dataframe, with each row representing a model and columns corresponding to each feature's coefficient. It must be noted that this test is exclusive to 'statsmodels' and 'R' models, other models will result in a 'SkipTestError'.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Discrepancies in how different models weight the same features\n",
       "- Unexpectedly high or low coefficients\n",
       "- The test is inapplicable to certain models because they are not from 'statsmodels' or 'R' libraries\n",
       "\n",
       "**Strengths**:\n",
       "- Enables insight into the training process of different models\n",
       "- Allows comparison of feature importance across models\n",
       "- Through the review of feature coefficients, the test provides a more transparent evaluation of the model and highlights significant weights and biases in the training procedure\n",
       "\n",
       "**Limitations**:\n",
       "- The test is only compatible with 'statsmodels' and 'R' regression models\n",
       "- While the test provides contrast in feature weightings among models, it does not establish the most appropriate or accurate weighting, thus remaining subject to interpretation\n",
       "- It does not account for potential overfitting or underfitting of models\n",
       "- The computed coefficients might not lead to effective performance on unseen data</td>\n",
       "      <td id=\"T_cf15d_row1_col3\" class=\"data row1 col3\" >validmind.model_validation.statsmodels.RegressionModelsCoeffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row2_col0\" class=\"data row2 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row2_col1\" class=\"data row2 col1\" >Box Pierce</td>\n",
       "      <td id=\"T_cf15d_row2_col2\" class=\"data row2 col2\" >**Purpose:** The Box-Pierce test is utilized to detect the presence of autocorrelation in a time-series dataset. Autocorrelation, or serial correlation, refers to the degree of similarity between observations based on the temporal spacing between them. This test is essential for affirming the quality of a time-series model by ensuring that the error terms in the model are random and do not adhere to a specific pattern.\n",
       "\n",
       "**Test Mechanism:** The implementation of the Box-Pierce test involves calculating a test statistic along with a corresponding p-value derived from the dataset features. These quantities are used to test the null hypothesis that posits the data to be independently distributed. This is achieved by iterating over every feature column in the time-series data and applying the `acorr_ljungbox` function of the statsmodels library. The function yields the Box-Pierce test statistic as well as the respective p-value, all of which are cached as test results.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- A low p-value, typically under 0.05 as per statistical convention, throws the null hypothesis of independence into question. This implies that the dataset potentially houses autocorrelations, thus indicating a high-risk scenario concerning model performance.\n",
       "- Large Box-Pierce test statistic values may indicate the presence of autocorrelation.\n",
       "\n",
       "**Strengths:**\n",
       "- Detects patterns in data that are supposed to be random, thereby ensuring no underlying autocorrelation.\n",
       "- Can be computed efficiently given its low computational complexity.\n",
       "- Can be widely applied to most regression problems, making it very versatile.\n",
       "\n",
       "**Limitations:**\n",
       "- Assumes homoscedasticity (constant variance) and normality of residuals, which may not always be the case in real-world datasets.\n",
       "- May exhibit reduced power for detecting complex autocorrelation schemes such as higher-order or negative correlations.\n",
       "- It only provides a general indication of the existence of autocorrelation, without providing specific insights into the nature or patterns of the detected autocorrelation.\n",
       "- In the presence of exhibits trends or seasonal patterns, the Box-Pierce test may yield misleading results.\n",
       "- Applicability is limited to time-series data, which limits its overall utility.</td>\n",
       "      <td id=\"T_cf15d_row2_col3\" class=\"data row2 col3\" >validmind.model_validation.statsmodels.BoxPierce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row3_col0\" class=\"data row3 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row3_col1\" class=\"data row3 col1\" >Regression Coeffs Plot</td>\n",
       "      <td id=\"T_cf15d_row3_col2\" class=\"data row3 col2\" >**Purpose**: The Regression Coefficients with Confidence Intervals plot and metric aims to understand the impact of predictor variables on the response variable in question. This understanding is achieved via the visualization and analysis of the regression model by presenting the coefficients derived from the model along with their associated 95% confidence intervals. By doing so, it offers insights into the variability and uncertainty associated with the model's estimates.\n",
       "\n",
       "**Test Mechanism**: The test begins by extracting the estimated coefficients and their related standard errors from the regression model under test. It then calculates and draws confidence intervals based on a 95% confidence level (a standard convention in statistics). These intervals provide a range wherein the true value can be expected to fall 95% of the time if the same regression were re-run multiple times with samples drawn from the same population. This information is then visualized as a bar plot, with the predictor variables and their coefficients on the x-axis and y-axis respectively and the confidence intervals represented as error bars.\n",
       "\n",
       "**Signs of High Risk**: * If the calculated confidence interval contains the zero value, it could mean the feature/coefficient in question doesn't significantly contribute to prediction in the model. * If there are multiple coefficients exhibiting this behavior, it might raise concerns about overall model reliability. * Very wide confidence intervals might indicate high uncertainty in the associated coefficient estimates.\n",
       "\n",
       "**Strengths**: * This metric offers a simple and easily comprehendible visualization of the significance and impact of individual predictor variables in a regression model. * By including confidence intervals, it enables an observer to evaluate the uncertainty around each coefficient estimate.\n",
       "\n",
       "**Limitations**: * The test is dependent on a few assumptions about the data, namely normality of residuals and independence of observations, which may not always be true for all types of datasets. * The test does not consider multi-collinearity (correlation among predictor variables), which can potentially distort the model and make interpretation of coefficients challenging. * The test's application is limited to regression tasks and tabular datasets and is not suitable for other types of machine learning assignments or data structures.</td>\n",
       "      <td id=\"T_cf15d_row3_col3\" class=\"data row3 col3\" >validmind.model_validation.statsmodels.RegressionCoeffsPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row4_col0\" class=\"data row4 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row4_col1\" class=\"data row4 col1\" >Regression Model Sensitivity Plot</td>\n",
       "      <td id=\"T_cf15d_row4_col2\" class=\"data row4 col2\" >**Purpose**: The Regression Sensitivity Plot metric is designed to perform sensitivity analysis on regression models. This metric aims to measure the impact of slight changes (shocks) applied to individual variables on the system's outcome while keeping all other variables constant. By doing so, it analyzes the effects of each independent variable on the dependent variable within the regression model and helps identify significant risk factors that could substantially influence the model's output.\n",
       "\n",
       "**Test Mechanism**: This metric operates by initially applying shocks of varying magnitudes, defined by specific parameters, to each of the model's features, one at a time. With all other variables held constant, a new prediction is made for each dataset subjected to shocks. Any changes in the model's predictions are directly attributed to the shocks applied. In the event that the transformation parameter is set to \"integrate\", initial predictions and target values undergo transformation via an integration function before being plotted. Lastly, a plot demonstrating observed values against predicted values for each model is generated, showcasing a distinct line graph illustrating predictions for each shock.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If the plot exhibits drastic alterations in model predictions consequent to minor shocks to an individual variable, it may indicate high risk. This underscores potentially high model sensitivity to changes in that variable, suggesting over-dependence on that variable for predictions.\n",
       "- Unusually high or unpredictable shifts in response to shocks may also denote potential model instability or overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric allows identification of variables strongly influencing the model outcomes, paving the way for understanding feature importance.\n",
       "- It generates visual plots which make the results easily interpretable even to non-technical stakeholders.\n",
       "- Beneficial in identifying overfitting and detecting unstable models that over-react to minor changes in variables.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric operates on the assumption that all other variables remain unchanged during the application of a shock. However, real-world situations where variables may possess intricate interdependencies may not always reflect this.\n",
       "- It is best compatible with linear models and may not effectively evaluate the sensitivity of non-linear model configurations.\n",
       "- The metric does not provide a numerical risk measure. It offers only a visual representation, which may invite subjectivity in interpretation.</td>\n",
       "      <td id=\"T_cf15d_row4_col3\" class=\"data row4 col3\" >validmind.model_validation.statsmodels.RegressionModelSensitivityPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row5_col0\" class=\"data row5 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row5_col1\" class=\"data row5 col1\" >Regression Models Performance</td>\n",
       "      <td id=\"T_cf15d_row5_col2\" class=\"data row5 col2\" >**Purpose**: This metric is used to evaluate and compare the performance of various regression models. Through the use of key statistical measures such as R-squared, Adjusted R-squared, and Mean Squared Error (MSE), the performance of different models in predicting dependent variables can be assessed both on the data used for training (in-sample) and new, unseen data (out-of-sample).\n",
       "\n",
       "**Test Mechanism**: The test evaluates a list of provided regression models. For each model, it calculates their in-sample and out-of-sample performance by deriving the model predictions for the training and testing datasets respectively, and then comparing these predictions to the actual values. In doing so, it calculates R-squared, Adjusted R-squared, and MSE for each model, stores the results, and returns them for comparison.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High Mean Squared Error (MSE) values.\n",
       "- Strikingly low values of R-squared and Adjusted R-squared.\n",
       "- A significant drop in performance when transitioning from in-sample to out-of-sample evaluations, signaling a potential overfitting issue.\n",
       "\n",
       "**Strengths**:\n",
       "- The test permits comparisons of multiple models simultaneously, providing an objective base for identifying the top-performing model.\n",
       "- It delivers both in-sample and out-of-sample evaluations, presenting performance data on unseen data.\n",
       "- The utilization of R-squared and Adjusted R-squared in conjunction with MSE allows for a detailed view of the model's explainability and error rate.\n",
       "\n",
       "**Limitations**:\n",
       "- This test is built around the assumption that the residuals of the regression model are normally distributed, which is a fundamental requirement for Ordinary Least Squares (OLS) regression; thus, it could be not suitable for models where this assumption is broken.\n",
       "- The test does not consider cases where higher R-squared or lower MSE values do not necessarily correlate with better predictive performance, particularly in instances of excessively complex models.</td>\n",
       "      <td id=\"T_cf15d_row5_col3\" class=\"data row5 col3\" >validmind.model_validation.statsmodels.RegressionModelsPerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row6_col0\" class=\"data row6 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row6_col1\" class=\"data row6 col1\" >Zivot Andrews Arch</td>\n",
       "      <td id=\"T_cf15d_row6_col2\" class=\"data row6 col2\" >**Purpose**: The Zivot-Andrews Arch metric is used to evaluate the order of integration for a time series data in a machine learning model. It's designed to test for stationarity, a crucial aspect in time series analysis where data points are not dependent on time. Stationarity means that the statistical properties such as mean, variance and autocorrelation are all constant over time.\n",
       "\n",
       "**Test Mechanism**: The Zivot-Andrews unit root test is performed on each feature in the dataset using the `ZivotAndrews` function from the `arch.unitroot` module. This function returns the Zivot-Andrews metric for each feature, which includes the statistical value, p-value (probability value), the number of used lags, and the number of observations. The p-value is later used to decide on the null hypothesis (the time series has a unit root and is non-stationary) based on a chosen level of significance.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high p-value can suggest high risk. This might indicate that there's insufficient evidence to reject the null hypothesis, which would mean the time series has a unit root and is therefore non-stationary.\n",
       "- Non-stationary time series data can lead to misleading statistics and unreliable machine learning models.\n",
       "\n",
       "**Strengths**:\n",
       "- The Zivot-Andrews Arch metric dynamically tests for stationarity against structural breaks in time series data, offering robust evaluation of stationarity in features.\n",
       "- This metric is especially beneficial with financial, economic, or other time-series data where data observations lack a consistent pattern and structural breaks may occur.\n",
       "\n",
       "**Limitations**:\n",
       "- The Zivot-Andrews Arch metric assumes that data is derived from a single-equation, autoregressive model. It may, therefore, not be appropriate for multivariate time series data or data which does not align with the autoregressive model assumption.\n",
       "- It might not take into account unexpected shocks or changes in the series trend which can both have a significant impact on the stationarity of the data.</td>\n",
       "      <td id=\"T_cf15d_row6_col3\" class=\"data row6 col3\" >validmind.model_validation.statsmodels.ZivotAndrewsArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row7_col0\" class=\"data row7 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row7_col1\" class=\"data row7 col1\" >Regression Model Outsample Comparison</td>\n",
       "      <td id=\"T_cf15d_row7_col2\" class=\"data row7 col2\" >**Purpose**: The RegressionModelOutsampleComparison test is designed to evaluate the predictive performance of multiple regression models by means of an out-of-sample test. The primary aim of this test is to validate the model's ability to generalize to unseen data, a common challenge in the context of overfitting. It does this by computing two critical metrics — Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), which provide a quantifiable measure of the model's prediction accuracy on the testing dataset.\n",
       "\n",
       "**Test Mechanism**: This test requires multiple models (specifically Ordinary Least Squares\n",
       "- OLS regression models) and a test dataset as inputs. Each model generates predictions using the test dataset. The residuals are then calculated and used to compute the MSE and RMSE for each model. The test outcomes, which include the model's name, its MSE, and RMSE, are recorded and returned in a structured dataframe format.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High values of MSE or RMSE indicate significant risk, signifying that the model's predictions considerably deviate from the actual values in the test dataset.\n",
       "- Consistently large discrepancies between training and testing performance across various models may indicate an issue with the input data itself or the model selection strategies employed.\n",
       "\n",
       "**Strengths**:\n",
       "- This test offers a comparative evaluation of multiple models' out-of-sample performance, enabling the selection of the best performing model.\n",
       "- The use of both MSE and RMSE provides insights into the model's prediction error. While MSE is sensitive to outliers, emphasizing larger errors, RMSE provides a more interpretable measure of average prediction error given that it's in the same unit as the dependent variable.\n",
       "\n",
       "**Limitations**:\n",
       "- The applicability of this test is limited to regression tasks, specifically OLS models.\n",
       "- The test operates under the assumption that the test dataset is a representative sample of the population. This might not always hold true and can result in less accurate insights.\n",
       "- The interpretability and the objectivity of the output (MSE and RMSE) can be influenced when the scale of the dependent variable varies significantly, or the distribution of residuals is heavily skewed or contains outliers.</td>\n",
       "      <td id=\"T_cf15d_row7_col3\" class=\"data row7 col3\" >validmind.model_validation.statsmodels.RegressionModelOutsampleComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row8_col0\" class=\"data row8 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row8_col1\" class=\"data row8 col1\" >Regression Model Forecast Plot Levels</td>\n",
       "      <td id=\"T_cf15d_row8_col2\" class=\"data row8 col2\" >**Purpose:** The `RegressionModelForecastPlotLevels` metric is designed to visually assess a series of regression models' performance. It achieves this by contrasting the models' forecasts with the observed data from the respective training and test datasets. The gauge of accuracy here involves determining the extent of closeness between forecasted and actual values. Accordingly, if any transformations are specified, the metric will handle transforming the data before making this comparison.\n",
       "\n",
       "**Test Mechanism:** The `RegressionModelForecastPlotLevels` class in Python initiates with a `transformation` parameter, which default aggregates to None. Initially, the class checks for the presence of model objects and raises a `ValueError` if none are found. Each model is then processed, creating predictive forecasts for both its training and testing datasets. These forecasts are then contrasted with the actual values and plotted. In situations where a specified transformation, like \"integrate,\" is specified, the class navigates the transformation steps (performing cumulative sums to generate a novel series, for instance). Finally, plots are produced that compare observed and forecasted values for both the raw and transformed datasets.\n",
       "\n",
       "**Signs of High Risk:** Indications of high risk or failure in the model's performance can be derived from checking the generated plots. When the forecasted values dramatically deviate from the observed values in either the training or testing datasets, it suggests a high risk situation. A significant deviation could be a symptom of either overfitting or underfitting, both scenarios are worrying. Such discrepancies could inhibit the model's ability to create precise, generalized results.\n",
       "\n",
       "**Strengths:**\n",
       "- Visual Evaluations: The metric provides a visual and comparative way of assessing multiple regression models at once. This allows easier interpretation and evaluation of their forecasting accuracy.\n",
       "- Transformation Handling: This metric can handle transformations like \"integrate,\" enhancing its breadth and flexibility in evaluating different models.\n",
       "- Detailed Perspective: By looking at the performance on both datasets (training and testing), the metric may give a detailed overview of the model.\n",
       "\n",
       "**Limitations:**\n",
       "- Subjectivity: Relying heavily on visual interpretations; assessments may differ from person to person.\n",
       "- Limited Transformation Capability: Currently, only the \"integrate\" transformation is supported, implying complex transformations might go unchecked or unhandled.\n",
       "- Overhead: The plotting mechanism may become computationally costly when applying to extensive datasets, increasing runtime.\n",
       "- Numerical Measurement: Although visualization is instrumental, a corresponding numerical measure would further reinforce the observations. However, this metric does not provide numerical measures.</td>\n",
       "      <td id=\"T_cf15d_row8_col3\" class=\"data row8 col3\" >validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row9_col0\" class=\"data row9 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row9_col1\" class=\"data row9 col1\" >Feature Importance And Significance</td>\n",
       "      <td id=\"T_cf15d_row9_col2\" class=\"data row9 col2\" >**Purpose**: The 'FeatureImportanceAndSignificance' test evaluates the statistical significance and the importance of features in the context of the machine learning model. By comparing the p-values from a regression model and the feature importances from a decision tree model, this test aids in determining the most significant variables from a statistical and a machine learning perspective, assisting in feature selection during the model development process.\n",
       "\n",
       "**Test Mechanism**: The test first compares the p-values from a regression model and the feature importances from a decision tree model. These values are normalized to ensure a uniform comparison. The 'p_threshold' parameter is used to determine what p-value is considered statistically significant and if the 'significant_only' parameter is true, only features with p-values below this threshold are included in the final output. The output from this test includes an interactive visualization displaying normalized p-values and the associated feature importances. The test throws an error if it does not receive both a regression model and a decision tree model.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Exceptionally high or low p-values, which suggest that a feature may not be significant or meaningful in the context of the model.\n",
       "- If many variables with small feature importance values have significant p-values, this could indicate that the model might be overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- Combines two perspectives statistical significance (p-values) and feature importance (decision tree model), making it a robust feature selection test.\n",
       "- Provides an interactive visualization making it easy to interpret and understand the results.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only works with a regression model and a decision tree model which may limit its applicability.\n",
       "- The test does not take into account potential correlations or causative relationships between features which may lead to misinterpretations of significance and importance.\n",
       "- Over-reliance on the p-value as a cut-off for feature significance can be seen as arbitrary and may not truly reflect the real-world importance of the feature.</td>\n",
       "      <td id=\"T_cf15d_row9_col3\" class=\"data row9 col3\" >validmind.model_validation.statsmodels.FeatureImportanceAndSignificance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row10_col0\" class=\"data row10 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row10_col1\" class=\"data row10 col1\" >L Jung Box</td>\n",
       "      <td id=\"T_cf15d_row10_col2\" class=\"data row10 col2\" >**Purpose**: The Ljung-Box test is a type of statistical test utilized to ascertain whether there are autocorrelations within a given dataset that differ significantly from zero. In the context of a machine learning model, this test is primarily used to evaluate data utilized in regression tasks, especially those involving time series and forecasting.\n",
       "\n",
       "**Test Mechanism**: The test operates by iterating over each feature within the training dataset and applying the `acorr_ljungbox` function from the `statsmodels.stats.diagnostic` library. This function calculates the Ljung-Box statistic and p-value for each feature. These results are then stored in a dictionary where the keys are the feature names and the values are dictionaries containing the statistic and p-value respectively. Generally, a lower p-value indicates a higher likelihood of significant autocorrelations within the feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk or failure in the model's performance relating to this test might be indicated by high Ljung-Box statistic values or low p-values.\n",
       "- These outcomes suggest the presence of significant autocorrelations in the respective features. If not properly consider or handle in the machine learning model, these can negatively affect model performance or bias.\n",
       "\n",
       "**Strengths**:\n",
       "- The Ljung-Box test is a powerful tool for detecting autocorrelations within datasets, especially in time series data.\n",
       "- It provides quantitative measures (statistic and p-value) that allow for precise evaluation of autocorrelation.\n",
       "- This test can be instrumental in avoiding issues related to autoregressive residuals and other challenges in regression models.\n",
       "\n",
       "**Limitations**:\n",
       "- The Ljung-Box test cannot detect all types of non-linearity or complex interrelationships among variables.\n",
       "- Testing individual features may not fully encapsulate the dynamics of the data if features interact with each other.\n",
       "- It is designed more for traditional statistical models and may not be fully compatible with certain types of complex machine learning models.</td>\n",
       "      <td id=\"T_cf15d_row10_col3\" class=\"data row10 col3\" >validmind.model_validation.statsmodels.LJungBox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row11_col0\" class=\"data row11 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row11_col1\" class=\"data row11 col1\" >Jarque Bera</td>\n",
       "      <td id=\"T_cf15d_row11_col2\" class=\"data row11 col2\" >**Purpose**: The purpose of the Jarque-Bera test as implemented in this metric is to determine if the features in the dataset of a given Machine Learning model follows a normal distribution. This is crucial for understanding the distribution and behavior of the model's features, as numerous statistical methods assume normal distribution of the data.\n",
       "\n",
       "**Test Mechanism**: The test mechanism involves computing the Jarque-Bera statistic, p-value, skew, and kurtosis for each feature in the dataset. It utilizes the 'jarque_bera' function from the 'statsmodels' library in Python, storing the results in a dictionary. The test evaluates the skewness and kurtosis to ascertain whether the dataset follows a normal distribution. A significant p-value (typically less than 0.05) implies that the data does not possess normal distribution.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high Jarque-Bera statistic and a low p-value (usually less than 0.05) indicates high-risk conditions.\n",
       "- Such results suggest the data significantly deviates from a normal distribution. If a machine learning model expects feature data to be normally distributed, these findings imply that it may not function as intended.\n",
       "\n",
       "**Strengths**:\n",
       "- This test provides insights into the shape of the data distribution, helping determine whether a given set of data follows a normal distribution.\n",
       "- This is particularly useful for risk assessment for models that assume a normal distribution of data.\n",
       "- By measuring skewness and kurtosis, it provides additional insights into the nature and magnitude of a distribution's deviation.\n",
       "\n",
       "**Limitations**:\n",
       "- The Jarque-Bera test only checks for normality in the data distribution. It cannot provide insights into other types of distributions.\n",
       "- Datasets that aren't normally distributed but follow some other distribution might lead to inaccurate risk assessments.\n",
       "- The test is highly sensitive to large sample sizes, often rejecting the null hypothesis (that data is normally distributed) even for minor deviations in larger datasets.</td>\n",
       "      <td id=\"T_cf15d_row11_col3\" class=\"data row11 col3\" >validmind.model_validation.statsmodels.JarqueBera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row12_col0\" class=\"data row12 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row12_col1\" class=\"data row12 col1\" >Phillips Perron Arch</td>\n",
       "      <td id=\"T_cf15d_row12_col2\" class=\"data row12 col2\" >**Purpose**: The Phillips-Perron (PP) test is used to establish the order of integration in time series data, testing a null hypothesis that a time series is unit-root non-stationary. This is vital in forecasting and understanding the stochastic behavior of data within machine learning models. Essentially, the PP test aids in confirming the robustness of results and generating valid predictions from regression analysis models.\n",
       "\n",
       "**Test Mechanism**: The PP test is conducted for each feature in the dataset. A data frame is created from the dataset, and for each column in this frame, the PhillipsPerron method calculates the statistic value, p-value, used lags, and number of observations. This process computes the PP metric for each feature and stores the results for future reference.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high P-value could imply that the series has a unit root and is therefore non-stationary.\n",
       "- Test statistic values that surpass critical values indicate additional evidence of non-stationarity.\n",
       "- A high 'usedlag' value for a series could point towards autocorrelation issues which could further impede the model's performance.\n",
       "\n",
       "**Strengths**:\n",
       "- Resilience against heteroskedasticity in the error term is a significant strength of the PP test.\n",
       "- Its capacity to handle long time series data.\n",
       "- Its ability to determine whether the time series is stationary or not, influencing the selection of suitable models for forecasting.\n",
       "\n",
       "**Limitations**:\n",
       "- The PP test can only be employed within a univariate time series framework.\n",
       "- The test relies on asymptotic theory, which means the test's power can significantly diminish for small sample sizes.\n",
       "- The need to convert non-stationary time series into stationary series through differencing might lead to loss of vital data points.</td>\n",
       "      <td id=\"T_cf15d_row12_col3\" class=\"data row12 col3\" >validmind.model_validation.statsmodels.PhillipsPerronArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row13_col0\" class=\"data row13 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row13_col1\" class=\"data row13 col1\" >Kolmogorov Smirnov</td>\n",
       "      <td id=\"T_cf15d_row13_col2\" class=\"data row13 col2\" >**Purpose**: This metric employs the Kolmogorov-Smirnov (KS) test to evaluate the distribution of a dataset's features. It specifically gauges whether the data from each feature aligns with a normal distribution, a common presumption in many statistical methods and machine learning models.\n",
       "\n",
       "**Test Mechanism**: This KS test calculates the KS statistic and the corresponding p-value for each column in a dataset. It achieves this by contrasting the cumulative distribution function of the dataset's feature with an ideal normal distribution. Subsequently, a feature-by-feature KS statistic and p-value are stored in a dictionary. The specific threshold p-value (the value below which we reject the hypothesis that the data is drawn from a normal distribution) is not firmly set within this implementation, allowing for definitional flexibility depending on the specific application.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Elevated KS statistic for a feature combined with a low p-value. This suggests a significant divergence between the feature's distribution and a normal one.\n",
       "- Features with notable deviations. These could create problems if the applicable model makes assumptions about normal data distribution, thereby representing a risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The KS test is acutely sensitive to differences in the location and shape of the empirical cumulative distribution functions of two samples.\n",
       "- It is non-parametric and does not presuppose any specific data distribution, making it adaptable to a range of datasets.\n",
       "- With its focus on individual features, it offers detailed insights into data distribution.\n",
       "\n",
       "**Limitations**:\n",
       "- The sensitivity of the KS test to disparities in data distribution tails can be excessive. Such sensitivity might prompt false alarms about non-normal distributions, particularly in situations where these tail tendencies are irrelevant to the model.\n",
       "- It could become less effective when applied to multivariate distributions, considering that it's primarily configured for univariate distributions.\n",
       "- As a goodness-of-fit test, the KS test does not identify specific types of non-normality, such as skewness or kurtosis, that could directly impact model fitting.</td>\n",
       "      <td id=\"T_cf15d_row13_col3\" class=\"data row13 col3\" >validmind.model_validation.statsmodels.KolmogorovSmirnov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row14_col0\" class=\"data row14 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row14_col1\" class=\"data row14 col1\" >Residuals Visual Inspection</td>\n",
       "      <td id=\"T_cf15d_row14_col2\" class=\"data row14 col2\" >**Purpose**: The main purpose of this metric is to visualize and analyze the residuals (the differences between the observed and predicted values) of a regression problem. It allows for a graphical exploration of the model's errors, helping to identify statistical patterns or anomalies that may indicate a systematic bias in the model's predictions. By inspecting the residuals, we can check how well the model fits the data and meets the assumptions of the model.\n",
       "\n",
       "**Test Mechanism**: The metric generates four common types of residual plots which are: a histogram with kernel density estimation, a quantile-quantile (Q-Q) plot, a residuals series dot plot, and an autocorrelation function (ACF) plot.\n",
       "\n",
       "- The residuals histogram with kernel density estimation visualizes the distribution of residuals and allows to check if they are normally distributed.\n",
       "- Q-Q plot compares the observed quantiles of the data to the quantiles of a standard normal distribution, helping to assess the normality of residuals.\n",
       "- A residuals dot plot indicates the variation in residuals over time, which helps in identifying any time-related pattern in residuals.\n",
       "- ACF plot visualizes the correlation of an observation with its previous observations, helping to pinpoint any seasonality effect within residuals.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Skewness or asymmetry in the histogram or a significant deviation from the straight line in the Q-Q plot, which indicates that the residuals aren't normally distributed.\n",
       "- Large spikes in the ACF plot, indicating that the residuals are correlated, in violation of the assumption that they are independent.\n",
       "- Non-random patterns in the dot plot of residuals, indicating potential model misspecification.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Visual analysis of residuals is a powerful yet simple way to understand a model’s behavior across the data set and to identify problems with the model's assumptions or its fit to the data.\n",
       "- The test is applicable to any regression model, irrespective of complexity.\n",
       "- By exploring residuals, we might uncover relationships that were not captured by the model, revealing opportunities for model improvement.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Visual tests are largely subjective and can be open to interpretation. Clear-cut decisions about the model based solely on these plots may not be possible.\n",
       "- The metrics from the test do not directly infer the action based on the results; domain-specific knowledge and expert judgement is often required to interpret the results.\n",
       "- These plots can indicate a problem with the model but they do not necessarily reveal the nature or cause of the problem.\n",
       "- The test assumes that the error terms are identically distributed, which might not always be the case in real-world scenarios.</td>\n",
       "      <td id=\"T_cf15d_row14_col3\" class=\"data row14 col3\" >validmind.model_validation.statsmodels.ResidualsVisualInspection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row15_col0\" class=\"data row15 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row15_col1\" class=\"data row15 col1\" >Shapiro Wilk</td>\n",
       "      <td id=\"T_cf15d_row15_col2\" class=\"data row15 col2\" >**Purpose**: The Shapiro-Wilk test is utilized to investigate whether a particular dataset conforms to the standard normal distribution. This analysis is crucial in machine learning modeling because the normality of the data can profoundly impact the performance of the model. This metric is especially useful in evaluating various features of the dataset in both classification and regression tasks.\n",
       "\n",
       "**Test Mechanism**: The Shapiro-Wilk test is conducted on each feature column of the training dataset to determine if the data contained fall within the normal distribution. The test presents a statistic and a p-value, with the p-value serving to validate or repudiate the null hypothesis, which is that the tested data is normally distributed.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A p-value that falls below 0.05 signifies a high risk as it discards the null hypothesis, indicating that the data does not adhere to the normal distribution.\n",
       "- For machine learning models built on the presumption of data normality, such an outcome could result in subpar performance or incorrect predictions.\n",
       "\n",
       "**Strengths**:\n",
       "- The Shapiro-Wilk test is esteemed for its level of accuracy, thereby making it particularly well-suited to datasets of small to moderate sizes.\n",
       "- It proves its versatility through its efficient functioning in both classification and regression tasks.\n",
       "- By separately testing each feature column, the Shapiro-Wilk test can raise an alarm if a specific feature does not comply with the normality.\n",
       "\n",
       "**Limitations**:\n",
       "- The Shapiro-Wilk test's sensitivity can be a disadvantage as it often rejects the null hypothesis (i.e., data is normally distributed), even for minor deviations, especially in large datasets. This may lead to unwarranted 'false alarms' of high risk by deeming the data as not normally distributed even if it approximates normal distribution.\n",
       "- Exceptional care must be taken in managing missing data or outliers prior to testing as these can greatly skew the results.\n",
       "- Lastly, the Shapiro-Wilk test is not optimally suited for processing data with pronounced skewness or kurtosis.</td>\n",
       "      <td id=\"T_cf15d_row15_col3\" class=\"data row15 col3\" >validmind.model_validation.statsmodels.ShapiroWilk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row16_col0\" class=\"data row16 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row16_col1\" class=\"data row16 col1\" >Regression Model Insample Comparison</td>\n",
       "      <td id=\"T_cf15d_row16_col2\" class=\"data row16 col2\" >**Purpose**: The RegressionModelInsampleComparison test metric is utilized to evaluate and compare the performance of multiple regression models trained on the same dataset. Key performance indicators for this comparison include statistics related to the goodness of fit\n",
       "- R-Squared, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
       "\n",
       "**Test Mechanism**: The methodology behind this test is as follows\n",
       "- - Firstly, a verification that the list of models to be tested is indeed not empty occurs.\n",
       "- Once confirmed, the In-Sample performance of the models is calculated by a private function, `_in_sample_performance_ols`, that executes the following steps:\n",
       "- Iterates through each model in the supplied list.\n",
       "- For each model, the function extracts the features (`X`) and the target (`y_true`) from the training dataset and computes the predicted target values (`y_pred`).\n",
       "- The performance metrics for the model are calculated using formulas for R-Squared, Adjusted R-Squared, MSE, and RMSE.\n",
       "- The results, including the computed metrics, variables of the model, and the model's identifier, are stored in a dictionary that is appended to a list.\n",
       "- The collected results are finally returned as a pandas dataframe.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Significantly low values for R-Squared or Adjusted R-Squared.\n",
       "- Significantly high values for MSE and RMSE. Please note that what constitutes as \"low\" or \"high\" will vary based on the specific context or domain in which the model is being utilized.\n",
       "\n",
       "**Strengths**:\n",
       "- Enables comparison of in-sample performance across different models on the same dataset, providing insights into which model fits the data the best.\n",
       "- Utilizes multiple evaluation methods (R-Squared, Adjusted R-Squared, MSE, RMSE), offering a comprehensive review of a model's performance.\n",
       "\n",
       "**Limitations**:\n",
       "- The test measures only in-sample performance, i.e., how well a model fits the data it was trained on. However, it does not give any information on the performance of the model on new, unseen, or out-of-sample data.\n",
       "- Higher in-sample performance might be a result of overfitting, where the model is just memorizing the training data. This test is sensitive to such cases.\n",
       "- The test does not consider additional key factors such as the temporal dynamics of the data, that is, the pattern of changes in data over time.\n",
       "- The test does not provide an automated mechanism to determine if the reported metrics are within acceptable ranges, necessitating human judgment.</td>\n",
       "      <td id=\"T_cf15d_row16_col3\" class=\"data row16 col3\" >validmind.model_validation.statsmodels.RegressionModelInsampleComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row17_col0\" class=\"data row17 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row17_col1\" class=\"data row17 col1\" >Regression Feature Significance</td>\n",
       "      <td id=\"T_cf15d_row17_col2\" class=\"data row17 col2\" >**Purpose**: The Regression Feature Significance metric assesses the significance of each feature in a given set of regression models. It creates a visualization displaying p-values for every feature of each model, assisting model developers in understanding which features are most influential in their models.\n",
       "\n",
       "**Test Mechanism**: The test mechanism involves going through each fitted regression model in a given list, extracting the model coefficients and p-values for each feature, and then plotting these values. The x-axis on the plot contains the p-values while the y-axis denotes the coefficients of each feature. A vertical red line is drawn at the threshold for p-value significance, which is 0.05 by default. Any features with p-values to the left of this line are considered statistically significant at the chosen level.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Any feature with a high p-value (greater than the threshold) is considered a potential high risk, as it suggests the feature is not statistically significant and may not be reliably contributing to the model's predictions.\n",
       "- A high number of such features may indicate problems with the model validation, variable selection, and overall reliability of the model predictions.\n",
       "\n",
       "**Strengths**:\n",
       "- Helps identify the features that significantly contribute to a model's prediction, providing insights into the feature importance.\n",
       "- Provides tangible, easy-to-understand visualizations to interpret the feature significance.\n",
       "- Facilitates comparison of feature importance across multiple models.\n",
       "\n",
       "**Limitations**:\n",
       "- This metric assumes model features are independent, which may not always be the case. Multicollinearity (high correlation amongst predictors) can cause high variance and unreliable statistical tests of significance.\n",
       "- The p-value strategy for feature selection doesn't take into account the magnitude of the effect, focusing solely on whether the feature is likely non-zero.\n",
       "- This test is specific to regression models and wouldn't be suitable for other types of ML models.\n",
       "- P-value thresholds are somewhat arbitrary and do not always indicate practical significance, only statistical significance.</td>\n",
       "      <td id=\"T_cf15d_row17_col3\" class=\"data row17 col3\" >validmind.model_validation.statsmodels.RegressionFeatureSignificance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row18_col0\" class=\"data row18 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row18_col1\" class=\"data row18 col1\" >Regression Model Summary</td>\n",
       "      <td id=\"T_cf15d_row18_col2\" class=\"data row18 col2\" >**Purpose**: This metric test evaluates the performance of regression models by measuring their predictive ability with regards to dependent variables given changes in the independent variables. Its measurement tools include conventional regression metrics such as R-Squared, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
       "\n",
       "**Test Mechanism**: This test employs the 'train_ds' attribute of the model to gather and analyze the training data. Initially, it fetches the independent variables and uses the model to make predictions on these given features. Subsequently, it calculates several standard regression performance metrics including R-Square, Adjusted R-Squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE), which quantify the approximation of the predicted responses to the actual responses.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low R-Squared and Adjusted R-Squared values. A poor fit between the model predictions and the true responses is indicated by low values of these metrics, suggesting the model explains a small fraction of the variance in the target variable.\n",
       "- High MSE and RMSE values represent a high prediction error and point to poor model performance.\n",
       "\n",
       "**Strengths**:\n",
       "- Offers an extensive evaluation of regression models by combining four key measures of model accuracy and fit.\n",
       "- Provides a comprehensive view of the model's performance.\n",
       "- Both the R-Squared and Adjusted R-Squared measures are readily interpretable. They represent the proportion of total variation in the dependent variable that can be explained by the independent variables.\n",
       "\n",
       "**Limitations**:\n",
       "- Applicable exclusively to regression models. It is not suited for evaluating binary classification models or time series models, thus limiting its scope.\n",
       "- Although RMSE and MSE are sound measures of prediction error, they might be sensitive to outliers, potentially leading to an overestimation of the model's prediction error.\n",
       "- A high R-squared or adjusted R-squared may not necessarily indicate a good model, especially in cases where the model is possibly overfitting the data.</td>\n",
       "      <td id=\"T_cf15d_row18_col3\" class=\"data row18 col3\" >validmind.model_validation.statsmodels.RegressionModelSummary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row19_col0\" class=\"data row19 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row19_col1\" class=\"data row19 col1\" >KPSS</td>\n",
       "      <td id=\"T_cf15d_row19_col2\" class=\"data row19 col2\" >**Purpose**: The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test is utilized to ensure the stationarity of data within the machine learning model. It specifically works on time-series data to establish the order of integration, which is a prime requirement for accurate forecasting, given the fundamental condition for any time series model is that the series should be stationary.\n",
       "\n",
       "**Test Mechanism**: This metric evaluates the KPSS score for every feature present in the dataset. Within the KPSS score, there are various components, namely: a statistic, a p-value, a used lag, and critical values. The core scheme behind the KPSS score is to test the hypothesis that an observable time series is stationary around a deterministic trend. If the computed statistic surpasses the critical value, the null hypothesis is dismissed, inferring the series is non-stationary.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High KPSS score represents a considerable risk, particularly if the calculated statistic is higher than the critical value.\n",
       "- If the null hypothesis is rejected and the series is recognized as non-stationary, it heavily influences the model's forecasting capability rendering it less effective.\n",
       "\n",
       "**Strengths**:\n",
       "- The KPSS test directly measures the stationarity of a series, allowing it to fulfill a key prerequisite for many time-series models, making it a valuable tool for model validation.\n",
       "- The logics underpinning the test are intuitive and simple, making it understandable and accessible for developers and risk management teams.\n",
       "\n",
       "**Limitations**:\n",
       "- The KPSS test presumes the absence of a unit root in the series and does not differentiate between series that are stationary and those border-lining stationarity.\n",
       "- The test might show restricted power against specific alternatives.\n",
       "- The reliability of the test is contingent on the number of lags selected, which introduces potential bias in the measurement.</td>\n",
       "      <td id=\"T_cf15d_row19_col3\" class=\"data row19 col3\" >validmind.model_validation.statsmodels.KPSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row20_col0\" class=\"data row20 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row20_col1\" class=\"data row20 col1\" >Lilliefors</td>\n",
       "      <td id=\"T_cf15d_row20_col2\" class=\"data row20 col2\" >**Purpose**: The purpose of this metric is to utilize the Lilliefors test, named in honor of the Swedish statistician Hubert Lilliefors, in order to assess whether the features of the machine learning model's training dataset conform to a normal distribution. This is done because the assumption of normal distribution plays a vital role in numerous statistical procedures as well as numerous machine learning models. Should the features fail to follow a normal distribution, some model types may not operate at optimal efficiency. This can potentially lead to inaccurate predictions.\n",
       "\n",
       "**Test Mechanism**: The application of this test happens across all feature columns within the training dataset. For each feature, the Lilliefors test returns a test statistic and p-value. The test statistic quantifies how far the feature's distribution is from an ideal normal distribution, whereas the p-value aids in determining the statistical relevance of this deviation. The final results are stored within a dictionary, the keys of which correspond to the name of the feature column, and the values being another dictionary which houses the test statistic and p-value.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- If the p-value corresponding to a specific feature sinks below a pre-established significance level, generally set at 0.05, then it can be deduced that the distribution of that feature significantly deviates from a normal distribution. This can present a high risk for models that assume normality, as these models may perform inaccurately or inefficiently in the presence of such a feature.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- One advantage of the Lilliefors test is its utility irrespective of whether the mean and variance of the normal distribution are known in advance. This makes it a more robust option in real-world situations where these values might not be known.\n",
       "- Second, the test has the ability to screen every feature column, offering a holistic view of the dataset.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Despite the practical applications of the Lilliefors test in validating normality, it does come with some limitations.\n",
       "- Firstly, it is only capable of testing unidimensional data, thus rendering it ineffective for datasets with interactions between features or multi-dimensional phenomena.\n",
       "- Additionally, the test might not be as sensitive as some other tests (like the Anderson-Darling test) in detecting deviations from a normal distribution.\n",
       "- Lastly, like any other statistical test, Lilliefors test may also produce false positives or negatives. Hence, banking solely on this test, without considering other characteristics of the data, may give rise to risks.</td>\n",
       "      <td id=\"T_cf15d_row20_col3\" class=\"data row20 col3\" >validmind.model_validation.statsmodels.Lilliefors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row21_col0\" class=\"data row21 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row21_col1\" class=\"data row21 col1\" >Runs Test</td>\n",
       "      <td id=\"T_cf15d_row21_col2\" class=\"data row21 col2\" >**Purpose**: The Runs Test is a statistical procedure used to determine whether the sequence of data extracted from the ML model behaves randomly or not. Specifically, it analyzes runs, sequences of consecutive positives or negatives, in the data to check if there are more or fewer runs than expected under the assumption of randomness. This can be an indication of some pattern, trend, or cycle in the model's output which may need attention.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism applies the Runs Test from the statsmodels module on each column of the training dataset. For every feature in the dataset, a Runs Test is executed, whose output includes a Runs Statistic and P-value. A low P-value suggests that data arrangement in the feature is not likely to be random. The results are stored in a dictionary where the keys are the feature names, and the values are another dictionary storing the test statistic and the P-value for each feature.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk is indicated when the P-value is close to zero.\n",
       "- If the p-value is less than a predefined significance level (like 0.05), it suggests that the runs (series of positive or negative values) in the model's output are not random and are longer or shorter than what is expected under a random scenario.\n",
       "- This would mean there's a high risk of non-random distribution of errors or model outcomes, suggesting potential issues with the model.\n",
       "\n",
       "**Strengths**:\n",
       "- The strength of the Runs Test is that it's straightforward and fast for detecting non-random patterns in data sequence.\n",
       "- It can validate assumptions of randomness, which is particularly valuable for checking error distributions in regression models, trendless time series data, and making sure a classifier doesn't favour one class over another.\n",
       "- Moreover, it can be applied to both classification and regression tasks, making it versatile.\n",
       "\n",
       "**Limitations**:\n",
       "- The test assumes that the data is independently and identically distributed (i.i.d.), which might not be the case for many real-world datasets.\n",
       "- The conclusion drawn from the low p-value indicating non-randomness does not provide information about the type or the source of the detected pattern.\n",
       "- Also, it is sensitive to extreme values (outliers), and overly large or small run sequences can influence the results.\n",
       "- Furthermore, this test does not provide model performance evaluation; it is used to detect patterns in the sequence of outputs only.</td>\n",
       "      <td id=\"T_cf15d_row21_col3\" class=\"data row21 col3\" >validmind.model_validation.statsmodels.RunsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row22_col0\" class=\"data row22 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row22_col1\" class=\"data row22 col1\" >DFGLS Arch</td>\n",
       "      <td id=\"T_cf15d_row22_col2\" class=\"data row22 col2\" >**Purpose**: The Dickey-Fuller GLS (DFGLS) Arch metric is utilized to determine the order of integration in time series data. For machine learning models dealing with time series and forecasting, this metric evaluates the existence of a unit root, thereby checking whether a time series is non-stationary. This analysis is a crucial initial step when dealing with time series data.\n",
       "\n",
       "**Test Mechanism**: This code implements the Dickey-Fuller GLS unit root test on each attribute of the dataset. This process involves iterating through every column of the dataset and applying the DFGLS test to assess the presence of a unit root. The resulting information, including the test statistic ('stat'), the p-value ('pvalue'), the quantity of lagged differences utilized in the regression ('usedlag'), and the number of observations ('nobs'), is subsequently stored.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high p-value for the DFGLS test represents a high risk. Specifically, a p-value above a typical threshold of 0.05 suggests that the time series data is quite likely to be non-stationary, thus presenting a high risk for generating unreliable forecasts.\n",
       "\n",
       "**Strengths**:\n",
       "- The Dickey-Fuller GLS test is a potent tool for checking the stationarity of time series data.\n",
       "- It helps to verify the assumptions of the models before the actual construction of the machine learning models proceeds.\n",
       "- The results produced by this metric offer a clear insight into whether the data is appropriate for specific machine learning models, especially those demanding the stationarity of time series data.\n",
       "\n",
       "**Limitations**:\n",
       "- Despite its benefits, the DFGLS test does present some drawbacks. It can potentially lead to inaccurate conclusions if the time series data incorporates a structural break.\n",
       "- If the time series tends to follow a trend while still being stationary, the test might misinterpret it, necessitating further detrending.\n",
       "- The test also presents challenges when dealing with shorter time series data or volatile data, not producing reliable results in these cases.</td>\n",
       "      <td id=\"T_cf15d_row22_col3\" class=\"data row22 col3\" >validmind.model_validation.statsmodels.DFGLSArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row23_col0\" class=\"data row23 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row23_col1\" class=\"data row23 col1\" >Auto ARIMA</td>\n",
       "      <td id=\"T_cf15d_row23_col2\" class=\"data row23 col2\" >**Purpose**: The AutoARIMA validation test is designed to evaluate and rank AutoRegressive Integrated Moving Average (ARIMA) models. These models are primarily used for forecasting time-series data. The validation test automatically fits multiple ARIMA models, with varying parameters, to every variable within the given dataset. The models are then ranked based on their Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) values, which provide a basis for the efficient model selection process.\n",
       "\n",
       "**Test Mechanism**: This metric proceeds by generating an array of feasible combinations of ARIMA model parameters which are within a prescribed limit. These limits include `max_p`, `max_d`, `max_q`; they represent the autoregressive, differencing, and moving average components respectively. Upon applying these sets of parameters, the validation test fits each ARIMA model to the time-series data provided. For each model, it subsequently proceeds to calculate and record both the BIC and AIC values, which serve as performance indicators for the model fit. Prior to this parameter fitting process, the Augmented Dickey-Fuller test for data stationarity is conducted on the data series. If a series is found to be non-stationary, a warning message is sent out, given that ARIMA models necessitate input series to be stationary.\n",
       "\n",
       "**Signs of High Risk**: * If the p-value of the Augmented Dickey-Fuller test for a variable exceeds 0.05, a warning is logged. This warning indicates that the series might not be stationary, leading to potentially inaccurate results. * Consistent failure in fitting ARIMA models (as made evident through logged errors) might disclose issues with either the data or model stability.\n",
       "\n",
       "**Strengths**: * The AutoARIMA validation test simplifies the often complex task of selecting the most suitable ARIMA model based on BIC and AIC criteria. * The mechanism incorporates a check for non-stationarity within the data, which is a critical prerequisite for ARIMA models. * The exhaustive search through all possible combinations of model parameters enhances the likelihood of identifying the best-fit model.\n",
       "\n",
       "**Limitations**: * This validation test can be computationally costly as it involves creating and fitting multiple ARIMA models for every variable. * Although the test checks for non-stationarity and logs warnings where present, it does not apply any transformations to the data to establish stationarity. * The selection of models leans solely on BIC and AIC criteria, which may not yield the best predictive model in all scenarios. * The test is only applicable to regression tasks involving time-series data, and may not work effectively for other types of machine learning tasks.</td>\n",
       "      <td id=\"T_cf15d_row23_col3\" class=\"data row23 col3\" >validmind.model_validation.statsmodels.AutoARIMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row24_col0\" class=\"data row24 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row24_col1\" class=\"data row24 col1\" >ADF Test</td>\n",
       "      <td id=\"T_cf15d_row24_col2\" class=\"data row24 col2\" >**Purpose**: The Augmented Dickey-Fuller (ADF) metric test is designed to evaluate the presence of a unit root in a time series. This essentially translates to assessing the stationarity of a time series dataset. This is vital in time series analysis, regression tasks, and forecasting, as these often need the data to be stationary.\n",
       "\n",
       "**Test Mechanism**: This test application utilizes the \"adfuller\" function from Python's “statsmodels” library. It applies this function to each column of the training dataset, subsequently calculating the ADF statistic, p-value, the number of lags used, and the number of observations in the sample for each column. If a column's p-value is lower than the predetermined threshold (usually 0.05), the series is considered stationary, and the test is deemed passed for that column.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A p-value that surpasses the threshold value indicates a high risk or potential model performance issue.\n",
       "- A high p-value suggests that the null hypothesis (of a unit root being present) cannot be rejected. This in turn suggests that the series is non-stationary which could potentially yield unreliable and falsified results for the model's performance and forecast.\n",
       "\n",
       "**Strengths**:\n",
       "- Archetypal Test for Stationarity: The ADF test is a comprehensive approach towards testing the stationarity of time series data. Such testing is vital for many machine learning and statistical models.\n",
       "- Detailed Output: The function generates detailed output, including the number of lags used and the number of observations, which adds to understanding a series’ behaviour.\n",
       "\n",
       "**Limitations**:\n",
       "- Dependence on Threshold: The result of this test freights heavily on the threshold chosen. Hence, an imprudent threshold value might lead to false acceptance or rejection of the null hypothesis.\n",
       "- Not Effective for Trending Data: The test suffers when it operates under the assumption that the data does not encapsulate any deterministic trend. In the presence of such a trend, it might falsely identify a series as non-stationary.\n",
       "- Potential for False Positives: The ADF test especially in the case of larger datasets, tends to reject the null hypothesis, escalating the chances of false positives.</td>\n",
       "      <td id=\"T_cf15d_row24_col3\" class=\"data row24 col3\" >validmind.model_validation.statsmodels.ADFTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row25_col0\" class=\"data row25 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row25_col1\" class=\"data row25 col1\" >Regression Model Forecast Plot</td>\n",
       "      <td id=\"T_cf15d_row25_col2\" class=\"data row25 col2\" >**Purpose:** The \"regression_forecast_plot\" is intended to visually depict the performance of one or more regression models by comparing the model's forecasted outcomes against actual observed values within a specified date range. This metric is especially useful in time-series models or any model where the outcome changes over time, allowing direct comparison of predicted vs actual values.\n",
       "\n",
       "**Test Mechanism:** This test generates a plot for each fitted model in the list. The x-axis represents the date ranging from the specified \"start_date\" to the \"end_date\", while the y-axis shows the value of the outcome variable. Two lines are plotted: one representing the forecasted values and the other representing the observed values. The \"start_date\" and \"end_date\" can be parameters of this test; if these parameters are not provided, they are set to the minimum and maximum date available in the dataset. The test verifies that the provided date range is within the limits of the available data.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- High risk or failure signs could be deduced visually from the plots if the forecasted line significantly deviates from the observed line, indicating the model's predicted values are not matching actual outcomes.\n",
       "- A model that struggles to handle the edge conditions like maximum and minimum data points could also be considered a sign of risk.\n",
       "\n",
       "**Strengths:**\n",
       "- Visualization: The plot provides an intuitive and clear illustration of how well the forecast matches the actual values, making it straightforward even for non-technical stakeholders to interpret.\n",
       "- Flexibility: It allows comparison for multiple models and for specified time periods.\n",
       "- Model Evaluation: It can be useful in identifying overfitting or underfitting situations, as these will manifest as discrepancies between the forecasted and observed values.\n",
       "\n",
       "**Limitations:**\n",
       "- Interpretation Bias: Interpretation of the plot is subjective and can lead to different conclusions by different evaluators.\n",
       "- Lack of Precision: Visual representation might not provide precise values of the deviation.\n",
       "- Inapplicability: Limited to cases where the order of data points (time-series) matters, it might not be of much use in problems that are not related to time series prediction.</td>\n",
       "      <td id=\"T_cf15d_row25_col3\" class=\"data row25 col3\" >validmind.model_validation.statsmodels.RegressionModelForecastPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row26_col0\" class=\"data row26 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row26_col1\" class=\"data row26 col1\" >ADF</td>\n",
       "      <td id=\"T_cf15d_row26_col2\" class=\"data row26 col2\" >**Purpose**: The Augmented Dickey-Fuller (ADF) test metric is used here to determine the order of integration, i.e., the stationarity of a given time series data. The stationary property of data is pivotal in many machine learning models as it impacts the reliability and effectiveness of predictions and forecasts.\n",
       "\n",
       "**Test Mechanism**: The ADF test starts by executing the ADF function from the statsmodels library on every feature of the dataset. Multiple outputs are generated for each run, including the ADF test statistic and p-value, count of lags used, the number of observations factored into the test, critical values at various confidence levels, and the maximized information criterion. These results are stored for each feature for subsequent analysis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- An inflated ADF statistic and high p-value (generally above 0.05) insinuate a high risk to the model's performance due to the presence of a unit root indicating non-stationarity.\n",
       "- Such non-stationarity might result in untrustworthy or insufficient forecasts.\n",
       "\n",
       "**Strengths**:\n",
       "- The ADF test is robust to more sophisticated correlation within the data, which empowers it to be deployed in settings where data might display complex stochastic behavior.\n",
       "- The ADF test provides explicit outputs like test statistics, critical values, and information criterion, thereby enhancing our understanding and transparency of the model validation process.\n",
       "\n",
       "**Limitations**:\n",
       "- The ADF test might demonstrate low statistical power, making it challenging to differentiate between a unit root and near-unit-root processes causing false negatives.\n",
       "- The test assumes the data follows an autoregressive process, which might not be the case all the time.\n",
       "- The ADF test finds it demanding to manage time series data with structural breaks.</td>\n",
       "      <td id=\"T_cf15d_row26_col3\" class=\"data row26 col3\" >validmind.model_validation.statsmodels.ADF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row27_col0\" class=\"data row27 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row27_col1\" class=\"data row27 col1\" >Durbin Watson Test</td>\n",
       "      <td id=\"T_cf15d_row27_col2\" class=\"data row27 col2\" >**Purpose**: The Durbin-Watson Test metric detects autocorrelation in time series data (where a set of data values influences their predecessors). Autocorrelation is a crucial factor for regression tasks as these often assume the independence of residuals. A model with significant autocorrelation may give unreliable predictions.\n",
       "\n",
       "**Test Mechanism**: Utilizing the `durbin_watson` function in the `statsmodels` Python library, the Durbin-Watson (DW) Test metric generates a statistical value for each feature of the training dataset. The function is looped over all columns of the dataset, calculating and caching the DW value for each column for further analysis. A DW metric value nearing 2 indicates no autocorrelation. Conversely, values approaching 0 suggest positive autocorrelation, and those leaning towards 4 imply negative autocorrelation.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If a feature's DW value significantly deviates from 2, it could signal a high risk due to potential autocorrelation issues in the dataset.\n",
       "- A value closer to '0' could imply positive autocorrelation, while a value nearer to '4' could point to negative autocorrelation, both leading to potentially unreliable prediction models.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric specializes in identifying autocorrelation in prediction model residuals.\n",
       "- Autocorrelation detection assists in diagnosing violation of various modeling technique assumptions, particularly in regression analysis and time-series data modeling.\n",
       "\n",
       "**Limitations**:\n",
       "- The Durbin-Watson Test mainly detects linear autocorrelation and could overlook other types of relationships.\n",
       "- The metric is highly sensitive to data points order. Shuffling the order could lead to notably different results.\n",
       "- The test only checks for first-order autocorrelation (between a variable and its immediate predecessor) and fails to detect higher order autocorrelation.</td>\n",
       "      <td id=\"T_cf15d_row27_col3\" class=\"data row27 col3\" >validmind.model_validation.statsmodels.DurbinWatsonTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row28_col0\" class=\"data row28 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row28_col1\" class=\"data row28 col1\" >Missing Values Risk</td>\n",
       "      <td id=\"T_cf15d_row28_col2\" class=\"data row28 col2\" >**Purpose**: The Missing Values Risk metric is specifically designed to assess and quantify the risk associated with missing values in the dataset used for machine learning model training. It measures two specific risks: the percentage of total data that are missing, and the percentage of all variables (columns) that contain some missing values.\n",
       "\n",
       "**Test Mechanism**: Initially, the metric calculates the total number of data points in the dataset and the count of missing values. It then inspects each variable (column) to determine how many contain at least one missing datapoint. By methodically counting missing datapoints across the entire dataset and each variable (column), it identifies the percentage of missing values in the entire dataset and the percentage of variables (columns) with such values.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Record high percentages in either of the risk measures could suggest a high risk.\n",
       "- If the dataset indicates a high percentage of missing values, it might significantly undermine the model's performance and credibility.\n",
       "- If a significant portion of variables (columns) in the dataset are missing values, this could make the model susceptible to bias and overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The metric offers valuable insights into the readiness of a dataset for model training as missing values can heavily destabilize both the model's performance and predictive capabilities.\n",
       "- The metric's quantification of the risks caused by missing values allows for the use of targeted methods to manage these values correctly- either through removal, imputation, or alternative strategies.\n",
       "- The metric has the flexibility to be applied to both classification and regression assignments, maintaining its utility across a wide range of models and scenarios.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The metric primarily identifies and quantifies the risk associated with missing values without suggesting specific mitigation strategies.\n",
       "- The metric does not ascertain whether the missing values are random or associated with an underlying issue in the stages of data collection or preprocessing.\n",
       "- However, the identification of the presence and scale of missing data is the essential initial step towards improving data quality.</td>\n",
       "      <td id=\"T_cf15d_row28_col3\" class=\"data row28 col3\" >validmind.data_validation.MissingValuesRisk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row29_col0\" class=\"data row29 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row29_col1\" class=\"data row29 col1\" >IQR Outliers Table</td>\n",
       "      <td id=\"T_cf15d_row29_col2\" class=\"data row29 col2\" >**Purpose**: The \"Interquartile Range Outliers Table\" (IQROutliersTable) metric has been designed for identifying and summarizing outliers within numerical features of a dataset using the Interquartile Range (IQR) method. The purpose of this exercise is crucial in the pre-processing of data as outliers can substantially distort the statistical analysis and debilitate the performance of machine learning models.\n",
       "\n",
       "**Test Mechanism**: The IQR, which is the range separating the first quartile (25th percentile) from the third quartile (75th percentile), is calculated for each numerical feature within the dataset. An outlier is defined as a data point falling below the \"Q1\n",
       "- 1.5 * IQR\" or above \"Q3 + 1.5 * IQR\" range. The metric then computes the number of outliers along with their minimum, 25th percentile, median, 75th percentile, and maximum values for each numerical feature. If no specific features are chosen, the metric will apply to all numerical features in the dataset. The default outlier threshold is set to 1.5, following the standard definition of outliers in statistical analysis, although it can be customized by the user.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- High risk is indicated by a large number of outliers in multiple features.\n",
       "- Outliers that are significantly distanced from the mean value of variables could potentially signal high risk.\n",
       "- Data entry errors or other data quality issues could be manifested through extremely high or low outlier values.\n",
       "\n",
       "**Strengths**:\n",
       "- It yields a comprehensive summary of outliers for each numerical feature within the dataset. This enables the user to pinpoint features with potential quality issues.\n",
       "- The IQR method is not overly affected by extremely high or low outlier values as it is based on quartile calculations.\n",
       "- The versatility of this metric grants the ability to customize the method to work on selected features and set a defined threshold for outliers.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric might cause false positives if the variable of interest veers away from a normal or near-normal distribution, notably in the case of skewed distributions.\n",
       "- It does not extend to provide interpretation or recommendations for tackling outliers and relies on the user or a data scientist to conduct further analysis of the results.\n",
       "- As it only functions on numerical features, it cannot be used for categorical data.\n",
       "- For data that has undergone heavy pre-processing, was manipulated, or inherently possesses a high kurtosis (heavy tails), the pre-set threshold may not be optimal for outlier detection.</td>\n",
       "      <td id=\"T_cf15d_row29_col3\" class=\"data row29 col3\" >validmind.data_validation.IQROutliersTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row30_col0\" class=\"data row30 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row30_col1\" class=\"data row30 col1\" >Skewness</td>\n",
       "      <td id=\"T_cf15d_row30_col2\" class=\"data row30 col2\" >**Purpose**: The purpose of the Skewness test is to measure the asymmetry in the distribution of data within a predictive machine learning model. Specifically, it evaluates the divergence of said distribution from a normal distribution. In understanding the level of skewness, we can potentially identify issues with data quality, an essential component for optimizing the performance of traditional machine learning models in both classification and regression settings.\n",
       "\n",
       "**Test Mechanism**: This test calculates skewness of numerical columns in a dataset, which is extracted from the DataFrame, specifically focusing on numerical data types. The skewness value is then contrasted against a predetermined maximum threshold, set by default to 1. The skewness value under review is deemed to have passed the test only if it is less than this maximum threshold; otherwise, the test is considered 'fail'. Subsequently, the test results of each column, together with the skewness value and column name, are cached.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- The presence of substantial skewness levels that significantly exceed the maximum threshold is an indication of skewed data distribution and subsequently high model risk.\n",
       "- Persistent skewness in data could signify that the foundational assumptions of the machine learning model may not be applicable, potentially leading to subpar model performance, erroneous predictions, or biased inferences.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Fast and efficient identification of unequal data\n",
       "- distributions within a machine learning model is enabled by the skewness test.\n",
       "- The maximum threshold parameter can be adjusted to meet the user's specific needs, enhancing the test's versatility.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The test only evaluates numeric columns, which means that data in non-numeric columns could still include bias or problematic skewness that this test does not capture.\n",
       "- The test inherently assumes that the data should follow a normal distribution, an expectation which may not always be met in real-world data.\n",
       "- The risk grading is largely dependent on a subjective threshold, which may result in excessive strictness or leniency depending upon selection. This factor might require expert input and recurrent iterations for refinement.</td>\n",
       "      <td id=\"T_cf15d_row30_col3\" class=\"data row30 col3\" >validmind.data_validation.Skewness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row31_col0\" class=\"data row31 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row31_col1\" class=\"data row31 col1\" >Duplicates</td>\n",
       "      <td id=\"T_cf15d_row31_col2\" class=\"data row31 col2\" >**Purpose**: The Duplicates test is designed to assess the data quality of an ML model by identifying any duplicate entries in the data set. It focuses on seeking out duplication in a specified text column or among the primary keys of the data set, which could have serious implications for the performance and integrity of the model. Duplicate entries could potentially skew the data distribution and influence model training inaccurately.\n",
       "\n",
       "**Test Mechanism**: This test operates by calculating the total number of duplicate entries in the data set. The algorithm will count duplicates within the 'text_column' if this property is specified. If primary keys are defined, the test will also be applied on them. The count of duplicates ('n_duplicates') is then compared to a predefined minimum threshold (the default 'min_threshold' is set at 1) to determine whether the test has passed or not. The results include the total number of duplicates as well as the percentage of duplicate rows in comparison to the overall dataset ('p_duplicates').\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large amount of duplicates, particularly those exceeding the predefined minimum threshold, point toward a high risk situation.\n",
       "- Overrepresentation of certain data which can lead to skewed results.\n",
       "- Indication of inefficient data collecting techniques leading to data redundancy.\n",
       "- Models that fail this test predominantly may necessitate a closer examination of their data preprocessing methods or source data.\n",
       "\n",
       "**Strengths**:\n",
       "- The Duplicates test is highly adaptable, being capable of being used with both text data and tabular data formats.\n",
       "- It is able to provide results both numerically and as a percentage of the total data set, allowing for a broader understanding of the extent of duplication.\n",
       "- Its utility lies in effectively flagging any data quality issues that could potentially upset model performance and generate erroneous predictions.\n",
       "\n",
       "**Limitations**:\n",
       "- The Duplicates test solely targets exact duplication in entries, meaning it may overlook near-duplicates or normalized forms of entries that might also affect data distribution and model integrity.\n",
       "- Data variations caused by errors, phrasing changes, or inconsistencies may not be detected.\n",
       "- A substantial number of duplicates in a datasets may not always denote poor data quality, as this can be dependent on the nature of the data and the problem being addressed.</td>\n",
       "      <td id=\"T_cf15d_row31_col3\" class=\"data row31 col3\" >validmind.data_validation.Duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row32_col0\" class=\"data row32 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row32_col1\" class=\"data row32 col1\" >Missing Values Bar Plot</td>\n",
       "      <td id=\"T_cf15d_row32_col2\" class=\"data row32 col2\" >**Purpose:** The 'MissingValuesBarPlot' metric provides a color-coded visual representation of the percentage of missing values for each column in an ML model's dataset. The primary purpose of this metric is to easily identify and quantify missing data, which are essential steps in data preprocessing. The presence of missing data can potentially skew the model's predictions and decrease its accuracy. Additionally, this metric uses a pre-set threshold to categorize various columns into ones that contain missing data above the threshold (high risk) and below the threshold (less risky).\n",
       "\n",
       "**Test Mechanism:** The test mechanism involves scanning each column in the input dataset and calculating the percentage of missing values. It then compares each column's missing data percentage with the predefined threshold, categorizing columns with missing data above the threshold as high-risk. The test generates a bar plot in which columns with missing data are represented on the y-axis and their corresponding missing data percentages are displayed on the x-axis. The color of each bar reflects the missing data percentage in relation to the threshold: grey for values below the threshold and light coral for those exceeding it. The user-defined threshold is represented by a red dashed line on the plot.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Columns with higher percentages of missing values beyond the threshold are high-risk. These are visually represented by light coral bars on the bar plot.\n",
       "\n",
       "**Strengths:**\n",
       "- Helps in quickly identifying and quantifying missing data across all columns of the dataset.\n",
       "- Facilitates pattern recognition through visual representation.\n",
       "- Enables customization of the level of risk tolerance via a user-defined threshold.\n",
       "- Supports both classification and regression tasks, sharing its versatility.\n",
       "\n",
       "**Limitations:**\n",
       "- It only considers the quantity of missing values, not differentiating between different types of missingness (Missing completely at random\n",
       "- MCAR, Missing at random\n",
       "- MAR, Not Missing at random\n",
       "- NMAR).\n",
       "- It doesn't offer insights into potential approaches for handling missing entries, such as various imputation strategies.\n",
       "- The metric does not consider possible impacts of the missing data on the model's accuracy or precision.\n",
       "- Interpretation of the findings and the next steps might require an expert understanding of the field.</td>\n",
       "      <td id=\"T_cf15d_row32_col3\" class=\"data row32 col3\" >validmind.data_validation.MissingValuesBarPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row33_col0\" class=\"data row33 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row33_col1\" class=\"data row33 col1\" >Dataset Description</td>\n",
       "      <td id=\"T_cf15d_row33_col2\" class=\"data row33 col2\" >**Purpose**: This metric, known as DatasetDescription, has been architected to furnish an extensive spectrum of descriptive statistics pertinent to the input data that fuels a machine learning model. The statistics generated by this metric include measures of central tendency, dispersion, in addition to frequency counts for each field, or variable, in the dataset. The purpose of these statistics lies in comprehending the nature of the data, identifying anomalies, and grasping the extent to which the data fulfills model assumptions.\n",
       "\n",
       "**Test Mechanism**: The testing function commences by morphing the input dataset back to its primary form by undoing any one-hot encoding, subsequently extracting each field from the dataset. Post this process, the script computes the descriptive statistics for each field, contingent on its data type (Numeric, Categorical, Boolean, Dummy, Text, Null). For numeric fields, the statistics include computations of mean, standard deviation, minimum, maximum, and varying percentiles. For categorical and boolean fields, the frequency counts of each category along with the most frequent category (or 'top') are computed. The script computes missing and distinct values for all fields, while also generating histograms for numeric and categorical types, as well as word count histograms for text\n",
       "- accomplished through counting the number of occurrences of each unique word.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant portion of missing or null values, which may hinder the model's performance.\n",
       "- Predominance of a single value (indicating low diversity) within a feature.\n",
       "- High variance in a numeric feature.\n",
       "- High number of distinct categories in categorical features.\n",
       "- High frequency of a specific word in text data as this might signal bias in the dataset.\n",
       "\n",
       "**Strengths**:\n",
       "- Capability to provide an all-inclusive, general summary of the dataset.\n",
       "- It helps to understand the distribution, dispersion, and central tendency of numeric features, moreover, the frequency distribution within categorical and text features.\n",
       "- It can efficaciously handle different data types, offering insights into the variety and distribution of field values.\n",
       "\n",
       "**Limitations**:\n",
       "- It does not provide insights regarding the relationships between different fields or features, as it focuses only on univariate analysis.\n",
       "- The statistical description for text data is severely limited, given it essentially forms a bag-of-words model.\n",
       "- The descriptive statistics generated, including mean and standard deviation, hold no meaningful value for ordinal categorical data.\n",
       "- This metric does not offer intelligence on how the derived data will influence the performance of the model.</td>\n",
       "      <td id=\"T_cf15d_row33_col3\" class=\"data row33 col3\" >validmind.data_validation.DatasetDescription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row34_col0\" class=\"data row34 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row34_col1\" class=\"data row34 col1\" >Scatter Plot</td>\n",
       "      <td id=\"T_cf15d_row34_col2\" class=\"data row34 col2\" >**Purpose**: The ScatterPlot metric is designed to offer a visual analysis of a given dataset by constructing a scatter plot matrix encapsulating all the dataset's features (or columns). Its primary function lies in unearthing relationships, patterns, or outliers across different features, thus providing both quantitative and qualitative insights into the multidimensional relationships within the dataset. This visual assessment aids in understanding the efficacy of the chosen features for model training and their overall suitability.\n",
       "\n",
       "**Test Mechanism**: Using the seaborn library, the ScatterPlot class creates the scatter plot matrix. The process includes retrieving all columns from the dataset, verifying their existence, and subsequently generating a pairplot for these columns. A kernel density estimate (kde) is utilized to present a smoother, univariate distribution along the grid's diagonal. The final plot is housed in an array of Figure objects, each wrapping a matplotlib figure instance for storage and future usage.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The emergence of non-linear or random patterns across different feature pairs. This may suggest intricate relationships unfit for linear presumptions.\n",
       "- A lack of clear patterns or clusters which might point to weak or non-existent correlations among features, thus creating a problem for certain model types.\n",
       "- The occurrence of outliers as visual outliers in your data can adversely influence the model's performance.\n",
       "\n",
       "**Strengths**:\n",
       "- It offers insight into the multidimensional relationships among multiple features.\n",
       "- It assists in identifying trends, correlations, and outliers which could potentially affect the model's performance.\n",
       "- As a diagnostic tool, it can validate whether certain assumptions made during the model-creation process, such as linearity, hold true.\n",
       "- The tool's versatility extends to its application for both regression and classification tasks.\n",
       "\n",
       "**Limitations**:\n",
       "- Scatter plot matrices may become cluttered and hard to decipher as the number of features escalates, resulting in complexity and confusion.\n",
       "- While extremely proficient in revealing pairwise relationships, these matrices may fail to illuminate complex interactions that involve three or more features.\n",
       "- These matrices are primarily visual tools, so the precision of quantitative analysis may be compromised.\n",
       "- If not clearly visible, outliers can be missed, which could negatively affect model performance.\n",
       "- It assumes that the dataset can fit into the computer's memory, which might not always be valid particularly for extremely large datasets.</td>\n",
       "      <td id=\"T_cf15d_row34_col3\" class=\"data row34 col3\" >validmind.data_validation.ScatterPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row35_col0\" class=\"data row35 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row35_col1\" class=\"data row35 col1\" >Time Series Outliers</td>\n",
       "      <td id=\"T_cf15d_row35_col2\" class=\"data row35 col2\" >**Purpose**: This test is designed to identify outliers in time-series data using the z-score method. It's vital for ensuring data quality before modeling, as outliers can skew predictive models and significantly impact their overall performance.\n",
       "\n",
       "**Test Mechanism**: The test processes a given dataset which must have datetime indexing, checks if a 'zscore_threshold' parameter has been supplied, and identifies columns with numeric data types. After finding numeric columns, the implementer then applies the z-score method to each numeric column, identifying outliers based on the threshold provided. Each outlier is listed together with their variable name, z-score, timestamp and relative threshold in a dictionary and converted to a DataFrame for convenient output. Additionally, it produces visual plots for each time series illustrating outliers in the context of the broader dataset. The 'zscore_threshold' parameter sets the limit beyond which a data point will be labeled as an outlier. The default threshold is set at 3, indicating that any data point that falls 3 standard deviations away from the mean will be marked as an outlier.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If many or substantial outliers are present within a dataset, this may be an indicator of high risk as it suggests that the dataset contains significant anomalies.\n",
       "- This could potentially affect the performance of the machine learning models, if not properly addressed.\n",
       "- Data points with z-scores higher than the set threshold would be flagged as outliers and could be considered as high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The z-score method is a popular and robust method for identifying outliers in a dataset.\n",
       "- Time series maintenance is simplified through requiring a datetime index.\n",
       "- Outliers are identified for each numeric feature individually.\n",
       "- Provides an elaborate report which shows variables, date, z-score and whether the test passed or failed.\n",
       "- Offers visual inspection for detected outliers in the respective time-series through plots.\n",
       "\n",
       "**Limitations**:\n",
       "- This test only identifies outliers in numeric columns, and won't identify outliers in categorical variables.\n",
       "- The utility and accuracy of z-scores can be limited if the data doesn't follow a normal distribution.\n",
       "- The method relies on a subjective z-score threshold for deciding what constitutes an outlier, which might not always be suitable depending on the dataset and the use case.\n",
       "- It does not address possible ways to handle identified outliers in the data.\n",
       "- The necessity for a datetime index could limit the extent of its application.</td>\n",
       "      <td id=\"T_cf15d_row35_col3\" class=\"data row35 col3\" >validmind.data_validation.TimeSeriesOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row36_col0\" class=\"data row36 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row36_col1\" class=\"data row36 col1\" >Tabular Categorical Bar Plots</td>\n",
       "      <td id=\"T_cf15d_row36_col2\" class=\"data row36 col2\" >**Purpose**: The purpose of this metric is to visually analyze categorical data using bar plots. It is intended to evaluate the dataset's composition by displaying the counts of each category in each categorical feature.\n",
       "\n",
       "**Test Mechanism**: The provided dataset is first checked to determine if it contains any categorical variables. If no categorical columns are found, the tool raises a ValueError. For each categorical variable in the dataset, a separate bar plot is generated. The number of occurrences for each category is calculated and displayed on the plot. If a dataset contains multiple categorical columns, multiple bar plots are produced.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- High risk could occur if the categorical variables exhibit an extreme imbalance, with categories having very few instances possibly being underrepresented in the model, which could affect the model's performance and its ability to generalize.\n",
       "- Another sign of risk is if there are too many categories in a single variable, which could lead to overfitting and make the model complex.\n",
       "\n",
       "**Strengths**: This metric provides a visual and intuitively understandable representation of categorical data, which aids in the analysis of variable distributions. By presenting model inputs in this way, we can easily identify imbalances or rare categories that could affect the model's performance.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- This method only works with categorical data, meaning it won't apply to numerical variables.\n",
       "- In addition, the method does not provide any informative value when there are too many categories, as the bar chart could become cluttered and hard to interpret.\n",
       "- It offers no insights into the model's performance or precision, but rather provides a descriptive analysis of the input.</td>\n",
       "      <td id=\"T_cf15d_row36_col3\" class=\"data row36 col3\" >validmind.data_validation.TabularCategoricalBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row37_col0\" class=\"data row37 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row37_col1\" class=\"data row37 col1\" >Auto Stationarity</td>\n",
       "      <td id=\"T_cf15d_row37_col2\" class=\"data row37 col2\" >**Purpose**: The AutoStationarity metric is intended to automatically detect and evaluate the stationary nature of each time series in a DataFrame. It incorporates the Augmented Dickey-Fuller (ADF) test, a statistical approach used to assess stationarity. Stationarity is a fundamental property suggesting that statistic features like mean and variance remain unchanged over time. This is necessary for many time-series models.\n",
       "\n",
       "**Test Mechanism**: The mechanism for the AutoStationarity test involves applying the Augmented Dicky-Fuller test to each time series within the given dataframe to assess if they are stationary. Every series in the dataframe is looped, using the ADF test up to a defined maximum order (configurable and by default set to 5). The p-value resulting from the ADF test is compared against a predetermined threshold (also configurable and by default set to 0.05). The time series is deemed stationary at its current differencing order if the p-value is less than the threshold.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant number of series not achieving stationarity even at the maximum order of differencing can indicate high risk or potential failure in the model.\n",
       "- This could suggest the series may not be appropriately modeled by a stationary process, hence other modeling approaches might be required.\n",
       "\n",
       " **Strengths**:\n",
       "- The key strength in this metric lies in the automation of the ADF test, enabling mass stationarity analysis across various time series and boosting the efficiency and credibility of the analysis.\n",
       "- The utilization of the ADF test, a widely accepted method for testing stationarity, lends authenticity to the results derived.\n",
       "- The introduction of the max order and threshold parameters give users the autonomy to determine their preferred levels of stringency in the tests.\n",
       "\n",
       "**Limitations**:\n",
       "- The Augumented Dicky-Fuller test and the stationarity test are not without their limitations. These tests are premised on the assumption that the series can be modeled by an autoregressive process, which may not always hold true.\n",
       "- The stationarity check is highly sensitive to the choice of threshold for the significance level; an extremely high or low threshold could lead to incorrect results regarding the stationarity properties.\n",
       "- There's also a risk of over-differencing if the maximum order is set too high, which could induce unnecessary cycles.</td>\n",
       "      <td id=\"T_cf15d_row37_col3\" class=\"data row37 col3\" >validmind.data_validation.AutoStationarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row38_col0\" class=\"data row38 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row38_col1\" class=\"data row38 col1\" >Descriptive Statistics</td>\n",
       "      <td id=\"T_cf15d_row38_col2\" class=\"data row38 col2\" >**Purpose**: The purpose of the Descriptive Statistics metric is to provide a comprehensive summary of both numerical and categorical data within a dataset. This involves statistics such as count, mean, standard deviation, minimum and maximum values for numerical data. For categorical data, it calculates the count, number of unique values, most common value and its frequency, and the proportion of the most frequent value relative to the total. The goal is to visualize the overall distribution of the variables in the dataset, aiding in understanding the model's behavior and predicting its performance.\n",
       "\n",
       "**Test Mechanism**: The testing mechanism utilizes two in-built functions of pandas dataframes: describe() for numerical fields and value_counts() for categorical fields. The describe() function pulls out several summary statistics, while value_counts() accounts for unique values. The resulting data is formatted into two distinct tables, one for numerical and another for categorical variable summaries. These tables provide a clear summary of the main characteristics of the variables, which can be instrumental in assessing the model's performance.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Skewed data or significant outliers can represent high risk. For numerical data, this may be reflected via a significant difference between the mean and median (50% percentile).\n",
       "- For categorical data, a lack of diversity (low count of unique values), or overdominance of a single category (high frequency of the top value) can indicate high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- This metric provides a comprehensive summary of the dataset, shedding light on the distribution and characteristics of the variables under consideration.\n",
       "- It is a versatile and robust method, applicable to both numerical and categorical data.\n",
       "- It helps highlight crucial anomalies such as outliers, extreme skewness, or lack of diversity, which are vital in understanding model behavior during testing and validation.\n",
       "\n",
       "**Limitations**:\n",
       "- While this metric offers a high-level overview of the data, it may fail to detect subtle correlations or complex patterns.\n",
       "- It does not offer any insights on the relationship between variables.\n",
       "- Alone, descriptive statistics cannot be used to infer properties about future unseen data.\n",
       "- It should be used in conjunction with other statistical tests to provide a comprehensive understanding of the model's data.</td>\n",
       "      <td id=\"T_cf15d_row38_col3\" class=\"data row38 col3\" >validmind.data_validation.DescriptiveStatistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row39_col0\" class=\"data row39 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row39_col1\" class=\"data row39 col1\" >Pearson Correlation Matrix</td>\n",
       "      <td id=\"T_cf15d_row39_col2\" class=\"data row39 col2\" >**Purpose**: This test is intended to evaluate the extent of linear dependency between all pairs of numerical variables in the given dataset. It provides the Pearson Correlation coefficient, which reveals any high correlations present. The purpose of doing this is to identify potential redundancy, as variables that are highly correlated can often be removed to reduce the dimensionality of the dataset without significantly impacting the model's performance.\n",
       "\n",
       "**Test Mechanism**: This metric test generates a correlation matrix for all numerical variables in the dataset using the Pearson correlation formula. A heat map is subsequently created to visualize this matrix effectively. The color of each point on the heat map corresponds to the magnitude and direction (positive or negative) of the correlation, with a range from -1 (perfect negative correlation) to 1 (perfect positive correlation). Any correlation coefficients higher than 0.7 (in absolute terms) are indicated in white in the heat map, suggesting a high degree of correlation.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of variables in the dataset showing a high degree of correlation (coefficients approaching ±1). This indicates redundancy within the dataset, suggesting that some variables may not be contributing new information to the model.\n",
       "- This could potentially lead to overfitting.\n",
       "\n",
       "**Strengths**:\n",
       "- The primary strength of this metric test is its ability to detect and quantify the linearity of relationships between variables. This allows for the identification of redundant variables, which in turn can help in simplifying models and potentially improving their performance.\n",
       "- The visualization aspect (heatmap) is another strength as it offers an easy-to-understand overview of the correlations, beneficial for those not comfortable navigating numerical matrices.\n",
       "\n",
       "**Limitations**:\n",
       "- The primary limitation of Pearson Correlation is its inability to detect non-linear relationships between variables, which can lead to missed opportunities for dimensionality reduction.\n",
       "- It only measures the degree of linear relationship and not the strength of effect of one variable on the other.\n",
       "- The cutoff value of 0.7 for high correlation is a somewhat arbitrary choice and some valid dependencies might be missed if they have a correlation coefficient less than this value.</td>\n",
       "      <td id=\"T_cf15d_row39_col3\" class=\"data row39 col3\" >validmind.data_validation.PearsonCorrelationMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row40_col0\" class=\"data row40 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row40_col1\" class=\"data row40 col1\" >Feature Target Correlation Plot</td>\n",
       "      <td id=\"T_cf15d_row40_col2\" class=\"data row40 col2\" >**Purpose**: This test is designed to graphically illustrate the correlations between distinct input features and the target output of a Machine Learning model. Understanding how each feature influences the model's predictions is crucial\n",
       "- a higher correlation indicates stronger influence of the feature on the target variable. This correlation study is especially advantageous during feature selection and for comprehending the model's operation.\n",
       "\n",
       "**Test Mechanism**: This FeatureTargetCorrelationPlot test computes and presents the correlations between the features and the target variable using a specific dataset. These correlations are calculated, graphically represented in a horizontal bar plot, and color-coded based on the strength of the correlation. A hovering template can also be utilized for informative tooltips. It is possible to specify the features to be analyzed and adjust the graph's height according to need.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- There are no strong correlations (either positive or negative) between features and the target variable. This could suggest high risk as the supplied features do not appear to significantly impact the prediction output.\n",
       "- The presence of duplicated correlation values might hint at redundancy in the feature set.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides visual assistance to interpreting correlations more effectively.\n",
       "- Gives a clear and simple tour of how each feature affects the model's target variable.\n",
       "- Beneficial for feature selection and grasping the model's prediction nature.\n",
       "- Precise correlation values for each feature are offered by the hover template, contributing to a granular-level comprehension.\n",
       "\n",
       "**Limitations**:\n",
       "- The test only accepts numerical data, meaning variables of other types need to be prepared beforehand.\n",
       "- The plot assumes all correlations to be linear, thus non-linear relationships might not be captured effectively.\n",
       "- Not apt for models that employ complex feature interactions, like Decision Trees or Neural Networks, as the test may not accurately reflect their importance.</td>\n",
       "      <td id=\"T_cf15d_row40_col3\" class=\"data row40 col3\" >validmind.data_validation.FeatureTargetCorrelationPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row41_col0\" class=\"data row41 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row41_col1\" class=\"data row41 col1\" >Tabular Numerical Histograms</td>\n",
       "      <td id=\"T_cf15d_row41_col2\" class=\"data row41 col2\" >**Purpose**: The purpose of this test is to provide visual analysis of numerical data through the generation of histograms for each numerical feature in the dataset. Histograms aid in the exploratory analysis of data, offering insight into the distribution of the data, skewness, presence of outliers, and central tendencies. It helps in understanding if the inputs to the model are normally distributed which is a common assumption in many machine learning algorithms.\n",
       "\n",
       "**Test Mechanism**: This test scans the provided dataset and extracts all the numerical columns. For each numerical column, it constructs a histogram using plotly, with 50 bins. The deployment of histograms offers a robust visual aid, ensuring unruffled identification and understanding of numerical data distribution patterns.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- A high degree of skewness\n",
       "- Unexpected data distributions\n",
       "- Existence of extreme outliers in the histograms These may indicate issues with the data that the model is receiving. If data for a numerical feature is expected to follow a certain distribution (like normal distribution) but does not, it could lead to sub-par performance by the model. As such these instances should be treated as high-risk indicators.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- This test provides a simple, easy-to-interpret visualization of how data for each numerical attribute is distributed.\n",
       "- It can help detect skewed values and outliers, that could potentially harm the AI model's performance.\n",
       "- It can be applied to large datasets and multiple numerical variables conveniently.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- This test only works with numerical data, thus ignoring non-numerical or categorical data.\n",
       "- It does not analyze relationships between different features, only the individual feature distributions.\n",
       "- It is a univariate analysis, and may miss patterns or anomalies that only appear when considering multiple variables together.\n",
       "- It does not provide any insight into how these features affect the output of the model; it is purely an input analysis tool.</td>\n",
       "      <td id=\"T_cf15d_row41_col3\" class=\"data row41 col3\" >validmind.data_validation.TabularNumericalHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row42_col0\" class=\"data row42 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row42_col1\" class=\"data row42 col1\" >High Cardinality</td>\n",
       "      <td id=\"T_cf15d_row42_col2\" class=\"data row42 col2\" >**Purpose**: The “High Cardinality” test is used to evaluate the number of unique values present in the categorical columns of a dataset. In this context, high cardinality implies the presence of a large number of unique, non-repetitive values in the dataset.\n",
       "\n",
       "**Test Mechanism**: The test first infers the dataset's type and then calculates an initial numeric threshold based on the test parameters. It only considers columns classified as \"Categorical\". For each of these columns, the number of distinct values (n_distinct) and the percentage of distinct values (p_distinct) are calculated. The test will pass if n_distinct is less than the calculated numeric threshold. Lastly, the results, which include details such as column name, number of distinct values, and pass/fail status, are compiled into a table.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A large number of distinct values (high cardinality) in one or more categorical columns implies a high risk.\n",
       "- A column failing the test (n_distinct >= num_threshold) is another indicator of high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The High Cardinality test is effective in early detection of potential overfitting and unwanted noise.\n",
       "- It aids in identifying potential outliers and inconsistencies, thereby improving data quality.\n",
       "- The test can be applied to both, classification and regression task types, demonstrating its versatility.\n",
       "\n",
       "**Limitations**:\n",
       "- The test is restricted to only \"Categorical\" data types and is thus not suitable for numerical or continuous features, limiting its scope.\n",
       "- The test does not consider the relevance or importance of unique values in categorical features, potentially causing it to overlook critical data points.\n",
       "- The threshold (both number and percent) used for the test is static and may not be optimal for diverse datasets and varied applications. Further mechanisms to adjust and refine this threshold could enhance its effectiveness.</td>\n",
       "      <td id=\"T_cf15d_row42_col3\" class=\"data row42 col3\" >validmind.data_validation.HighCardinality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row43_col0\" class=\"data row43 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row43_col1\" class=\"data row43 col1\" >Missing Values</td>\n",
       "      <td id=\"T_cf15d_row43_col2\" class=\"data row43 col2\" >**Purpose**: This test is designed to evaluate the quality of a dataset by measuring the number of missing values across all features. The objective is to ensure that the ratio of missing data to total data is less than a predefined threshold, defaulting to 1, to maintain the data quality necessary for reliable predictive strength in a machine learning model.\n",
       "\n",
       "**Test Mechanism**: The mechanism for this test involves iterating through each column of the dataset, counting missing values (represented as NaNs), and calculating the percentage they represent against the total number of rows. The test then checks if these missing value counts are less than the predefined `min_threshold`. The results are shown in a table summarizing each column, the number of missing values, the percentage of missing values in each column, and a Pass/Fail status based on the threshold comparison.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- When the number of missing values in any column exceeds the `min_threshold` value, it indicates a high risk.\n",
       "- A high risk is also flagged when missing values are present across many columns. In both instances, the test would return a \"Fail\" mark.\n",
       "\n",
       "**Strengths**:\n",
       "- The test offers a quick and granular identification of missing data across each feature in the dataset.\n",
       "- It provides an effective, straightforward means of maintaining data quality, which is vital for constructing efficient machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- Even though the test can efficiently identify missing values, it does not suggest the root causes of these missing values or recommend ways to impute or handle them.\n",
       "- The test might overlook features with a significant amount of missing data, but still less than the `min_threshold`. This could impact the model, especially if `min_threshold` is set too high.\n",
       "- The test does not account for data encoded as values (like \"-999\" or \"None\"), which might not technically classify as missing but could bear similar implications.</td>\n",
       "      <td id=\"T_cf15d_row43_col3\" class=\"data row43 col3\" >validmind.data_validation.MissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row44_col0\" class=\"data row44 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row44_col1\" class=\"data row44 col1\" >Rolling Stats Plot</td>\n",
       "      <td id=\"T_cf15d_row44_col2\" class=\"data row44 col2\" >**Purpose**: The `RollingStatsPlot` metric is employed to gauge the stationarity of time series data in a given dataset. This metric specifically evaluates the rolling mean and rolling standard deviation of the dataset over a pre-specified window size. The rolling mean provides an understanding of the average trend in the data, while the rolling standard deviation gauges the volatility of the data within the window. It is critical in preparing time series data for modeling as it reveals key insights into data behavior across time.\n",
       "\n",
       "**Test Mechanism**: This mechanism is comprised of two steps. Initially, the rolling mean and standard deviation for each of the dataset's columns are calculated over a window size, which can be user-specified or by default set to 12 data points. Then, the calculated rolling mean and standard deviation are visualized via separate plots, illustrating the trends and volatility in the dataset. A straightforward check is conducted to ensure the existence of columns in the dataset, and to verify that the given dataset has been indexed by its date and time—a necessary prerequisites for time series analysis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The presence of non-stationary patterns in either the rolling mean or the rolling standard deviation plots, which could indicate trends or seasonality in the data that may affect the performance of time series models.\n",
       "- Missing columns in the dataset, which would prevent the execution of this metric correctly.\n",
       "- The detection of NaN values in the dataset, which may need to be addressed before the metric can proceed successfully.\n",
       "\n",
       "**Strengths**:\n",
       "- Offers visualizations of trending behaviour and volatility within the data, facilitating a broader understanding of the dataset's inherent characteristics.\n",
       "- Checks of the dataset's integrity, such as existence of all required columns and the availability of a datetime index.\n",
       "- Adjusts to accommodate various window sizes, thus allowing accurate analysis of data with differing temporal granularities.\n",
       "- Considers each column of the data individually, thereby accommodating multi-feature datasets.\n",
       "\n",
       "**Limitations**:\n",
       "- For all columns, a fixed-size window is utilised. This may not accurately capture patterns in datasets where different features may require different optimal window sizes.\n",
       "- Requires the dataset to be indexed by date and time, hence it may not be useable for datasets without a timestamp index.\n",
       "- Primarily serves for data visualization as it does not facilitate any quantitative measures for stationarity, such as through statistical tests. Therefore, the interpretation is subjective and depends heavily on modeler discretion.</td>\n",
       "      <td id=\"T_cf15d_row44_col3\" class=\"data row44 col3\" >validmind.data_validation.RollingStatsPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row45_col0\" class=\"data row45 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row45_col1\" class=\"data row45 col1\" >Dataset Correlations</td>\n",
       "      <td id=\"T_cf15d_row45_col2\" class=\"data row45 col2\" >**Purpose**: The DatasetCorrelations metric is employed to examine the relationship between variables in a dataset, specifically designed for numerical and categorical data types. Using Pearson's R, Cramer's V, and Correlation ratios, it helps in understanding the linear relationship between numerical variables, association between categorical ones, and between numerical-categorical variables respectively. This allows for better awareness regarding dependency between features, which is crucial for optimizing model performance and understanding the model's behavior and predictors.\n",
       "\n",
       "**Test Mechanism**: During its execution, DatasetCorrelations initiates the calculation of the aforementioned correlation coefficients for the provided dataset. It leverages the built-in method 'get_correlations()', populating the 'correlations' attribute in the dataset object. It then invokes 'get_correlation_plots()' to generate graphical representations of these correlations. Finally, the correlation details and figures are cached for further study and analysis. The test does not dictate specific thresholds or grading scales.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- High correlation levels between input variables (multicollinearity), which can jeopardize the interpretability of the model and lead to overfitting.\n",
       "- The absence of any significant correlations, suggesting the variables may not have predictive power.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Comprehensive coverage of the correlation study of numerical, categorical, and numerical-categorical variables, negating the need for multiple individual tests.\n",
       "- Along with numerical correlation values, it provides visualization plots for a more intuitive understanding of relationships between variables.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Limitations of this metric include detecting only linear relationships and associations; nonlinear relationships may go unnoticed.\n",
       "- The absence of specified thresholds for determining correlation significance means the interpretation of the results is dependent on the user's expertise.\n",
       "- It doesn't manage missing values in the dataset, which need to be treated beforehand.</td>\n",
       "      <td id=\"T_cf15d_row45_col3\" class=\"data row45 col3\" >validmind.data_validation.DatasetCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row46_col0\" class=\"data row46 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row46_col1\" class=\"data row46 col1\" >Tabular Description Tables</td>\n",
       "      <td id=\"T_cf15d_row46_col2\" class=\"data row46 col2\" >**Purpose**: The main purpose of this metric is to gather and present the descriptive statistics of numerical, categorical, and datetime variables present in a dataset. The attributes it measures include the count, mean, minimum and maximum values, percentage of missing values, data types of fields, and unique values for categorical fields, among others.\n",
       "\n",
       "**Test Mechanism**: The test first segregates the variables in the dataset according to their data types (numerical, categorical, or datetime). Then, it compiles summary statistics for each type of variable. The specifics of these statistics vary depending on the type of variable:\n",
       "\n",
       "- For numerical variables, the metric extracts descriptors like count, mean, minimum and maximum values, count of missing values, and data types.\n",
       "- For categorical variables, it counts the number of unique values, displays unique values, counts missing values, and identifies data types.\n",
       "- For datetime variables, it counts the number of unique values, identifies the earliest and latest dates, counts missing values, and identifies data types.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Masses of missing values in the descriptive statistics results could hint at high risk or failure, indicating potential data collection, integrity, and quality issues.\n",
       "- Detection of inappropriate distributions for numerical variables, like having negative values for variables that are always supposed to be positive.\n",
       "- Identifying inappropriate data types, like a continuous variable being encoded as a categorical type.\n",
       "\n",
       "**Strengths**:\n",
       "- Provides a comprehensive overview of the dataset.\n",
       "- Gives a snapshot into the essence of the numerical, categorical, and datetime fields.\n",
       "- Identifies potential data quality issues such as missing values or inconsistencies crucial for building credible machine learning models.\n",
       "- The metadata, including the data type and missing value information, are vital for anyone including data scientists dealing with the dataset before the modeling process.\n",
       "\n",
       "**Limitations**:\n",
       "- It does not perform any deeper statistical analysis or tests on the data.\n",
       "- It does not handle issues such as outliers, or relationships between variables.\n",
       "- It offers no insights into potential correlations or possible interactions between variables.\n",
       "- It does not investigate the potential impact of missing values on the performance of the machine learning models.\n",
       "- It does not explore potential transformation requirements that may be necessary to enhance the performance of the chosen algorithm.</td>\n",
       "      <td id=\"T_cf15d_row46_col3\" class=\"data row46 col3\" >validmind.data_validation.TabularDescriptionTables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row47_col0\" class=\"data row47 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row47_col1\" class=\"data row47 col1\" >Auto MA</td>\n",
       "      <td id=\"T_cf15d_row47_col2\" class=\"data row47 col2\" >**Purpose**: The `AutoMA` metric serves an essential role of automated decision-making for selecting the optimal Moving Average (MA) order for every variable in a given time series dataset. The selection is dependent on the minimalization of BIC (Bayesian Information Criterion) and AIC (Akaike Information Criterion); these are established statistical tools used for model selection. Furthermore, prior to the commencement of the model fitting process, the algorithm conducts a stationarity test (Augmented Dickey-Fuller test) on each series.\n",
       "\n",
       "**Test Mechanism**: Starting off, the `AutoMA` algorithm checks whether the `max_ma_order` parameter has been provided. It consequently loops through all variables in the dataset, carrying out the Dickey-Fuller test for stationarity. For each stationary variable, it fits an ARIMA model for orders running from 0 to `max_ma_order`. The result is a list showcasing the BIC and AIC values of the ARIMA models based on different orders. The MA order, which yields the smallest BIC, is chosen as the 'best MA order' for every single variable. The final results include a table summarizing the auto MA analysis and another table listing the best MA order for each variable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- When a series is non-stationary (p-value>0.05 in the Dickey-Fuller test), the produced result could be inaccurate.\n",
       "- Any error that arises in the process of fitting the ARIMA models, especially with a higher MA order, can potentially indicate risks and might need further investigation.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric facilitates automation in the process of selecting the MA order for time series forecasting. This significantly saves time and reduces efforts conventionally necessary for manual hyperparameter tuning.\n",
       "- The use of both BIC and AIC enhances the likelihood of selecting the most suitable model.\n",
       "- The metric ascertains the stationarity of the series prior to model fitting, thus ensuring that the underlying assumptions of the MA model are fulfilled.\n",
       "\n",
       "**Limitations**:\n",
       "- If the time series fails to be stationary, the metric may yield inaccurate results. Consequently, it necessitates pre-processing steps to stabilize the series before fitting the ARIMA model.\n",
       "- The metric adopts a rudimentary model selection process based on BIC and doesn't consider other potential model selection strategies. Depending on the specific dataset, other strategies could be more appropriate.\n",
       "- The 'max_ma_order' parameter must be manually input which doesn't always guarantee optimal performance, especially when configured too low.\n",
       "- The computation time increases with the rise in `max_ma_order`, hence, the metric may become computationally costly for larger values.</td>\n",
       "      <td id=\"T_cf15d_row47_col3\" class=\"data row47 col3\" >validmind.data_validation.AutoMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row48_col0\" class=\"data row48 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row48_col1\" class=\"data row48 col1\" >Unique Rows</td>\n",
       "      <td id=\"T_cf15d_row48_col2\" class=\"data row48 col2\" >**Purpose**: The UniqueRows test is designed to gauge the quality of the data supplied to the machine learning model by verifying that the count of distinct rows in the dataset exceeds a specific threshold, thereby ensuring a varied collection of data. Diversity in data is essential for training an unbiased and robust model that excels when faced with novel data.\n",
       "\n",
       "**Test Mechanism**: The testing process starts with calculating the total number of rows in the dataset. Subsequently, the count of unique rows is determined for each column in the dataset. If the percentage of unique rows (calculated as the ratio of unique rows to the overall row count) is less than the prescribed minimum percentage threshold given as a function parameter, the test is passed. The results are cached and a final pass or fail verdict is given based on whether all columns have successfully passed the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A lack of diversity in data columns, demonstrated by a count of unique rows that falls short of the preset minimum percentage threshold, is indicative of high risk.\n",
       "- This lack of variety in the data signals potential issues with data quality, possibly leading to overfitting in the model and issues with generalization, thus posing a significant risk.\n",
       "\n",
       "**Strengths**:\n",
       "- The UniqueRows test is efficient in evaluating the data's diversity across each information column in the dataset.\n",
       "- This test provides a quick, systematic method to assess data quality based on uniqueness, which can be pivotal in developing effective and unbiased machine learning models.\n",
       "\n",
       "**Limitations**:\n",
       "- A limitation of the UniqueRows test is its assumption that the data's quality is directly proportionate to its uniqueness, which may not always hold true. There might be contexts where certain non-unique rows are essential and should not be overlooked.\n",
       "- The test does not consider the relative 'importance' of each column in predicting the output, treating all columns equally.\n",
       "- This test may not be suitable or useful for categorical variables, where the count of unique categories is inherently limited.</td>\n",
       "      <td id=\"T_cf15d_row48_col3\" class=\"data row48 col3\" >validmind.data_validation.UniqueRows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row49_col0\" class=\"data row49 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row49_col1\" class=\"data row49 col1\" >Too Many Zero Values</td>\n",
       "      <td id=\"T_cf15d_row49_col2\" class=\"data row49 col2\" >**Purpose**: The 'TooManyZeroValues' test is utilized to identify numerical columns in the dataset that may present a quantity of zero values considered excessive. The aim is to detect situations where these may implicate data sparsity or a lack of variation, limiting their effectiveness within a machine learning model. The definition of 'too many' is quantified as a percentage of total values, with a default set to 3%.\n",
       "\n",
       "**Test Mechanism**: This test is conducted by looping through each column in the dataset and categorizing those that pertain to numerical data. On identifying a numerical column, the function computes the total quantity of zero values and their ratio to the total row count. Should the proportion exceed a pre-set threshold parameter, set by default at 0.03 or 3%, the column is considered to have failed the test. The results for each column are summarised and reported, indicating the count and percentage of zero values for each numerical column, alongside a status indicating whether the column has passed or failed the test.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Indicators evidencing a high risk connected with this test would include numerical columns showing a high ratio of zero values when compared to the total count of rows (exceeding a pre-determined threshold).\n",
       "- Columns characterized by zero values across the board suggest a complete lack of data variation, signifying high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- Assists in highlighting columns featuring an excess of zero values that could otherwise go unnoticed within a large dataset.\n",
       "- Provides the flexibility to alter the threshold that determines when the quantity of zero values becomes 'too many', thus catering to specific needs of a particular analysis or model.\n",
       "- Offers feedback in the form of both counts and percentages of zero values, which allows a closer inspection of the distribution and proportion of zeros within a column.\n",
       "- Targets specifically numerical data, thereby avoiding inappropriate application to non-numerical columns and mitigating the risk of false test failures.\n",
       "\n",
       "**Limitations**:\n",
       "- Is exclusively designed to check for zero values, and doesn’t assesses the potential impact of other values that could affect the dataset, such as extremely high or low figures, missing values or outliers.\n",
       "- Lacks the ability to detect a repetitive pattern of zeros, which could be significant in time-series or longitudinal data.\n",
       "- Zero values can actually be meaningful in some contexts, therefore tagging them as 'too many' could potentially misinterpret the data to some extent.\n",
       "- This test does not take into consideration the context of the dataset, and fails to recognize that within certain columns, a high number of zero values could be quite normal and not necessarily an indicator of poor data quality.\n",
       "- Cannot evaluate non-numerical or categorical columns, which might bring with them different types of concerns or issues.</td>\n",
       "      <td id=\"T_cf15d_row49_col3\" class=\"data row49 col3\" >validmind.data_validation.TooManyZeroValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row50_col0\" class=\"data row50 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row50_col1\" class=\"data row50 col1\" >High Pearson Correlation</td>\n",
       "      <td id=\"T_cf15d_row50_col2\" class=\"data row50 col2\" >**Purpose**: The High Pearson Correlation test measures the linear relationship between features in a dataset, with the main goal of identifying high correlations that might indicate feature redundancy or multicollinearity. Identification of such issue allows developers and risk management teams to properly deal with potential impacts on the machine learning model's performance and interpretability.\n",
       "\n",
       "**Test Mechanism**: The test works by generating pairwise Pearson correlations for all features in the dataset, then sorting and eliminating duplicate and self-correlations. It assigns a Pass or Fail based on whether the absolute value of the correlation coefficient surpasses a pre-set threshold (defaulted at 0.3). It lastly returns the top ten strongest correlations regardless of passing or failing status.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk indication would be the presence of correlation coefficients exceeding the threshold.\n",
       "- If the features share a strong linear relationship, this could lead to potential multicollinearity and model overfitting.\n",
       "- Redundancy of variables can undermine the interpretability of the model due to uncertainty over the authenticity of individual variable's predictive power.\n",
       "\n",
       "**Strengths**:\n",
       "- The High Pearson Correlation test provides a quick and simple means of identifying relationships between feature pairs.\n",
       "- It generates a transparent output which not only displays pairs of correlated variables but also delivers the Pearson correlation coefficient and a Pass or Fail status for each.\n",
       "- It aids early identification of potential multicollinearity issues that may disrupt model training.\n",
       "\n",
       "**Limitations**:\n",
       "- The Pearson correlation test can only delineate linear relationships. It fails to shed light on nonlinear relationships or dependencies.\n",
       "- It is sensitive to outliers where a few outliers could notably affect the correlation coefficient.\n",
       "- It is limited to identifying redundancy only within feature pairs. When three or more variables are linearly dependent, it may fail to spot this complex relationship.\n",
       "- The top 10 result filter might not fully capture the richness of the data; an option to configure the number of retained results could be helpful.</td>\n",
       "      <td id=\"T_cf15d_row50_col3\" class=\"data row50 col3\" >validmind.data_validation.HighPearsonCorrelation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row51_col0\" class=\"data row51 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row51_col1\" class=\"data row51 col1\" >AC Fand PACF Plot</td>\n",
       "      <td id=\"T_cf15d_row51_col2\" class=\"data row51 col2\" >**Purpose**: The ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) plot test is employed to analyze time series data in machine learning models. It illuminates the correlation of the data over time by plotting the correlation of the series with its own lags (ACF), and the correlations after removing effects already accounted for by earlier lags (PACF). This information can identify trends, such as seasonality, degrees of autocorrelation, and inform the selection of order parameters for AutoRegressive Integrated Moving Average (ARIMA) models.\n",
       "\n",
       "**Test Mechanism**: The `ACFandPACFPlot` test accepts a dataset with a time-based index. It first confirms the index is of a datetime type, then handles any NaN values. The test subsequently generates ACF and PACF plots for each column in the dataset, producing a subplot for each. If the dataset doesn't include key columns, an error is returned.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- Sudden drops in the correlation at a specific lag might signal a model at high risk.\n",
       "- Consistent high correlation across multiple lags could also indicate non-stationarity in the data, which may suggest that a model estimated on this data won't generalize well to future, unknown data.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- ACF and PACF plots offer clear graphical representations of the correlations in time series data.\n",
       "- These plots are effective at revealing important data characteristics such as seasonality, trends, and correlation patterns.\n",
       "- The insights from these plots aid in better model configuration, particularly in the selection of ARIMA model parameters.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- ACF and PACF plots are exclusively for time series data and hence, can't be applied to all ML models.\n",
       "- These plots require large, consistent datasets as gaps could lead to misleading results.\n",
       "- The plots can only represent linear correlations and fail to capture any non-linear relationships within the data.\n",
       "- The plots might be difficult for non-experts to interpret and should not replace more advanced analyses.</td>\n",
       "      <td id=\"T_cf15d_row51_col3\" class=\"data row51 col3\" >validmind.data_validation.ACFandPACFPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row52_col0\" class=\"data row52 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row52_col1\" class=\"data row52 col1\" >Heatmap Feature Correlations</td>\n",
       "      <td id=\"T_cf15d_row52_col2\" class=\"data row52 col2\" >**Purpose:** The HeatmapFeatureCorrelations metric is utilized to evaluate the degree of interrelationships between pairs of input features within a dataset. This metric allows us to visually comprehend the correlation patterns through a heatmap, which can be essential in understanding which features may contribute most significantly to the performance of the model. Features that have high intercorrelation can potentially reduce the model's ability to learn, thus impacting the overall performance and stability of the machine learning model.\n",
       "\n",
       "**Test Mechanism:** The metric executes the correlation test by computing the Pearson correlations for all pairs of numerical features. It then generates a heatmap plot using seaborn, a Python data visualization library. The colormap ranges from -1 to 1, indicating perfect negative correlation and perfect positive correlation respectively. A 'declutter' option is provided which, if set to true, removes variable names and numerical correlations from the plot to provide a more streamlined view. The size of feature names and correlation coefficients can be controlled through 'fontsize' parameters.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- Indicators of potential risk include features with high absolute correlation values.\n",
       "- A significant degree of multicollinearity might lead to instabilities in the trained model and can also result in overfitting.\n",
       "- The presence of multiple homogeneous blocks of high positive or negative correlation within the plot might indicate redundant or irrelevant features included within the dataset.\n",
       "\n",
       "**Strengths:**\n",
       "- The strength of this metric lies in its ability to visually represent the extent and direction of correlation between any two numeric features, which aids in the interpretation and understanding of complex data relationships.\n",
       "- The heatmap provides an immediate and intuitively understandable representation, hence, it is extremely useful for high-dimensional datasets where extracting meaningful relationships might be challenging.\n",
       "\n",
       "**Limitations:**\n",
       "- The central limitation might be that it can only calculate correlation between numeric features, making it unsuitable for categorical variables unless they are already numerically encoded in a meaningful manner.\n",
       "- It uses Pearson's correlation, which only measures linear relationships between features. It may perform poorly in cases where the relationship is non-linear.\n",
       "- Large feature sets might result in cluttered and difficult-to-read correlation heatmaps, especially when the 'declutter' option is set to false.</td>\n",
       "      <td id=\"T_cf15d_row52_col3\" class=\"data row52 col3\" >validmind.data_validation.HeatmapFeatureCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row53_col0\" class=\"data row53 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row53_col1\" class=\"data row53 col1\" >Time Series Frequency</td>\n",
       "      <td id=\"T_cf15d_row53_col2\" class=\"data row53 col2\" >**Purpose**: The purpose of the TimeSeriesFrequency test is to evaluate the consistency in the frequency of data points in a time-series dataset. This test inspects the intervals or duration between each data point to determine if a fixed pattern (such as daily, weekly, or monthly) exists. The identification of such patterns is crucial to time-series analysis as any irregularities could lead to erroneous results and hinder the model's capacity for identifying trends and patterns.\n",
       "\n",
       "**Test Mechanism**: Initially, the test checks if the dataframe index is in datetime format. Subsequently, it utilizes pandas' `infer_freq` method to identify the frequency of each data series within the dataframe. The `infer_freq` method attempts to establish the frequency of a time series and returns both the frequency string and a dictionary relating these strings to their respective labels. The test compares the frequencies of all datasets. If they share a common frequency, the test passes, but it fails if they do not. Additionally, Plotly is used to create a frequency plot, offering a visual depiction of the time differences between consecutive entries in the dataframe index.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The test fails, indicating multiple unique frequencies within the dataset. This failure could suggest irregular intervals between observations, potentially interrupting pattern recognition or trend analysis.\n",
       "- The presence of missing or null frequencies could be an indication of inconsistencies in data or gaps within the data collection process.\n",
       "\n",
       "**Strengths**:\n",
       "- This test uses a systematic approach to checking the consistency of data frequency within a time-series dataset.\n",
       "- It increases the model's reliability by asserting the consistency of observations over time, an essential factor in time-series analysis.\n",
       "- The test generates a visual plot, providing an intuitive representation of the dataset's frequency distribution, which caters to visual learners and aids in interpretation and explanation.\n",
       "\n",
       "**Limitations**:\n",
       "- This test is only applicable to time-series datasets and hence not suitable for other types of datasets.\n",
       "- The `infer_freq` method might not always correctly infer frequency when faced with missing or irregular data points.\n",
       "- Depending on context or the model under development, mixed frequencies might sometimes be acceptable, but this test considers them a failing condition.</td>\n",
       "      <td id=\"T_cf15d_row53_col3\" class=\"data row53 col3\" >validmind.data_validation.TimeSeriesFrequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row54_col0\" class=\"data row54 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row54_col1\" class=\"data row54 col1\" >Dataset Split</td>\n",
       "      <td id=\"T_cf15d_row54_col2\" class=\"data row54 col2\" >**Purpose:** The DatasetSplit test is designed to evaluate and visualize the distribution of data among training, testing, and validation datasets, if available, within a given machine learning model. The main purpose is to assess whether the model's datasets are split appropriately, as an imbalanced split might affect the model's ability to learn from the data and generalize to unseen data.\n",
       "\n",
       "**Test Mechanism:** The DatasetSplit test first calculates the total size of all available datasets in the model. Then, for each individual dataset, the methodology involves determining the size of the dataset and its proportion relative to the total size. The results are then conveniently summarized in a table that shows dataset names, sizes, and proportions. Absolute size and proportion of the total dataset size are displayed for each individual dataset.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- A very small training dataset, which may result in the model not learning enough from the data.\n",
       "- A very large training dataset and a small test dataset, which may lead to model overfitting and poor generalization to unseen data.\n",
       "- A small or non-existent validation dataset, which might complicate the model's performance assessment.\n",
       "\n",
       "**Strengths:**\n",
       "- The DatasetSplit test provides a clear, understandable visualization of dataset split proportions, which can highlight any potential imbalance in dataset splits quickly.\n",
       "- It covers a wide range of task types including classification, regression, and text-related tasks.\n",
       "- The metric is not tied to any specific data type and is applicable to tabular data, time series data, or text data.\n",
       "\n",
       "**Limitations:**\n",
       "- The DatasetSplit test does not provide any insight into the quality or diversity of the data within each split, just the size and proportion.\n",
       "- The test does not give any recommendations or adjustments for imbalanced datasets.\n",
       "- Potential lack of compatibility with more complex modes of data splitting (for example, stratified or time-based splits) could limit the applicability of this test.</td>\n",
       "      <td id=\"T_cf15d_row54_col3\" class=\"data row54 col3\" >validmind.data_validation.DatasetSplit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row55_col0\" class=\"data row55 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row55_col1\" class=\"data row55 col1\" >Spread Plot</td>\n",
       "      <td id=\"T_cf15d_row55_col2\" class=\"data row55 col2\" >**Purpose**: The SpreadPlot metric is intended to graphically illustrate and analyse the relationships between pairs of time series variables within a given dataset. This facilitated understanding helps in identifying and assessing potential time series correlations, like cointegration, between the variables.\n",
       "\n",
       "**Test Mechanism**: The SpreadPlot metric operates by computing and representing the spread between each pair of time series variables in the dataset. In particular, the difference between two variables is calculated and presented as a line graph. This method is iterated for each unique pair of variables in the dataset.\n",
       "\n",
       "**Signs of High Risk**: Potential indicators of high risk related to the SpreadPlot metric might include:\n",
       "\n",
       "- Large fluctuations in the spread over a given timespan\n",
       "- Unexpected patterns or trends that may signal a potential risk in the underlying correlations between the variables\n",
       "- Presence of significant missing data or extreme outlier values, which could potentially skew the spread and indicate high risk\n",
       "\n",
       "**Strengths**: The SpreadPlot metric provides several key advantages:\n",
       "\n",
       "- It allows for thorough visual examination and interpretation of the correlations between time-series pairs\n",
       "- It aids in revealing complex relationships like cointegration\n",
       "- It enhances interpretability by visualising the relationships, thereby helping in spotting outliers and trends\n",
       "- It is capable of handling numerous variable pairs from the dataset through a versatile and adaptable process\n",
       "\n",
       "**Limitations**: Despite its advantages, the SpreadPlot metric does have certain drawbacks:\n",
       "\n",
       "- It primarily serves as a visualisation tool and does not offer quantitative measurements or statistics to objectively determine relationships\n",
       "- It heavily relies on the quality and granularity of the data\n",
       "- missing data or outliers can notably disturb the interpretation of the relationships\n",
       "- It can become inefficient or difficult to interpret with a high number of variables due to the profuse number of plots\n",
       "- It might not completely capture intricate non-linear relationships between the variables</td>\n",
       "      <td id=\"T_cf15d_row55_col3\" class=\"data row55 col3\" >validmind.data_validation.SpreadPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row56_col0\" class=\"data row56 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row56_col1\" class=\"data row56 col1\" >Time Series Line Plot</td>\n",
       "      <td id=\"T_cf15d_row56_col2\" class=\"data row56 col2\" >**Purpose**: The TimeSeriesLinePlot metric is designed to generate and analyze time series data through the creation of line plots. This assists in the initial inspection of the data by providing a visual representation of patterns, trends, seasonality, irregularity, and anomalies that may be present in the dataset over a period of time.\n",
       "\n",
       "**Test Mechanism**: The mechanism for this Python class involves extracting the column names from the provided dataset and subsequently generates line plots for each column using the Plotly Python library. For every column in the dataset, a time-series line plot is created where the values are plotted against the dataset's datetime index. It is important to note that indexes that are not of datetime type will result in a ValueError.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Presence of time-series data that does not have datetime indices.\n",
       "- Provided columns do not exist in the provided dataset.\n",
       "- The detection of anomalous patterns or irregularities in the time-series plots, indicating potential high model instability or probable predictive error.\n",
       "\n",
       "**Strengths**:\n",
       "- The visual representation of complex time series data, which simplifies understanding and helps in recognizing temporal trends, patterns, and anomalies.\n",
       "- The adaptability of the metric, which allows it to effectively work with multiple time series within the same dataset.\n",
       "- Enables the identification of anomalies and irregular patterns through visual inspection, assisting in spotting potential data or model performance problems.\n",
       "\n",
       "**Limitations**:\n",
       "- The effectiveness of the metric is heavily reliant on the quality and patterns of the provided time series data.\n",
       "- Exclusively a visual tool, it lacks the capability to provide quantitative measurements, making it less effective for comparing and ranking multiple models or when specific numerical diagnostics are needed.\n",
       "- The metric necessitates that the time-specific data has been transformed into a datetime index, with the data formatted correctly.\n",
       "- The metric has an inherent limitation in that it cannot extract deeper statistical insights from the time series data, which can limit its efficacy with complex data structures and phenomena.</td>\n",
       "      <td id=\"T_cf15d_row56_col3\" class=\"data row56 col3\" >validmind.data_validation.TimeSeriesLinePlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row57_col0\" class=\"data row57 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row57_col1\" class=\"data row57 col1\" >Auto Seasonality</td>\n",
       "      <td id=\"T_cf15d_row57_col2\" class=\"data row57 col2\" >**Purpose:** The AutoSeasonality metric's purpose is to automatically detect and identify the best seasonal order or period for each variable in a time series dataset. This detection helps to quantify periodic patterns and seasonality that reoccur at fixed intervals of time in the data. This is especially significant for forecasting-based models, where understanding the seasonality component can drastically improve prediction accuracy.\n",
       "\n",
       "**Test Mechanism:** This metric uses the seasonal decomposition method from the Statsmodels Python library. The function takes the 'additive' model type for each variable and applies it within the prescribed range of 'min_period' and 'max_period'. The function decomposes the seasonality for each period in the range and calculates the mean residual error for each period. The seasonal period that results in the minimum residuals is marked as the 'Best Period'. The test results include the 'Best Period', the calculated residual errors, and a determination of 'Seasonality' or 'No Seasonality'.\n",
       "\n",
       "**Signs of High Risk:**\n",
       "- If the optimal seasonal period (or 'Best Period') is consistently at the maximum or minimum limit of the offered range for a majority of variables, it may suggest that the range set does not adequately capture the true seasonal pattern in the series.\n",
       "- A high average 'Residual Error' for the selected 'Best Period' could indicate issues with the model's performance.\n",
       "\n",
       "**Strengths:**\n",
       "- The metric offers an automatic approach to identifying and quantifying the optimal seasonality, providing a robust method for analyzing time series datasets.\n",
       "- It is applicable to multiple variables in a dataset, providing a comprehensive evaluation of each variable's seasonality.\n",
       "- The use of concrete and measurable statistical methods improves the objectivity and reproducibility of the model.\n",
       "\n",
       "**Limitations:**\n",
       "- This AutoSeasonality metric may not be suitable if the time series data exhibits random walk behaviour or lacks clear seasonality, as the seasonal decomposition model may not be appropriate.\n",
       "- The defined range for the seasonal period (min_period and max_period) can influence the outcomes. If the actual seasonality period lies outside this range, this method will not be able to identify the true seasonal order.\n",
       "- This metric may not be able to fully interpret complex patterns that go beyond the simple additive model for seasonal decomposition.\n",
       "- The tool may incorrectly infer seasonality if random fluctuations in the data match the predefined seasonal period range.</td>\n",
       "      <td id=\"T_cf15d_row57_col3\" class=\"data row57 col3\" >validmind.data_validation.AutoSeasonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row58_col0\" class=\"data row58 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row58_col1\" class=\"data row58 col1\" >Engle Granger Coint</td>\n",
       "      <td id=\"T_cf15d_row58_col2\" class=\"data row58 col2\" >**Purpose**: The intent of this Engle-Granger cointegration test is to explore and quantify the degree of co-movement between pairs of time series variables in a dataset. This is particularly useful in enhancing the accuracy of predictive regressions whenever the underlying variables are co-integrated, i.e., they move together over time.\n",
       "\n",
       "**Test Mechanism**: The test first drops any non-applicable values from the input dataset and then iterates over each pair of variables to apply the Engle-Granger cointegration test. The test generates a 'p' value, which is then compared against a pre-specified threshold (0.05 by default). The pair is labeled as 'Cointegrated' if the 'p' value is less than or equal to the threshold or 'Not cointegrated' otherwise. A summary table is returned by the metric showing cointegration results for each variable pair.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk might be indicated if a significant number of variables that were hypothesized to be cointegrated do not pass the test.\n",
       "- Another sign of high risk is if a considerable number of 'p' values are close to the threshold. This is a risk because minor fluctuations in the data can switch the decision between 'Cointegrated' and 'Not cointegrated'.\n",
       "\n",
       "**Strengths**:\n",
       "- The Engle-Granger cointegration test provides an effective way to analyze relationships between time series, particularly in contexts where it's essential to check if variables are moving together in a statistically significant manner.\n",
       "- It is useful in various domains, especially finance or economics. Here, predictive models often hinge on understanding how different variables move together over time.\n",
       "\n",
       "**Limitations**:\n",
       "- The Engle-Granger cointegration test assumes that the time series are integrated of the same order, which isn't always true in multivariate time series datasets.\n",
       "- The presence of non-stationary characteristics in the series or structural breaks can result in falsely positive or negative cointegration results.\n",
       "- The test may not perform well for small sample sizes due to lack of statistical power. Therefore, it should be used with caution, and whenever possible, supplemented with other predictive indicators for a more robust model evaluation.</td>\n",
       "      <td id=\"T_cf15d_row58_col3\" class=\"data row58 col3\" >validmind.data_validation.EngleGrangerCoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row59_col0\" class=\"data row59 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_cf15d_row59_col1\" class=\"data row59 col1\" >Time Series Missing Values</td>\n",
       "      <td id=\"T_cf15d_row59_col2\" class=\"data row59 col2\" >**Purpose**: This test is designed to validate the quality of a historical time-series dataset by verifying that the number of missing values is below a specified threshold. As time-series models greatly depend on the continuity and temporality of data points, missing values could compromise the model's performance. Consequently, this test aims to ensure data quality and readiness for the machine learning model, safeguarding its predictive capacity.\n",
       "\n",
       "**Test Mechanism**: The test method commences by validating if the dataset has a datetime index, if not, an error is raised. It establishes a lower limit threshold for missing values and performs a missing values check on each column of the dataset. An object for the test result is created stating whether the number of missing values is within the specified threshold. Additionally, the test calculates the percentage of missing values alongside the raw count.\n",
       "\n",
       "To aid in data visualization, the test generates two plots\n",
       "- a bar plot and a heatmap, to better illustrate the distribution and quantity of missing values per variable. The test results, a count of missing values, the percentage of missing values, and a pass/fail status are returned in a results table.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The number of missing values in any column of the dataset surpasses the threshold, marking a failure and a high-risk scenario. The reasons could range from incomplete data collection, faulty sensors to data preprocessing errors.\n",
       "- A continuous visual 'streak' in the heatmap may indicate a systematic error during data collection, pointing towards another potential risk source.\n",
       "\n",
       "**Strengths**:\n",
       "- Effectively identifies missing values which could adversely affect the model's performance.\n",
       "- Applicable and customizable through the threshold parameter across different data sets.\n",
       "- Goes beyond raw numbers by calculating the percentage of missing values, offering a more relative understanding of data scarcity.\n",
       "- Includes a robust visualization mechanism for easy and fast understanding of data quality.\n",
       "\n",
       "**Limitations**:\n",
       "- Although it identifies missing values, the test does not provide solutions to handle them.\n",
       "- The test demands that the dataset should have a datetime index, hence limiting its use only to time series analysis.\n",
       "- The test's sensitivity to the 'min_threshold' parameter may raise false alarms if set too strictly or may overlook problematic data if set too loosely.\n",
       "- Solely focuses on the 'missingness' of the data and might fall short in addressing other aspects of data quality.</td>\n",
       "      <td id=\"T_cf15d_row59_col3\" class=\"data row59 col3\" >validmind.data_validation.TimeSeriesMissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row60_col0\" class=\"data row60 col0\" >DatasetMetadata</td>\n",
       "      <td id=\"T_cf15d_row60_col1\" class=\"data row60 col1\" >Dataset Metadata</td>\n",
       "      <td id=\"T_cf15d_row60_col2\" class=\"data row60 col2\" >**Purpose**: The `DatasetMetadata` test is primarily aimed at collecting and logging essential descriptive statistics related to the training datasets. This test generates essential metadata such as the types of tasks (classification, regression, text_classification, text_summarization) and tags (tabular_data, time_series_data, text_data) associated with the datasets. This transparency facilitates model validation by linking different metrics and test results to the originating dataset.\n",
       "\n",
       "**Test Mechanism**: Rather than conducting a test or implementing a grading scale, this class collects and logs dataset metadata. During post-initialization, the metadata is linked to the dataset object. The `run` method produces a `TestSuiteDatasetResult` object, which is assigned a unique ID and is bound to a dataset. The dataset metadata is associated with this ID for use in future, more focused, validation procedures.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The metadata is incomplete or incorrect which can lead to inaccuracies in model risk assessment.\n",
       "- Dataset labels or types are missing, leading to issues in further model validation or mispresentations.\n",
       "\n",
       "**Strengths**:\n",
       "- The class brings transparency to model validation exercises by providing detailed information about the dataset.\n",
       "- It assists in error diagnosis and behaviors correlation to the model.\n",
       "- Ensures the correctness of tasks and data types associations and allows superior model explanations.\n",
       "- Supports dataset versioning by logging each dataset's metadata, maintaining a trackable history of alterations.\n",
       "\n",
       "**Limitations**:\n",
       "- The `DatasetMetadata` class's completeness and accuracy might be questionable, especially if metadata isn't appropriately added or is inaccurate.\n",
       "- It doesn't involve the evaluation of the dataset's quality or the direct validation of model predictions, hence it should be combined with other tests for a more comprehensive assessment.\n",
       "- The class cannot detect potential bias in the dataset. For bias detection, separate tests specifically tailored towards fairness and bias detection would be necessary.</td>\n",
       "      <td id=\"T_cf15d_row60_col3\" class=\"data row60 col3\" >validmind.data_validation.DatasetMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row61_col0\" class=\"data row61 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row61_col1\" class=\"data row61 col1\" >Time Series Histogram</td>\n",
       "      <td id=\"T_cf15d_row61_col2\" class=\"data row61 col2\" >**Purpose**: The purpose of this metric is to perform a histogram analysis on time-series data. It primarily assesses the distribution of values within a dataset over a period of time, typically used for regression tasks. The types of data that this metric can be applicable to are diverse, ranging from internet traffic and stock prices to weather data. This analysis provides valuable insights into the probability distribution, skewness, and peakness (kurtosis) underlying the data.\n",
       "\n",
       "**Test Mechanism**: This test operates on a specific column within the dataset that is required to have a datetime type index. It goes through each column in the given dataset, creating a histogram with Seaborn's histplot function. In cases where the dataset includes more than one time-series (i.e., more than one column with a datetime type index), a distinct histogram is plotted for each series. Additionally, a kernel density estimate (KDE) line is drawn for each histogram, providing a visualization of the data's underlying probability distribution. The x and y-axis labels are purposely hidden to concentrate solely on the data distribution.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- The dataset lacks a column with a datetime type index.\n",
       "- The specified columns do not exist within the dataset.\n",
       "- The data distribution within the histogram demonstrates high degrees of skewness or kurtosis, which could bias the model.\n",
       "- Outliers that differ significantly from the primary data distribution are present.\n",
       "\n",
       "**Strengths**:\n",
       "- It serves as a visual diagnostic tool, offering an ideal starting point for understanding the overall behavior and distribution trends within the dataset.\n",
       "- It is effective for both single and multiple time-series data analysis.\n",
       "- The Kernel Density Estimation (KDE) line provides a smooth estimate of the overall trend in data distribution.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric only presents a high-level view of data distribution and does not offer specific numeric measures such as skewness or kurtosis.\n",
       "- The histogram does not display precise data values; due to the data grouping into bins, some detail is inevitably lost, marking a trade-off between precision and general overview.\n",
       "- The histogram cannot handle non-numeric data columns.\n",
       "- The histogram's shape may be sensitive to the number of bins used.</td>\n",
       "      <td id=\"T_cf15d_row61_col3\" class=\"data row61 col3\" >validmind.data_validation.TimeSeriesHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row62_col0\" class=\"data row62 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row62_col1\" class=\"data row62 col1\" >Lagged Correlation Heatmap</td>\n",
       "      <td id=\"T_cf15d_row62_col2\" class=\"data row62 col2\" >**Purpose**: The LaggedCorrelationHeatmap metric is utilized to appraise and illustrate the correlation between the target variable and delayed copies (lags) of independent variables in a time-series dataset. It assists in revealing relationships in time-series data where the influence of an independent variable on the dependent variable is not immediate but occurs after a period (lags).\n",
       "\n",
       "**Test Mechanism**: To execute this test, Python's Pandas library pairs with Plotly to perform computations and present the visualization in the form of a heatmap. The test begins by extracting the target variable and corresponding independent variables from the dataset. Then, generation of lags of independent variables takes place, followed by the calculation of correlation between these lagged variables and the target variable. The outcome is a correlation matrix that gets recorded and illustrated as a heatmap, where different color intensities represent the strength of the correlation, making patterns easier to identify.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Insignificant correlations across the heatmap, indicating a lack of noteworthy relationships between variables.\n",
       "- Correlations that break intuition or previous understanding, suggesting potential issues with the dataset or the model.\n",
       "\n",
       "**Strengths**:\n",
       "- This metric serves as an exceptional tool for exploring and visualizing time-dependent relationships between features and the target variable in a time-series dataset.\n",
       "- It aids in identifying delayed effects that might go unnoticed with other correlation measures.\n",
       "- The heatmap offers an intuitive visual representation of time-dependent correlations and influences.\n",
       "\n",
       "**Limitations**:\n",
       "- The metric presumes linear relationships between variables, potentially ignoring non-linear relationships.\n",
       "- The correlation considered is linear; therefore, intricate non-linear interactions might be overlooked.\n",
       "- The metric is only applicable for time-series data, limiting its utility outside of this context.\n",
       "- The number of lags chosen can significantly influence the results; too many lags can render the heatmap difficult to interpret, while too few might overlook delayed effects.\n",
       "- This metric does not take into account any causal relationships, but merely demonstrates correlation.</td>\n",
       "      <td id=\"T_cf15d_row62_col3\" class=\"data row62 col3\" >validmind.data_validation.LaggedCorrelationHeatmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row63_col0\" class=\"data row63 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row63_col1\" class=\"data row63 col1\" >Seasonal Decompose</td>\n",
       "      <td id=\"T_cf15d_row63_col2\" class=\"data row63 col2\" >**Purpose**: This test utilizes the Seasonal Decomposition of Time Series by Loess (STL) method to decompose a dataset into its fundamental components: observed, trend, seasonal, and residuals. The purpose is to identify implicit patterns, majorly any seasonality, in the dataset's features which aid in developing a more comprehensive understanding and effectively validating the dataset.\n",
       "\n",
       "**Test Mechanism**: The testing process exploits the `seasonal_decompose` function from the `statsmodels.tsa.seasonal` library to evaluate each feature in the dataset. It isolates each feature into four components: observed, trend, seasonal, and residuals, and generates essentially six subplot graphs per feature for visual interpretation of the results. Prior to the seasonal decomposition, non-finite values are scrutinized and removed thus, ensuring reliability in the analysis.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- **Non-Finiteness**: If a dataset carries too many non-finite values it might flag high risk as these values are omitted before conducting the seasonal decomposition.\n",
       "- **Frequent Warnings**: The test could be at risk if it chronically fails to infer frequency for a scrutinized feature.\n",
       "- **High Seasonality**: A high seasonal component could potentially render forecasts unreliable due to overwhelming seasonal variation.\n",
       "\n",
       "**Strengths**:\n",
       "- **Seasonality Detection**: The code aptly discerns hidden seasonality patterns in the features of datasets.\n",
       "- **Visualization**: The test facilitates interpretation and comprehension via graphical representations.\n",
       "- **Unrestricted Usage**: The code is not confined to any specific regression model, thereby promoting wide-ranging applicability.\n",
       "\n",
       "**Limitations**:\n",
       "- **Dependence on Assumptions**: The test presumes that features in the dataset are periodically distributed. If no frequency could be inferred for a variable, that feature is excluded from the test.\n",
       "- **Handling Non-finite Values**: The test disregards non-finite values during the analysis which could potentially result in incomplete understanding of the dataset.\n",
       "- **Unreliability with Noisy Datasets**: The test tends to produce unreliable results when used with heavy noise present in the dataset.</td>\n",
       "      <td id=\"T_cf15d_row63_col3\" class=\"data row63 col3\" >validmind.data_validation.SeasonalDecompose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row64_col0\" class=\"data row64 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row64_col1\" class=\"data row64 col1\" >IQR Outliers Bar Plot</td>\n",
       "      <td id=\"T_cf15d_row64_col2\" class=\"data row64 col2\" >**Purpose**: The InterQuartile Range Outliers Bar Plot (IQROutliersBarPlot) metric aims to visually analyze and evaluate the extent of outliers in numeric variables based on percentiles. Its primary purpose is to clarify the dataset's distribution, flag possible abnormalities in it and gauge potential risks associated with processing potentially skewed data, which can affect the machine learning model's predictive prowess.\n",
       "\n",
       "**Test Mechanism**: The examination invokes a series of steps:\n",
       "\n",
       "1. For every numeric feature in the dataset, the 25th percentile (Q1) and 75th percentile (Q3) are calculated before deriving the Interquartile Range (IQR), the difference between Q1 and Q3. 2. Subsequently, the metric calculates the lower and upper thresholds by subtracting Q1 from the `threshold` times IQR and adding Q3 to `threshold` times IQR, respectively. The default `threshold` is set at 1.5. 3. Any value in the feature that falls below the lower threshold or exceeds the upper threshold is labeled as an outlier. 4. The number of outliers are tallied for different percentiles, such as [0-25], [25-50], [50-75], and [75-100]. 5. These counts are employed to construct a bar plot for the feature, showcasing the distribution of outliers across different percentiles.\n",
       "\n",
       "**Signs of High Risk**: High risk or a potential lapse in the model's performance could be unveiled by the following signs:\n",
       "\n",
       "- A prevalence of outliers in the data, potentially skewing its distribution.\n",
       "- Outliers dominating higher percentiles (75-100) which implies the presence of extreme values, capable of severely influencing the model's performance.\n",
       "- Certain features harboring most of their values as outliers, which signifies that these features might not contribute positively to the model's forecasting ability.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- Effectively identifies outliers in the data through visual means, facilitating easier comprehension and offering insights into the outliers' possible impact on the model.\n",
       "- Provides flexibility by accommodating all numeric features or a chosen subset.\n",
       "- Task-agnostic in nature; it is viable for both classification and regression tasks.\n",
       "- Can handle large datasets as its operation does not hinge on computationally heavy operations.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- Its application is limited to numerical variables and does not extend to categorical ones.\n",
       "- Relies on a predefined threshold (default being 1.5) for outlier identification, which may not be suitable for all cases.\n",
       "- Only reveals the presence and distribution of outliers and does not provide insights into how these outliers might affect the model's predictive performance.\n",
       "- The assumption that data is unimodal and symmetric may not always hold true. In cases with non-normal distributions, the results can be misleading.</td>\n",
       "      <td id=\"T_cf15d_row64_col3\" class=\"data row64 col3\" >validmind.data_validation.IQROutliersBarPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row65_col0\" class=\"data row65 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row65_col1\" class=\"data row65 col1\" >Auto AR</td>\n",
       "      <td id=\"T_cf15d_row65_col2\" class=\"data row65 col2\" >**Purpose**:\n",
       "\n",
       "The AutoAR test is intended to automatically identify the Autoregressive (AR) order of a time series by utilizing the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC). AR order is crucial in forecasting tasks as it dictates the quantity of prior terms in the sequence to use for predicting the current term. The objective is to select the most fitting AR model that encapsulates the trend and seasonality in the time series data.\n",
       "\n",
       "**Test Mechanism**:\n",
       "\n",
       "The test mechanism operates by iterating through a possible range of AR orders up to a defined maximum. An AR model is fitted for each order, and the corresponding BIC and AIC are computed. BIC and AIC statistical measures are designed to penalize models for complexity, preferring simpler models that fit the data proficiently. To verify the stationarity of the time series, the Augmented Dickey-Fuller test is executed. The AR order, BIC, and AIC findings, are compiled into a dataframe for effortless comparison. Then, the AR order with the smallest BIC is established as the desirable order for each variable.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "\n",
       "- An augmented Dickey Fuller test p-value > 0.05, indicating the time series isn't stationary, may lead to inaccurate results.\n",
       "- Problems with the model fitting procedure, such as computational or convergence issues.\n",
       "- Continuous selection of the maximum specified AR order may suggest insufficient set limit.\n",
       "\n",
       "**Strengths**:\n",
       "\n",
       "- The test independently pinpoints the optimal AR order, thereby reducing potential human bias.\n",
       "- It strikes a balance between model simplicity and goodness-of-fit to avoid overfitting.\n",
       "- Has the capability to account for stationarity in a time series, an essential aspect for dependable AR modelling.\n",
       "- The results are aggregated into an comprehensive table, enabling an easy interpretation.\n",
       "\n",
       "**Limitations**:\n",
       "\n",
       "- The tests need a stationary time series input.\n",
       "- They presume a linear relationship between the series and its lags.\n",
       "- The search for the best model is constrained by the maximum AR order supplied in the parameters. Therefore, a low max_ar_order could result in subpar outcomes.\n",
       "- AIC and BIC may not always agree on the selection of the best model. This potentially requires the user to juggle interpretational choices.</td>\n",
       "      <td id=\"T_cf15d_row65_col3\" class=\"data row65 col3\" >validmind.data_validation.AutoAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf15d_row66_col0\" class=\"data row66 col0\" >Metric</td>\n",
       "      <td id=\"T_cf15d_row66_col1\" class=\"data row66 col1\" >Tabular Date Time Histograms</td>\n",
       "      <td id=\"T_cf15d_row66_col2\" class=\"data row66 col2\" >**Purpose**: The `TabularDateTimeHistograms` metric is designed to provide graphical insight into the distribution of time intervals in a machine learning model's datetime data. By plotting histograms of differences between consecutive date entries in all datetime variables, it enables an examination of the underlying pattern of time series data and identification of anomalies.\n",
       "\n",
       "**Test Mechanism**: This test operates by first identifying all datetime columns and extracting them from the dataset. For each datetime column, it next computes the differences (in days) between consecutive dates, excluding zero values, and visualizes these differences in a histogram. The seaborn library's histplot function is used to generate histograms, which are labeled appropriately and provide a graphical representation of the frequency of different day intervals in the dataset.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- If no datetime columns are detected in the dataset, this would lead to a ValueError. Hence, the absence of datetime columns signifies a high risk.\n",
       "- A severely skewed or irregular distribution depicted in the histogram may indicate possible complications with the data, such as faulty timestamps or abnormalities.\n",
       "\n",
       "**Strengths**:\n",
       "- The metric offers a visual overview of time interval frequencies within the dataset, supporting the recognition of inherent patterns.\n",
       "- Histogram plots can aid in the detection of potential outliers and data anomalies, contributing to an assessment of data quality.\n",
       "- The metric is versatile, compatible with a range of task types, including classification and regression, and can work with multiple datetime variables if present.\n",
       "\n",
       "**Limitations**:\n",
       "- A major weakness of this metric is its dependence on the visual examination of data, as it does not provide a measurable evaluation of the model.\n",
       "- The metric might overlook complex or multi-dimensional trends in the data.\n",
       "- The test is only applicable to datasets containing datetime columns and will fail if such columns are unavailable.\n",
       "- The interpretation of the histograms relies heavily on the domain expertise and experience of the reviewer.</td>\n",
       "      <td id=\"T_cf15d_row66_col3\" class=\"data row66 col3\" >validmind.data_validation.TabularDateTimeHistograms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d591b340>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.list_tests(task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `task` parameter is designed for pinpointing tests that align with a specific task type. For instance, to find tests tailored for 'regression' tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_53aa1 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_53aa1_row0_col0, #T_53aa1_row0_col1, #T_53aa1_row0_col2, #T_53aa1_row0_col3, #T_53aa1_row1_col0, #T_53aa1_row1_col1, #T_53aa1_row1_col2, #T_53aa1_row1_col3, #T_53aa1_row2_col0, #T_53aa1_row2_col1, #T_53aa1_row2_col2, #T_53aa1_row2_col3, #T_53aa1_row3_col0, #T_53aa1_row3_col1, #T_53aa1_row3_col2, #T_53aa1_row3_col3, #T_53aa1_row4_col0, #T_53aa1_row4_col1, #T_53aa1_row4_col2, #T_53aa1_row4_col3, #T_53aa1_row5_col0, #T_53aa1_row5_col1, #T_53aa1_row5_col2, #T_53aa1_row5_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_53aa1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_53aa1_level0_col0\" class=\"col_heading level0 col0\" >Test Type</th>\n",
       "      <th id=\"T_53aa1_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_53aa1_level0_col2\" class=\"col_heading level0 col2\" >Description</th>\n",
       "      <th id=\"T_53aa1_level0_col3\" class=\"col_heading level0 col3\" >ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_53aa1_row0_col0\" class=\"data row0 col0\" >Metric</td>\n",
       "      <td id=\"T_53aa1_row0_col1\" class=\"data row0 col1\" >Confusion Matrix</td>\n",
       "      <td id=\"T_53aa1_row0_col2\" class=\"data row0 col2\" >**Purpose**: The Confusion Matrix tester is designed to assess the performance of a classification Machine Learning model. This performance is evaluated based on how well the model is able to correctly classify True Positives, True Negatives, False Positives, and False Negatives\n",
       "- fundamental aspects of model accuracy.\n",
       "\n",
       "**Test Mechanism**: The mechanism used involves taking the predicted results (`y_test_predict`) from the classification model and comparing them against the actual values (`y_test_true`). A confusion matrix is built using the unique labels extracted from `y_test_true`, employing scikit-learn's metrics. The matrix is then visually rendered with the help of Plotly's `create_annotated_heatmap` function. A heatmap is created which provides a two-dimensional graphical representation of the model's performance, showcasing distributions of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
       "\n",
       "**Signs of High Risk**: Indicators of high risk related to the model include:\n",
       "- High numbers of False Positives (FP) and False Negatives (FN), depicting that the model is not effectively classifying the values.\n",
       "- Low numbers of True Positives (TP) and True Negatives (TN), implying that the model is struggling with correctly identifying class labels.\n",
       "\n",
       "**Strengths**: The Confusion Matrix tester brings numerous strengths:\n",
       "- It provides a simplified yet comprehensive visual snapshot of the classification model's predictive performance.\n",
       "- It distinctly brings out True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), thus, making it easier to focus on potential areas of improvement.\n",
       "- The matrix is beneficial in dealing with multi-class classification problems as it can provide a simple view of complex model performances.\n",
       "- It aids in understanding the different types of errors that the model could potentially make, as it provides in-depth insights into Type-I and Type-II errors.\n",
       "\n",
       "**Limitations**: Despite its various strengths, the Confusion Matrix tester does exhibit some limitations:\n",
       "- In cases of unbalanced classes, the effectiveness of the confusion matrix might be lessened. It may wrongly interpret the accuracy of a model that is essentially just predicting the majority class.\n",
       "- It does not provide a single unified statistic that could evaluate the overall performance of the model. Different aspects of the model's performance are evaluated separately instead.\n",
       "- It mainly serves as a descriptive tool and does not offer the capability for statistical hypothesis testing.\n",
       "- Risks of misinterpretation exist because the matrix doesn't directly provide precision, recall, or F1-score data. These metrics have to be computed separately.</td>\n",
       "      <td id=\"T_53aa1_row0_col3\" class=\"data row0 col3\" >validmind.model_validation.sklearn.ConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_53aa1_row1_col0\" class=\"data row1 col0\" >Metric</td>\n",
       "      <td id=\"T_53aa1_row1_col1\" class=\"data row1 col1\" >Precision Recall Curve</td>\n",
       "      <td id=\"T_53aa1_row1_col2\" class=\"data row1 col2\" >**Purpose**: The Precision Recall Curve metric is intended to evaluate the trade-off between precision and recall in classification models, particularly binary classification models. It assesses the model's capacity to produce accurate results (high precision), as well as its ability to capture a majority of all positive instances (high recall).\n",
       "\n",
       "**Test Mechanism**: The test extracts ground truth labels and prediction probabilities from the model's test dataset. It applies the precision_recall_curve method from the sklearn metrics module to these extracted labels and predictions, which computes a precision-recall pair for each possible threshold. This calculation results in an array of precision and recall scores that can be plotted against each other to form the Precision-Recall Curve. This curve is then visually represented by using Plotly's scatter plot.\n",
       "\n",
       "**Signs of High Risk**: * A lower area under the Precision-Recall Curve signifies high risk. * This corresponds to a model yielding a high amount of false positives (low precision) and/or false negatives (low recall). * If the curve is closer to the bottom left of the plot, rather than being closer to the top right corner, it can be a sign of high risk.\n",
       "\n",
       "**Strengths**: * This metric aptly represents the balance between precision (minimizing false positives) and recall (minimizing false negatives), which is especially critical in scenarios where both values are significant. * Through the graphic representation, it enables an intuitive understanding of the model's performance across different threshold levels.\n",
       "\n",
       "**Limitations**: * This metric is only applicable to binary classification models – it raises errors for multiclass classification models or Foundation models. * It may not fully represent the overall accuracy of the model if the cost of false positives and false negatives are extremely different, or if the dataset is heavily imbalanced.</td>\n",
       "      <td id=\"T_53aa1_row1_col3\" class=\"data row1 col3\" >validmind.model_validation.sklearn.PrecisionRecallCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_53aa1_row2_col0\" class=\"data row2 col0\" >Metric</td>\n",
       "      <td id=\"T_53aa1_row2_col1\" class=\"data row2 col1\" >ROC Curve</td>\n",
       "      <td id=\"T_53aa1_row2_col2\" class=\"data row2 col2\" >**Purpose**: The Receiver Operating Characteristic (ROC) curve is designed to evaluate the performance of binary classification models. This curve illustrates the balance between the True Positive Rate (TPR) and False Positive Rate (FPR) across various threshold levels. In combination with the Area Under the Curve (AUC), the ROC curve aims to measure the model's discrimination ability between the two defined classes in a binary classification problem (e.g., default vs non-default). Ideally, a higher AUC score signifies superior model performance in accurately distinguishing between the positive and negative classes.\n",
       "\n",
       "**Test Mechanism**: First, this script selects the target model and datasets that require binary classification. It then calculates the predicted probabilities for the test set, and uses this data, along with the true outcomes, to generate and plot the ROC curve. Additionally, it concludes a line signifying randomness (AUC of 0.5). The AUC score for the model's ROC curve is also computed, presenting a numerical estimation of the model's performance. If any Infinite values are detected in the ROC threshold, these are effectively eliminated. The resulting ROC curve, AUC score, and thresholds are consequently saved for future reference.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A high risk is potentially linked to the model's performance if the AUC score drops below or nears 0.5.\n",
       "- Another warning sign would be the ROC curve lying closer to the line of randomness, indicating no discriminative ability.\n",
       "- For the model to be deemed competent at its classification tasks, it is crucial that the AUC score is significantly above 0.5.\n",
       "\n",
       "**Strengths**:\n",
       "- This ROC Curve offers an inclusive visual depiction of a model's discriminative power throughout all conceivable classification thresholds, unlike other metrics that solely disclose model performance at one fixed threshold.\n",
       "- Despite the proportions of the dataset, the AUC Score, which represents the entire ROC curve as a single data point, continues to be consistent, proving to be the ideal choice for such situations.\n",
       "\n",
       "**Limitations**:\n",
       "- The primary limitation is that this test is exclusively structured for binary classification tasks, thus limiting its application towards other model types.\n",
       "- Furthermore, its performance might be subpar with models that output probabilities highly skewed towards 0 or 1.\n",
       "- At the extreme, the ROC curve could reflect high performance even when the majority of classifications are incorrect, provided that the model's ranking format is retained. This phenomenon is commonly termed the \"Class Imbalance Problem\".</td>\n",
       "      <td id=\"T_53aa1_row2_col3\" class=\"data row2 col3\" >validmind.model_validation.sklearn.ROCCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_53aa1_row3_col0\" class=\"data row3 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_53aa1_row3_col1\" class=\"data row3 col1\" >Training Test Degradation</td>\n",
       "      <td id=\"T_53aa1_row3_col2\" class=\"data row3 col2\" >**Purpose**: The 'TrainingTestDegradation' class serves as a test to verify that the degradation in performance between the training and test datasets does not exceed a predefined threshold. This test serves as a measure to check the model's ability to generalize from its training data to unseen test data. It assesses key classification metric scores such as accuracy, precision, recall and f1 score, to verify the model's robustness and reliability.\n",
       "\n",
       "**Test Mechanism**: The code applies several predefined metrics including accuracy, precision, recall and f1 scores to the model's predictions for both the training and test datasets. It calculates the degradation as the difference between the training score and test score divided by the training score. The test is considered successful if the degradation for each metric is less than the preset maximum threshold of 10%. The results are summarized in a table showing each metric's train score, test score, degradation percentage, and pass/fail status.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A degradation percentage that exceeds the maximum allowed threshold of 10% for any of the evaluated metrics.\n",
       "- A high difference or gap between the metric scores on the training and the test datasets.\n",
       "- The 'Pass/Fail' column displaying 'Fail' for any of the evaluated metrics.\n",
       "\n",
       "**Strengths**:\n",
       "- This test provides a quantitative measure of the model's ability to generalize to unseen data, which is key for predicting its practical real-world performance.\n",
       "- By evaluating multiple metrics, it takes into account different facets of model performance and enables a more holistic evaluation.\n",
       "- The use of a variable predefined threshold allows the flexibility to adjust the acceptability criteria for different scenarios.\n",
       "\n",
       "**Limitations**:\n",
       "- The test compares raw performance on training and test data, but does not factor in the nature of the data. Areas with less representation in the training set, for instance, might still perform poorly on unseen data.\n",
       "- It requires good coverage and balance in the test and training datasets to produce reliable results, which may not always be available.\n",
       "- The test is currently only designed for classification tasks.</td>\n",
       "      <td id=\"T_53aa1_row3_col3\" class=\"data row3 col3\" >validmind.model_validation.sklearn.TrainingTestDegradation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_53aa1_row4_col0\" class=\"data row4 col0\" >Metric</td>\n",
       "      <td id=\"T_53aa1_row4_col1\" class=\"data row4 col1\" >Log Regression Confusion Matrix</td>\n",
       "      <td id=\"T_53aa1_row4_col2\" class=\"data row4 col2\" >**Purpose**: The Logistic Regression Confusion Matrix is a metric used to measure the performance of a logistic regression classification model. This metric is particularly useful for scenarios where a model's predictions are formulated by thresholding probabilities. The main advantage of this approach is that it includes true positives, true negatives, false positives, and false negatives in its assessment, providing a more comprehensive overview of the model's effectiveness in distinguishing between correct and incorrect classifications.\n",
       "\n",
       "**Test Mechanism**: The methodology behind the Logistic Regression Confusion Matrix uses the `sklearn.metrics.confusion_matrix` function from the Python library to generate a matrix. This matrix is created by comparing the model's predicted probabilities, which are initially converted to binary predictions using a predetermined cut-off threshold (default is 0.5), against the actual classes. The matrix's design consists of the predicted class labels forming the x-axis, and the actual class labels forming the y-axis, with each cell containing the record of true positives, true negatives, false positives, and false negatives respectively.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- A significant number of false positives and false negatives, indicating that the model is incorrectly classifying instances.\n",
       "- The counts of true positives and true negatives being substantially lower than projected, positioning this as a potential high-risk indicator.\n",
       "\n",
       "**Strengths**:\n",
       "- Simple, intuitive, and provides a comprehensive understanding of the model's performance.\n",
       "- Provides a detailed breakdown of error types, improving transparency.\n",
       "- Offers flexible adaptation for diverse prediction scenarios by allowing adjustments to the cut-off threshold, and enabling exploration of trade-offs between precision (minimizing false positives) and recall (minimizing false negatives).\n",
       "\n",
       "**Limitations**:\n",
       "- Acceptable performance on majority classes but potential poor performance on minority classes in imbalanced datasets, as the confusion matrix may supply misleading results.\n",
       "- Lack of insight into the severity of the mistakes and the cost trade-off between different types of misclassification.\n",
       "- Selection of the cut-off threshold can significantly alter the interpretation, and a poorly chosen threshold may lead to erroneous conclusions.</td>\n",
       "      <td id=\"T_53aa1_row4_col3\" class=\"data row4 col3\" >validmind.model_validation.statsmodels.LogRegressionConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_53aa1_row5_col0\" class=\"data row5 col0\" >Metric</td>\n",
       "      <td id=\"T_53aa1_row5_col1\" class=\"data row5 col1\" >GINI Table</td>\n",
       "      <td id=\"T_53aa1_row5_col2\" class=\"data row5 col2\" >**Purpose**: The 'GINITable' metric is designed to evaluate the performance of a classification model by emphasizing its discriminatory power. Specifically, it calculates and presents three important metrics\n",
       "- the Area under the ROC Curve (AUC), the GINI coefficient, and the Kolmogov-Smirnov (KS) statistic\n",
       "- for both training and test datasets.\n",
       "\n",
       "**Test Mechanism**: Using a dictionary for storing performance metrics for both the training and test datasets, the 'GINITable' metric calculates each of these metrics sequentially. The Area under the ROC Curve (AUC) is calculated via the `roc_auc_score` function from the Scikit-Learn library. The GINI coefficient, a measure of statistical dispersion, is then computed by doubling the AUC and subtracting 1. Finally, the Kolmogov-Smirnov (KS) statistic is calculated via the `roc_curve` function from Scikit-Learn, with the False Positive Rate (FPR) subtracted from the True Positive Rate (TPR) and the maximum value taken from the resulting data. These metrics are then stored in a pandas DataFrame for convenient visualization.\n",
       "\n",
       "**Signs of High Risk**:\n",
       "- Low values for performance metrics may suggest a reduction in model performance, particularly a low AUC which indicates poor classification performance, or a low GINI coefficient, which could suggest a decreased ability to discriminate different classes.\n",
       "- A high KS value may be an indicator of potential overfitting, as this generally signifies a substantial divergence between positive and negative distributions.\n",
       "- Significant discrepancies between the performance on the training dataset and the test dataset may present another signal of high risk.\n",
       "\n",
       "**Strengths**:\n",
       "- Offers three key performance metrics (AUC, GINI, and KS) in one test, providing a more comprehensive evaluation of the model.\n",
       "- Provides a direct comparison between the model's performance on training and testing datasets, which aids in identifying potential underfitting or overfitting.\n",
       "- The applied metrics are class-distribution invariant, thereby remaining effective for evaluating model performance even when dealing with imbalanced datasets.\n",
       "- Presents the metrics in a user-friendly table format for easy comprehension and analysis.\n",
       "\n",
       "**Limitations**:\n",
       "- The GINI coefficient and KS statistic are both dependent on the AUC value. Therefore, any errors in the calculation of the latter will adversely impact the former metrics too.\n",
       "- Mainly suited for binary classification models and may require modifications for effective application in multi-class scenarios.\n",
       "- The metrics used are threshold-dependent and may exhibit high variability based on the chosen cut-off points.\n",
       "- The test does not incorporate a method to efficiently handle missing or inefficiently processed data, which could lead to inaccuracies in the metrics if the data is not appropriately preprocessed.</td>\n",
       "      <td id=\"T_53aa1_row5_col3\" class=\"data row5 col3\" >validmind.model_validation.statsmodels.GINITable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d591b1c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.list_tests(tags=[\"model_performance\", \"visualization\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tags` parameter facilitates searching tests by their tags. For instance, if you're keen on tests associated with 'model_performance' and 'visualization':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb32519",
   "metadata": {},
   "source": [
    "To work with a specific set of tests programmatically, you can store the results in a variable. For instance, let's list all regression tests and store them in `regression_tests` for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['validmind.model_validation.ModelMetadata',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelsCoeffs',\n",
       " 'validmind.model_validation.statsmodels.BoxPierce',\n",
       " 'validmind.model_validation.statsmodels.RegressionCoeffsPlot',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelSensitivityPlot',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelsPerformance',\n",
       " 'validmind.model_validation.statsmodels.ZivotAndrewsArch',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelOutsampleComparison',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels',\n",
       " 'validmind.model_validation.statsmodels.FeatureImportanceAndSignificance',\n",
       " 'validmind.model_validation.statsmodels.LJungBox',\n",
       " 'validmind.model_validation.statsmodels.JarqueBera',\n",
       " 'validmind.model_validation.statsmodels.PhillipsPerronArch',\n",
       " 'validmind.model_validation.statsmodels.KolmogorovSmirnov',\n",
       " 'validmind.model_validation.statsmodels.ResidualsVisualInspection',\n",
       " 'validmind.model_validation.statsmodels.ShapiroWilk',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelInsampleComparison',\n",
       " 'validmind.model_validation.statsmodels.RegressionFeatureSignificance',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelSummary',\n",
       " 'validmind.model_validation.statsmodels.KPSS',\n",
       " 'validmind.model_validation.statsmodels.Lilliefors',\n",
       " 'validmind.model_validation.statsmodels.RunsTest',\n",
       " 'validmind.model_validation.statsmodels.DFGLSArch',\n",
       " 'validmind.model_validation.statsmodels.AutoARIMA',\n",
       " 'validmind.model_validation.statsmodels.ADFTest',\n",
       " 'validmind.model_validation.statsmodels.RegressionModelForecastPlot',\n",
       " 'validmind.model_validation.statsmodels.ADF',\n",
       " 'validmind.model_validation.statsmodels.DurbinWatsonTest',\n",
       " 'validmind.data_validation.MissingValuesRisk',\n",
       " 'validmind.data_validation.IQROutliersTable',\n",
       " 'validmind.data_validation.Skewness',\n",
       " 'validmind.data_validation.Duplicates',\n",
       " 'validmind.data_validation.MissingValuesBarPlot',\n",
       " 'validmind.data_validation.DatasetDescription',\n",
       " 'validmind.data_validation.ScatterPlot',\n",
       " 'validmind.data_validation.TimeSeriesOutliers',\n",
       " 'validmind.data_validation.TabularCategoricalBarPlots',\n",
       " 'validmind.data_validation.AutoStationarity',\n",
       " 'validmind.data_validation.DescriptiveStatistics',\n",
       " 'validmind.data_validation.PearsonCorrelationMatrix',\n",
       " 'validmind.data_validation.FeatureTargetCorrelationPlot',\n",
       " 'validmind.data_validation.TabularNumericalHistograms',\n",
       " 'validmind.data_validation.HighCardinality',\n",
       " 'validmind.data_validation.MissingValues',\n",
       " 'validmind.data_validation.RollingStatsPlot',\n",
       " 'validmind.data_validation.DatasetCorrelations',\n",
       " 'validmind.data_validation.TabularDescriptionTables',\n",
       " 'validmind.data_validation.AutoMA',\n",
       " 'validmind.data_validation.UniqueRows',\n",
       " 'validmind.data_validation.TooManyZeroValues',\n",
       " 'validmind.data_validation.HighPearsonCorrelation',\n",
       " 'validmind.data_validation.ACFandPACFPlot',\n",
       " 'validmind.data_validation.HeatmapFeatureCorrelations',\n",
       " 'validmind.data_validation.TimeSeriesFrequency',\n",
       " 'validmind.data_validation.DatasetSplit',\n",
       " 'validmind.data_validation.SpreadPlot',\n",
       " 'validmind.data_validation.TimeSeriesLinePlot',\n",
       " 'validmind.data_validation.AutoSeasonality',\n",
       " 'validmind.data_validation.EngleGrangerCoint',\n",
       " 'validmind.data_validation.TimeSeriesMissingValues',\n",
       " 'validmind.data_validation.DatasetMetadata',\n",
       " 'validmind.data_validation.TimeSeriesHistogram',\n",
       " 'validmind.data_validation.LaggedCorrelationHeatmap',\n",
       " 'validmind.data_validation.SeasonalDecompose',\n",
       " 'validmind.data_validation.IQROutliersBarPlot',\n",
       " 'validmind.data_validation.AutoAR',\n",
       " 'validmind.data_validation.TabularDateTimeHistograms']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_tests = vt.list_tests(task=\"regression\", pretty=False)\n",
    "regression_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delving into Test Details with `describe_test`\n",
    "\n",
    "After identifying a set of potential tests, you might want to explore the specifics of an individual test. The `describe_test` function provides a deep dive into the details of a test. It reveals the test name, description, ID, test type, and required inputs. Below, we showcase how to describe a test using its ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_76b82 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_76b82_row0_col0, #T_76b82_row0_col1, #T_76b82_row1_col0, #T_76b82_row1_col1, #T_76b82_row2_col0, #T_76b82_row2_col1, #T_76b82_row3_col0, #T_76b82_row3_col1, #T_76b82_row4_col0, #T_76b82_row4_col1, #T_76b82_row5_col0, #T_76b82_row5_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_76b82\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_76b82_level0_col0\" class=\"col_heading level0 col0\" ></th>\n",
       "      <th id=\"T_76b82_level0_col1\" class=\"col_heading level0 col1\" > </th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_76b82_row0_col0\" class=\"data row0 col0\" >ID:</td>\n",
       "      <td id=\"T_76b82_row0_col1\" class=\"data row0 col1\" >validmind.model_validation.sklearn.ConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_76b82_row1_col0\" class=\"data row1 col0\" >Name:</td>\n",
       "      <td id=\"T_76b82_row1_col1\" class=\"data row1 col1\" >Confusion Matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_76b82_row2_col0\" class=\"data row2 col0\" >Description:</td>\n",
       "      <td id=\"T_76b82_row2_col1\" class=\"data row2 col1\" >**Purpose**: The Confusion Matrix tester is designed to assess the performance of a classification Machine Learning model. This performance is evaluated based on how well the model is able to correctly classify True Positives, True Negatives, False Positives, and False Negatives\n",
       "- fundamental aspects of model accuracy.\n",
       "\n",
       "**Test Mechanism**: The mechanism used involves taking the predicted results (`y_test_predict`) from the classification model and comparing them against the actual values (`y_test_true`). A confusion matrix is built using the unique labels extracted from `y_test_true`, employing scikit-learn's metrics. The matrix is then visually rendered with the help of Plotly's `create_annotated_heatmap` function. A heatmap is created which provides a two-dimensional graphical representation of the model's performance, showcasing distributions of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
       "\n",
       "**Signs of High Risk**: Indicators of high risk related to the model include:\n",
       "- High numbers of False Positives (FP) and False Negatives (FN), depicting that the model is not effectively classifying the values.\n",
       "- Low numbers of True Positives (TP) and True Negatives (TN), implying that the model is struggling with correctly identifying class labels.\n",
       "\n",
       "**Strengths**: The Confusion Matrix tester brings numerous strengths:\n",
       "- It provides a simplified yet comprehensive visual snapshot of the classification model's predictive performance.\n",
       "- It distinctly brings out True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), thus, making it easier to focus on potential areas of improvement.\n",
       "- The matrix is beneficial in dealing with multi-class classification problems as it can provide a simple view of complex model performances.\n",
       "- It aids in understanding the different types of errors that the model could potentially make, as it provides in-depth insights into Type-I and Type-II errors.\n",
       "\n",
       "**Limitations**: Despite its various strengths, the Confusion Matrix tester does exhibit some limitations:\n",
       "- In cases of unbalanced classes, the effectiveness of the confusion matrix might be lessened. It may wrongly interpret the accuracy of a model that is essentially just predicting the majority class.\n",
       "- It does not provide a single unified statistic that could evaluate the overall performance of the model. Different aspects of the model's performance are evaluated separately instead.\n",
       "- It mainly serves as a descriptive tool and does not offer the capability for statistical hypothesis testing.\n",
       "- Risks of misinterpretation exist because the matrix doesn't directly provide precision, recall, or F1-score data. These metrics have to be computed separately.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_76b82_row3_col0\" class=\"data row3 col0\" >Test Type:</td>\n",
       "      <td id=\"T_76b82_row3_col1\" class=\"data row3 col1\" >Metric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_76b82_row4_col0\" class=\"data row4 col0\" >Required Inputs:</td>\n",
       "      <td id=\"T_76b82_row4_col1\" class=\"data row4 col1\" >['model']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_76b82_row5_col0\" class=\"data row5 col0\" >Params:</td>\n",
       "      <td id=\"T_76b82_row5_col1\" class=\"data row5 col1\" >{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d59197e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.describe_test(test_id=\"validmind.model_validation.sklearn.ConfusionMatrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By harnessing the functionalities presented in this guide, you can adeptly navigate and employ the ValidMind test suite tailored to your model and dataset validation requirements. We hope this walkthrough proves invaluable and inspires you to further explore the expansive ValidMind API for an even broader scope of capabilities and details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
