{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis of financial data using Hugging face LLM models\n",
    "This notebook aims to provide an introduction to documenting an LLM model using the ValidMind Developer Framework. The use case presented is a sentiment analysis of financial pharse data (https://huggingface.co/datasets/financial_phrasebank).\n",
    "\n",
    "- Initializing the ValidMind Developer Framework\n",
    "- Running a test various tests to quickly generate document about the data and model\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "To use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\n",
    "\n",
    "If you don't already have one, you should also [create a documentation project](https://docs.validmind.ai/guide/create-your-first-documentation-project.html) on the ValidMind platform. You will use this project to upload your documentation and test results.\n",
    "\n",
    "## Install the client library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the client library\n",
    "\n",
    "In a browser, go to the **Client Integration** page of your documentation project and click **Copy to clipboard** next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n",
    "\n",
    "::: {.column-margin}\n",
    "::: {.callout-tip}\n",
    "This step requires a documentation project. [Learn how you can create one](https://docs.validmind.ai/guide/create-your-first-documentation-project.html).\n",
    ":::\n",
    ":::\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidAPICredentialsError",
     "evalue": "Invalid API credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidAPICredentialsError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_54455/347011668.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidmind\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m vm.init(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mapi_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://api.prod.validmind.ai/api/v1/tracking\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/github/validmind-python/validmind/api_client.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(project, api_key, api_secret, api_host)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_api_host\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0msend_single_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/github/validmind-python/validmind/api_client.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(project, api_key, api_secret, api_host)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0m__ping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# if the api host is https, assume we're not in dev mode and send to sentry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/github/validmind-python/validmind/api_client.py\u001b[0m in \u001b[0;36m__ping\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mraise_api_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mclient_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/github/validmind-python/validmind/errors.py\u001b[0m in \u001b[0;36mraise_api_error\u001b[0;34m(error_string)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPIRequestError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mInvalidAPICredentialsError\u001b[0m: Invalid API credentials"
     ]
    }
   ],
   "source": [
    "## Replace the code below with the code snippet from your project ## \n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key = \"...\",\n",
    "    api_secret = \"...\",\n",
    "    project = \"...\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "In this section, we'll load the financial phrasebank dataset, which will be the foundation for our sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_54455/3718354069.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./datasets/bbc_text_cls.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/bbc_text_cls.csv')\n",
    "sample = df.sample(10)\n",
    "sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP data quality tests\n",
    "\n",
    "Before we proceed with the analysis, it's crucial to ensure the quality of our NLP data. This section runs a data quality test plan to validate the data's integrity and suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_ds = vm.init_dataset(dataset=df, type=\"generic\", text_column='Sentence', target_column=\"Sentiment\")\n",
    "text_data_test_plan = vm.run_test_plan(\"text_data_quality\",\n",
    "                                       dataset=vm_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example model wrapper\n",
    "\n",
    "The following code template showcases how to wrap a Hugging Face model for compatibility with the ValidMind Developer Framework. We will load an example model using the transformers API and then run some predictions on our test dataset.\n",
    "\n",
    "The ValidMind developer framework provides support for Hugging Face transformers out of the box, so in the following section we will show how to initialize multiple transformers models with the `init_model` function, removing the need for a custom wrapper. In cases where you need extra pre-processing or post-processing steps, you can use the following code template as a starting point to wrap your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "\n",
    "class Sentiment_HuggingFace:\n",
    "    \"\"\"\n",
    "    A VM Model instance wrapper only requires a predict and predict_proba method.\n",
    "    \"\"\"\n",
    "\n",
    "    predicted_prob_values = None\n",
    "\n",
    "    def __init__(self, pipeline_task, model_name=None, model=None, tokenizer=None):\n",
    "        self.model_name = model_name\n",
    "        self.pipeline_task = pipeline_task\n",
    "        self.model = pipeline(pipeline_task, model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def predict(self, data):\n",
    "        data = [str(datapoint) for datapoint in data]\n",
    "        results = []\n",
    "        results = self.model(data)\n",
    "        results_df = pd.DataFrame(results)\n",
    "        self.predicted_prob_values = results_df.score.values\n",
    "        return results_df.label.values\n",
    "\n",
    "    def predict_proba(self):\n",
    "        if self.predicted_prob_values is None:\n",
    "            raise ValueError(\"First run predict method to retrieve predicted probabilities\")\n",
    "        return self.predicted_prob_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "sentiment_model_hf = Sentiment_HuggingFace(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "df_test = df.head(15)\n",
    "\n",
    "y_pred = sentiment_model_hf.predict(df_test.Sentence.values.tolist())\n",
    "y_pred_prob = sentiment_model_hf.predict_proba()\n",
    "df_results = df_test.copy()\n",
    "df_results['y_pred'] = y_pred\n",
    "df_results['y_pred_prob'] = y_pred_prob\n",
    "df_results.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hugging Face: FinancialBERT-Sentiment-Analysis\n",
    "\n",
    "https://huggingface.co/ahmedrachid/FinancialBERT-Sentiment-Analysis\n",
    "\n",
    "Let's now explore integrating and testing FinancialBERT, a model designed specifically for sentiment analysis in the financial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "hfmodel = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test dataset with 100 rows only\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=df.head(1000),\n",
    "    text_column=\"Sentence\",\n",
    "    target_column=\"Sentiment\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VM model\n",
    "\n",
    "When initializing a VM model, we pre-calculate predictions on the test dataset. This operation can take a long time for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_model = vm.init_model(\n",
    "    hfmodel,\n",
    "    train_ds=vm_test_ds,\n",
    "    test_ds=vm_test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some errors are expected since some binary classifier metrics are not applicable to all models\n",
    "full_suite = vm.run_test_suite(\n",
    "    \"binary_classifier_model_validation\",\n",
    "    dataset=vm_test_ds,\n",
    "    model=vm_model\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face: distilroberta-finetuned-financial-news-sentiment-analysis\n",
    "\n",
    "https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\n",
    "\n",
    "The distilroberta model was fine-tuned on the phrasebank dataset: https://huggingface.co/datasets/financial_phrasebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "hfmodel = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_model = vm.init_model(\n",
    "    hfmodel,\n",
    "    test_ds=vm_test_ds,\n",
    "    train_ds=vm_test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_suite = vm.run_test_suite(\n",
    "    \"binary_classifier_model_validation\",\n",
    "    dataset=vm_test_ds,\n",
    "    model=vm_model\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hugging Face: financial-roberta-large-sentiment\n",
    "\n",
    "https://huggingface.co/soleimanian/financial-roberta-large-sentiment\n",
    "\n",
    "The financial-roberta-large model is another financial sentiment analysis model trained on large amounts of data including:\n",
    "\n",
    "- Financial Statements\n",
    "- Earnings Announcements\n",
    "- Earnings Call Transcripts\n",
    "- Corporate Social Responsibility (CSR) Reports\n",
    "- Environmental, Social, and Governance (ESG) News\n",
    "- Financial News\n",
    "- Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"soleimanian/financial-roberta-large-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"soleimanian/financial-roberta-large-sentiment\")\n",
    "hfmodel = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is larger so predictions will take longer. Feel free to adjust the dataset size to make it faster.\n",
    "vm_model = vm.init_model(\n",
    "    hfmodel,\n",
    "    test_ds=vm_test_ds,\n",
    "    train_ds=vm_test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_suite = vm.run_test_suite(\n",
    "    \"binary_classifier_model_validation\",\n",
    "    dataset=vm_test_ds,\n",
    "    model=vm_model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
