{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ValidMind Introduction for Model Developers\n",
    "\n",
    "This interactive notebook guides you through the process of documenting a model with the ValidMind Developer Framework. It uses a binary classification model as an example, but the same principles apply to other model types.\n",
    "\n",
    "As part of the notebook, you will learn how to start documenting a model as a **Model Developer** persona. At this stage the assumption is that there has been a [Model Documentation template](https://docs.validmind.com/guide/swap-documentation-templates.html#view-current-templates) defined in the platform.\n",
    "\n",
    "## Overview of the Notebook\n",
    "\n",
    "1. Initializing the ValidMind Developer Framework:\n",
    "\n",
    "   ValidMind’s developer framework provides a rich collection of documentation tools and test suites, from documenting descriptions of your dataset to validation testing your models for weak spots and overfit areas.\n",
    "\n",
    "2. Start the model development process with raw data and run out-of-the box tests and add evidence to model documentation\n",
    "\n",
    "   In this stage the notebook will provide you details on how to access ValidMind's test repository to individual tests that you will use as building blocks to ensure a model is being built appropriately. The goal is to show how to run tests, investigate results and add tests / evidence to the documentation.\n",
    "\n",
    "   For a full list of out-of-box tests please refer to: https://docs.validmind.com/guide/test-descriptions.html\n",
    "\n",
    "3. Next we are going to build upon the previous step, but the focus here is implementation of Custom Tests\n",
    "\n",
    "   In this stage the notebook will provide details on how to implement custom tests. Usually, model developers have a lot of their own custom tests and it is important to include this within the model documentation. We will show how you how to include custom tests and then how they can be implemented within the documentation as additional evidence.\n",
    "\n",
    "4. The final part of the notebook will show you how to ensure completion of documentation\n",
    "\n",
    "   In this stage the notebook will provide details on how to ensure that model documentation and associated sections in the model documentation have been built out, and if there are any changes to testing due to additional data processing or data analysis requirements. The notebook will show how to update results for existing tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidMind at a glance\n",
    "\n",
    "ValidMind's platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "If this is your first time trying out ValidMind, you can make use of the following resources alongside this notebook:\n",
    "\n",
    "- [Get started](https://docs.validmind.ai/guide/get-started.html) — The basics, including key concepts, and how our products work\n",
    "- [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html) — The path for developers, more code samples, and our developer reference\n",
    "\n",
    "It is important to note that in order to connect to the Developer Framework you will have to access this through our API's using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "### New to ValidMind?\n",
    "\n",
    "For access to all features available in this notebook, create a free ValidMind account.\n",
    "\n",
    "Signing up is FREE — [**Sign up now**](https://app.prod.validmind.ai)\n",
    ":::\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing the ValidMind Developer Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library\n",
    "\n",
    "Please note the following recommended Python versions to utilize:\n",
    "\n",
    "- Python version 3.7 > x <= 3.11\n",
    "\n",
    "The client library provides Python support for the ValidMind Developer Framework. To install it run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a new model in ValidMind UI and initialize the client library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "Get your code snippet:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
    "\n",
    "3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
    "\n",
    "   For example, to register a model for use with this notebook, select:\n",
    "\n",
    "   - Documentation template: `Binary classification`\n",
    "   - Use case: `Marketing/Sales - Attrition/Churn Management`\n",
    "\n",
    "   You can fill in other options according to your preference.\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify & preview the documentation template\n",
    "\n",
    "Here we want to verify that we have connected with ValidMnd and that the appropriate template is selected. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
    "\n",
    "You will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the `vm.preview_template()` function from the ValidMind library and note the empty sections:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm.preview_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's observe the the list of all available tests in the ValidMind Developer Framework:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm.tests.list_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start the model development process with raw data and run out-of-the box tests and add evidence to model documentation\n",
    "\n",
    "In this section we will provide details on how to understand individual tests available in ValidMind, how you can access each test, run it and change parameters if necessary. You will be using an example dataset provided by ValidMind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from validmind.datasets.classification import customer_churn as demo_dataset\n",
    "\n",
    "df_raw = demo_dataset.load_data()\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some data quality assessments by running a few individual tests related to data assessment. You will be using the `vm.tests.list_tests()` function above in combination with `vm.tests.list_tags()` and `vm.tests.list_task_types()` to find which prebuilt tests are relevant for data quality assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Get the list of available tags\n",
    "sorted(vm.tests.list_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Get the list of available task types\n",
    "sorted(vm.tests.list_task_types())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass `tags` and `task_types` as parameters to the `vm.tests.list_tests()` function to filter the tests based on the tags and task types. For example, to find tests related to tabular data quality for classification models, you can call `list_tests()` like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm.tests.list_tests(task=\"classification\", tags=[\"tabular_data\", \"data_quality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ValidMind datasets\n",
    "\n",
    "Now we assume we have identified some tests we want to run with regards to the data we are intending to use. The next step is to connect your data with a ValidMind dataset object. This step is always necessary every time you want to connect a dataset to documentation and produce tests through ValidMind. You only need to do it one time per dataset.\n",
    "\n",
    "You can initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
    "\n",
    "This function takes a number of arguments:\n",
    "\n",
    "- `dataset` — the raw dataset that you want to provide as input to tests\n",
    "- `input_id` - a unique identifier that allows tracking what inputs are used when running each individual test\n",
    "- `target_column` — a required argument if tests require access to true values. This is the name of the target column in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# vm_raw_dataset is now a VMDataset object that you can pass to any ValidMind test\n",
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=df_raw,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=\"Exited\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some tabular data tests\n",
    "\n",
    "Individual tests can be easily run by calling the `run_test` function provided by the `validmind.tests` module. The function takes the following arguments:\n",
    "\n",
    "- `test_id`: The ID of the test to run. To find a particular test and get its ID, refer to the [explore_tests](../how_to/explore_tests.ipynb) notebook. Look above for example after running 'vm.test_suites.describe_suite' as column 'Test ID' will contain the id.\n",
    "- `params`: A dictionary of parameters for the test. These will override any `default_params` set in the test definition. Refer to the [explore_tests](../how_to/explore_tests.ipynb) notebook to find the default parameters for a test. See below for examples.\n",
    "\n",
    "The inputs expected by a test can also be found in the test definition. Let's take `validmind.data_validation.DescriptiveStatistics` as an example. Note that the output of the `describe_test()` function below shows that this test expects a `dataset` as input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm.tests.describe_test(\"validmind.data_validation.DescriptiveStatistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a few tests to assess the quality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.DescriptiveStatistics\",\n",
    "    inputs={\"dataset\": vm_raw_dataset},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test2 = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_raw_dataset},\n",
    "    params={\"min_percent_threshold\": 30},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the class imbalance test did not pass accordig to the value of `min_percent_threshold` we have set. Here is how you can re-run the test on some processed data to address this data quality issue. In this case we apply a very simple rebalance technique to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_raw_new = df_raw.sample(frac=1)  # Create a copy of the raw dataset\n",
    "\n",
    "# Create a balanced dataset with the same number of exited and not exited customers\n",
    "exited_df = df_raw_new.loc[df_raw_new[\"Exited\"] == 1]\n",
    "not_exited_df = df_raw_new.loc[df_raw_new[\"Exited\"] == 0].sample(n=exited_df.shape[0])\n",
    "\n",
    "new_df = pd.concat([exited_df, not_exited_df])\n",
    "new_df_raw = new_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new raw dataset you can re-run the individual test to see if it passes the class imbalance test requirement. Remember to register new VM dataset object since that is the type of input required by `run_test()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Register new data and now 'vm_raw_dataset_new' is the new dataset object of interest\n",
    "vm_raw_dataset_new = vm.init_dataset(\n",
    "    dataset=new_df_raw,\n",
    "    input_id=\"new_df_raw\",\n",
    "    target_column=\"Exited\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_raw_dataset_new},\n",
    "    params={\"min_percent_threshold\": 30},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize Test Output\n",
    "\n",
    "Below is an example on how you can utilize the output from a ValidMind test for futher use, for example, if you want to remove highly correlated features then the below shows how you can get a pearson's correlation matrix, use the output to reduce the feature list for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "corr_results = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.HighPearsonCorrelation\",\n",
    "    params={\"max_threshold\": 0.3},\n",
    "    inputs={\"dataset\": vm_raw_dataset_new},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to remove highly correlated features from the dataset. `corr_results` is an object of type `ThresholdTestResult` and we can inspects its individual `results` to get access to the features that failed the test. In general, all ValidMind tests can return two different types of results:\n",
    "\n",
    "- [MetricResult](https://docs.validmind.ai/validmind/validmind/vm_models.html#MetricResult): most metrics return this type of result\n",
    "- [ThresholdTestResult](https://docs.validmind.ai/validmind/validmind/vm_models.html#ThresholdTest): metrics that compare a metric to a threshold return this type of result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(corr_results.test_results)\n",
    "print(\"test_name: \", corr_results.test_results.test_name)\n",
    "print(\"params: \", corr_results.test_results.params)\n",
    "print(\"passed: \", corr_results.test_results.passed)\n",
    "print(\"results: \", corr_results.test_results.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the `results` and extract a list of features that failed the test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "corr_results.test_results.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the highly correlated features and create a new VM dataset object. Note the use of different `input_id`s. This allows tracking the inputs used when running each individual test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "high_correlation_features = [\n",
    "    result.column\n",
    "    for result in corr_results.test_results.results\n",
    "    if result.passed == False\n",
    "]\n",
    "high_correlation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Remove the highly correlated features from the dataset\n",
    "new_df_raw.drop(columns=high_correlation_features, inplace=True)\n",
    "\n",
    "# Re-initialize the dataset object\n",
    "vm_raw_dataset_new = vm.init_dataset(\n",
    "    dataset=new_df_raw,\n",
    "    input_id=\"new_df_raw_no_age\",\n",
    "    target_column=\"Exited\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running the test with the reduced feature set should pass the test. You can also plot the correlation matrix to visualize the new correlation between features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "corr_results = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.HighPearsonCorrelation\",\n",
    "    params={\"max_threshold\": 0.3},\n",
    "    inputs={\"dataset\": vm_raw_dataset_new},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "corr_results = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.PearsonCorrelationMatrix\",\n",
    "    inputs={\"dataset\": vm_raw_dataset_new},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documenting the results based on two datasets\n",
    "\n",
    "We have now done some analysis on two different datasets and we should able to document why certain things were done to the raw data with testing to support it. Every test result returned by the `run_test()` function has a `.log()` method that can be used to log the test results to ValidMind. When logging individual results to ValidMind you will need to manually add those results in a specific section of the model documentation.\n",
    "\n",
    "When using `run_documentation_tests()`, it's possible to automatically populate a section with the results of all tests that were registered in the documentation template.\n",
    "\n",
    "To populate the data preparation section of the documentatio, you will now complete the following steps:\n",
    "\n",
    "1. Run `run_documentation_tests()` using `vm_raw_dataset_new` as input\n",
    "2. Log the individual result high correlation test using `vm_raw_dataset` (no data cleanup)\n",
    "3. Log the individual result high correlation test using `vm_raw_dataset_new` (balanced classes and reduced features)\n",
    "\n",
    "After adding test driven blocks for steps #2 and #3 you will be able to explain the changes made to the raw data by editing the default description of the test result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run `run_documentation_tests()` using `vm_raw_dataset_new` as input\n",
    "\n",
    "`run_documentation_tests()` allows you to run multiple tests at once and log the results to the documentation. The function takes the following arguments:\n",
    "\n",
    "- `inputs`: any inputs to be passed to the tests\n",
    "- `config`: a dictionary `<test_id>:<test_config>` that allows configuring each test individually. Each test config has the following form:\n",
    "  - `params`: individual test parameters\n",
    "  - `inputs`: individual test inputs. When passed, this overrides any inputs passed from the `run_documentation_tests()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    \"validmind.data_validation.ClassImbalance\": {\n",
    "        \"params\": {\"min_percent_threshold\": 30},\n",
    "    },\n",
    "    \"validmind.data_validation.HighPearsonCorrelation\": {\n",
    "        \"params\": {\"max_threshold\": 0.3},\n",
    "    },\n",
    "}\n",
    "\n",
    "tests_suite = vm.run_documentation_tests(\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset_new,\n",
    "    },\n",
    "    config=test_config,\n",
    "    section=[\"data_preparation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log the individual result high correlation test using `vm_raw_dataset` (no data cleanup)\n",
    "\n",
    "Here you can use a custom `result_id` to tag the individual result with a unique identifier. This `result_id` can be appended to `test_id` with a `:` separator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.HighPearsonCorrelation:vm_raw_dataset\",\n",
    "    params={\"max_threshold\": 0.3},\n",
    "    inputs={\"dataset\": vm_raw_dataset},\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log the individual result high correlation test using `vm_raw_dataset_new` (balanced classes and reduced features)\n",
    "\n",
    "Repeat the same process as above but with the new dataset, using a new `result_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.HighPearsonCorrelation:vm_raw_dataset_new\",\n",
    "    params={\"max_threshold\": 0.3},\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset_new,\n",
    "    },\n",
    ")\n",
    "result.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add individual test results to model documentation\n",
    "\n",
    "You can now visit the documentation page for the model you connected to at the beginning of this notebook and add a new content block in the relevant section.\n",
    "\n",
    "To do this, go to the documentation page of your model and navigate to the `Data Preparation` -> `Correlations and Interactions` section. Then hover after the \"Pearson Correlation Matrix\" content block to reveal the `+` button as shown in the screenshot below.\n",
    "\n",
    "![screenshot showing insert button for test-driven blocks](../images/insert-test-driven-block.png)\n",
    "\n",
    "Click on the `+` button and select `Test-Driven Block`. This will open a dialog where you can select `Threshold Test` as the type of the test-driven content block, and then select the `High Pearson Correlation Vm Raw Dataset Test` metric. This will show a preview of the result and it should match the results shown above.\n",
    "\n",
    "![screenshot showing the selected test result in the dialog](../images/selecting-high-pearson-correlation-test.png)\n",
    "\n",
    "Finally, click on the `Insert block` button to add the test result to the documentation. You'll now see two individual results for the high correlation test in the `Correlations and Interactions` section of the documentation. To finalize the documentation, you can edit the test result's description block to explain the changes made to the raw data and the reasons behind them as we can see in the screenshot below.\n",
    "\n",
    "![screenshot showing the high pearson correlation block](../images/high-pearson-correlation-block.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "\n",
    "We have focused so far on the data assessment and pre-processing that usually occurs prior to any models being built. Now we are going to assume we have built a model and now we want to incorporate some model results in our documentation.\n",
    "\n",
    "Let's train a simple logistic regression model on the dataset and evaluate its performance. You will use the `LogisticRegression` class from the `sklearn.linear_model` and use ValidMind tests to evaluate the model's performance.\n",
    "\n",
    "Before training the model, we need to encode the categorical features in the dataset. You will use the `OneHotEncoder` class from the `sklearn.preprocessing` module to encode the categorical features. The categorical features in the dataset are `Geography` and `Gender`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "new_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "new_df_raw = pd.get_dummies(\n",
    "    new_df_raw, columns=[\"Geography\", \"Gender\"], drop_first=True\n",
    ")\n",
    "new_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the input and target variables\n",
    "X = new_df_raw.drop(\"Exited\", axis=1)\n",
    "y = new_df_raw[\"Exited\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Logistic Regression grid params\n",
    "log_reg_params = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"solver\": [\"liblinear\"],\n",
    "}\n",
    "\n",
    "# Grid search for Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Logistic Regression best estimator\n",
    "log_reg = grid_log_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model evaluation objects and assigning predictions\n",
    "\n",
    "The last step for evaluating the model's performance is to initialize the ValidMind datasets ande model objects and assign model predictions to each dataset. You will use the `init_dataset`, `init_model` and `assign_predictions` functions to initialize these objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "TRAIN = X_train\n",
    "TRAIN[\"Exited\"] = y_train\n",
    "TEST = X_test\n",
    "TEST[\"Exited\"] = y_test\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    input_id=\"train_dataset_final\",\n",
    "    dataset=TRAIN,\n",
    "    target_column=\"Exited\",\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    input_id=\"test_dataset_final\",\n",
    "    dataset=TEST,\n",
    "    target_column=\"Exited\",\n",
    ")\n",
    "\n",
    "# Register the model\n",
    "vm_model = vm.init_model(log_reg, input_id=\"log_reg_model_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been registered you can assign model predictions to the training and test datasets. The `assign_predictions()` method from the `Dataset` object can link existing predictions to any number of models. If no prediction values are passed, the method will compute predictions automatically:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vm_train_ds.assign_predictions(model=vm_model)\n",
    "vm_test_ds.assign_predictions(model=vm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model evaluation tests\n",
    "\n",
    "In this part, we focus on running the tests within the model development section of the model documentation. After running this function, only the tests associated with this section will be executed, and the corresponding section in the model documentation will be updated. In the example below, you will focus on only running tests for the \"model development\" section of the document.\n",
    "\n",
    "Note the additional config that is passed to `run_documentation_tests()`. This allows you to override inputs or params in certain tests. In our case, we want to explicitly use the `vm_train_ds` for the `validmind.model_validation.sklearn.ClassifierPerformance:in_sample` test, since it's supposed to run on the training dataset and not the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance:in_sample\": {\n",
    "        \"inputs\": {\n",
    "            \"dataset\": vm_train_ds,\n",
    "            \"model\": vm_model,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "results = vm.run_documentation_tests(\n",
    "    section=[\"model_development\"],\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,  # Any test that requires a single dataset will use vm_test_ds\n",
    "        \"model\": vm_model,\n",
    "        \"datasets\": (\n",
    "            vm_train_ds,\n",
    "            vm_test_ds,\n",
    "        ),  # Any test that requires multiple datasets will use vm_train_ds and vm_test_ds\n",
    "    },\n",
    "    config=test_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom metrics implementation\n",
    "\n",
    "This section assumes that model developers already have a repository of custom made tests that they consider critical to include in the documentation. Here we will provide details on how to easily integrate your custom tests with ValidMind.\n",
    "\n",
    "For a more in-depth introduction to custom metrics, refer to this [notebook](../code_samples/custom_tests/implementing_custom_tests.ipynb).\n",
    "\n",
    "A custom metric is any function that takes as arguments a set of inputs and optionally some parameters and returns one or more outputs. The function can be as simple or as complex as you need it to be. It can use external libraries, make API calls, or do anything else that you can do in Python. The only requirement is that the function signature and return values can be \"understood\" and handled by the ValidMind developer framework. As such, custom metrics offer added flexibility by extending the default metrics provided by ValidMind, enabling you to document any type of model or use case.\n",
    "\n",
    "In the following example, you will learn how to implement a custom metric that calculates the confusion matrix for a binary classification model. You will see that the custom metric function is just a regular Python function that can include and require any Python library as you see fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a confusion matrix plot\n",
    "\n",
    "To understand how to create a custom metric from anything, let's first create a confusion matrix plot using the `confusion_matrix` function from the `sklearn.metrics` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# Get the predicted classes\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix, display_labels=[False, True]\n",
    ")\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a @vm.metric wrapper that will allow you to create a reusable custom metric. Note the following changes in the code below:\n",
    "\n",
    "- The function `confusion_matrix` takes two arguments `dataset` and `model`. This is a `VMDataset` and `VMModel` object respectively.\n",
    "  - `VMDataset` objects allow you to access the dataset's true (target) values by accessing the `.y` attribute.\n",
    "  - `VMDataset` objects allow you to access the predictions for a given model by accessing the `.y_pred()` method.\n",
    "- The function docstring provides a description of what the metric does. This will be displayed along with the result in this notebook as well as in the ValidMind platform.\n",
    "- The function body calculates the confusion matrix using the `sklearn.metrics.confusion_matrix` function as we just did above.\n",
    "- The function then returns the `ConfusionMatrixDisplay.figure_` object - this is important as the ValidMind framework expects the output of the custom metric to be a plot or a table.\n",
    "- The `@vm.metric` decorator is doing the work of creating a wrapper around the function that will allow it to be run by the ValidMind framework. It also registers the metric so it can be found by the ID `my_custom_metrics.ConfusionMatrix` (see the section below on how test IDs work in ValidMind and why this format is important)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "@vm.metric(\"my_custom_metrics.ConfusionMatrix\")\n",
    "def confusion_matrix(dataset, model):\n",
    "    \"\"\"The confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known.\n",
    "\n",
    "    The confusion matrix is a 2x2 table that contains 4 values:\n",
    "\n",
    "    - True Positive (TP): the number of correct positive predictions\n",
    "    - True Negative (TN): the number of correct negative predictions\n",
    "    - False Positive (FP): the number of incorrect positive predictions\n",
    "    - False Negative (FN): the number of incorrect negative predictions\n",
    "\n",
    "    The confusion matrix can be used to assess the holistic performance of a classification model by showing the accuracy, precision, recall, and F1 score of the model on a single figure.\n",
    "    \"\"\"\n",
    "    y_true = dataset.y\n",
    "    y_pred = dataset.y_pred(model_id=model.input_id)\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(\n",
    "        confusion_matrix=confusion_matrix, display_labels=[False, True]\n",
    "    )\n",
    "    cm_display.plot()\n",
    "\n",
    "    plt.close()  # close the plot to avoid displaying it\n",
    "\n",
    "    return cm_display.figure_  # return the figure object itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now run the newly created custom metric on both the training and test datasets using the `run_test()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "result = vm.tests.run_test(\n",
    "    \"my_custom_metrics.ConfusionMatrix:training_dataset\",\n",
    "    inputs={\"model\": vm_model, \"dataset\": vm_train_ds},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "result = vm.tests.run_test(\n",
    "    \"my_custom_metrics.ConfusionMatrix:test_dataset\",\n",
    "    inputs={\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding parameters to custom metrics\n",
    "\n",
    "Custom metrics can take parameters just like any other function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register external test providers (custom test)\n",
    "\n",
    "We will now declare a local filesystem test provider that allows loading tests from a local folder. Fror this to work we just need to specify the root folder under which the provider class will look for tests. For this demo, it is the `./tests/` directory.\n",
    "\n",
    "[PLACEHOLDER FOR TEAM TO ADD MORE DETAILS ON THE FLOW HERE]  \n",
    "WE NEED HOW THE CODE SHOULD BE STRUCTURED AND GOAL: MAKE IT AS EASY AS POSSIBLE\n",
    "CAN WE ADD MULTIPLE TESTS IN THE PYTHON FILE?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import LocalTestProvider\n",
    "\n",
    "# First we are going define a name so that we can always refer back and find our custom tests. In this example \"gbc_test_provides\" is the identifier\n",
    "gbc_namespace = \"gbc_test_provider\"\n",
    "\n",
    "# Setting up the connection to where the custom testing code lives.\n",
    "local_test_provider = LocalTestProvider(root_folder=\"./tests/\")\n",
    "\n",
    "# Now let's register the test under the name we defined above\n",
    "vm.tests.register_test_provider(\n",
    "    namespace=gbc_namespace,\n",
    "    test_provider=local_test_provider,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing & Executing Custom Test in Model Documentation\n",
    "\n",
    "Let's now build a sample custom test that includes the outputs from a demo function called `get_marginal_bad_rates`. Inside the `tests/` directory next to this notebook you will find a file called `MarginalBadRateTest.py`. This file contains the custom test definition that we will run in the next cell. If you open that file you'll see how we invoke the `get_marginal_bad_rates` function from the `run()` method provided by the test interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom test is found by searching for the name space created above with the Python file name 'MarginalBadRateTest'\n",
    "# This runs the test on the dataset object 'vm_train_ds' with model object 'vm_model'\n",
    "test = vm.tests.run_test(\n",
    "    test_id=f\"{gbc_namespace}.MarginalBadRateTest\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the parameters and implement in Model Documentation\n",
    "\n",
    "Note how we have defined the following property in the custom test class (i.e. parameter in custom test):\n",
    "\n",
    "```python\n",
    "default_params = {\"bins\": 10}\n",
    "```\n",
    "\n",
    "This allows you to pass parameters to the test when running it. Let's try to re-running the test with 15 bins instead. In this custom test the bins affecting the figures and table output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test is run exactly the same as before but now you can see an additional line; 'params={\"bins\":15}' which will overwrite default bin value of 10\n",
    "\n",
    "test = vm.tests.run_test(\n",
    "    test_id=f\"{gbc_namespace}.MarginalBadRateTest\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    params={\"bins\": 15},\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using another dataset\n",
    "\n",
    "The inputs to the test can also can be changed. Let's try to re-run the test with the test dataset instead of the training dataset.\n",
    "\n",
    "[PLACEHOLDER CAN WE IMPLEMENT TWO DATASET RESULTS FOR ONE TEST RUN?]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    test_id=f\"{zopa_namespace}.MarginalBadRateTest\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate Custom Test in Model Documentation [PLACEHOLDER TEAM - IS there a way to incorporate the test programatically without going to UI?]\n",
    "\n",
    "Now, let's try visualizing these results in the ValidMind dashboard. Since we have called `test.log()` when running these tests their results are automatically logged to the ValidMind platform.\n",
    "\n",
    "Go to the ValidMind UI, select your model in the registry and go to the documentation page of your model and navigate to the `Model Development` -> `Model Evaluation` section. Then hover between any existing content block to reveal the `+` button as shown in the screenshot below.\n",
    "\n",
    "![screenshot showing insert button for test-driven blocks](images/insert-test-driven-block.png)\n",
    "\n",
    "Click on the `+` button and select `Test-Driven Block`. This will open a dialog where you can select `Metric` as the type of the test-driven content block, and then select the `GBC Test Provider Marginal Bad Rate Test` metric. This will show a preview of the composite metric and it should match the results shown above.\n",
    "\n",
    "![screenshot showing the selected composite metric in the dialog](images/selecting-bad-rates-metric.png)\n",
    "\n",
    "Finally, click on the `Insert block` button to add the composite metric to the documentation. You'll see the composite metric displayed in the documentation and now anytime you run `run_documentation_tests()`, the `Model Performance` composite metric will be run as part of the test suite. Let's go ahead and connect to the documentation project and run the tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finalize Testing and Documentation\n",
    "\n",
    "In this section we will show how to finalize the testing and documentation by showing the following items:\n",
    "\n",
    "1. How to run documentation and update the configuration so we can implement custom tests and additional tests in documentation sections\n",
    "2. How to overwrite individual tests with new data or new model\n",
    "3. How to go deeper in the configuration of parameters for model diagnosis testing\n",
    "4. MORE? (specific to model development persona....)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Programtically change the documentation configuration\n",
    "\n",
    "Below you will observe how you can first preview the current configuration using the `vm.get_test_suite().get_default_config()` interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "project_test_suite = vm.get_test_suite()\n",
    "config = project_test_suite.get_default_config()\n",
    "print(\"Suite Config: \\n\", json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Updating config\n",
    "\n",
    "The test configuration can be updated to fit with your use case and requirements but below you can see examples where several datasets are provided.\n",
    "\n",
    "[PLACEHOLDER CAN WE PROVIDE EXAMPLES ON HOW TO ADD A TEST IN A SECTION - PREFERABLY A CUSTOM TET?]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"validmind.data_validation.DatasetSplit\": {\n",
    "        \"inputs\": {\"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.PopulationStabilityIndex\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ConfusionMatrix\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance:in_sample\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_train_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance:out_of_sample\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.PrecisionRecallCurve\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ROCCurve\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.TrainingTestDegradation\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.MinimumAccuracy\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.MinimumF1Score\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.MinimumROCAUCScore\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.PermutationFeatureImportance\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.SHAPGlobalImportance\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run documentation tests\n",
    "\n",
    "You can now run all documentation tests and pass an extra `config` parameter that overrides input and parameter configuration for the tests specified in the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_suite = vm.run_documentation_tests(\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_ds,\n",
    "        \"model\": vm_model,\n",
    "        \"datasets\": (vm_train_ds, vm_test_ds),\n",
    "    },\n",
    "    config=config,\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Overwrite a test that has been docmented\n",
    "\n",
    "In this example we are showing how you can easily overwrite a test results. For example, let's assume you did some inital testing and logged results but for some reason you had to change the data used for model training and testing and as a consequence updated tests have to be implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Configure parameters for model diagnosis tests\n",
    "\n",
    "Each test has its default parameters and their values depending on the use case you are trying to solve. ValidMind's developer framework exposes these parameters at the user level so that they can be adjusted based on requirements.\n",
    "\n",
    "The config can be applied to a specific test to override the default configuration parameters.\n",
    "\n",
    "The format of the config is:\n",
    "\n",
    "```\n",
    "config = {\n",
    "    \"<test1_id>\": {\n",
    "        \"<default_param_1>\": value,\n",
    "        \"<default_param_2>\": value,\n",
    "    },\n",
    "     \"<test2_id>\": {\n",
    "        \"<default_param_1>\": value,\n",
    "        \"<default_param_2>\": value,\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "Users can input the configuration to `run_documentation_tests()` and `run_test_suite()` using **`config`**, allowing fine-tuning the suite according to the specific configuration requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the example below we are making the test more specific for certain columns. For example, in test Weak Spot Diagnosis I only want to perform this test on Age and Balance features.\n",
    "\n",
    "config = {\n",
    "    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n",
    "        \"params\": {\n",
    "            \"cut_off_percentage\": 3,\n",
    "            \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"],\n",
    "        },\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n",
    "        \"params\": {\n",
    "            \"features_columns\": [\"Age\", \"Balance\"],\n",
    "            \"accuracy_gap_threshold\": 85,\n",
    "        },\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n",
    "        \"params\": {\n",
    "            \"features_columns\": [\"Balance\", \"Tenure\"],\n",
    "            \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "            \"accuracy_decay_threshold\": 4,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "full_suite = vm.run_documentation_tests(\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"datasets\": (vm_train_ds, vm_test_ds),\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    config=config,\n",
    "    section=\"model_diagnosis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "You can look at the results of this test plan right in the notebook where you ran the code, as you would expect. But there is a better way: view the test results as part of your model documentation right in the ValidMind Platform UI:\n",
    "\n",
    "1. In the [Platform UI](https://app.prod.validmind.ai), go to the **Documentation** page for the model you registered earlier.\n",
    "\n",
    "2. Expand **Model Development**\n",
    "\n",
    "What you can see now is a more easily consumable version of the model diagnosis tests you just performed, along with other parts of your model documentation that still need to be completed.\n",
    "\n",
    "If you want to learn more about where you are in the model documentation process, take a look at <a href=\"https://docs.validmind.ai/guide/get-started-developer-framework.html#how-do-i-use-the-framework\"> How do I use the framework? </a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
