{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ValidMind Introduction for Model Developers\n",
    "\n",
    "This interactive notebook guides you through the process of documenting a model with the ValidMind Developer Framework. It uses a binary classification model as an example, but the same principles apply to other model types.\n",
    "\n",
    "As part of the notebook, you will learn how to start documenting a model as a **Model Developer** persona. At this stage the assumption is that there has been a [Model Documentation template](https://docs.validmind.com/guide/swap-documentation-templates.html#view-current-templates) defined in the platform.\n",
    "\n",
    "## Overview of the Notebook\n",
    "\n",
    "1. Initializing the ValidMind Developer Framework:\n",
    "\n",
    "   ValidMind’s developer framework provides a rich collection of documentation tools and test suites, from documenting descriptions of your dataset to validation testing your models for weak spots and overfit areas.\n",
    "\n",
    "2. Start Model Development process with raw data and run out-of-the box tests and add evidence to model documentation\n",
    "\n",
    "   In this stage the notebook will provide you details on how to access ValidMind's test repository to individual tests that you will use as building blocks to ensure a model is being built appropriately. The goal is to show how to run tests, investigate results and add tests / evidence to the documentation.\n",
    "\n",
    "   For a full list of out-of-box tests please refer to: https://docs.validmind.com/guide/test-descriptions.html\n",
    "\n",
    "3. Next we are going to build upon the previous step, but the focus here is implementation of Custom Tests\n",
    "\n",
    "   In this stage the notebook will provide details on how to implement custom tests. Usually, model developers have a lot of their own custom tests and it is important to include this within the model documentation. We will show how you how to include custom tests and then how they can be implemented within the documentation as additional evidence.\n",
    "\n",
    "4. The final part of the notebook will show you how to ensure completion of documentation\n",
    "\n",
    "   In this stage the notebook will provide details on how to ensure that model documentation and associated sections in the model documentation have been built out, and if there are any changes to testing due to additional data processing or data analysis requirements. The notebook will show how to update results for existing tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidMind at a glance\n",
    "\n",
    "ValidMind's platform enables organizations to identify, document, and manage model risks for all types of models, including AI/ML models, LLMs, and statistical models. As a model developer, you use the ValidMind Developer Framework to automate documentation and validation tests, and then use the ValidMind AI Risk Platform UI to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "If this is your first time trying out ValidMind, you can make use of the following resources alongside this notebook:\n",
    "\n",
    "- [Get started](https://docs.validmind.ai/guide/get-started.html) — The basics, including key concepts, and how our products work\n",
    "- [Get started with the ValidMind Developer Framework](https://docs.validmind.ai/guide/get-started-developer-framework.html) — The path for developers, more code samples, and our developer reference\n",
    "\n",
    "It is important to note that in order to connect to the Developer Framework you will have to access this through our API's using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "### New to ValidMind?\n",
    "\n",
    "For access to all features available in this notebook, create a free ValidMind account.\n",
    "\n",
    "Signing up is FREE — [**Sign up now**](https://app.prod.validmind.ai)\n",
    ":::\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the client library\n",
    "\n",
    "Please note the following recommended Python versions to utilize:\n",
    "\n",
    "- Python version 3.7 > x <= 3.11\n",
    "\n",
    "The client library provides Python support for the ValidMind Developer Framework. To install it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing the ValidMind Developer Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the client library\n",
    "\n",
    "The client library provides Python support for the ValidMind Developer Framework. You can install it directly in your Python environment or install it directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a new model in ValidMind UI and initialize the client library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the client library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "Get your code snippet:\n",
    "\n",
    "1. In a browser, log into the [Platform UI](https://app.prod.validmind.ai).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register new model**.\n",
    "\n",
    "3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/register-models-in-model-inventory.html))\n",
    "\n",
    "   For example, to register a model for use with this notebook, select:\n",
    "\n",
    "   - Documentation template: `Binary classification`\n",
    "   - Use case: `Marketing/Sales - Attrition/Churn Management`\n",
    "\n",
    "   You can fill in other options according to your preference.\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    project=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify & Preview the documentation template\n",
    "\n",
    "Here we want to verify that we have connected and that the appropriate template is selected. A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
    "\n",
    "[PLACEHOLDER - JOHN's SANDBOX ENVIRONMENT SHOWING DESCRIPTION OF EACH TEST AVAILABLE]\n",
    "\n",
    "You will upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the `vm.preview_template()` function from the ValidMind library and note the empty sections:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.preview_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's observe the the list of all available tests in the ValidMind Developer Framework:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start Model Development process by leveraging ValidMind out-of-the-box tests\n",
    "\n",
    "In this section we will provide details on how to understand individual tests available in ValidMind, how we can access each test, run it and change parameters if necessary. We are using the dataset provided to Model Developers as the example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.datasets.classification import customer_churn as demo_dataset\n",
    "\n",
    "df_raw = demo_dataset.load_data()\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some data quality assessments by running a few individual tests related to data assessment. We will be using the 'vm.tests.list_suites()' above as the guide on which tests to run. Below you will observe how we selected a suite of tests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is how you can obtain tags to search for:\n",
    "vm.tests.list_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the task is the target you are predicting, i.e. in this example classification otherwise options can be found using 'list_task_types()'\n",
    "print(vm.tests.list_task_types())\n",
    "vm.tests.list_tests(task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can be even more specific by using tags to drill down into the appropriate out-of-box tests\n",
    "vm.tests.list_tests(\n",
    "    tags=[\"model_performance\", \"visualization\", \"sklearn\"], task=\"classification\"\n",
    ")  # MAKE THIS SPECIFIC TO DATA QUALITY ASSESSMENT TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ValidMind datasets\n",
    "\n",
    "Now we assume we have identified some tests we want to run with regards to the data we are intending to use. First thing is to register the data inorder to Initialize ValidMind objects. Everytime you want to connect a dataset to documentation and produce tests through ValidMind this step is always necessary. You only need to do it one time per dataset.\n",
    "\n",
    "Before you can run tests, you must first initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
    "\n",
    "This function takes a number of arguments:\n",
    "\n",
    "- `dataset` — the raw dataset that you want to provide as input to tests\n",
    "- `input_id` - a unique identifier that allows tracking what inputs are used when running each individual test\n",
    "- `target_column` — a required argument if tests require access to true values. This is the name of the target column in the dataset\n",
    "\n",
    "With all datasets ready, you can now initialize the raw, training and test datasets (`train_df` and `test_df`) created earlier into their own dataset objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):\n",
    "\n",
    "NOTE: below the object 'vm_raw_dataset' is the dataset object that we will use to pass in to ValidMind tests as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vm_raw_dataset is now the dataset object\n",
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=df_raw,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=\"Class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is one way to see available tests for tag \"tabular_dataset\"\n",
    "vm.test_suites.describe_suite(\"tabular_dataset\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test\n",
    "\n",
    "Individual tests can be easily run by calling the `run_test` function provided by the `validmind.tests` module. The function takes the following arguments:\n",
    "\n",
    "- `test_id`: The ID of the test to run. To find a particular test and get its, refer to the `explore_tests.ipynb` notebook. Look above for example after running 'vm.test_suites.describe_suite' as column 'Test ID' will contain the id.\n",
    "- `params`: A dictionary of parameters for the test. These will override any `default_params` set in the test definition. Refer to the `explore_tests.ipynb` notebook to find the default parameters for a test. See below for examples.\n",
    "\n",
    "You can then pass in any inputs for the test as keyword arguments. Most likely, these will be `dataset` and `model` objects. Again, you may refer to the [PLACEHOLDER JOHN's SANDBOX FOR TEST DESCRIPTION]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is example on producing descriptive statistic\n",
    "test = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.DescriptiveStatistics\",\n",
    "    inputs={\"dataset\": vm_raw_dataset},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is example on class imbalance test for Target\n",
    "test2 = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_raw_dataset},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the class imbalance test did not pass our data quality standards here is how we can re-run the test on some processed data to address the issue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple treatment to make it more dataset distribution of target more in-line with best-practice\n",
    "df_raw_new = df_raw.sample(frac=1)  # create a copy of the raw dataset\n",
    "# split the classes\n",
    "fraud_df = df_raw_new.loc[df_raw_new[\"Class\"] == 1]\n",
    "non_fraud_df = df_raw_new.loc[df_raw_new[\"Class\"] == 0][\n",
    "    : len(fraud_df) * 8\n",
    "]  # using the positive class as undersampled, adding x8 larger non-positive class dataframe\n",
    "# new_df\n",
    "new_df = pd.concat([fraud_df, non_fraud_df])\n",
    "# Shuffle dataframe rows\n",
    "new_df_raw = new_df.sample(frac=1, random_state=42)\n",
    "new_df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new raw dataset and let's run the individual test to see if it passes the class imbalance test requirement. BUT FIRST, remember to register new dataset and then run the test and notice new dataset object being used as the input to the test 'tests.run_test':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register new data and now 'vm_raw_dataset_new' is the new dataset object of interest\n",
    "vm_raw_dataset_new = vm.init_dataset(\n",
    "    dataset=new_df_raw,\n",
    "    input_id=\"new_df_raw\",\n",
    "    target_column=\"Class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can continue and see if test passes. NOTE: dataset is now set to 'vm_raw_dataset_new'\n",
    "test = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_raw_dataset_new},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize Test Output\n",
    "\n",
    "Below is an example on how you can utilize the output from a ValidMind test for futher use, for example, if you want to remove highly correlated features then the below shows how you can get a pearson's correlation matrix, use the output to reduce the feature list for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.HighPearsonCorrelation\",\n",
    "    params={\"max_threshold\": 0.5},\n",
    "    inputs={\"dataset\": vm_raw_dataset_new},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER - DO SOMETHING WITH THE OUTPUT to reduce feature list - how we can work with validmind test output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documenting the results based on two datasets\n",
    "\n",
    "We have now done some analysis on two different datasets and we should document why certain things was done to the raw data, and testing should support this. Here we are providing a simple example, where we are providing correlation matrix for two different datasets\n",
    "\n",
    "Now specify inputs and params for individual tests using config parameter. The results for the both the datasets will be visible in the documentation. The inputs in the config get priority over global inputs in the run_documentation_tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall the two dataset objects: 'vm_raw_dataset' & 'vm_raw_dataset_new'\n",
    "config = {\n",
    "    \"validmind.data_validation.HighPearsonCorrelation:vm_raw_dataset\": {\n",
    "        \"params\": {\"max_threshold\": 0.5},\n",
    "        \"inputs\": {\"dataset\": vm_raw_dataset},\n",
    "    },\n",
    "    \"validmind.data_validation.HighPearsonCorrelation:vm_raw_dataset_new\": {\n",
    "        \"params\": {\"max_threshold\": 0.5},\n",
    "        \"inputs\": {\"dataset\": vm_raw_dataset_new},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Note that section is under data preperation\n",
    "tests_suite = vm.run_documentation_tests(\n",
    "    inputs={\n",
    "        \"dataset\": vm_dataset,  # WHY IS THIS NEEED - CHECK WITH TEAM ON WHY\n",
    "    },\n",
    "    config=config,\n",
    "    section=[\"data_preparation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the individual tests using the `run_test`\n",
    "\n",
    "Now run the `HighPearsonCorrelation` tests for the two datasets. The results for the both the datasets will be visible in the documentation. Note that test.log() is logging the test results in the documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER THE BELOW CODE SEEMS REDUNDANT RELATIVE TO THE CELL ABOVE -  VERIFY WITH TEAM IF THIS IS THE EASIEST WAY.\n",
    "\n",
    "test = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.HighPearsonCorrelation:vm_raw_dataset\",\n",
    "    params={\"max_threshold\": 0.5},\n",
    "    inputs={\"dataset\": vm_raw_dataset},\n",
    ")\n",
    "test.log()\n",
    "\n",
    "test = vm.tests.run_test(\n",
    "    test_id=\"validmind.data_validation.HighPearsonCorrelation:vm_raw_dataset_new\",\n",
    "    params={\"max_threshold\": 0.5},\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset_new,\n",
    "    },\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a larger portion of Tests\n",
    "\n",
    "Next, let's see how we can run whole sections / suites of tests instead of doing one-by-one as we would likely want to do this in the Model Development process. Note that running parts of the testing which will be populated in the documentation is a function of the template and test mapping associated with the template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vm.run_documentation_tests(\n",
    "    section=\"data_preparation\", inputs={\"dataset\": vm_raw_dataset_new}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "\n",
    "We have focused so far on the data assessment and pre-processing that usually occurs prior to any models being built. Now we are going to assume we have built a model and now we want to incorporate some model results in our documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a simple model based on lastest new_df_raw\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data out\n",
    "X = new_df_raw.drop(\"Class\", axis=1)\n",
    "y = new_df_raw[\"Class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_params = {\"C\": [0.5, 0.7, 0.9, 1], \"kernel\": [\"rbf\", \"poly\", \"sigmoid\", \"linear\"]}\n",
    "grid_svc = GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "\n",
    "# SVC best estimator\n",
    "svc = grid_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to ValidMind structure we add the Target with the Model Inputs, so let's combine test and train into two datasets:\n",
    "# I need to redo the X_train, X_test, y_train, y_test  as we need it to be overall train and test\n",
    "TRAIN = X_train\n",
    "TRAIN[\"Class\"] = y_train\n",
    "TEST = X_test\n",
    "TEST[\"Class\"] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, always remember to connect the datasets with ValidMind but in addition we will also connect the model to ValidMind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the two new datasets as test and training\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    input_id=\"train_dataset_final\",\n",
    "    dataset=TRAIN,\n",
    "    target_column=\"Class\",\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    input_id=\"test_dataset_final\",\n",
    "    dataset=TEST,\n",
    "    target_column=\"Class\",\n",
    ")\n",
    "\n",
    "\n",
    "# Register the model\n",
    "vm_model = vm.init_model(svc, input_id=\"svc_model_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign predictions to the datasets\n",
    "\n",
    "Once the Model has been registered with the corresponding train and test we can use these datasets directly together with the model to assign predictions. We can now use the `assign_predictions()` method from the `Dataset` object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds.assign_predictions(model=vm_model)\n",
    "vm_test_ds.assign_predictions(model=vm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we focus on running the tests within the model development section of the model documentation. After running this function, only the tests associated with this section will be executed, and the corresponding section in the model documentation will be updated. In the example below, model development and model diagnosis sections are being run and where the train and test datasets are linked with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this and observe in the output the sections being filled.\n",
    "results = vm.run_documentation_tests(\n",
    "    section=[\"model_development\", \"model_diagnosis\"],\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset_new,\n",
    "        \"model\": vm_model,\n",
    "        \"datasets\": (vm_train_ds, vm_test_ds),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the entire suite of tests\n",
    "\n",
    "Here you will observe how you can run all the tests that have been pre-defined in the documentation template, e.g. under Model Diagnosis section certain tests have been mapped in setting up the appropriate documentation template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_suite = vm.run_documentation_tests(\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset_new,\n",
    "        \"model\": vm_model,\n",
    "        \"datasets\": (vm_train_ds, vm_test_ds),\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom Metrics and Threshold Tests Implementation\n",
    "\n",
    "[PLACEHOLDER - DO WE NEED TO ADD JOHN's NEW PROCESS HERE with Custom Metric function decorator? https://github.com/validmind/developer-framework/blob/john6797/sc-3718/create-decorator-for-registering-one-off/notebooks/code_samples/custom_tests/implementing_custom_tests.ipynb]\n",
    "\n",
    "This next session assumes that Model Developers already have a repository of custom made tests and analysis that is critical to include in the documentation. In this sub-section we will provide details on how to easily implement your custom tests in ValidMind before showing how to use the test.\n",
    "\n",
    "[PLACEHOLDER HERE ON OVERVIEW IMAGE ON HOW PROCESS LOOKS LIKE]\n",
    "\n",
    "Custom metrics offer added flexibility by extending the default metrics provided by ValidMind, enabling you to document any type of model or use case. Both metrics and threshold tests assess models but they differ in approach: _metrics_ measure a range of dataset or model behaviors, while _threshold tests_ yield a pass or fail result based on specific criteria. These instructions include the code required to:\n",
    "\n",
    "- Create a metric class signature\n",
    "- Implement a custom metric\n",
    "- Test the custom metric\n",
    "- Add a `summary()` method to the custom metric\n",
    "- Add figures to a metric\n",
    "\n",
    "As a reminder we are utilizing the previous steps in the future steps. More specifically:\n",
    "\n",
    "- `vm_model` is the Support Vector Classifier model object from ValidMind\n",
    "- `vm_raw_dataset_new` is the final pre-processed dataset object used for training and testing of model\n",
    "- `vm_train_ds` & `vm_test_ds` are the two dataset objects used to train and test the model\n",
    "\n",
    "Finally, recall that predictions have been assigned through `assign_predictions`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register external test providers (custom test)\n",
    "\n",
    "We will now declare a local filesystem test provider that allows loading tests from a local folder. Fror this to work we just need to specify the root folder under which the provider class will look for tests. For this demo, it is the `./tests/` directory.\n",
    "\n",
    "[PLACEHOLDER FOR TEAM TO ADD MORE DETAILS ON THE FLOW HERE]  \n",
    "WE NEED HOW THE CODE SHOULD BE STRUCTURED AND GOAL: MAKE IT AS EASY AS POSSIBLE\n",
    "CAN WE ADD MULTIPLE TESTS IN THE PYTHON FILE?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import LocalTestProvider\n",
    "\n",
    "# First we are going define a name so that we can always refer back and find our custom tests. In this example \"gbc_test_provides\" is the identifier\n",
    "gbc_namespace = \"gbc_test_provider\"\n",
    "\n",
    "# Setting up the connection to where the custom testing code lives.\n",
    "local_test_provider = LocalTestProvider(root_folder=\"./tests/\")\n",
    "\n",
    "# Now let's register the test under the name we defined above\n",
    "vm.tests.register_test_provider(\n",
    "    namespace=gbc_namespace,\n",
    "    test_provider=local_test_provider,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing & Executing Custom Test in Model Documentation\n",
    "\n",
    "Let's now build a sample custom test that includes the outputs from a demo function called `get_marginal_bad_rates`. Inside the `tests/` directory next to this notebook you will find a file called `MarginalBadRateTest.py`. This file contains the custom test definition that we will run in the next cell. If you open that file you'll see how we invoke the `get_marginal_bad_rates` function from the `run()` method provided by the test interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom test is found by searching for the name space created above with the Python file name 'MarginalBadRateTest'\n",
    "# This runs the test on the dataset object 'vm_train_ds' with model object 'vm_model'\n",
    "test = vm.tests.run_test(\n",
    "    test_id=f\"{gbc_namespace}.MarginalBadRateTest\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the parameters and implement in Model Documentation\n",
    "\n",
    "Note how we have defined the following property in the custom test class (i.e. parameter in custom test):\n",
    "\n",
    "```python\n",
    "default_params = {\"bins\": 10}\n",
    "```\n",
    "\n",
    "This allows you to pass parameters to the test when running it. Let's try to re-running the test with 15 bins instead. In this custom test the bins affecting the figures and table output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test is run exactly the same as before but now you can see an additional line; 'params={\"bins\":15}' which will overwrite default bin value of 10\n",
    "\n",
    "test = vm.tests.run_test(\n",
    "    test_id=f\"{gbc_namespace}.MarginalBadRateTest\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    params={\"bins\": 15},\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using another dataset\n",
    "\n",
    "The inputs to the test can also can be changed. Let's try to re-run the test with the test dataset instead of the training dataset.\n",
    "\n",
    "[PLACEHOLDER CAN WE IMPLEMENT TWO DATASET RESULTS FOR ONE TEST RUN?]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    test_id=f\"{zopa_namespace}.MarginalBadRateTest\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate Custom Test in Model Documentation [PLACEHOLDER TEAM - IS there a way to incorporate the test programatically without going to UI?]\n",
    "\n",
    "Now, let's try visualizing these results in the ValidMind dashboard. Since we have called `test.log()` when running these tests their results are automatically logged to the ValidMind platform.\n",
    "\n",
    "Go to the ValidMind UI, select your model in the registry and go to the documentation page of your model and navigate to the `Model Development` -> `Model Evaluation` section. Then hover between any existing content block to reveal the `+` button as shown in the screenshot below.\n",
    "\n",
    "![screenshot showing insert button for test-driven blocks](images/insert-test-driven-block.png)\n",
    "\n",
    "Click on the `+` button and select `Test-Driven Block`. This will open a dialog where you can select `Metric` as the type of the test-driven content block, and then select the `GBC Test Provider Marginal Bad Rate Test` metric. This will show a preview of the composite metric and it should match the results shown above.\n",
    "\n",
    "![screenshot showing the selected composite metric in the dialog](images/selecting-bad-rates-metric.png)\n",
    "\n",
    "Finally, click on the `Insert block` button to add the composite metric to the documentation. You'll see the composite metric displayed in the documentation and now anytime you run `run_documentation_tests()`, the `Model Performance` composite metric will be run as part of the test suite. Let's go ahead and connect to the documentation project and run the tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finalize Testing and Documentation\n",
    "\n",
    "In this section we will show how to finalize the testing and documentation by showing the following items:\n",
    "\n",
    "1. How to run documentation and update the configuration so we can implement custom tests and additional tests in documentation sections\n",
    "2. How to overwrite individual tests with new data or new model\n",
    "3. How to go deeper in the configuration of parameters for model diagnosis testing\n",
    "4. MORE? (specific to model development persona....)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Programtically change the documentation configuration\n",
    "\n",
    "Below you will observe how you can first preview the current configuration using the `vm.get_test_suite().get_default_config()` interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "project_test_suite = vm.get_test_suite()\n",
    "config = project_test_suite.get_default_config()\n",
    "print(\"Suite Config: \\n\", json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Updating config\n",
    "\n",
    "The test configuration can be updated to fit with your use case and requirements but below you can see examples where several datasets are provided.\n",
    "\n",
    "[PLACEHOLDER CAN WE PROVIDE EXAMPLES ON HOW TO ADD A TEST IN A SECTION - PREFERABLY A CUSTOM TET?]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"validmind.data_validation.DatasetSplit\": {\n",
    "        \"inputs\": {\"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.PopulationStabilityIndex\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ConfusionMatrix\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance:in_sample\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_train_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance:out_of_sample\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.PrecisionRecallCurve\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.ROCCurve\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.TrainingTestDegradation\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.MinimumAccuracy\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.MinimumF1Score\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.MinimumROCAUCScore\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.PermutationFeatureImportance\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.SHAPGlobalImportance\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"dataset\": vm_test_ds},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n",
    "        \"inputs\": {\"model\": vm_model, \"datasets\": (vm_train_ds, vm_test_ds)},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run documentation tests\n",
    "\n",
    "You can now run all documentation tests and pass an extra `config` parameter that overrides input and parameter configuration for the tests specified in the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_suite = vm.run_documentation_tests(\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_ds,\n",
    "        \"model\": vm_model,\n",
    "        \"datasets\": (vm_train_ds, vm_test_ds),\n",
    "    },\n",
    "    config=config,\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Overwrite a test that has been docmented\n",
    "\n",
    "In this example we are showing how you can easily overwrite a test results. For example, let's assume you did some inital testing and logged results but for some reason you had to change the data used for model training and testing and as a consequence updated tests have to be implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Configure parameters for model diagnosis tests\n",
    "\n",
    "Each test has its default parameters and their values depending on the use case you are trying to solve. ValidMind's developer framework exposes these parameters at the user level so that they can be adjusted based on requirements.\n",
    "\n",
    "The config can be applied to a specific test to override the default configuration parameters.\n",
    "\n",
    "The format of the config is:\n",
    "\n",
    "```\n",
    "config = {\n",
    "    \"<test1_id>\": {\n",
    "        \"<default_param_1>\": value,\n",
    "        \"<default_param_2>\": value,\n",
    "    },\n",
    "     \"<test2_id>\": {\n",
    "        \"<default_param_1>\": value,\n",
    "        \"<default_param_2>\": value,\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "Users can input the configuration to `run_documentation_tests()` and `run_test_suite()` using **`config`**, allowing fine-tuning the suite according to the specific configuration requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the example below we are making the test more specific for certain columns. For example, in test Weak Spot Diagnosis I only want to perform this test on Age and Balance features.\n",
    "\n",
    "config = {\n",
    "    \"validmind.model_validation.sklearn.OverfitDiagnosis\": {\n",
    "        \"params\": {\n",
    "            \"cut_off_percentage\": 3,\n",
    "            \"feature_columns\": [\"Age\", \"Balance\", \"Tenure\", \"NumOfProducts\"],\n",
    "        },\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.WeakspotsDiagnosis\": {\n",
    "        \"params\": {\n",
    "            \"features_columns\": [\"Age\", \"Balance\"],\n",
    "            \"accuracy_gap_threshold\": 85,\n",
    "        },\n",
    "    },\n",
    "    \"validmind.model_validation.sklearn.RobustnessDiagnosis\": {\n",
    "        \"params\": {\n",
    "            \"features_columns\": [\"Balance\", \"Tenure\"],\n",
    "            \"scaling_factor_std_dev_list\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "            \"accuracy_decay_threshold\": 4,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "full_suite = vm.run_documentation_tests(\n",
    "    inputs={\n",
    "        \"dataset\": vm_train_ds,\n",
    "        \"datasets\": (vm_train_ds, vm_test_ds),\n",
    "        \"model\": vm_model,\n",
    "    },\n",
    "    config=config,\n",
    "    section=\"model_diagnosis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "You can look at the results of this test plan right in the notebook where you ran the code, as you would expect. But there is a better way: view the test results as part of your model documentation right in the ValidMind Platform UI:\n",
    "\n",
    "1. In the [Platform UI](https://app.prod.validmind.ai), go to the **Documentation** page for the model you registered earlier.\n",
    "\n",
    "2. Expand **Model Development**\n",
    "\n",
    "What you can see now is a more easily consumable version of the model diagnosis tests you just performed, along with other parts of your model documentation that still need to be completed.\n",
    "\n",
    "If you want to learn more about where you are in the model documentation process, take a look at <a href=\"https://docs.validmind.ai/guide/get-started-developer-framework.html#how-do-i-use-the-framework\"> How do I use the framework? </a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
