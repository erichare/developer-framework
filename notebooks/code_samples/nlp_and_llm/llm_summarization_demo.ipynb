{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Using LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def _format_cell_text(text, width=50):\n",
    "    \"\"\"Private function to format a cell's text.\"\"\"\n",
    "    return \"\\n\".join([textwrap.fill(line, width=width) for line in text.split(\"\\n\")])\n",
    "\n",
    "\n",
    "def _format_dataframe_for_tabulate(df):\n",
    "    \"\"\"Private function to format the entire DataFrame for tabulation.\"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Format all string columns\n",
    "    for column in df_out.columns:\n",
    "        # Check if column is of type object (likely strings)\n",
    "        if df_out[column].dtype == object:\n",
    "            df_out[column] = df_out[column].apply(_format_cell_text)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def _dataframe_to_html_table(df):\n",
    "    \"\"\"Private function to convert a DataFrame to an HTML table.\"\"\"\n",
    "    headers = df.columns.tolist()\n",
    "    table_data = df.values.tolist()\n",
    "    return tabulate(table_data, headers=headers, tablefmt=\"html\")\n",
    "\n",
    "\n",
    "def display_formatted_dataframe(df, num_rows=None):\n",
    "    \"\"\"Primary function to format and display a DataFrame.\"\"\"\n",
    "    if num_rows is not None:\n",
    "        df = df.head(num_rows)\n",
    "    formatted_df = _format_dataframe_for_tabulate(df)\n",
    "    html_table = _dataframe_to_html_table(formatted_df)\n",
    "    display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  project = \"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first install the `datasets` library from huggingface\n",
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train_df = cnn_dataset.data[\"train\"].to_pandas()\n",
    "val_df = cnn_dataset.data[\"validation\"].to_pandas()\n",
    "test_df = cnn_dataset.data[\"test\"].to_pandas()\n",
    "\n",
    "train_df = train_df[[\"article\", \"highlights\"]]\n",
    "train_df = train_df.head(5)\n",
    "test_df = test_df[[\"article\", \"highlights\"]]\n",
    "test_df = test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_formatted_dataframe(train_df, num_rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.models import FoundationModel, Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise Exception(\"OPENAI_API_KEY not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model = OpenAI()\n",
    "\n",
    "\n",
    "def call_model(prompt):\n",
    "    return (\n",
    "        model.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an AI with expertise in summarizing financial news.\n",
    "Your task is to provide a concise summary of the specific news article provided below.\n",
    "Before proceeding, take a moment to understand the context and nuances of the financial terminology used in the article.\n",
    "\n",
    "Article to Summarize:\n",
    "\n",
    "```\n",
    "{article}\n",
    "```\n",
    "\n",
    "Please respond with a concise summary of the article's main points.\n",
    "Ensure that your summary is based on the content of the article and not on external information or assumptions.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_variables = [\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    text_column=\"article\",\n",
    "    target_column=\"highlights\",\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df,\n",
    "    input_id=\"test_dataset\",\n",
    "    text_column=\"article\",\n",
    "    target_column=\"highlights\",\n",
    ")\n",
    "\n",
    "vm_model = FoundationModel(\n",
    "    predict_fn=call_model,\n",
    "    prompt=Prompt(\n",
    "        template=prompt_template,\n",
    "        variables=prompt_variables,\n",
    "    ),\n",
    "    input_id=\"llm_model\"\n",
    ")\n",
    "\n",
    "# Assign model predictions to the test dataset\n",
    "vm_test_ds.assign_predictions(vm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vm_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "- Text Description\n",
    "- Common Words \n",
    "- Punctuations\n",
    "- StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.TextDescription\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.CommonWords\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.Punctuations\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.data_validation.nlp.StopWords\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings \n",
    "\n",
    "- Cosine Similarity Distribution\n",
    "- Cluster Distribution\n",
    "- Descriptive Analytics\n",
    "- Stability Analysis Keyword\n",
    "- Stability Analysis Random Noise\n",
    "- Stability Analysis Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "embedding_model = pipeline(\n",
    "    'feature-extraction', \n",
    "    model='bert-base-uncased', \n",
    "    tokenizer='bert-base-uncased',\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "vm_embedding_model = vm.init_model(\n",
    "    model=embedding_model,\n",
    "    input_id=\"bert_embedding_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.embeddings.CosineSimilarityDistribution\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_embedding_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.embeddings.ClusterDistribution\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_embedding_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.embeddings.DescriptiveAnalytics\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_embedding_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisKeyword\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_embedding_model,\n",
    "    },\n",
    "    params = {\n",
    "        \"text_column\": \"article\",\n",
    "        \"keyword_dict\": {\"finance\": \"financial\"},\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisRandomNoise\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_embedding_model,\n",
    "    },\n",
    "    params = {\n",
    "        \"text_column\": \"article\",\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.embeddings.StabilityAnalysisSynonyms\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_embedding_model,\n",
    "    },\n",
    "    params = {\n",
    "        \"text_column\": \"article\",\n",
    "        \"probability:\": 0.1,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Validation\n",
    "\n",
    "- Bias\n",
    "- Clarity\n",
    "- Conciseness\n",
    "- Delimitation\n",
    "- NegativeInstruction\n",
    "- Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Bias\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Clarity\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Conciseness\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Delimitation\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.NegativeInstruction\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.prompt_validation.Specificity\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_code = False  \n",
    "if run_code:\n",
    "\n",
    "    test_suite_results = vm.run_test_suite(\n",
    "        \"prompt_validation\",\n",
    "        inputs={\n",
    "            \"dataset\": vm_test_ds,\n",
    "            \"model\": vm_model,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Tests\n",
    "\n",
    "- Token Disparity\n",
    "- Rouge Metrics\n",
    "- Rouge Metrics Aggregate\n",
    "- Bert Score\n",
    "- Bert Score Aggregate\n",
    "- Contextual Recall\n",
    "- Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.MeteorScore\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.TokenDisparity\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.RougeMetrics\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.RougeMetricsAggregate\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.BertScore\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.BertScoreAggregate\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.ContextualRecall\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.BleuScore\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Toxicity Tests\n",
    "\n",
    "- Toxicity Score\n",
    "- Toxicity Histogram\n",
    "- Regard Score\n",
    "- Regard Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.ToxicityScore\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.ToxicityHistogram\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.RegardScore\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vm.tests.run_test(\n",
    "    \"validmind.model_validation.RegardHistogram\",\n",
    "    inputs = {\n",
    "        \"dataset\": vm_test_ds,\n",
    "        \"model\": vm_model,\n",
    "    }\n",
    ")\n",
    "test.log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
