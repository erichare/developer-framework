{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Scorecard Model Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Credit risk Scorecard** model created from the Lending Club dataset is instrumental in computing the Probability of Default (PD), a key factor in ECL calculations. This scorecard assesses several credit characteristics of potential borrowers, like their credit history, income, outstanding debts, and more, each of which is assigned a specific score. By combining these scores, we derive a total score for each borrower, which translates into an estimated Point-in-Time (PiT) PD. The PiT PD reflects the borrower's likelihood of default at a specific point in time, accounting for both current and foreseeable future conditions.\n",
    "\n",
    "Additionally, for a holistic view of credit risk, it's essential to estimate the Lifetime PD. The Lifetime PD, as the name suggests, predicts the borrower's likelihood of default throughout the life of the exposure, taking into account potential future changes in the economic and financial conditions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Pandas backend loaded 1.5.3\n",
      "INFO: Numpy backend loaded 1.22.3\n",
      "INFO: Pyspark backend NOT loaded\n",
      "INFO: Python backend loaded\n",
      "2023-08-10 15:30:31,733 - INFO(validmind.api_client): Connected to ValidMind. Project: [6] Credit Risk Scorecard - Initial Validation (clk00h0u800x9qjy67gduf5om)\n",
      "INFO: Connected to ValidMind. Project: [6] Credit Risk Scorecard - Initial Validation (clk00h0u800x9qjy67gduf5om)\n"
     ]
    }
   ],
   "source": [
    "from notebooks.probability_of_default.helpers.Developer import Developer\n",
    "from notebooks.probability_of_default.helpers.scorecard_tasks import *\n",
    "from notebooks.probability_of_default.helpers.model_development_tasks import *\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"clk00h0u800x9qjy67gduf5om\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_column = \"default\"\n",
    "\n",
    "lending_club_url = \"https://vmai.s3.us-west-1.amazonaws.com/datasets/lending_club_loan_data_2007_2014.csv\"\n",
    "\n",
    "preliminary_features_to_drop = [\n",
    "    \"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n",
    "    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n",
    "    'earliest_cr_line', 'issue_d']\n",
    "\n",
    "final_features_to_drop = ['addr_state', 'total_rec_int', 'loan_amnt',\n",
    "                    'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n",
    "                    'total_pymnt_inv', 'last_pymnt_amnt', \"inq_last_6mths\"]\n",
    "\n",
    "min_missing_percentage = 80"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Developer Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove_features_missing_values'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Developer class\n",
    "developer = Developer()\n",
    "\n",
    "# Register developer tasks\n",
    "developer.add_task(\n",
    "    task_id=\"import_raw_data\", \n",
    "    task=import_raw_data,\n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"drop_features\",\n",
    "    task=drop_features,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"add_default_definition\",\n",
    "    task=add_default_definition,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_term_column\",\n",
    "    task=convert_term_column,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_emp_length_column\",\n",
    "    task=convert_emp_length_column,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_inq_last_6mths_column\",\n",
    "    task=convert_inq_last_6mths_column,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"data_split\",\n",
    "    task=data_split,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"drop_categories\",\n",
    "    task=drop_categories,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_to_woe\",\n",
    "    task=convert_to_woe,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"add_constant\",\n",
    "    task=add_constant,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"train_model\",\n",
    "    task=train_model,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"remove_features_missing_values\",\n",
    "    task=remove_features_missing_values,  \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'import_raw_data'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing raw data from: https://vmai.s3.us-west-1.amazonaws.com/datasets/lending_club_loan_data_2007_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Dev/github/validmind-python/notebooks/probability_of_default/helpers/model_development_tasks.py:11: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_out = pd.read_csv(source)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully with 466285 rows and 75 columns.\n"
     ]
    }
   ],
   "source": [
    "df_1 = developer.execute_task(\n",
    "    area_id = \"data_description\",\n",
    "    task_id = \"import_raw_data\", \n",
    "    inputs = [lending_club_url],\n",
    "    validation_tests = [\"descriptive_statistics\", \"missing_values_bar_plot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_features'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 33 columns.\n",
      "Columns remaining after dropping: 42\n"
     ]
    }
   ],
   "source": [
    "df_2 = developer.execute_task(\n",
    "    area_id = \"data_preparation\",\n",
    "    task_id = \"drop_features\", \n",
    "    inputs = [df_1, preliminary_features_to_drop],\n",
    "    validation_tests = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'add_default_definition'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'loan_status' to target column...\n",
      "Removed 239071 rows with undefined 'loan_status' values.\n",
      "Converted 'loan_status' to 'default' and set its data type to integer.\n",
      "'loan_status' column has been removed from the DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Dev/github/validmind-python/notebooks/probability_of_default/helpers/scorecard_tasks.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[default_column] = df[default_column].astype(int)\n",
      "/Users/juanvalidmind/Dev/github/validmind-python/notebooks/probability_of_default/helpers/scorecard_tasks.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=[\"loan_status\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_3 = developer.execute_task(\n",
    "    area_id = \"data_preparation\",\n",
    "    task_id = \"add_default_definition\", \n",
    "    inputs = [df_2, default_column],\n",
    "    validation_tests = [\"missing_values_bar_plot\",\n",
    "                        \"class_imbalance\", \n",
    "                        \"iqr_outliers_table\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'remove_features_missing_values'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing missing values in the dataset...\n",
      "Found 18 features with more than 80% missing values.\n",
      "Dropping the following columns: mths_since_last_major_derog, annual_inc_joint, dti_joint, verification_status_joint, open_acc_6m, open_il_6m, open_il_12m, open_il_24m, mths_since_rcnt_il, total_bal_il, il_util, open_rv_12m, open_rv_24m, max_bal_bc, all_util, inq_fi, total_cu_tl, inq_last_12m\n"
     ]
    }
   ],
   "source": [
    "df_4 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"remove_features_missing_values\", \n",
    "    inputs=[df_3, min_missing_percentage],\n",
    "    validation_tests=[\"missing_values_bar_plot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'convert_term_column'...\n",
      "\n",
      "INFO: Executing task 'convert_emp_length_column'...\n",
      "\n",
      "INFO: Executing task 'convert_inq_last_6mths_column'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"convert_term_column\", \n",
    "    inputs=[df_4]\n",
    ")\n",
    "\n",
    "df_6 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"convert_emp_length_column\", \n",
    "    inputs=[df_5]\n",
    ")\n",
    "\n",
    "df_7 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"convert_inq_last_6mths_column\", \n",
    "    inputs=[df_6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'data_split'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 181771 rows and 23 columns.\n",
      "Test data has 45443 rows and 23 columns.\n"
     ]
    }
   ],
   "source": [
    "df_train_1, df_test_1 = developer.execute_task(\n",
    "    area_id=\"data_sampling\",\n",
    "    task_id=\"data_split\", \n",
    "    inputs=[df_7, default_column],\n",
    "    validation_tests=[\"tabular_numerical_histograms\", \n",
    "                      \"high_cardinality\", \n",
    "                      \"tabular_categorical_bar_plots\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_categories'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows retained with purpose 'debt_consolidation' or 'credit_card': 142293\n",
      "Rows after removing grades 'F' or 'G': 137816\n",
      "Rows after removing sub_grades starting with 'F' or 'G': 137816\n",
      "Rows after removing home_ownership values 'OTHER', 'NONE', or 'ANY': 137723\n",
      "Total rows dropped: 44048\n",
      "Rows retained with purpose 'debt_consolidation' or 'credit_card': 35532\n",
      "Rows after removing grades 'F' or 'G': 34349\n",
      "Rows after removing sub_grades starting with 'F' or 'G': 34349\n",
      "Rows after removing home_ownership values 'OTHER', 'NONE', or 'ANY': 34322\n",
      "Total rows dropped: 11121\n"
     ]
    }
   ],
   "source": [
    "df_train_2 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_categories\", \n",
    "    inputs=[df_train_1],\n",
    "    validation_tests=[\"target_rate_bar_plots\"]\n",
    ")\n",
    "\n",
    "df_test_2 = drop_categories(df_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_features'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 10 columns.\n",
      "Columns remaining after dropping: 14\n",
      "Dropped 10 columns.\n",
      "Columns remaining after dropping: 14\n"
     ]
    }
   ],
   "source": [
    "df_train_3 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_train_2, final_features_to_drop],\n",
    "    validation_tests=[\"chi_squared_features_table\", \n",
    "                      \"anova_one_way_table\", \n",
    "                      \"pearson_correlation_matrix\", \n",
    "                      \"feature_target_correlation_plot\",\n",
    "                      \"woe_bin_table\",\n",
    "                      \"woe_bin_table\",   # with different parameters\n",
    "                      \"woe_bin_plots\"]\n",
    ")\n",
    "\n",
    "df_test_3 = drop_features(df_test_2, final_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 15:31:05,613 - INFO(validmind.client): Pandas dataset detected. Initializing VM Dataset instance...\n",
      "INFO: Pandas dataset detected. Initializing VM Dataset instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with breaks_adj: {'int_rate': [5, 10, 15]}\n",
      "Performing binning with breaks_adj: {'int_rate': [5, 10, 15]}\n",
      "[INFO] creating woe binning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning: There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "  warnings.warn('There are blank strings in {} columns, which are replaced with NaN. \\n (ColumnNames: {})'.format(\n"
     ]
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.WOEBinTable import WOEBinTable\n",
    "\n",
    "params = {\n",
    "    \"breaks_adj\": {\n",
    "        \"int_rate\": [5,10,15]}  \n",
    "     }\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train_3, target_column=default_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = WOEBinTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_dic = metric.result.metric.value['woe_iv']\n",
    "woe_df = pd.DataFrame(woe_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'convert_to_woe'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 13 features to WoE values.\n",
      "[INFO] converting into woe values ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning: There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "  warnings.warn('There are blank strings in {} columns, which are replaced with NaN. \\n (ColumnNames: {})'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted features to WoE values.\n",
      "Converting 13 features to WoE values.\n",
      "[INFO] converting into woe values ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvalidmind/Library/Caches/pypoetry/virtualenvs/validmind-X_uvMH0R-py3.10/lib/python3.10/site-packages/scorecardpy/condition_fun.py:79: UserWarning: There are blank strings in 1 columns, which are replaced with NaN. \n",
      " (ColumnNames: emp_length)\n",
      "  warnings.warn('There are blank strings in {} columns, which are replaced with NaN. \\n (ColumnNames: {})'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted features to WoE values.\n"
     ]
    }
   ],
   "source": [
    "df_train_4 = developer.execute_task(\n",
    "    area_id=\"feature_engineering\",\n",
    "    task_id=\"convert_to_woe\", \n",
    "    inputs=[df_train_3, woe_df, default_column],\n",
    ")     \n",
    "\n",
    "df_test_4 = convert_to_woe(df_test_3, woe_df, default_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'add_constant'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added constant to dataframe. Number of columns went from 14 to 15.\n",
      "Added constant to dataframe. Number of columns went from 14 to 15.\n"
     ]
    }
   ],
   "source": [
    "df_train_5 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"add_constant\", \n",
    "    inputs=[df_train_4]\n",
    ")\n",
    "\n",
    "df_test_5 = add_constant(df_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'train_model'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with 14 features and 137723 data points.\n",
      "Model trained successfully.\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:               137723\n",
      "Model:                            GLM   Df Residuals:                   137709\n",
      "Model Family:                Binomial   Df Model:                           13\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -59975.\n",
      "Date:                Thu, 10 Aug 2023   Deviance:                   1.1995e+05\n",
      "Time:                        15:31:17   Pearson chi2:                 1.38e+05\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):            0.06635\n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "const                      -1.5257      0.007   -204.231      0.000      -1.540      -1.511\n",
      "sub_grade_woe               0.3735      0.051      7.386      0.000       0.274       0.473\n",
      "home_ownership_woe          0.6607      0.066      9.999      0.000       0.531       0.790\n",
      "purpose_woe                 0.2789      0.090      3.095      0.002       0.102       0.455\n",
      "emp_length_woe              0.7761      0.060     12.916      0.000       0.658       0.894\n",
      "term_woe                    0.5146      0.022     23.195      0.000       0.471       0.558\n",
      "grade_woe                   0.2865      0.049      5.878      0.000       0.191       0.382\n",
      "Unnamed: 0_woe              0.7117      0.045     15.917      0.000       0.624       0.799\n",
      "int_rate_woe                0.1799      0.028      6.455      0.000       0.125       0.234\n",
      "open_acc_woe                2.5244      0.220     11.458      0.000       2.093       2.956\n",
      "annual_inc_woe              1.1000      0.033     33.221      0.000       1.035       1.165\n",
      "total_acc_woe               0.9871      0.128      7.713      0.000       0.736       1.238\n",
      "verification_status_woe     0.2869      0.044      6.584      0.000       0.201       0.372\n",
      "installment_woe             0.7332      0.073     10.022      0.000       0.590       0.877\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_fit_1 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"train_model\", \n",
    "    inputs=[df_train_5, default_column]\n",
    ")\n",
    "\n",
    "print(model_fit_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'drop_features'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 columns.\n",
      "Columns remaining after dropping: 15\n",
      "Dropped 0 columns.\n",
      "Columns remaining after dropping: 15\n"
     ]
    }
   ],
   "source": [
    "model_features_to_drop = []\n",
    "\n",
    "df_train_6 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_train_5, model_features_to_drop]\n",
    ")\n",
    "\n",
    "df_test_6 = drop_features(df_test_5, model_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Executing task 'train_model'...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with 14 features and 137723 data points.\n",
      "Model trained successfully.\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:               137723\n",
      "Model:                            GLM   Df Residuals:                   137709\n",
      "Model Family:                Binomial   Df Model:                           13\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -59975.\n",
      "Date:                Thu, 10 Aug 2023   Deviance:                   1.1995e+05\n",
      "Time:                        15:31:17   Pearson chi2:                 1.38e+05\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):            0.06635\n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "const                      -1.5257      0.007   -204.231      0.000      -1.540      -1.511\n",
      "sub_grade_woe               0.3735      0.051      7.386      0.000       0.274       0.473\n",
      "home_ownership_woe          0.6607      0.066      9.999      0.000       0.531       0.790\n",
      "purpose_woe                 0.2789      0.090      3.095      0.002       0.102       0.455\n",
      "emp_length_woe              0.7761      0.060     12.916      0.000       0.658       0.894\n",
      "term_woe                    0.5146      0.022     23.195      0.000       0.471       0.558\n",
      "grade_woe                   0.2865      0.049      5.878      0.000       0.191       0.382\n",
      "Unnamed: 0_woe              0.7117      0.045     15.917      0.000       0.624       0.799\n",
      "int_rate_woe                0.1799      0.028      6.455      0.000       0.125       0.234\n",
      "open_acc_woe                2.5244      0.220     11.458      0.000       2.093       2.956\n",
      "annual_inc_woe              1.1000      0.033     33.221      0.000       1.035       1.165\n",
      "total_acc_woe               0.9871      0.128      7.713      0.000       0.736       1.238\n",
      "verification_status_woe     0.2869      0.044      6.584      0.000       0.201       0.372\n",
      "installment_woe             0.7332      0.073     10.022      0.000       0.590       0.877\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_fit_2 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"train_model\", \n",
    "    inputs=[df_train_6, default_column],\n",
    "    validation_tests = [\"regression_coeffs_plot\", \n",
    "                        \"regression_models_coeffs\", \n",
    "                        \"log_regression_confusion_matrix\", \n",
    "                        \"regression_roc_curve\", \"gini_table\", \n",
    "                        \"logistic_reg_prediction_histogram\", \n",
    "                        \"logistic_reg_cumulative_prob\", \n",
    "                        \"scorecard_histogram\"]\n",
    ")\n",
    "\n",
    "print(model_fit_2.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area ID</th>\n",
       "      <th>Task ID</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "      <th>Validation Tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_description</td>\n",
       "      <td>import_raw_data</td>\n",
       "      <td>lending_club_url</td>\n",
       "      <td>df_1</td>\n",
       "      <td>descriptive_statistics<br>missing_values_bar_plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>drop_features</td>\n",
       "      <td>df_1, preliminary_features_to_drop</td>\n",
       "      <td>df_2</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>add_default_definition</td>\n",
       "      <td>df_2, default_column</td>\n",
       "      <td>df_3</td>\n",
       "      <td>missing_values_bar_plot<br>class_imbalance<br>iqr_outliers_table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>remove_features_missing_values</td>\n",
       "      <td>df_3, min_missing_percentage</td>\n",
       "      <td>df_4</td>\n",
       "      <td>missing_values_bar_plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>convert_term_column</td>\n",
       "      <td>df_4</td>\n",
       "      <td>df_5</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>convert_emp_length_column</td>\n",
       "      <td>df_5</td>\n",
       "      <td>df_6</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data_preparation</td>\n",
       "      <td>convert_inq_last_6mths_column</td>\n",
       "      <td>df_6</td>\n",
       "      <td>df_7</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data_sampling</td>\n",
       "      <td>data_split</td>\n",
       "      <td>df_7, default_column</td>\n",
       "      <td>df_train_1,df_test_1</td>\n",
       "      <td>tabular_numerical_histograms<br>high_cardinality<br>tabular_categorical_bar_plots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>exploratory_data_analysis</td>\n",
       "      <td>drop_categories</td>\n",
       "      <td>df_train_1</td>\n",
       "      <td>df_train_2</td>\n",
       "      <td>target_rate_bar_plots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>exploratory_data_analysis</td>\n",
       "      <td>drop_features</td>\n",
       "      <td>df_train_2, final_features_to_drop</td>\n",
       "      <td>df_train_3</td>\n",
       "      <td>chi_squared_features_table<br>anova_one_way_table<br>pearson_correlation_matrix<br>feature_target_correlation_plot<br>woe_bin_table<br>woe_bin_table<br>woe_bin_plots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>feature_engineering</td>\n",
       "      <td>convert_to_woe</td>\n",
       "      <td>df_train_3, woe_df, default_column</td>\n",
       "      <td>df_train_4</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model_training</td>\n",
       "      <td>add_constant</td>\n",
       "      <td>df_train_4</td>\n",
       "      <td>df_train_5</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model_training</td>\n",
       "      <td>train_model</td>\n",
       "      <td>df_train_5, default_column</td>\n",
       "      <td>model_fit_1</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model_training</td>\n",
       "      <td>drop_features</td>\n",
       "      <td>df_train_5, model_features_to_drop</td>\n",
       "      <td>df_train_6</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model_training</td>\n",
       "      <td>train_model</td>\n",
       "      <td>df_train_6, default_column</td>\n",
       "      <td>model_fit_2</td>\n",
       "      <td>regression_coeffs_plot<br>regression_models_coeffs<br>log_regression_confusion_matrix<br>regression_roc_curve<br>gini_table<br>logistic_reg_prediction_histogram<br>logistic_reg_cumulative_prob<br>scorecard_histogram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_validation = developer.show_validation_plan()\n",
    "display(HTML(df_validation.to_html(escape=False)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Saved 10 objects to datasets/scorecard_data_and_models.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['df_1',\n",
       " 'df_3',\n",
       " 'df_4',\n",
       " 'df_train_1',\n",
       " 'df_train_2',\n",
       " 'df_train_3',\n",
       " 'df_train_6',\n",
       " 'df_test_6',\n",
       " 'model_fit_2',\n",
       " 'df_validation']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_to_store = {\n",
    "    \"df_1\": df_1,\n",
    "    \"df_3\": df_3,\n",
    "    \"df_4\": df_4,\n",
    "    \"df_train_1\": df_train_1,\n",
    "    \"df_train_2\": df_train_2,\n",
    "    \"df_train_3\": df_train_3,\n",
    "    \"df_train_6\": df_train_6,\n",
    "    \"df_test_6\": df_test_6,\n",
    "    \"model_fit_2\": model_fit_2,\n",
    "    \"df_validation\": df_validation,\n",
    "}\n",
    "\n",
    "developer.save_objects_to_pickle(\n",
    "    filename=\"datasets/scorecard_data_and_models.pkl\", \n",
    "    objects_to_save=objects_to_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
