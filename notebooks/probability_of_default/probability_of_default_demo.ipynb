{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorecard Calibration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point-in-Time (PiT) calibration is a method used to adjust the predicted probabilities of a model so that they align with the actual observed default rates. If the observed default rate at the time of calibration is the same as it was when the scorecard was built, then the calibration process would not lead to any changes in the predicted probabilities, and hence, no improvement in the model's performance as measured by the confusion matrix or other metrics.\n",
    "\n",
    "The purpose of calibration is to ensure that the model's predicted probabilities accurately reflect the actual likelihood of the event (in this case, default). If the underlying default rate hasn't changed since the model was built, then the model's predictions are already well-calibrated, and further calibration isn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scorecard(model, X, target_score, target_odds, pdo):\n",
    "    # Compute probability of default\n",
    "    X_copy = X.copy()\n",
    "    X_copy[\"probabilities\"] = model.predict(X_copy)\n",
    "\n",
    "    # Compute scores\n",
    "    factor = pdo / np.log(2)\n",
    "    offset = target_score - (factor * np.log(target_odds))\n",
    "\n",
    "    X_copy[\"score\"] = offset + factor * np.log(\n",
    "        X_copy[\"probabilities\"] / (1 - X_copy[\"probabilities\"])\n",
    "    )\n",
    "    return X_copy\n",
    "\n",
    "target_score = 600\n",
    "target_odds = 50\n",
    "pdo = 20\n",
    "X_train_scores = create_scorecard(model_fit_glm, X_train, target_score, target_odds, pdo)\n",
    "X_train_scores = X_train_scores['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "X_train_scores = sm.add_constant(X_train_scores)\n",
    "calibrated_model = sm.GLM(y_train, X_train_scores, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "calibrated_model_fit_glm = calibrated_model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(calibrated_model_fit_glm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df_calibrated_train and df_calibrated_test\n",
    "df_calibrated_train = pd.concat([X_train_scores, y_train], axis=1)\n",
    "\n",
    "# Update test data \n",
    "X_test_scores = create_scorecard(model_fit_glm, X_test, target_score, target_odds, pdo)\n",
    "X_test_scores = X_test_scores['score']\n",
    "X_test_scores = sm.add_constant(X_test_scores)\n",
    "df_calibrated_test = pd.concat([X_test_scores, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VM dataset\n",
    "vm_train_calibrated_ds = vm.init_dataset(dataset=df_calibrated_train,\n",
    "                        target_column=target_column)\n",
    "vm_test_calibrated_ds = vm.init_dataset(dataset=df_calibrated_test,\n",
    "                        target_column=target_column)\n",
    "\n",
    "# Create VM model\n",
    "vm_model_glm_calibrated = vm.init_model(\n",
    "    model = calibrated_model_fit_glm, \n",
    "    train_ds=vm_train_calibrated_ds, \n",
    "    test_ds=vm_test_calibrated_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogRegressionConfusionMatrix import LogRegressionConfusionMatrix\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm_calibrated)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"cut_off_threshold\": 0.5,\n",
    "}\n",
    "\n",
    "metric = LogRegressionConfusionMatrix(test_context, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Default by Rating Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.PDRatingClassPlot import PDRatingClassPlot\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"rating_classes\": ['A','B','C','D'],\n",
    "    \"title\": \"PD by Rating Class\",\n",
    "}\n",
    "test_context = TestContext(model= vm_model_glm_calibrated)\n",
    "metric = PDRatingClassPlot(test_context, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
