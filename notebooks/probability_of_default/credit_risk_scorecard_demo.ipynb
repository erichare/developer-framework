{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Scorecard Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to ValidMind Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 18:03:57,574 - INFO - api_client - Connected to ValidMind. Project: [3] PD Model - Initial Validation (cliwzqjgv00001fy6869rlav9)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of Tests used in this Demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_653c4 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_653c4_row0_col0, #T_653c4_row0_col1, #T_653c4_row0_col2, #T_653c4_row0_col3, #T_653c4_row1_col0, #T_653c4_row1_col1, #T_653c4_row1_col2, #T_653c4_row1_col3, #T_653c4_row2_col0, #T_653c4_row2_col1, #T_653c4_row2_col2, #T_653c4_row2_col3, #T_653c4_row3_col0, #T_653c4_row3_col1, #T_653c4_row3_col2, #T_653c4_row3_col3, #T_653c4_row4_col0, #T_653c4_row4_col1, #T_653c4_row4_col2, #T_653c4_row4_col3, #T_653c4_row5_col0, #T_653c4_row5_col1, #T_653c4_row5_col2, #T_653c4_row5_col3, #T_653c4_row6_col0, #T_653c4_row6_col1, #T_653c4_row6_col2, #T_653c4_row6_col3, #T_653c4_row7_col0, #T_653c4_row7_col1, #T_653c4_row7_col2, #T_653c4_row7_col3, #T_653c4_row8_col0, #T_653c4_row8_col1, #T_653c4_row8_col2, #T_653c4_row8_col3, #T_653c4_row9_col0, #T_653c4_row9_col1, #T_653c4_row9_col2, #T_653c4_row9_col3, #T_653c4_row10_col0, #T_653c4_row10_col1, #T_653c4_row10_col2, #T_653c4_row10_col3, #T_653c4_row11_col0, #T_653c4_row11_col1, #T_653c4_row11_col2, #T_653c4_row11_col3, #T_653c4_row12_col0, #T_653c4_row12_col1, #T_653c4_row12_col2, #T_653c4_row12_col3, #T_653c4_row13_col0, #T_653c4_row13_col1, #T_653c4_row13_col2, #T_653c4_row13_col3, #T_653c4_row14_col0, #T_653c4_row14_col1, #T_653c4_row14_col2, #T_653c4_row14_col3, #T_653c4_row15_col0, #T_653c4_row15_col1, #T_653c4_row15_col2, #T_653c4_row15_col3, #T_653c4_row16_col0, #T_653c4_row16_col1, #T_653c4_row16_col2, #T_653c4_row16_col3, #T_653c4_row17_col0, #T_653c4_row17_col1, #T_653c4_row17_col2, #T_653c4_row17_col3, #T_653c4_row18_col0, #T_653c4_row18_col1, #T_653c4_row18_col2, #T_653c4_row18_col3, #T_653c4_row19_col0, #T_653c4_row19_col1, #T_653c4_row19_col2, #T_653c4_row19_col3, #T_653c4_row20_col0, #T_653c4_row20_col1, #T_653c4_row20_col2, #T_653c4_row20_col3, #T_653c4_row21_col0, #T_653c4_row21_col1, #T_653c4_row21_col2, #T_653c4_row21_col3, #T_653c4_row22_col0, #T_653c4_row22_col1, #T_653c4_row22_col2, #T_653c4_row22_col3, #T_653c4_row23_col0, #T_653c4_row23_col1, #T_653c4_row23_col2, #T_653c4_row23_col3, #T_653c4_row24_col0, #T_653c4_row24_col1, #T_653c4_row24_col2, #T_653c4_row24_col3, #T_653c4_row25_col0, #T_653c4_row25_col1, #T_653c4_row25_col2, #T_653c4_row25_col3, #T_653c4_row26_col0, #T_653c4_row26_col1, #T_653c4_row26_col2, #T_653c4_row26_col3, #T_653c4_row27_col0, #T_653c4_row27_col1, #T_653c4_row27_col2, #T_653c4_row27_col3, #T_653c4_row28_col0, #T_653c4_row28_col1, #T_653c4_row28_col2, #T_653c4_row28_col3, #T_653c4_row29_col0, #T_653c4_row29_col1, #T_653c4_row29_col2, #T_653c4_row29_col3, #T_653c4_row30_col0, #T_653c4_row30_col1, #T_653c4_row30_col2, #T_653c4_row30_col3, #T_653c4_row31_col0, #T_653c4_row31_col1, #T_653c4_row31_col2, #T_653c4_row31_col3, #T_653c4_row32_col0, #T_653c4_row32_col1, #T_653c4_row32_col2, #T_653c4_row32_col3, #T_653c4_row33_col0, #T_653c4_row33_col1, #T_653c4_row33_col2, #T_653c4_row33_col3, #T_653c4_row34_col0, #T_653c4_row34_col1, #T_653c4_row34_col2, #T_653c4_row34_col3, #T_653c4_row35_col0, #T_653c4_row35_col1, #T_653c4_row35_col2, #T_653c4_row35_col3, #T_653c4_row36_col0, #T_653c4_row36_col1, #T_653c4_row36_col2, #T_653c4_row36_col3, #T_653c4_row37_col0, #T_653c4_row37_col1, #T_653c4_row37_col2, #T_653c4_row37_col3, #T_653c4_row38_col0, #T_653c4_row38_col1, #T_653c4_row38_col2, #T_653c4_row38_col3, #T_653c4_row39_col0, #T_653c4_row39_col1, #T_653c4_row39_col2, #T_653c4_row39_col3, #T_653c4_row40_col0, #T_653c4_row40_col1, #T_653c4_row40_col2, #T_653c4_row40_col3, #T_653c4_row41_col0, #T_653c4_row41_col1, #T_653c4_row41_col2, #T_653c4_row41_col3, #T_653c4_row42_col0, #T_653c4_row42_col1, #T_653c4_row42_col2, #T_653c4_row42_col3, #T_653c4_row43_col0, #T_653c4_row43_col1, #T_653c4_row43_col2, #T_653c4_row43_col3, #T_653c4_row44_col0, #T_653c4_row44_col1, #T_653c4_row44_col2, #T_653c4_row44_col3, #T_653c4_row45_col0, #T_653c4_row45_col1, #T_653c4_row45_col2, #T_653c4_row45_col3, #T_653c4_row46_col0, #T_653c4_row46_col1, #T_653c4_row46_col2, #T_653c4_row46_col3, #T_653c4_row47_col0, #T_653c4_row47_col1, #T_653c4_row47_col2, #T_653c4_row47_col3, #T_653c4_row48_col0, #T_653c4_row48_col1, #T_653c4_row48_col2, #T_653c4_row48_col3, #T_653c4_row49_col0, #T_653c4_row49_col1, #T_653c4_row49_col2, #T_653c4_row49_col3, #T_653c4_row50_col0, #T_653c4_row50_col1, #T_653c4_row50_col2, #T_653c4_row50_col3, #T_653c4_row51_col0, #T_653c4_row51_col1, #T_653c4_row51_col2, #T_653c4_row51_col3, #T_653c4_row52_col0, #T_653c4_row52_col1, #T_653c4_row52_col2, #T_653c4_row52_col3, #T_653c4_row53_col0, #T_653c4_row53_col1, #T_653c4_row53_col2, #T_653c4_row53_col3, #T_653c4_row54_col0, #T_653c4_row54_col1, #T_653c4_row54_col2, #T_653c4_row54_col3, #T_653c4_row55_col0, #T_653c4_row55_col1, #T_653c4_row55_col2, #T_653c4_row55_col3, #T_653c4_row56_col0, #T_653c4_row56_col1, #T_653c4_row56_col2, #T_653c4_row56_col3, #T_653c4_row57_col0, #T_653c4_row57_col1, #T_653c4_row57_col2, #T_653c4_row57_col3, #T_653c4_row58_col0, #T_653c4_row58_col1, #T_653c4_row58_col2, #T_653c4_row58_col3, #T_653c4_row59_col0, #T_653c4_row59_col1, #T_653c4_row59_col2, #T_653c4_row59_col3, #T_653c4_row60_col0, #T_653c4_row60_col1, #T_653c4_row60_col2, #T_653c4_row60_col3, #T_653c4_row61_col0, #T_653c4_row61_col1, #T_653c4_row61_col2, #T_653c4_row61_col3, #T_653c4_row62_col0, #T_653c4_row62_col1, #T_653c4_row62_col2, #T_653c4_row62_col3, #T_653c4_row63_col0, #T_653c4_row63_col1, #T_653c4_row63_col2, #T_653c4_row63_col3, #T_653c4_row64_col0, #T_653c4_row64_col1, #T_653c4_row64_col2, #T_653c4_row64_col3, #T_653c4_row65_col0, #T_653c4_row65_col1, #T_653c4_row65_col2, #T_653c4_row65_col3, #T_653c4_row66_col0, #T_653c4_row66_col1, #T_653c4_row66_col2, #T_653c4_row66_col3, #T_653c4_row67_col0, #T_653c4_row67_col1, #T_653c4_row67_col2, #T_653c4_row67_col3, #T_653c4_row68_col0, #T_653c4_row68_col1, #T_653c4_row68_col2, #T_653c4_row68_col3, #T_653c4_row69_col0, #T_653c4_row69_col1, #T_653c4_row69_col2, #T_653c4_row69_col3, #T_653c4_row70_col0, #T_653c4_row70_col1, #T_653c4_row70_col2, #T_653c4_row70_col3, #T_653c4_row71_col0, #T_653c4_row71_col1, #T_653c4_row71_col2, #T_653c4_row71_col3, #T_653c4_row72_col0, #T_653c4_row72_col1, #T_653c4_row72_col2, #T_653c4_row72_col3, #T_653c4_row73_col0, #T_653c4_row73_col1, #T_653c4_row73_col2, #T_653c4_row73_col3, #T_653c4_row74_col0, #T_653c4_row74_col1, #T_653c4_row74_col2, #T_653c4_row74_col3, #T_653c4_row75_col0, #T_653c4_row75_col1, #T_653c4_row75_col2, #T_653c4_row75_col3, #T_653c4_row76_col0, #T_653c4_row76_col1, #T_653c4_row76_col2, #T_653c4_row76_col3, #T_653c4_row77_col0, #T_653c4_row77_col1, #T_653c4_row77_col2, #T_653c4_row77_col3, #T_653c4_row78_col0, #T_653c4_row78_col1, #T_653c4_row78_col2, #T_653c4_row78_col3, #T_653c4_row79_col0, #T_653c4_row79_col1, #T_653c4_row79_col2, #T_653c4_row79_col3, #T_653c4_row80_col0, #T_653c4_row80_col1, #T_653c4_row80_col2, #T_653c4_row80_col3, #T_653c4_row81_col0, #T_653c4_row81_col1, #T_653c4_row81_col2, #T_653c4_row81_col3, #T_653c4_row82_col0, #T_653c4_row82_col1, #T_653c4_row82_col2, #T_653c4_row82_col3, #T_653c4_row83_col0, #T_653c4_row83_col1, #T_653c4_row83_col2, #T_653c4_row83_col3, #T_653c4_row84_col0, #T_653c4_row84_col1, #T_653c4_row84_col2, #T_653c4_row84_col3, #T_653c4_row85_col0, #T_653c4_row85_col1, #T_653c4_row85_col2, #T_653c4_row85_col3, #T_653c4_row86_col0, #T_653c4_row86_col1, #T_653c4_row86_col2, #T_653c4_row86_col3, #T_653c4_row87_col0, #T_653c4_row87_col1, #T_653c4_row87_col2, #T_653c4_row87_col3, #T_653c4_row88_col0, #T_653c4_row88_col1, #T_653c4_row88_col2, #T_653c4_row88_col3, #T_653c4_row89_col0, #T_653c4_row89_col1, #T_653c4_row89_col2, #T_653c4_row89_col3, #T_653c4_row90_col0, #T_653c4_row90_col1, #T_653c4_row90_col2, #T_653c4_row90_col3, #T_653c4_row91_col0, #T_653c4_row91_col1, #T_653c4_row91_col2, #T_653c4_row91_col3, #T_653c4_row92_col0, #T_653c4_row92_col1, #T_653c4_row92_col2, #T_653c4_row92_col3, #T_653c4_row93_col0, #T_653c4_row93_col1, #T_653c4_row93_col2, #T_653c4_row93_col3, #T_653c4_row94_col0, #T_653c4_row94_col1, #T_653c4_row94_col2, #T_653c4_row94_col3, #T_653c4_row95_col0, #T_653c4_row95_col1, #T_653c4_row95_col2, #T_653c4_row95_col3, #T_653c4_row96_col0, #T_653c4_row96_col1, #T_653c4_row96_col2, #T_653c4_row96_col3, #T_653c4_row97_col0, #T_653c4_row97_col1, #T_653c4_row97_col2, #T_653c4_row97_col3, #T_653c4_row98_col0, #T_653c4_row98_col1, #T_653c4_row98_col2, #T_653c4_row98_col3, #T_653c4_row99_col0, #T_653c4_row99_col1, #T_653c4_row99_col2, #T_653c4_row99_col3, #T_653c4_row100_col0, #T_653c4_row100_col1, #T_653c4_row100_col2, #T_653c4_row100_col3, #T_653c4_row101_col0, #T_653c4_row101_col1, #T_653c4_row101_col2, #T_653c4_row101_col3, #T_653c4_row102_col0, #T_653c4_row102_col1, #T_653c4_row102_col2, #T_653c4_row102_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_653c4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_653c4_level0_col0\" class=\"col_heading level0 col0\" >Test Type</th>\n",
       "      <th id=\"T_653c4_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_653c4_level0_col2\" class=\"col_heading level0 col2\" >Description</th>\n",
       "      <th id=\"T_653c4_level0_col3\" class=\"col_heading level0 col3\" >ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row0_col0\" class=\"data row0 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row0_col1\" class=\"data row0 col1\" >ModelMetadata</td>\n",
       "      <td id=\"T_653c4_row0_col2\" class=\"data row0 col2\" >Custom class to collect the following metadata for a model:\n",
       "    - Model architecture\n",
       "    - Model hyperparameters\n",
       "    - Model task type</td>\n",
       "      <td id=\"T_653c4_row0_col3\" class=\"data row0 col3\" >validmind.model_validation.ModelMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row1_col0\" class=\"data row1 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row1_col1\" class=\"data row1 col1\" >ClassifierOutOfSamplePerformance</td>\n",
       "      <td id=\"T_653c4_row1_col2\" class=\"data row1 col2\" >Test that outputs the performance of the model on the test data.</td>\n",
       "      <td id=\"T_653c4_row1_col3\" class=\"data row1 col3\" >validmind.model_validation.sklearn.ClassifierOutOfSamplePerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row2_col0\" class=\"data row2 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row2_col1\" class=\"data row2 col1\" >RobustnessDiagnosis</td>\n",
       "      <td id=\"T_653c4_row2_col2\" class=\"data row2 col2\" >Test robustness of model by perturbing the features column values by adding noise within scale\n",
       "    stardard deviation.</td>\n",
       "      <td id=\"T_653c4_row2_col3\" class=\"data row2 col3\" >validmind.model_validation.sklearn.RobustnessDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row3_col0\" class=\"data row3 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row3_col1\" class=\"data row3 col1\" >SHAPGlobalImportance</td>\n",
       "      <td id=\"T_653c4_row3_col2\" class=\"data row3 col2\" >SHAP Global Importance</td>\n",
       "      <td id=\"T_653c4_row3_col3\" class=\"data row3 col3\" >validmind.model_validation.sklearn.SHAPGlobalImportance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row4_col0\" class=\"data row4 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row4_col1\" class=\"data row4 col1\" >ConfusionMatrix</td>\n",
       "      <td id=\"T_653c4_row4_col2\" class=\"data row4 col2\" >Confusion Matrix</td>\n",
       "      <td id=\"T_653c4_row4_col3\" class=\"data row4 col3\" >validmind.model_validation.sklearn.ConfusionMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row5_col0\" class=\"data row5 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row5_col1\" class=\"data row5 col1\" >ClassifierInSamplePerformance</td>\n",
       "      <td id=\"T_653c4_row5_col2\" class=\"data row5 col2\" >Test that outputs the performance of the model on the training data.</td>\n",
       "      <td id=\"T_653c4_row5_col3\" class=\"data row5 col3\" >validmind.model_validation.sklearn.ClassifierInSamplePerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row6_col0\" class=\"data row6 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row6_col1\" class=\"data row6 col1\" >OverfitDiagnosis</td>\n",
       "      <td id=\"T_653c4_row6_col2\" class=\"data row6 col2\" >Test that identify overfit regions with high residuals by histogram slicing techniques.</td>\n",
       "      <td id=\"T_653c4_row6_col3\" class=\"data row6 col3\" >validmind.model_validation.sklearn.OverfitDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row7_col0\" class=\"data row7 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row7_col1\" class=\"data row7 col1\" >PermutationFeatureImportance</td>\n",
       "      <td id=\"T_653c4_row7_col2\" class=\"data row7 col2\" >The Feature Importance plot below calculates a score representing the\n",
       "    importance of each feature in the model. A higher score indicates\n",
       "    that the specific input feature will have a larger effect on the\n",
       "    predictive power of the model.\n",
       "\n",
       "    The importance score is calculated using Permutation Feature\n",
       "    Importance. Permutation feature importance measures the decrease of\n",
       "    model performance after the feature's values have been permuted, which\n",
       "    breaks the relationship between the feature and the true outcome. A\n",
       "    feature is \"important\" if shuffling its values increases the model\n",
       "    error, because in this case the model relied on the feature for the\n",
       "    prediction. A feature is \"unimportant\" if shuffling its values leaves\n",
       "    the model error unchanged, because in this case the model ignored the\n",
       "    feature for the prediction.</td>\n",
       "      <td id=\"T_653c4_row7_col3\" class=\"data row7 col3\" >validmind.model_validation.sklearn.PermutationFeatureImportance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row8_col0\" class=\"data row8 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row8_col1\" class=\"data row8 col1\" >MinimumROCAUCScore</td>\n",
       "      <td id=\"T_653c4_row8_col2\" class=\"data row8 col2\" >Test that the model's ROC AUC score on the validation dataset meets or\n",
       "    exceeds a predefined threshold.</td>\n",
       "      <td id=\"T_653c4_row8_col3\" class=\"data row8 col3\" >validmind.model_validation.sklearn.MinimumROCAUCScore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row9_col0\" class=\"data row9 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row9_col1\" class=\"data row9 col1\" >PrecisionRecallCurve</td>\n",
       "      <td id=\"T_653c4_row9_col2\" class=\"data row9 col2\" >Precision Recall Curve</td>\n",
       "      <td id=\"T_653c4_row9_col3\" class=\"data row9 col3\" >validmind.model_validation.sklearn.PrecisionRecallCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row10_col0\" class=\"data row10 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row10_col1\" class=\"data row10 col1\" >ClassifierPerformance</td>\n",
       "      <td id=\"T_653c4_row10_col2\" class=\"data row10 col2\" >Test that outputs the performance of the model on the training or test data.</td>\n",
       "      <td id=\"T_653c4_row10_col3\" class=\"data row10 col3\" >validmind.model_validation.sklearn.ClassifierPerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row11_col0\" class=\"data row11 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row11_col1\" class=\"data row11 col1\" >MinimumF1Score</td>\n",
       "      <td id=\"T_653c4_row11_col2\" class=\"data row11 col2\" >Test that the model's F1 score on the validation dataset meets or\n",
       "    exceeds a predefined threshold.</td>\n",
       "      <td id=\"T_653c4_row11_col3\" class=\"data row11 col3\" >validmind.model_validation.sklearn.MinimumF1Score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row12_col0\" class=\"data row12 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row12_col1\" class=\"data row12 col1\" >ROCCurve</td>\n",
       "      <td id=\"T_653c4_row12_col2\" class=\"data row12 col2\" >ROC Curve</td>\n",
       "      <td id=\"T_653c4_row12_col3\" class=\"data row12 col3\" >validmind.model_validation.sklearn.ROCCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row13_col0\" class=\"data row13 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row13_col1\" class=\"data row13 col1\" >TrainingTestDegradation</td>\n",
       "      <td id=\"T_653c4_row13_col2\" class=\"data row13 col2\" >Test that the degradation in performance between the training and test datasets\n",
       "    does not exceed a predefined threshold.</td>\n",
       "      <td id=\"T_653c4_row13_col3\" class=\"data row13 col3\" >validmind.model_validation.sklearn.TrainingTestDegradation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row14_col0\" class=\"data row14 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row14_col1\" class=\"data row14 col1\" >WeakspotsDiagnosis</td>\n",
       "      <td id=\"T_653c4_row14_col2\" class=\"data row14 col2\" >Test that identify weak regions with high residuals by histogram slicing techniques.</td>\n",
       "      <td id=\"T_653c4_row14_col3\" class=\"data row14 col3\" >validmind.model_validation.sklearn.WeakspotsDiagnosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row15_col0\" class=\"data row15 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row15_col1\" class=\"data row15 col1\" >PopulationStabilityIndex</td>\n",
       "      <td id=\"T_653c4_row15_col2\" class=\"data row15 col2\" >Population Stability Index between two datasets</td>\n",
       "      <td id=\"T_653c4_row15_col3\" class=\"data row15 col3\" >validmind.model_validation.sklearn.PopulationStabilityIndex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row16_col0\" class=\"data row16 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row16_col1\" class=\"data row16 col1\" >MinimumAccuracy</td>\n",
       "      <td id=\"T_653c4_row16_col2\" class=\"data row16 col2\" >Test that the model's prediction accuracy on a dataset meets or\n",
       "    exceeds a predefined threshold.</td>\n",
       "      <td id=\"T_653c4_row16_col3\" class=\"data row16 col3\" >validmind.model_validation.sklearn.MinimumAccuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row17_col0\" class=\"data row17 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row17_col1\" class=\"data row17 col1\" >RegressionModelsCoeffs</td>\n",
       "      <td id=\"T_653c4_row17_col2\" class=\"data row17 col2\" >Test that outputs the coefficients of stats library regression models.</td>\n",
       "      <td id=\"T_653c4_row17_col3\" class=\"data row17 col3\" >validmind.model_validation.statsmodels.RegressionModelsCoeffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row18_col0\" class=\"data row18 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row18_col1\" class=\"data row18 col1\" >BoxPierce</td>\n",
       "      <td id=\"T_653c4_row18_col2\" class=\"data row18 col2\" >The Box-Pierce test is a statistical test used to determine\n",
       "    whether a given set of data has autocorrelations\n",
       "    that are different from zero.</td>\n",
       "      <td id=\"T_653c4_row18_col3\" class=\"data row18 col3\" >validmind.model_validation.statsmodels.BoxPierce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row19_col0\" class=\"data row19 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row19_col1\" class=\"data row19 col1\" >RegressionModelSensitivityPlot</td>\n",
       "      <td id=\"T_653c4_row19_col2\" class=\"data row19 col2\" >This metric performs sensitivity analysis applying shocks to one variable at a time.</td>\n",
       "      <td id=\"T_653c4_row19_col3\" class=\"data row19 col3\" >validmind.model_validation.statsmodels.RegressionModelSensitivityPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row20_col0\" class=\"data row20 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row20_col1\" class=\"data row20 col1\" >RegressionModelsPerformance</td>\n",
       "      <td id=\"T_653c4_row20_col2\" class=\"data row20 col2\" >Test that outputs the comparison of stats library regression models.</td>\n",
       "      <td id=\"T_653c4_row20_col3\" class=\"data row20 col3\" >validmind.model_validation.statsmodels.RegressionModelsPerformance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row21_col0\" class=\"data row21 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row21_col1\" class=\"data row21 col1\" >ZivotAndrewsArch</td>\n",
       "      <td id=\"T_653c4_row21_col2\" class=\"data row21 col2\" >Zivot-Andrews unit root test for\n",
       "    establishing the order of integration of time series</td>\n",
       "      <td id=\"T_653c4_row21_col3\" class=\"data row21 col3\" >validmind.model_validation.statsmodels.ZivotAndrewsArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row22_col0\" class=\"data row22 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row22_col1\" class=\"data row22 col1\" >RegressionModelOutsampleComparison</td>\n",
       "      <td id=\"T_653c4_row22_col2\" class=\"data row22 col2\" >Test that evaluates the performance of different regression models on a separate test dataset\n",
       "    that was not used to train the models.</td>\n",
       "      <td id=\"T_653c4_row22_col3\" class=\"data row22 col3\" >validmind.model_validation.statsmodels.RegressionModelOutsampleComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row23_col0\" class=\"data row23 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row23_col1\" class=\"data row23 col1\" >RegressionModelForecastPlotLevels</td>\n",
       "      <td id=\"T_653c4_row23_col2\" class=\"data row23 col2\" >This metric creates a plot of forecast vs observed for each model in the list.</td>\n",
       "      <td id=\"T_653c4_row23_col3\" class=\"data row23 col3\" >validmind.model_validation.statsmodels.RegressionModelForecastPlotLevels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row24_col0\" class=\"data row24 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row24_col1\" class=\"data row24 col1\" >ScorecardHistogram</td>\n",
       "      <td id=\"T_653c4_row24_col2\" class=\"data row24 col2\" >Score Histogram</td>\n",
       "      <td id=\"T_653c4_row24_col3\" class=\"data row24 col3\" >validmind.model_validation.statsmodels.ScorecardHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row25_col0\" class=\"data row25 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row25_col1\" class=\"data row25 col1\" >FeatureImportanceAndSignificance</td>\n",
       "      <td id=\"T_653c4_row25_col2\" class=\"data row25 col2\" >This metric class computes and visualizes the feature importance and statistical significance\n",
       "    within a model's context. It compares the p-values from a regression model with the feature importances\n",
       "    from a decision tree model. The significance filter can be turned on or off, allowing for flexibility\n",
       "    in feature selection. The p-values and feature importances are normalized for comparison and visualization.</td>\n",
       "      <td id=\"T_653c4_row25_col3\" class=\"data row25 col3\" >validmind.model_validation.statsmodels.FeatureImportanceAndSignificance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row26_col0\" class=\"data row26 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row26_col1\" class=\"data row26 col1\" >LJungBox</td>\n",
       "      <td id=\"T_653c4_row26_col2\" class=\"data row26 col2\" >The Ljung-Box test is a statistical test used to determine\n",
       "    whether a given set of data has autocorrelations\n",
       "    that are different from zero.</td>\n",
       "      <td id=\"T_653c4_row26_col3\" class=\"data row26 col3\" >validmind.model_validation.statsmodels.LJungBox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row27_col0\" class=\"data row27 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row27_col1\" class=\"data row27 col1\" >JarqueBera</td>\n",
       "      <td id=\"T_653c4_row27_col2\" class=\"data row27 col2\" >The Jarque-Bera test is a statistical test used to determine\n",
       "    whether a given set of data follows a normal distribution.</td>\n",
       "      <td id=\"T_653c4_row27_col3\" class=\"data row27 col3\" >validmind.model_validation.statsmodels.JarqueBera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row28_col0\" class=\"data row28 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row28_col1\" class=\"data row28 col1\" >PhillipsPerronArch</td>\n",
       "      <td id=\"T_653c4_row28_col2\" class=\"data row28 col2\" >Phillips-Perron (PP) unit root test for\n",
       "    establishing the order of integration of time series</td>\n",
       "      <td id=\"T_653c4_row28_col3\" class=\"data row28 col3\" >validmind.model_validation.statsmodels.PhillipsPerronArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row29_col0\" class=\"data row29 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row29_col1\" class=\"data row29 col1\" >KolmogorovSmirnov</td>\n",
       "      <td id=\"T_653c4_row29_col2\" class=\"data row29 col2\" >The Kolmogorov-Smirnov metric is a statistical test used to determine\n",
       "    whether a given set of data follows a normal distribution.</td>\n",
       "      <td id=\"T_653c4_row29_col3\" class=\"data row29 col3\" >validmind.model_validation.statsmodels.KolmogorovSmirnov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row30_col0\" class=\"data row30 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row30_col1\" class=\"data row30 col1\" >ResidualsVisualInspection</td>\n",
       "      <td id=\"T_653c4_row30_col2\" class=\"data row30 col2\" >Log plots for visual inspection of residuals</td>\n",
       "      <td id=\"T_653c4_row30_col3\" class=\"data row30 col3\" >validmind.model_validation.statsmodels.ResidualsVisualInspection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row31_col0\" class=\"data row31 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row31_col1\" class=\"data row31 col1\" >ShapiroWilk</td>\n",
       "      <td id=\"T_653c4_row31_col2\" class=\"data row31 col2\" >The Shapiro-Wilk test is a statistical test used to determine\n",
       "    whether a given set of data follows a normal distribution.</td>\n",
       "      <td id=\"T_653c4_row31_col3\" class=\"data row31 col3\" >validmind.model_validation.statsmodels.ShapiroWilk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row32_col0\" class=\"data row32 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row32_col1\" class=\"data row32 col1\" >RegressionModelInsampleComparison</td>\n",
       "      <td id=\"T_653c4_row32_col2\" class=\"data row32 col2\" >Test that output the comparison of stats library regression models.</td>\n",
       "      <td id=\"T_653c4_row32_col3\" class=\"data row32 col3\" >validmind.model_validation.statsmodels.RegressionModelInsampleComparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row33_col0\" class=\"data row33 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row33_col1\" class=\"data row33 col1\" >RegressionFeatureSignificance</td>\n",
       "      <td id=\"T_653c4_row33_col2\" class=\"data row33 col2\" >This metric creates a plot of p-values for each model in the list.</td>\n",
       "      <td id=\"T_653c4_row33_col3\" class=\"data row33 col3\" >validmind.model_validation.statsmodels.RegressionFeatureSignificance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row34_col0\" class=\"data row34 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row34_col1\" class=\"data row34 col1\" >LogRegPredictionHistogram</td>\n",
       "      <td id=\"T_653c4_row34_col2\" class=\"data row34 col2\" >Probability of Default (PD) Histogram</td>\n",
       "      <td id=\"T_653c4_row34_col3\" class=\"data row34 col3\" >validmind.model_validation.statsmodels.LogRegPredictionHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row35_col0\" class=\"data row35 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row35_col1\" class=\"data row35 col1\" >RegressionModelSummary</td>\n",
       "      <td id=\"T_653c4_row35_col2\" class=\"data row35 col2\" >Test that output the summary of regression models of statsmodel library.</td>\n",
       "      <td id=\"T_653c4_row35_col3\" class=\"data row35 col3\" >validmind.model_validation.statsmodels.RegressionModelSummary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row36_col0\" class=\"data row36 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row36_col1\" class=\"data row36 col1\" >LogRegCumProb</td>\n",
       "      <td id=\"T_653c4_row36_col2\" class=\"data row36 col2\" >Cumulative Probability Metric for Logistic Regression Models</td>\n",
       "      <td id=\"T_653c4_row36_col3\" class=\"data row36 col3\" >validmind.model_validation.statsmodels.LogRegCumProb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row37_col0\" class=\"data row37 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row37_col1\" class=\"data row37 col1\" >KPSS</td>\n",
       "      <td id=\"T_653c4_row37_col2\" class=\"data row37 col2\" >Kwiatkowski-Phillips-Schmidt-Shin (KPSS) unit root test for\n",
       "    establishing the order of integration of time series</td>\n",
       "      <td id=\"T_653c4_row37_col3\" class=\"data row37 col3\" >validmind.model_validation.statsmodels.KPSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row38_col0\" class=\"data row38 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row38_col1\" class=\"data row38 col1\" >Lilliefors</td>\n",
       "      <td id=\"T_653c4_row38_col2\" class=\"data row38 col2\" >The Lilliefors test is a statistical test used to determine\n",
       "    whether a given set of data follows a normal distribution.</td>\n",
       "      <td id=\"T_653c4_row38_col3\" class=\"data row38 col3\" >validmind.model_validation.statsmodels.Lilliefors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row39_col0\" class=\"data row39 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row39_col1\" class=\"data row39 col1\" >RunsTest</td>\n",
       "      <td id=\"T_653c4_row39_col2\" class=\"data row39 col2\" >The runs test is a statistical test used to determine whether a given set\n",
       "    of data has runs of positive and negative values that are longer than expected\n",
       "    under the null hypothesis of randomness.</td>\n",
       "      <td id=\"T_653c4_row39_col3\" class=\"data row39 col3\" >validmind.model_validation.statsmodels.RunsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row40_col0\" class=\"data row40 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row40_col1\" class=\"data row40 col1\" >DFGLSArch</td>\n",
       "      <td id=\"T_653c4_row40_col2\" class=\"data row40 col2\" >Dickey-Fuller GLS unit root test for\n",
       "    establishing the order of integration of time series</td>\n",
       "      <td id=\"T_653c4_row40_col3\" class=\"data row40 col3\" >validmind.model_validation.statsmodels.DFGLSArch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row41_col0\" class=\"data row41 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row41_col1\" class=\"data row41 col1\" >AutoARIMA</td>\n",
       "      <td id=\"T_653c4_row41_col2\" class=\"data row41 col2\" >Automatically fits multiple ARIMA models for each variable and ranks them by BIC and AIC.</td>\n",
       "      <td id=\"T_653c4_row41_col3\" class=\"data row41 col3\" >validmind.model_validation.statsmodels.AutoARIMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row42_col0\" class=\"data row42 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row42_col1\" class=\"data row42 col1\" >ADFTest</td>\n",
       "      <td id=\"T_653c4_row42_col2\" class=\"data row42 col2\" >Augmented Dickey-Fuller Metric for establishing the order of integration of\n",
       "    time series</td>\n",
       "      <td id=\"T_653c4_row42_col3\" class=\"data row42 col3\" >validmind.model_validation.statsmodels.ADFTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row43_col0\" class=\"data row43 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row43_col1\" class=\"data row43 col1\" >GINITable</td>\n",
       "      <td id=\"T_653c4_row43_col2\" class=\"data row43 col2\" >Compute and display the AUC, GINI, and KS for train and test sets.</td>\n",
       "      <td id=\"T_653c4_row43_col3\" class=\"data row43 col3\" >validmind.model_validation.statsmodels.GINITable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row44_col0\" class=\"data row44 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row44_col1\" class=\"data row44 col1\" >RegressionModelForecastPlot</td>\n",
       "      <td id=\"T_653c4_row44_col2\" class=\"data row44 col2\" >This metric creates a plot of forecast vs observed for each model in the list.</td>\n",
       "      <td id=\"T_653c4_row44_col3\" class=\"data row44 col3\" >validmind.model_validation.statsmodels.RegressionModelForecastPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row45_col0\" class=\"data row45 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row45_col1\" class=\"data row45 col1\" >RegressionROCCurve</td>\n",
       "      <td id=\"T_653c4_row45_col2\" class=\"data row45 col2\" >Regression ROC Curve</td>\n",
       "      <td id=\"T_653c4_row45_col3\" class=\"data row45 col3\" >validmind.model_validation.statsmodels.RegressionROCCurve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row46_col0\" class=\"data row46 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row46_col1\" class=\"data row46 col1\" >ADF</td>\n",
       "      <td id=\"T_653c4_row46_col2\" class=\"data row46 col2\" >Augmented Dickey-Fuller unit root test for establishing the order of integration of\n",
       "    time series</td>\n",
       "      <td id=\"T_653c4_row46_col3\" class=\"data row46 col3\" >validmind.model_validation.statsmodels.ADF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row47_col0\" class=\"data row47 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row47_col1\" class=\"data row47 col1\" >DurbinWatsonTest</td>\n",
       "      <td id=\"T_653c4_row47_col2\" class=\"data row47 col2\" >The Durbin-Watson Metric is a statistical test that\n",
       "    can be used to detect autocorrelation in a time series.</td>\n",
       "      <td id=\"T_653c4_row47_col3\" class=\"data row47 col3\" >validmind.model_validation.statsmodels.DurbinWatsonTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row48_col0\" class=\"data row48 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row48_col1\" class=\"data row48 col1\" >IQROutliersTable</td>\n",
       "      <td id=\"T_653c4_row48_col2\" class=\"data row48 col2\" >Detects the outliers in numerical features using the Interquartile Range (IQR) method.\n",
       "    The input dataset is required.</td>\n",
       "      <td id=\"T_653c4_row48_col3\" class=\"data row48 col3\" >validmind.data_validation.IQROutliersTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row49_col0\" class=\"data row49 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row49_col1\" class=\"data row49 col1\" >BivariateFeaturesBarPlots</td>\n",
       "      <td id=\"T_653c4_row49_col2\" class=\"data row49 col2\" >Generates a visual analysis of categorical data by plotting bivariate feautres bar plots.\n",
       "    The input dataset and features_pairs are required.</td>\n",
       "      <td id=\"T_653c4_row49_col3\" class=\"data row49 col3\" >validmind.data_validation.BivariateFeaturesBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row50_col0\" class=\"data row50 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row50_col1\" class=\"data row50 col1\" >Skewness</td>\n",
       "      <td id=\"T_653c4_row50_col2\" class=\"data row50 col2\" >The skewness test measures the extent to which a distribution of\n",
       "    values differs from a normal distribution. A positive skew describes\n",
       "    a longer tail of values in the right and a negative skew describes a\n",
       "    longer tail of values in the left.</td>\n",
       "      <td id=\"T_653c4_row50_col3\" class=\"data row50 col3\" >validmind.data_validation.Skewness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row51_col0\" class=\"data row51 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row51_col1\" class=\"data row51 col1\" >Duplicates</td>\n",
       "      <td id=\"T_653c4_row51_col2\" class=\"data row51 col2\" >The duplicates test measures the number of duplicate rows found in\n",
       "    the dataset. If a primary key column is specified, the dataset is\n",
       "    checked for duplicate primary keys as well.</td>\n",
       "      <td id=\"T_653c4_row51_col3\" class=\"data row51 col3\" >validmind.data_validation.Duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row52_col0\" class=\"data row52 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row52_col1\" class=\"data row52 col1\" >MissingValuesBarPlot</td>\n",
       "      <td id=\"T_653c4_row52_col2\" class=\"data row52 col2\" >Generates a visual analysis of missing values by plotting horizontal bar plots with colored bars and a threshold line.\n",
       "    The input dataset is required.</td>\n",
       "      <td id=\"T_653c4_row52_col3\" class=\"data row52 col3\" >validmind.data_validation.MissingValuesBarPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row53_col0\" class=\"data row53 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row53_col1\" class=\"data row53 col1\" >DatasetDescription</td>\n",
       "      <td id=\"T_653c4_row53_col2\" class=\"data row53 col2\" >Collects a set of descriptive statistics for a dataset</td>\n",
       "      <td id=\"T_653c4_row53_col3\" class=\"data row53 col3\" >validmind.data_validation.DatasetDescription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row54_col0\" class=\"data row54 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row54_col1\" class=\"data row54 col1\" >ScatterPlot</td>\n",
       "      <td id=\"T_653c4_row54_col2\" class=\"data row54 col2\" >Generates a visual analysis of data by plotting a scatter plot matrix for all columns\n",
       "    in the dataset. The input dataset can have multiple columns (features) if necessary.</td>\n",
       "      <td id=\"T_653c4_row54_col3\" class=\"data row54 col3\" >validmind.data_validation.ScatterPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row55_col0\" class=\"data row55 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row55_col1\" class=\"data row55 col1\" >TimeSeriesOutliers</td>\n",
       "      <td id=\"T_653c4_row55_col2\" class=\"data row55 col2\" >Test that find outliers for time series data using the z-score method</td>\n",
       "      <td id=\"T_653c4_row55_col3\" class=\"data row55 col3\" >validmind.data_validation.TimeSeriesOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row56_col0\" class=\"data row56 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row56_col1\" class=\"data row56 col1\" >TabularCategoricalBarPlots</td>\n",
       "      <td id=\"T_653c4_row56_col2\" class=\"data row56 col2\" >Generates a visual analysis of categorical data by plotting bar plots.\n",
       "    The input dataset can have multiple categorical variables if necessary.\n",
       "    In this case, we produce a separate plot for each categorical variable.</td>\n",
       "      <td id=\"T_653c4_row56_col3\" class=\"data row56 col3\" >validmind.data_validation.TabularCategoricalBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row57_col0\" class=\"data row57 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row57_col1\" class=\"data row57 col1\" >AutoStationarity</td>\n",
       "      <td id=\"T_653c4_row57_col2\" class=\"data row57 col2\" >Automatically detects stationarity for each time series in a DataFrame\n",
       "    using the Augmented Dickey-Fuller (ADF) test.</td>\n",
       "      <td id=\"T_653c4_row57_col3\" class=\"data row57 col3\" >validmind.data_validation.AutoStationarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row58_col0\" class=\"data row58 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row58_col1\" class=\"data row58 col1\" >DescriptiveStatistics</td>\n",
       "      <td id=\"T_653c4_row58_col2\" class=\"data row58 col2\" >Collects a set of descriptive statistics for a dataset, both for\n",
       "    numerical and categorical variables</td>\n",
       "      <td id=\"T_653c4_row58_col3\" class=\"data row58 col3\" >validmind.data_validation.DescriptiveStatistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row59_col0\" class=\"data row59 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row59_col1\" class=\"data row59 col1\" >ANOVAOneWayTable</td>\n",
       "      <td id=\"T_653c4_row59_col2\" class=\"data row59 col2\" >Perform an ANOVA F-test for each numerical variable with the target.\n",
       "    The input dataset and target column are required.</td>\n",
       "      <td id=\"T_653c4_row59_col3\" class=\"data row59 col3\" >validmind.data_validation.ANOVAOneWayTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row60_col0\" class=\"data row60 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row60_col1\" class=\"data row60 col1\" >PearsonCorrelationMatrix</td>\n",
       "      <td id=\"T_653c4_row60_col2\" class=\"data row60 col2\" >Extracts the Pearson correlation coefficient for all pairs of numerical variables\n",
       "    in the dataset. This metric is useful to identify highly correlated variables\n",
       "    that can be removed from the dataset to reduce dimensionality.</td>\n",
       "      <td id=\"T_653c4_row60_col3\" class=\"data row60 col3\" >validmind.data_validation.PearsonCorrelationMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row61_col0\" class=\"data row61 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row61_col1\" class=\"data row61 col1\" >FeatureTargetCorrelationPlot</td>\n",
       "      <td id=\"T_653c4_row61_col2\" class=\"data row61 col2\" >Generates a visual analysis of correlations between features and target by plotting a bar plot.\n",
       "    The input dataset is required.</td>\n",
       "      <td id=\"T_653c4_row61_col3\" class=\"data row61 col3\" >validmind.data_validation.FeatureTargetCorrelationPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row62_col0\" class=\"data row62 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row62_col1\" class=\"data row62 col1\" >TabularNumericalHistograms</td>\n",
       "      <td id=\"T_653c4_row62_col2\" class=\"data row62 col2\" >Generates a visual analysis of numerical data by plotting the histogram.\n",
       "    The input dataset can have multiple numerical variables if necessary.\n",
       "    In this case, we produce a separate plot for each numerical variable.</td>\n",
       "      <td id=\"T_653c4_row62_col3\" class=\"data row62 col3\" >validmind.data_validation.TabularNumericalHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row63_col0\" class=\"data row63 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row63_col1\" class=\"data row63 col1\" >IsolationForestOutliers</td>\n",
       "      <td id=\"T_653c4_row63_col2\" class=\"data row63 col2\" >Isolation Forest.\n",
       "    This class implements the Isolation Forest algorithm for anomaly detection.\n",
       "    Attributes:\n",
       "        name (str): The name of the Isolation Forest.\n",
       "        default_params (dict): The default parameters for the Isolation Forest.\n",
       "        required_context (list): The required context for running the Isolation Forest.\n",
       "\n",
       "    Methods:\n",
       "        description(): Returns the description of the Isolation Forest.\n",
       "        run(): Runs the Isolation Forest algorithm.</td>\n",
       "      <td id=\"T_653c4_row63_col3\" class=\"data row63 col3\" >validmind.data_validation.IsolationForestOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row64_col0\" class=\"data row64 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row64_col1\" class=\"data row64 col1\" >IQROutliersPlots</td>\n",
       "      <td id=\"T_653c4_row64_col2\" class=\"data row64 col2\" >Generates a visual analysis of the outliers for numeric variables.\n",
       "    The input dataset is required.</td>\n",
       "      <td id=\"T_653c4_row64_col3\" class=\"data row64 col3\" >validmind.data_validation.IQROutliersPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row65_col0\" class=\"data row65 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row65_col1\" class=\"data row65 col1\" >ChiSquaredFeaturesTable</td>\n",
       "      <td id=\"T_653c4_row65_col2\" class=\"data row65 col2\" >Perform a Chi-Squared test of independence for each categorical variable with the target.\n",
       "    The input dataset and target column are required.</td>\n",
       "      <td id=\"T_653c4_row65_col3\" class=\"data row65 col3\" >validmind.data_validation.ChiSquaredFeaturesTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row66_col0\" class=\"data row66 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row66_col1\" class=\"data row66 col1\" >HighCardinality</td>\n",
       "      <td id=\"T_653c4_row66_col2\" class=\"data row66 col2\" >The high cardinality test measures the number of unique\n",
       "    values found in categorical columns.</td>\n",
       "      <td id=\"T_653c4_row66_col3\" class=\"data row66 col3\" >validmind.data_validation.HighCardinality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row67_col0\" class=\"data row67 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row67_col1\" class=\"data row67 col1\" >MissingValues</td>\n",
       "      <td id=\"T_653c4_row67_col2\" class=\"data row67 col2\" >Test that the number of missing values in the dataset across all features\n",
       "    is less than a threshold</td>\n",
       "      <td id=\"T_653c4_row67_col3\" class=\"data row67 col3\" >validmind.data_validation.MissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row68_col0\" class=\"data row68 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row68_col1\" class=\"data row68 col1\" >RollingStatsPlot</td>\n",
       "      <td id=\"T_653c4_row68_col2\" class=\"data row68 col2\" >This class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.</td>\n",
       "      <td id=\"T_653c4_row68_col3\" class=\"data row68 col3\" >validmind.data_validation.RollingStatsPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row69_col0\" class=\"data row69 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row69_col1\" class=\"data row69 col1\" >DatasetCorrelations</td>\n",
       "      <td id=\"T_653c4_row69_col2\" class=\"data row69 col2\" >Extracts the correlation matrix for a dataset. The following coefficients\n",
       "    are calculated:\n",
       "    - Pearson's R for numerical variables\n",
       "    - Cramer's V for categorical variables\n",
       "    - Correlation ratios for categorical-numerical variables</td>\n",
       "      <td id=\"T_653c4_row69_col3\" class=\"data row69 col3\" >validmind.data_validation.DatasetCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row70_col0\" class=\"data row70 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row70_col1\" class=\"data row70 col1\" >TabularDescriptionTables</td>\n",
       "      <td id=\"T_653c4_row70_col2\" class=\"data row70 col2\" >Collects a set of descriptive statistics for a tabular dataset, for\n",
       "    numerical, categorical and datetime variables.</td>\n",
       "      <td id=\"T_653c4_row70_col3\" class=\"data row70 col3\" >validmind.data_validation.TabularDescriptionTables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row71_col0\" class=\"data row71 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row71_col1\" class=\"data row71 col1\" >AutoMA</td>\n",
       "      <td id=\"T_653c4_row71_col2\" class=\"data row71 col2\" >Automatically detects the MA order of a time series using both BIC and AIC.</td>\n",
       "      <td id=\"T_653c4_row71_col3\" class=\"data row71 col3\" >validmind.data_validation.AutoMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row72_col0\" class=\"data row72 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row72_col1\" class=\"data row72 col1\" >UniqueRows</td>\n",
       "      <td id=\"T_653c4_row72_col2\" class=\"data row72 col2\" >Test that the number of unique rows is greater than a threshold</td>\n",
       "      <td id=\"T_653c4_row72_col3\" class=\"data row72 col3\" >validmind.data_validation.UniqueRows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row73_col0\" class=\"data row73 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row73_col1\" class=\"data row73 col1\" >TooManyZeroValues</td>\n",
       "      <td id=\"T_653c4_row73_col2\" class=\"data row73 col2\" >The zeros test finds columns that have too many zero values.</td>\n",
       "      <td id=\"T_653c4_row73_col3\" class=\"data row73 col3\" >validmind.data_validation.TooManyZeroValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row74_col0\" class=\"data row74 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row74_col1\" class=\"data row74 col1\" >HighPearsonCorrelation</td>\n",
       "      <td id=\"T_653c4_row74_col2\" class=\"data row74 col2\" >Test that the pairwise Pearson correlation coefficients between the\n",
       "    features in the dataset do not exceed a specified threshold.</td>\n",
       "      <td id=\"T_653c4_row74_col3\" class=\"data row74 col3\" >validmind.data_validation.HighPearsonCorrelation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row75_col0\" class=\"data row75 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row75_col1\" class=\"data row75 col1\" >ACFandPACFPlot</td>\n",
       "      <td id=\"T_653c4_row75_col2\" class=\"data row75 col2\" >Plots ACF and PACF for a given time series dataset.</td>\n",
       "      <td id=\"T_653c4_row75_col3\" class=\"data row75 col3\" >validmind.data_validation.ACFandPACFPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row76_col0\" class=\"data row76 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row76_col1\" class=\"data row76 col1\" >BivariateHistograms</td>\n",
       "      <td id=\"T_653c4_row76_col2\" class=\"data row76 col2\" >Generates a visual analysis of categorical data by plotting bivariate histograms.\n",
       "    The input dataset and variable_pairs are required.</td>\n",
       "      <td id=\"T_653c4_row76_col3\" class=\"data row76 col3\" >validmind.data_validation.BivariateHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row77_col0\" class=\"data row77 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row77_col1\" class=\"data row77 col1\" >HeatmapFeatureCorrelations</td>\n",
       "      <td id=\"T_653c4_row77_col2\" class=\"data row77 col2\" >Generates a visual analysis of correlations by plotting a heatmap.\n",
       "    The input dataset is required.</td>\n",
       "      <td id=\"T_653c4_row77_col3\" class=\"data row77 col3\" >validmind.data_validation.HeatmapFeatureCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row78_col0\" class=\"data row78 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row78_col1\" class=\"data row78 col1\" >TimeSeriesFrequency</td>\n",
       "      <td id=\"T_653c4_row78_col2\" class=\"data row78 col2\" >Test that detect frequencies in the data</td>\n",
       "      <td id=\"T_653c4_row78_col3\" class=\"data row78 col3\" >validmind.data_validation.TimeSeriesFrequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row79_col0\" class=\"data row79 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row79_col1\" class=\"data row79 col1\" >DatasetSplit</td>\n",
       "      <td id=\"T_653c4_row79_col2\" class=\"data row79 col2\" >Attempts to extract information about the dataset split from the\n",
       "    provided training, test and validation datasets.</td>\n",
       "      <td id=\"T_653c4_row79_col3\" class=\"data row79 col3\" >validmind.data_validation.DatasetSplit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row80_col0\" class=\"data row80 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row80_col1\" class=\"data row80 col1\" >SpreadPlot</td>\n",
       "      <td id=\"T_653c4_row80_col2\" class=\"data row80 col2\" >This class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.</td>\n",
       "      <td id=\"T_653c4_row80_col3\" class=\"data row80 col3\" >validmind.data_validation.SpreadPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row81_col0\" class=\"data row81 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row81_col1\" class=\"data row81 col1\" >TimeSeriesLinePlot</td>\n",
       "      <td id=\"T_653c4_row81_col2\" class=\"data row81 col2\" >Generates a visual analysis of time series data by plotting the\n",
       "    raw time series. The input dataset can have multiple time series\n",
       "    if necessary. In this case we produce a separate plot for each time series.</td>\n",
       "      <td id=\"T_653c4_row81_col3\" class=\"data row81 col3\" >validmind.data_validation.TimeSeriesLinePlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row82_col0\" class=\"data row82 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row82_col1\" class=\"data row82 col1\" >WOEIVPlots</td>\n",
       "      <td id=\"T_653c4_row82_col2\" class=\"data row82 col2\" >Generates a visual analysis of the WoE and IV values distribution for categorical variables.\n",
       "    The input dataset is required.</td>\n",
       "      <td id=\"T_653c4_row82_col3\" class=\"data row82 col3\" >validmind.data_validation.WOEIVPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row83_col0\" class=\"data row83 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row83_col1\" class=\"data row83 col1\" >AutoSeasonality</td>\n",
       "      <td id=\"T_653c4_row83_col2\" class=\"data row83 col2\" >Automatically detects the optimal seasonal order for a time series dataset\n",
       "    using the seasonal_decompose method.</td>\n",
       "      <td id=\"T_653c4_row83_col3\" class=\"data row83 col3\" >validmind.data_validation.AutoSeasonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row84_col0\" class=\"data row84 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row84_col1\" class=\"data row84 col1\" >BivariateScatterPlots</td>\n",
       "      <td id=\"T_653c4_row84_col2\" class=\"data row84 col2\" >Generates a visual analysis of categorical data by plotting bivariate scatter plots.\n",
       "    The input dataset and variable_pairs are required.</td>\n",
       "      <td id=\"T_653c4_row84_col3\" class=\"data row84 col3\" >validmind.data_validation.BivariateScatterPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row85_col0\" class=\"data row85 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row85_col1\" class=\"data row85 col1\" >EngleGrangerCoint</td>\n",
       "      <td id=\"T_653c4_row85_col2\" class=\"data row85 col2\" >Test for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.</td>\n",
       "      <td id=\"T_653c4_row85_col3\" class=\"data row85 col3\" >validmind.data_validation.EngleGrangerCoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row86_col0\" class=\"data row86 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row86_col1\" class=\"data row86 col1\" >TimeSeriesMissingValues</td>\n",
       "      <td id=\"T_653c4_row86_col2\" class=\"data row86 col2\" >Test that the number of missing values is less than a threshold</td>\n",
       "      <td id=\"T_653c4_row86_col3\" class=\"data row86 col3\" >validmind.data_validation.TimeSeriesMissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row87_col0\" class=\"data row87 col0\" >DatasetMetadata</td>\n",
       "      <td id=\"T_653c4_row87_col1\" class=\"data row87 col1\" >DatasetMetadata</td>\n",
       "      <td id=\"T_653c4_row87_col2\" class=\"data row87 col2\" >Custom class to collect a set of descriptive statistics for a dataset.\n",
       "    This class will log dataset metadata via `log_dataset` instead of a metric.\n",
       "    Dataset metadata is necessary to initialize dataset object that can be related\n",
       "    to different metrics and test results</td>\n",
       "      <td id=\"T_653c4_row87_col3\" class=\"data row87 col3\" >validmind.data_validation.DatasetMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row88_col0\" class=\"data row88 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row88_col1\" class=\"data row88 col1\" >TimeSeriesHistogram</td>\n",
       "      <td id=\"T_653c4_row88_col2\" class=\"data row88 col2\" >Generates a visual analysis of time series data by plotting the\n",
       "    histogram. The input dataset can have multiple time series if\n",
       "    necessary. In this case we produce a separate plot for each time series.</td>\n",
       "      <td id=\"T_653c4_row88_col3\" class=\"data row88 col3\" >validmind.data_validation.TimeSeriesHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row89_col0\" class=\"data row89 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row89_col1\" class=\"data row89 col1\" >LaggedCorrelationHeatmap</td>\n",
       "      <td id=\"T_653c4_row89_col2\" class=\"data row89 col2\" >Generates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.</td>\n",
       "      <td id=\"T_653c4_row89_col3\" class=\"data row89 col3\" >validmind.data_validation.LaggedCorrelationHeatmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row90_col0\" class=\"data row90 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row90_col1\" class=\"data row90 col1\" >SeasonalDecompose</td>\n",
       "      <td id=\"T_653c4_row90_col2\" class=\"data row90 col2\" >Calculates seasonal_decompose metric for each of the dataset features</td>\n",
       "      <td id=\"T_653c4_row90_col3\" class=\"data row90 col3\" >validmind.data_validation.SeasonalDecompose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row91_col0\" class=\"data row91 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row91_col1\" class=\"data row91 col1\" >DefaultRateBarPlots</td>\n",
       "      <td id=\"T_653c4_row91_col2\" class=\"data row91 col2\" >Generates a visual analysis of loan default ratios by plotting bar plots.\n",
       "    The input dataset can have multiple categorical variables if necessary.\n",
       "    In this case, we produce a separate row of plots for each categorical variable.</td>\n",
       "      <td id=\"T_653c4_row91_col3\" class=\"data row91 col3\" >validmind.data_validation.DefaultRateBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row92_col0\" class=\"data row92 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row92_col1\" class=\"data row92 col1\" >ClassImbalance</td>\n",
       "      <td id=\"T_653c4_row92_col2\" class=\"data row92 col2\" >The class imbalance test measures the disparity between the majority\n",
       "    class and the minority class in the target column.</td>\n",
       "      <td id=\"T_653c4_row92_col3\" class=\"data row92 col3\" >validmind.data_validation.ClassImbalance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row93_col0\" class=\"data row93 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row93_col1\" class=\"data row93 col1\" >AutoAR</td>\n",
       "      <td id=\"T_653c4_row93_col2\" class=\"data row93 col2\" >Automatically detects the AR order of a time series using both BIC and AIC.</td>\n",
       "      <td id=\"T_653c4_row93_col3\" class=\"data row93 col3\" >validmind.data_validation.AutoAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row94_col0\" class=\"data row94 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row94_col1\" class=\"data row94 col1\" >TabularDateTimeHistograms</td>\n",
       "      <td id=\"T_653c4_row94_col2\" class=\"data row94 col2\" >Generates a visual analysis of datetime data by plotting histograms of\n",
       "    differences between consecutive dates. The input dataset can have multiple\n",
       "    datetime variables if necessary. In this case, we produce a separate plot\n",
       "    for each datetime variable.</td>\n",
       "      <td id=\"T_653c4_row94_col3\" class=\"data row94 col3\" >validmind.data_validation.TabularDateTimeHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row95_col0\" class=\"data row95 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row95_col1\" class=\"data row95 col1\" >WOEIVTable</td>\n",
       "      <td id=\"T_653c4_row95_col2\" class=\"data row95 col2\" >Calculate the Weight of Evidence (WoE) and Information Value (IV) of categorical features.\n",
       "    The input dataset and target column are required.</td>\n",
       "      <td id=\"T_653c4_row95_col3\" class=\"data row95 col3\" >validmind.data_validation.WOEIVTable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row96_col0\" class=\"data row96 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row96_col1\" class=\"data row96 col1\" >Duplicates</td>\n",
       "      <td id=\"T_653c4_row96_col2\" class=\"data row96 col2\" >The duplicates test measures the number of duplicate entries found in the text_column\n",
       "    of the dataset. If a primary key column is specified, the dataset is checked for\n",
       "    duplicate primary keys as well.</td>\n",
       "      <td id=\"T_653c4_row96_col3\" class=\"data row96 col3\" >validmind.data_validation.nlp.Duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row97_col0\" class=\"data row97 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row97_col1\" class=\"data row97 col1\" >Punctuations</td>\n",
       "      <td id=\"T_653c4_row97_col2\" class=\"data row97 col2\" >Punctuations(test_context: validmind.vm_models.test_context.TestContext, params: dict = None, result: validmind.vm_models.test_plan_result.TestPlanMetricResult = None)</td>\n",
       "      <td id=\"T_653c4_row97_col3\" class=\"data row97 col3\" >validmind.data_validation.nlp.Punctuations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row98_col0\" class=\"data row98 col0\" >Metric</td>\n",
       "      <td id=\"T_653c4_row98_col1\" class=\"data row98 col1\" >CommonWords</td>\n",
       "      <td id=\"T_653c4_row98_col2\" class=\"data row98 col2\" >CommonWords(test_context: validmind.vm_models.test_context.TestContext, params: dict = None, result: validmind.vm_models.test_plan_result.TestPlanMetricResult = None)</td>\n",
       "      <td id=\"T_653c4_row98_col3\" class=\"data row98 col3\" >validmind.data_validation.nlp.CommonWords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row99_col0\" class=\"data row99 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row99_col1\" class=\"data row99 col1\" >Hashtags</td>\n",
       "      <td id=\"T_653c4_row99_col2\" class=\"data row99 col2\" >The purpose of this test is to identify and analyze the most frequently used hashtags in a given text column of a dataset.</td>\n",
       "      <td id=\"T_653c4_row99_col3\" class=\"data row99 col3\" >validmind.data_validation.nlp.Hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row100_col0\" class=\"data row100 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row100_col1\" class=\"data row100 col1\" >Mentions</td>\n",
       "      <td id=\"T_653c4_row100_col2\" class=\"data row100 col2\" >Mentions(test_context: validmind.vm_models.test_context.TestContext, params: dict = None, result: validmind.vm_models.test_result.TestResults = None)</td>\n",
       "      <td id=\"T_653c4_row100_col3\" class=\"data row100 col3\" >validmind.data_validation.nlp.Mentions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row101_col0\" class=\"data row101 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row101_col1\" class=\"data row101 col1\" >ClassImbalance</td>\n",
       "      <td id=\"T_653c4_row101_col2\" class=\"data row101 col2\" >The class imbalance test measures the disparity between the majority\n",
       "    class and the minority class in the target column.</td>\n",
       "      <td id=\"T_653c4_row101_col3\" class=\"data row101 col3\" >validmind.data_validation.nlp.ClassImbalance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_653c4_row102_col0\" class=\"data row102 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_653c4_row102_col1\" class=\"data row102 col1\" >StopWords</td>\n",
       "      <td id=\"T_653c4_row102_col2\" class=\"data row102 col2\" >StopWords(test_context: validmind.vm_models.test_context.TestContext, params: dict = None, result: validmind.vm_models.test_result.TestResults = None)</td>\n",
       "      <td id=\"T_653c4_row102_col3\" class=\"data row102 col3\" >validmind.data_validation.nlp.StopWords</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2889c9ea0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.tests.list_tests()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Lending Club Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe Raw Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Missing Values in Raw Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"threshold\": 70,\n",
    "          \"fig_height\": 1100}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Target Variable\n",
    "\n",
    "**Definition of Default**\n",
    "\n",
    "We categorizing `Fully Paid` loans as \"default = 0\" and `Charged Off` loans as \"default = 1\". This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk). \n",
    "\n",
    "Loans with `Current` status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add `default` Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_column(df, target_column):\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    # Convert target column to integer\n",
    "    df[target_column] = df[target_column].astype(int)\n",
    "    return df\n",
    "\n",
    "target_column = 'default'\n",
    "df = add_target_column(df, target_column)\n",
    "\n",
    "# Drop 'loan_status' variable \n",
    "df.drop(columns='loan_status', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Unused Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "                    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "                    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n",
    "                    'earliest_cr_line', 'issue_d']\n",
    "\n",
    "df = df.drop(columns=unused_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop\n",
    "\n",
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df, min_missing_count)\n",
    "df.drop(columns=variables_to_drop, axis=1, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Type of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def clean_term_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to remove 'months' string from the 'term' column and convert it to categorical\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].str.replace(' months', '')\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "def clean_emp_length_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to clean 'emp_length' column and convert it to categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column].fillna(value = 0, inplace=True)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "def clean_inq_last_6mths(df, column):\n",
    "    \"\"\"\n",
    "    Function to convert 'inq_last_6mths' column into categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "clean_emp_length_column(df, 'emp_length')\n",
    "clean_term_column(df, 'term')\n",
    "clean_inq_last_6mths(df, 'inq_last_6mths')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_columns(df):\n",
    "        numerical_columns = df.select_dtypes(\n",
    "            include=[\"int\", \"float\", \"uint\"]\n",
    "        ).columns.tolist()\n",
    "        return numerical_columns\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "        categorical_columns = df.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns.tolist()\n",
    "        return categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.IQROutliersPlots import IQROutliersPlots\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "num_features = get_numerical_columns(df)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"threshold\": 1.5,\n",
    "          \"fig_width\": 500}\n",
    "\n",
    "metric = IQROutliersPlots(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Outliers using IQR Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_outliers(series, threshold=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return series[(series < lower_bound) | (series > upper_bound)]\n",
    "\n",
    "def remove_iqr_outliers(df, target_column, threshold=1.5):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols.remove(target_column)  # Exclude target_column from numerical columns\n",
    "    for col in num_cols:\n",
    "        outliers = compute_outliers(df[col], threshold)\n",
    "        df = df[~df[col].isin(outliers)]\n",
    "    return df\n",
    "\n",
    "df = remove_iqr_outliers(df, target_column, threshold=1.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Method\n",
    "\n",
    "We employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the `stratify = y` parameter is set, it ensures that the distribution of the target variable (`y`) in the test set is the same as that in the original dataset. \n",
    "\n",
    "This is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test \n",
    "X = df.drop(target_column, axis = 1)\n",
    "y = df[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 42, stratify = y)\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance\n",
    "\n",
    "Class imbalance is a common issue in credit risk scorecards and datasets like the Lending Club's. This imbalance arises when the number of defaulting loans (negative class) is significantly smaller than the number of loans that are paid off (positive class). Such imbalance can lead to biased models that favor the majority class, thus affecting predictive performance. \n",
    "\n",
    "Special techniques like oversampling, undersampling, or cost-sensitive learning are often needed to ensure that the minority class is appropriately represented during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = ClassImbalance(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Cardinality of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plots of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Rate by Categorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.DefaultRateBarPlots import DefaultRateBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": target_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = DefaultRateBarPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Squared Test on Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "cat_features = get_categorical_columns(df_train)\n",
    "params = {\"cat_features\": cat_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ChiSquaredFeaturesTable(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA Test on Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "num_features = get_numerical_columns(df_train)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ANOVAOneWayTable(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap Correlation of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations of Numerical Features with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"features\": None,\n",
    "          \"fig_height\": 600}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_categorical_features = ['addr_state']\n",
    "drop_numerical_features = ['total_rec_int', 'loan_amnt',\n",
    "                           'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n",
    "                           'total_pymnt_inv', 'last_pymnt_amnt',]\n",
    "\n",
    "df_train.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)\n",
    "\n",
    "# Update df_test \n",
    "df_test.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def encode_numerical_features(df):\n",
    "    \n",
    "    # term\n",
    "    df['term'] = df['term'].replace({' 36': '36M', ' 60': '60M'})\n",
    "\n",
    "    # emp_length_int\n",
    "    df['emp_length'] = df['emp_length'].replace('10+', '10')  # Replace '10+' with '10'\n",
    "    df['emp_length'] = pd.to_numeric(df['emp_length'], errors='coerce')  # Convert to numeric\n",
    "    df['emp_length'].fillna(-1, inplace=True)\n",
    "    bins = [0,1,2,3,5,8,10,999]\n",
    "    df['emp_length_bucket'] = pd.cut(df['emp_length'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='emp_length', inplace=True)\n",
    "\n",
    "    # inq_last_6mths\n",
    "    df['inq_last_6mths'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n",
    "    df['inq_last_6mths_bucket'] = pd.cut(df['inq_last_6mths'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='inq_last_6mths', inplace=True)\n",
    "    \n",
    "    # total_acc\n",
    "    df['total_acc'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 999]\n",
    "    df['total_acc_bucket'] = pd.cut(df['total_acc'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='total_acc', inplace=True)\n",
    "\n",
    "    # annual_inc\n",
    "    df['annual_inc'].fillna(-1, inplace=True)\n",
    "    df['annual_inc_1000'] = df['annual_inc']/1000\n",
    "    bins = [-1, 0, 10, 20, 30, 40, 50, 75, 100, 150, 250, 1000, 10000]\n",
    "    df['annual_inc_bucket'] = pd.cut(df['annual_inc_1000'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='annual_inc', inplace=True)\n",
    "    df.drop(columns='annual_inc_1000', inplace=True)\n",
    "    \n",
    "    # int_rate\n",
    "    df['int_rate'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n",
    "    df['int_rate_bucket'] = pd.cut(df['int_rate'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='int_rate', inplace=True)\n",
    "\n",
    "    # installment\n",
    "    df['installment'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 100, 200, 300, 400, 500, 750, 1000, 1500]\n",
    "    df['installment_bucket'] = pd.cut(df['installment'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='installment', inplace=True)\n",
    "\n",
    "    # open_acc\n",
    "    df['open_acc'].replace(\"N/A\", 1, inplace=True)\n",
    "    df['open_acc'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 8, 10, 100]\n",
    "    df['open_acc_bucket'] = pd.cut(df['open_acc'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='open_acc', inplace=True)\n",
    "\n",
    "def find_categorical_features(df):\n",
    "    # Get the column names of features with the data type \"category\"\n",
    "    categorical_features = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    return categorical_features\n",
    "\n",
    "\n",
    "def convert_categorical_to_object(df):\n",
    "    # Find the categorical features\n",
    "    categorical_features = find_categorical_features(df)\n",
    "\n",
    "    # Convert the categorical features to object type\n",
    "    df[categorical_features] = df[categorical_features].astype(str)\n",
    "\n",
    "\n",
    "encode_numerical_features(df_train)\n",
    "convert_categorical_to_object(df_train)\n",
    "\n",
    "# Update df_test\n",
    "encode_numerical_features(df_test)\n",
    "convert_categorical_to_object(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight of Evidence (WoE) and Infomation Value (IV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEIVTable import WOEIVTable\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"order_by\": [\"Feature\", \"WoE\"]\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_iv_df = metric.result.metric.value['woe_iv']\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def coarse_classing(df, mappings):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_new = df.copy()\n",
    "\n",
    "    # Loop through each feature and merge set\n",
    "    for feature, merge_sets in mappings.items():\n",
    "        for merge_set in merge_sets:\n",
    "            # Merge the specified categories into a new category\n",
    "            df_new[feature] = df_new[feature].apply(lambda x: f\"[{','.join(merge_set)}]\" if x in merge_set else x)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "def shorten_category_names(df, max_length=20, suffix=\"...\"):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for feature in df_new.columns:\n",
    "        # Check if the column has the \"object\" data type\n",
    "        if df_new[feature].dtype.name == 'object':\n",
    "            # Shorten long category names\n",
    "            df_new[feature] = df_new[feature].apply(lambda x: x[:max_length] + suffix if len(x) > max_length else x)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Create a dictionary of features and the sets to merge\n",
    "mappings = {\n",
    "    'sub_grade': [['B2','B3','B4','B5','C3','D1'], ['C1','C2','C4','C5'], ['D3','D4','D5','E3','G4'], ['E1','E2','E4','E5','F1','F2','F3','F4','G1','G2','G3','G5','F5']],\n",
    "    'grade': [['F','G']],\n",
    "    'purpose': [['wedding','major_purchase'], ['credit_card','car'], ['debt_consolidation','other','vacation'], ['medical','moving','house','educational'], ['renewable_energy','small_business']],\n",
    "    'home_ownership': [['MORTGAGE','OWN','RENT']],\n",
    "    'annual_inc_bucket': [['[250, 1000)','[100, 150)','[150, 250)','[1000, 10000)'], ['[50, 75)','[40, 50)'], ['[10, 20)','[0, 10)']],\n",
    "    'emp_length_bucket': [['[2, 3)','[40, 50)','[3, 5)','[1, 2)','[0, 1)','[5, 8)','[8, 10)']],\n",
    "    'inq_last_6mths_bucket': [['[4, 5)','[1, 2)'], ['[5, 10)','[3, 4)']],\n",
    "    'installment_bucket': [['[300, 400)','[200, 300)','[0, 100)'], ['[400, 500)', '[500, 750)']],\n",
    "    'total_acc_bucket': [['[20, 25)','[30, 35)','[15, 20)','[45, 50)','[40, 45)','[35, 40)','[10, 15)','[5, 10)']],\n",
    "    'open_acc_bucket': [['[5, 8)','[8, 10)','[10, 100)','[4, 5)'], ['[1, 2)','[2, 3)']]\n",
    "}\n",
    "\n",
    "df_train = coarse_classing(df_train, mappings)\n",
    "df_train = df_train[~df_train['home_ownership'].isin(['OTHER', 'NONE'])]\n",
    "df_train.drop(columns=\"home_ownership\", inplace=True)\n",
    "df_train = shorten_category_names(df_train, max_length=15, suffix=\"...\")\n",
    "\n",
    "# Update df_test\n",
    "df_test = coarse_classing(df_test, mappings)\n",
    "df_test = df_test[~df_test['home_ownership'].isin(['OTHER', 'NONE'])]\n",
    "df_test.drop(columns=\"home_ownership\", inplace=True)\n",
    "df_test = shorten_category_names(df_test, max_length=15, suffix=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEIVPlots import WOEIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"fig_height\": 500,\n",
    "    \"fig_height\": 500,\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Features with WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"order_by\": [\"Feature\", \"WoE\"]\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_iv_df = metric.result.metric.value['woe_iv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categories(woe_df, original_df):\n",
    "    for feature in woe_df['Feature'].unique():\n",
    "        woe_categories = woe_df[woe_df['Feature'] == feature]['Category'].unique()\n",
    "        original_categories = original_df[feature].unique()\n",
    "        \n",
    "        # Check categories in WoE table that are not in original DataFrame\n",
    "        for category in woe_categories:\n",
    "            if category not in original_categories:\n",
    "                print(f\"Category '{category}' not found in feature '{feature}' in original DataFrame.\")\n",
    "                \n",
    "        # Check categories in original DataFrame that are not in WoE table\n",
    "        for category in original_categories:\n",
    "            if category not in woe_categories:\n",
    "                print(f\"Category '{category}' in feature '{feature}' not found in WoE table.\")\n",
    "\n",
    "                \n",
    "check_categories(woe_iv_df, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def woe_encoder(woe_df, original_df, target):\n",
    "    # Create a new DataFrame with the same columns as original_df\n",
    "    woe_encoded_df = pd.DataFrame(columns=original_df.columns, index=original_df.index)\n",
    "\n",
    "    # Loop through each feature-category and assign the corresponding WoE value as float\n",
    "    for feature in woe_df['Feature'].unique():\n",
    "        # Check that the feature exists in the original DataFrame\n",
    "        if feature not in original_df.columns:\n",
    "            print(f\"Feature {feature} not found in original DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        feature_woe = woe_df[woe_df['Feature'] == feature]\n",
    "        woe_dict = dict(zip(feature_woe['Category'], feature_woe['WoE']))\n",
    "\n",
    "        # Check that the categories exist in the original DataFrame\n",
    "        # Converting both to strings to avoid datatype issues\n",
    "        original_categories = original_df[feature].astype(str).unique()\n",
    "        woe_categories = feature_woe['Category'].astype(str).unique()\n",
    "        \n",
    "        # Two-way check:\n",
    "        # 1. For each category in the original DataFrame, check if it exists in the WoE DataFrame\n",
    "        missing_from_woe = [category for category in original_categories if category not in woe_categories]\n",
    "        if missing_from_woe:\n",
    "            print(f\"Categories {missing_from_woe} from original DataFrame not found in WoE DataFrame for feature {feature}.\")\n",
    "            \n",
    "        # 2. For each category in the WoE DataFrame, check if it exists in the original DataFrame\n",
    "        missing_from_original = [category for category in woe_categories if category not in original_categories]\n",
    "        if missing_from_original:\n",
    "            print(f\"Categories {missing_from_original} from WoE DataFrame not found in original DataFrame for feature {feature}.\")\n",
    "        \n",
    "        # Also converting original dataframe feature to string before replacement\n",
    "        woe_encoded_df[feature] = original_df[feature].astype(str).replace(woe_dict).astype(float)\n",
    "\n",
    "    # Check that the target exists in the original DataFrame\n",
    "    if target not in original_df.columns:\n",
    "        print(f\"Target {target} not found in original DataFrame. Returning None...\")\n",
    "        return None\n",
    "\n",
    "    # Add the target column to the new DataFrame\n",
    "    woe_encoded_df[target] = original_df[target]\n",
    "\n",
    "    return woe_encoded_df\n",
    "\n",
    "\n",
    "df_train = woe_encoder(woe_iv_df, df_train, target='default')\n",
    "\n",
    "# Update df_test\n",
    "df_test = woe_encoder(woe_iv_df, df_test, target='default')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit GLM Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Create X_train, y_train and X_test, y_test\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n",
    "y_train = df_train[target_column]\n",
    "X_train = df_train.drop(target_column, axis=1)\n",
    "\n",
    "# Add constant to X_train for intercept term\n",
    "X_train = sm.add_constant(X_train)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Update df_test\n",
    "y_test = df_test[target_column]\n",
    "X_test = df_test.drop(target_column, axis=1)\n",
    "X_test = sm.add_constant(X_test)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "model_fit_glm = model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(model_fit_glm.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ValidMind Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerate VM dataset\n",
    "vm_train_ds = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "vm_test_ds = vm.init_dataset(dataset=df_test,\n",
    "                        target_column=target_column)\n",
    "\n",
    "# Create VM model\n",
    "vm_model_glm = vm.init_model(\n",
    "    model = model_fit_glm, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.sklearn.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = ConfusionMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionROCCurve import RegressionROCCurve\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = RegressionROCCurve(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GINI and Kolmogorov-Smirnov (KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.GINITable import GINITable\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = GINITable(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Probability of Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegPredictionHistogram import LogisticRegPredictionHistogram\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Histogram of Probability of Default\",\n",
    "}\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = LogisticRegPredictionHistogram(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegCumulativeProb import LogisticRegCumulativeProb\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Cumulative Probability of Default\",\n",
    "}\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = LogisticRegCumulativeProb(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Credit Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.ScorecardHistogram import ScorecardHistogram\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"target_score\": 600,\n",
    "    \"target_odds\": 50,\n",
    "    \"pdo\": 20,\n",
    "    \"title\": \"Histogram of Credit Scores\",\n",
    "}\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = ScorecardHistogram(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
