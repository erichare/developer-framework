{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Default Model using ValidMind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Connect Notebook to ValidMind Project\n",
    "- Step 2: Import Raw Data\n",
    "- Step 3: Data Description\n",
    "- Step 4: Data Preparation\n",
    "- Step 5: Data Description on Preprocessed Data \n",
    "- Step 6: Univariate Analysis\n",
    "- Step 7: Multivariate Analysis\n",
    "- Step 8: Model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect Notebook to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect Notebook to ValidMind Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 18:32:56,810 - INFO - api_client - Connected to ValidMind. Project: [3] PD Model - Initial Validation (cliwzqjgv00001fy6869rlav9)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Raw Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Lending Club Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39717 entries, 0 to 39716\n",
      "Columns: 111 entries, id to total_il_high_credit_limit\n",
      "dtypes: float64(74), int64(13), object(24)\n",
      "memory usage: 33.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_66996/2449495096.py:4: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the zip file\n",
    "# filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2011/lending_club_loan_data_2007_2011.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Description "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 18:32:57,203 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-26 18:32:57,204 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5af7bf8f69a4e649b3c03f7113196f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>This section provides descriptive statistics for numerical, categorical and date…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 18:33:21,202 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-26 18:33:21,203 - INFO - dataset - Inferring dataset types...\n",
      "/Users/juanvalidmind/Dev/github/validmind-python/validmind/tests/data_validation/MissingValuesBarPlot.py:60: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([\"{:.1f}%\".format(x) for x in ax.get_yticks()])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80a259a87b2467da6a9d0da98d4265e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of missing values by plotting bar plots with colored…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"xticks_fontsize\": 8}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_missing_values(data, threshold=80):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = (data.isnull().sum() / len(data)) * 100\n",
    "\n",
    "    # Sort missing value percentages in ascending order\n",
    "    missing_percentages_sorted = missing_percentages.sort_values(ascending=True)\n",
    "\n",
    "    # Create a list to store the colors for each bar\n",
    "    colors = []\n",
    "\n",
    "    # Iterate through the missing percentages and assign colors based on the threshold\n",
    "    for value in missing_percentages_sorted.values:\n",
    "        if value < threshold:\n",
    "            colors.append('grey')\n",
    "        else:\n",
    "            colors.append('lightcoral')\n",
    "\n",
    "    # Create a bar plot of missing value percentages\n",
    "    plt.figure()\n",
    "    ax = sns.barplot(x=missing_percentages_sorted.index, y=missing_percentages_sorted.values, palette=colors)\n",
    "\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=9)\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Missing Value Percentage (%)')\n",
    "    plt.title('Missing Values')\n",
    "\n",
    "    # Update y-axis labels to show one decimal place\n",
    "    ax.set_yticklabels(['{:.1f}%'.format(x) for x in ax.get_yticks()])\n",
    "\n",
    "    # Draw a red line at the specified threshold\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', label='Threshold: {}%'.format(threshold))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.MissingValues import MissingValues\n",
    "metric = MissingValues(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Target Variable\n",
    "\n",
    "**Definition of Default**\n",
    "\n",
    "We categorizing `Fully Paid` loans as \"default = 0\" and `Charged Off` loans as \"default = 1\". This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk). \n",
    "\n",
    "Loans with `Current` status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add `default` Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_column(df, target_column):\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    # Convert target column to integer\n",
    "    df[target_column] = df[target_column].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'default'\n",
    "df_prep = add_target_column(df, target_column)\n",
    "\n",
    "# Drop 'loan_status' variable \n",
    "df_prep.drop(columns='loan_status', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unused Variables\n",
    "\n",
    "Remove all the **Demographic** and **Customer Behavioural** features which is of no use for default analysis for credit approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "                    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "                    \"pub_rec_bankruptcies\", \"chargeoff_within_12_mths\", \"tax_liens\", \"delinq_amnt\"]\n",
    "df_prep = df_prep.drop(columns=unused_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df_prep, min_missing_count)\n",
    "df_prep.drop(columns=variables_to_drop, axis=1, inplace=True)\n",
    "\n",
    "df_prep.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df_prep.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with loan_status as \"Current\"\n",
    "# df_prep = df_prep[df_prep[\"loan_status\"].apply(lambda x: False if x == \"Current\" else True)]\n",
    "\n",
    "# Update loan_status as Fully Paid to 0 and Charged Off to 1\n",
    "#df_no_current[\"loan_status\"] = df_no_current[\"loan_status\"].apply(lambda x: 0 if x == \"Fully Paid\" else 1)\n",
    "\n",
    "# Convert 'emp_length' to string type\n",
    "#df_no_current[\"emp_length\"] = df_no_current[\"emp_length\"].astype(str)\n",
    "\n",
    "# Update emp_length feature with continuous values as int\n",
    "# where (< 1 year) is assumed as 0 and 10+ years is assumed as 10 and rest are stored as their magnitude\n",
    "#df_no_current[\"emp_length\"] = pd.to_numeric(df_no_current[\"emp_length\"].apply(lambda x: 0 if \"<\" in x else (x.split('+')[0] if \"+\" in x else x.split()[0])))\n",
    "\n",
    "# Look through the purpose value counts\n",
    "#loan_purpose_values = df_no_current[\"purpose\"].value_counts() * 100 / df_no_current.shape[0]\n",
    "\n",
    "# Remove rows with less than 1% of value counts in particular purpose \n",
    "#loan_purpose_delete = loan_purpose_values[loan_purpose_values < 1].index.values\n",
    "#df_processed = df_no_current[[False if p in loan_purpose_delete else True for p in df_no_current[\"purpose\"]]]\n",
    "\n",
    "# Update int_rate, revol_util without % sign and as numeric type\n",
    "#df_processed[\"int_rate\"] = pd.to_numeric(df_processed[\"int_rate\"].apply(lambda x:x.split('%')[0]))\n",
    "#df_processed[\"revol_util\"] = pd.to_numeric(df_processed[\"revol_util\"].apply(lambda x:x.split('%')[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Sampling \n",
    "\n",
    "We employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the `stratify = y` parameter is set, it ensures that the distribution of the target variable (`y`) in the test set is the same as that in the original dataset. \n",
    "\n",
    "This is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test \n",
    "X = df_prep.drop(target_column, axis = 1)\n",
    "y = df_prep[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 42, stratify = y)\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Cleaning \n",
    "\n",
    "We perform data cleaning after splitting the data into training and testing sets to prevent data leakage, maintain the test set's independence, and avoid overfitting. Data leakage can occur when information from the test set influences the training set, leading to overly optimistic performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def clean_date_columns(df: pd.DataFrame, columns: List[str], date_format: str = \"%b-%y\") -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Converts date columns to datetime format and create a new column \n",
    "    as a difference between today and the respective date.\n",
    "    \"\"\"\n",
    "    # Ensure the columns exist in the dataframe\n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    today_date = pd.to_datetime(datetime.now())\n",
    "    for column in columns:\n",
    "        # convert to datetime format\n",
    "        df[column] = pd.to_datetime(df[column], format=date_format)\n",
    "        # calculate the difference in months and add to a new column\n",
    "        df['mths_since_' + column] = round((today_date - df[column]) / np.timedelta64(1, 'M')).clip(lower=0)\n",
    "        # drop the original date column\n",
    "        df.drop(columns = [column], inplace = True)\n",
    "    return df\n",
    "\n",
    "def clean_term_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to remove 'months' string from the 'term' column and convert it to categorical\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].str.replace(' months', '')\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_rate_columns(df, column):\n",
    "    \"\"\"\n",
    "    Clean interest rate column. Remove the '%' sign and convert to numeric.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    column (str): Name of the interest rate column to be cleaned.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].str.replace('%', '')\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "\n",
    "def clean_emp_length_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to clean 'emp_length' column and convert it to categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column].fillna(value = 0, inplace=True)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_inq_last_6mths(df, column):\n",
    "    \"\"\"\n",
    "    Function to convert 'inq_last_6mths' column into categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_date_columns(df_train, ['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "clean_rate_columns(df_train, 'int_rate')\n",
    "clean_rate_columns(df_train, 'revol_util')\n",
    "clean_emp_length_column(df_train, 'emp_length')\n",
    "clean_term_column(df_train, 'term')\n",
    "clean_inq_last_6mths(df_train, 'inq_last_6mths')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NaN values from `mths_since_last_pymnt_d` and `mths_since_last_credit_pull_d` before performing statistical tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(subset=['mths_since_last_pymnt_d', 'mths_since_last_credit_pull_d'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_columns(df):\n",
    "        numerical_columns = df.select_dtypes(\n",
    "            include=[\"int\", \"float\"]\n",
    "        ).columns.tolist()\n",
    "        return numerical_columns\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "        categorical_columns = df.select_dtypes(\n",
    "            include=[\"object\", \"category\", \"uint8\"]\n",
    "        ).columns.tolist()\n",
    "        return categorical_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Test on Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_squared_categorical_feature_selection(df, cat_vars, target):\n",
    "    \"\"\"\n",
    "    Performs a Chi-Squared test of independence for each categorical variable with the target.\n",
    "\n",
    "    :param df: DataFrame containing the data\n",
    "    :param cat_vars: list of column names which are categorical\n",
    "    :param target: target variable name\n",
    "    :return: DataFrame with p-values and Chi-squared statistic. Each row is a categorical variable, \n",
    "             columns are 'Variable', 'Chi-squared statistic', 'p-value'.\n",
    "    \"\"\"\n",
    "    # Ensure the columns exist in the dataframe\n",
    "    for var in cat_vars:\n",
    "        if var not in df.columns:\n",
    "            raise ValueError(f\"The column '{var}' does not exist in the dataframe.\")\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"The target column '{target}' does not exist in the dataframe.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for var in cat_vars:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df[var], df[target])\n",
    "\n",
    "        # Perform the Chi-Square test\n",
    "        chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Add the result to the list of results\n",
    "        results.append([var, chi2, p])\n",
    "\n",
    "    # Convert results to a DataFrame and return\n",
    "    results_df = pd.DataFrame(results, columns=['Variable', 'Chi-squared statistic', 'p-value'])\n",
    "\n",
    "    # Sort by p-value in ascending order\n",
    "    results_df = results_df.sort_values(by='p-value')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = get_categorical_columns(df_train)\n",
    "chi_squared_categorical_feature_selection(df_train, categorical_columns, target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Test on Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "def anova_numerical_feature_selection(df, num_vars, target):\n",
    "    \"\"\"\n",
    "    Performs an ANOVA F-test for each numerical variable with the target.\n",
    "\n",
    "    :param df: DataFrame containing the data\n",
    "    :param num_vars: list of column names which are numerical\n",
    "    :param target: target variable name\n",
    "    :return: DataFrame with p-values and F statistic. Each row is a numerical variable, \n",
    "             columns are 'Variable', 'F statistic', 'p-value'.\n",
    "    \"\"\"\n",
    "    # Ensure the columns exist in the dataframe\n",
    "    for var in num_vars:\n",
    "        if var not in df.columns:\n",
    "            raise ValueError(f\"The column '{var}' does not exist in the dataframe.\")\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"The target column '{target}' does not exist in the dataframe.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for var in num_vars:\n",
    "        # Perform the ANOVA test\n",
    "        class_0 = df[df[target] == 0][var]\n",
    "        class_1 = df[df[target] == 1][var]\n",
    "        \n",
    "        f, p = f_oneway(class_0, class_1)\n",
    "\n",
    "        # Add the result to the list of results\n",
    "        results.append([var, f, p])\n",
    "\n",
    "    # Convert results to a DataFrame and return\n",
    "    results_df = pd.DataFrame(results, columns=['Variable', 'F statistic', 'p-value'])\n",
    "\n",
    "    # Sort by p-value in ascending order\n",
    "    results_df = results_df.sort_values(by='p-value')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = get_numerical_columns(df_train)\n",
    "anova_numerical_feature_selection(df_train, numerical_columns, target=target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Correlation of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_heatmap(df, declutter=False):\n",
    "    # Compute Pearson correlations\n",
    "    correlations = df.corr(method='pearson')\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # If declutter option is true, do not show correlation coefficients and variable names\n",
    "    if declutter:\n",
    "        sns.heatmap(correlations, cmap='coolwarm', vmin=-1, vmax=1, ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "    else:\n",
    "        sns.heatmap(correlations, cmap='coolwarm', vmin=-1, vmax=1, annot=True, fmt=\".2f\", ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "\n",
    "    # Rotate the x labels if declutter is False\n",
    "    if not declutter:\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation_heatmap(df_train, declutter=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations of Numerical Features with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_target_correlation(df, target_col, variables, declutter=False):\n",
    "    \"\"\"\n",
    "    Create a bar plot showing the correlation of selected features with the target variable.\n",
    "\n",
    "    :param df: DataFrame containing the data\n",
    "    :param target_col: Name of the target column\n",
    "    :param variables: List of feature column names\n",
    "    :param declutter: Boolean value indicating whether to declutter the x-axis labels (default: False)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"The target column '{target_col}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Filter numerical features\n",
    "    numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Filter selected variables\n",
    "    selected_features = numerical_features[variables]\n",
    "\n",
    "    # Perform check if all selected features are numerical\n",
    "    if not set(variables).issubset(selected_features.columns):\n",
    "        raise ValueError(\"Selected features contain non-numerical columns.\")\n",
    "\n",
    "    # Add the target variable to the selected features DataFrame\n",
    "    selected_features[target_col] = df[target_col]\n",
    "\n",
    "    # Compute correlations with the target variable\n",
    "    correlations = selected_features.corr()[target_col].drop(target_col)\n",
    "\n",
    "    # Sort correlations in descending order\n",
    "    correlations = correlations.sort_values(ascending=False)\n",
    "\n",
    "    # Create the bar plot with adjusted width and ordered bars\n",
    "    plt.figure()\n",
    "    ax = sns.barplot(x=correlations.values, y=correlations.index, palette='coolwarm_r', order=correlations.index)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.xlabel(None)\n",
    "    plt.ylabel(None)\n",
    "    plt.title(f\"Correlation of Features vs Target Variable ({target_col})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.legend().remove()\n",
    "\n",
    "    if declutter:\n",
    "        plt.ylabel(f\"{len(correlations)} Features\")\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    else:\n",
    "        for i, v in enumerate(correlations.values):\n",
    "            ax.text(v + 0.01, i, str(round(v, 2)), va='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "numerical_columns = get_numerical_columns(df_train)\n",
    "plot_feature_target_correlation(df_train, 'default', numerical_columns, declutter=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_categorical_features = ['term', 'sub_grade', 'emp_length', 'inq_last_6mths', 'addr_state']\n",
    "drop_numerical_features = ['installment', 'mths_since_last_credit_pull_d', 'total_rec_int',\n",
    "                           'mths_since_earliest_cr_line', 'mths_since_issue_d', 'open_acc',\n",
    "                           'pub_rec']\n",
    "df_train.drop(columns=drop_categorical_features+drop_numerical_features, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WoE and IV of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_woe_iv(df, target_column, features=None):\n",
    "    \"\"\"\n",
    "    Calculate the Weight of Evidence (WoE) and Information Value (IV)\n",
    "    of categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed. It should contain the target column.\n",
    "    target_column (str): Name of the target column in the DataFrame.\n",
    "    features (list, optional): List of feature names for which WoE and IV is to be calculated. \n",
    "                               If None, all features in df will be used.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the WoE and IV for each category of the feature(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no specific features specified, use all columns in the DataFrame\n",
    "    if features is None:\n",
    "        features = df.drop(target_column, axis=1).columns.tolist()\n",
    "\n",
    "    # Create a dataframe to store WoE and IV values\n",
    "    master = []\n",
    "    \n",
    "    for feature in features:\n",
    "        lst = []\n",
    "        \n",
    "        # For each unique category in the feature\n",
    "        for val in df[feature].unique():\n",
    "            lst.append({\n",
    "                'Variable': feature,\n",
    "                'Value': val,\n",
    "                'All': df[df[feature] == val].count()[feature],\n",
    "                'Good': df[(df[feature] == val) & (df[target_column] == 0)].count()[feature],\n",
    "                'Bad': df[(df[feature] == val) & (df[target_column] == 1)].count()[feature]\n",
    "            })\n",
    "            \n",
    "        dset = pd.DataFrame(lst)\n",
    "        \n",
    "        # Calculate WoE and IV\n",
    "        dset['Distr_Good'] = dset['Good'] / dset['Good'].sum()\n",
    "        dset['Distr_Bad'] = dset['Bad'] / dset['Bad'].sum()\n",
    "        dset['WoE'] = np.log(dset['Distr_Good'] / dset['Distr_Bad'])\n",
    "        dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n",
    "\n",
    "        master.append(dset)\n",
    "    \n",
    "    master_dset = pd.concat(master, ignore_index=True)\n",
    "    \n",
    "    return master_dset.sort_values(by=['Variable', 'WoE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['grade', 'purpose', 'verification_status', 'home_ownership']\n",
    "woe_iv_df_numerical = calculate_woe_iv(df_train, target_column, categorical_columns)\n",
    "display(woe_iv_df_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WoEandIVPlots import WoEandIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": categorical_columns,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WoE and IV of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_numerical_variables(df, columns_list, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Bin the specified numerical columns into categories.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    bins (int or sequence, optional): Number of bins to create, or a sequence representing the bins.\n",
    "    labels (list, optional): Labels for the bins. Must be the same length as the resulting bins.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the new binned columns.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in columns_list:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # Convert the bin intervals to strings\n",
    "            df_copy[column+'_bin'] = pd.cut(df[column], bins=bins, labels=labels).astype(str)\n",
    "        else:\n",
    "            raise ValueError(f\"Column {column} is not numerical.\")\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin numerical features for WoE and IV analysis\n",
    "#numerical_features_to_bin = get_numerical_columns(df_train)\n",
    "#df_train = bin_numerical_variables(df_train, numerical_features_to_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_columns = [f+'_bin' for f in numerical_features_to_bin]\n",
    "#woe_iv_df_numerical = calculate_woe_iv(df_train, target_column, categorical_columns)\n",
    "#display(woe_iv_df_numerical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_variables(df, columns_list):\n",
    "    \"\"\"\n",
    "    Generate dummy variables for specified columns in the DataFrame,\n",
    "    concatenate them with the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): DataFrame after processing.\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column + \"_\", drop_first=False)\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "categorical_columns = ['purpose', 'grade', 'verification_status', 'home_ownership']\n",
    "df_train = add_dummy_variables(df_train, categorical_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "clean_date_columns(df_test, ['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "clean_rate_columns(df_test, 'int_rate')\n",
    "clean_rate_columns(df_test, 'revol_util')\n",
    "clean_emp_length_column(df_test, 'emp_length')\n",
    "clean_term_column(df_test, 'term')\n",
    "clean_inq_last_6mths(df_test, 'inq_last_6mths')\n",
    "\n",
    "# Drop NaN values \n",
    "df_test.dropna(subset=['mths_since_last_pymnt_d', 'mths_since_last_credit_pull_d'], inplace=True)\n",
    "\n",
    "# Drop features\n",
    "df_test.drop(columns=drop_categorical_features + drop_numerical_features, inplace=True)\n",
    "\n",
    "# Dummy features \n",
    "categorical_columns = ['purpose', 'grade', 'verification_status', 'home_ownership']\n",
    "df_test = add_dummy_variables(df_test, categorical_columns)\n",
    "\n",
    "# Reindex df_train and df_test\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train = df_train.drop(target_column, axis=1)  \n",
    "y_train = df_train[target_column]  \n",
    "\n",
    "X_test = df_test.drop(target_column, axis=1)  \n",
    "y_test = df_test[target_column]\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "model_fit = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that X_test and y_test are your testing data and labels\n",
    "y_pred = model_fit.predict(X_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_fit.feature_importances_\n",
    "feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "display(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features\n",
    "selected_features = feature_importances.head(15)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = selected_features['Feature'].tolist()\n",
    "print(selected_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Filter training and test data\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "# Instantiate the model\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = log_reg.coef_\n",
    "intercept = log_reg.intercept_\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients: \", coefficients)\n",
    "print(\"Intercept: \", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Model Fit Coefficients to Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate odds ratios\n",
    "odds_ratios = np.exp(coefficients).reshape(-1)\n",
    "\n",
    "# Define the scaling factor and base points\n",
    "scaling_factor = 20 / np.log(2)\n",
    "base_points = 500\n",
    "\n",
    "# Calculate the scores for each coefficient\n",
    "scores = scaling_factor * np.log(odds_ratios)\n",
    "scores = base_points - scores\n",
    "\n",
    "# Print the scores for each coefficient\n",
    "for feature, score in zip(selected_features, scores):\n",
    "    print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of Numerical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 'df' is your DataFrame and 'column_name' is the name of the column\n",
    "unique_values = df['inq_last_6mths'].unique()\n",
    "print(unique_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Cardinality of Categorical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots of Categorical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Ratios by Categorical Feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.DefaultRatioBarPlots import DefaultRatioBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": target_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = DefaultRatioBarPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Bar Plots of Default Ratios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateFeaturesBarPlots import BivariateFeaturesBarPlots\n",
    "\n",
    "# Pass target column to validmind dataset\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure the metric\n",
    "features_pairs = {'home_ownership': 'grade', \n",
    "                  'purpose': 'grade',\n",
    "                  'grade': 'verification_status'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "}\n",
    "\n",
    "metric = BivariateFeaturesBarPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots by Default Status"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateScatterPlots import BivariateScatterPlots\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateScatterPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Histograms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateHistograms import BivariateHistograms\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateHistograms(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Engineering "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dummy Catergorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_variables(df, columns_list):\n",
    "    \"\"\"\n",
    "    Generate dummy variables for specified columns in the DataFrame,\n",
    "    concatenate them with the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column + \":\", drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = add_dummy_variables(df_train, ['grade', 'home_ownership', 'verification_status', 'purpose'])\n",
    "# df_test = add_dummy_variables(df_test, ['grade', 'home_ownership', 'verification_status', 'purpose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the X_test DataFrame to match the column structure of the X_train DataFrame\n",
    "# df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight of Evidence (WoE) Binning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a modelling perspective, the **WoE** allows us to transform raw variables into a format which provides a more robust base for statistical analysis. Specifically, the WoE measures the predictive power of an individual class of a categorical variable, distinguishing between 'good' (non-defaulters) and 'bad' (defaulters) risks. This is accomplished by comparing the distribution of 'good' and 'bad' risks within a specific category to the overall 'good'/'bad' distribution. If the 'good'/'bad' ratio of a particular category is significantly divergent from the overall ratio, it suggests that category is a strong predictor of credit risk.\n",
    "\n",
    "**Information Value (IV)**, on the other hand, is a fundamental metric we use to quantify the predictive power of each input variable in our scorecards. The IV is calculated by taking the sum of the differences between the WoE of each category and the overall WoE, multiplied by the WoE of that category. In other words, IV measures the total amount of 'information' or predictive power a variable brings to the model. For example, variables with an IV between 0.1 and 0.3 provide a weak predictive power, those between 0.3 and 0.5 a medium predictive power, and those with an IV greater than 0.5 have strong predictive power. Therefore, we utilize the IV to prioritize variables for inclusion in the model and to ensure the model's stability and accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WoE and IV for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_woe_iv_df = calculate_woe_iv(df_train, target_column, categorical_features)\n",
    "#display(categorical_woe_iv_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WoEandIVPlots import WoEandIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": categorical_features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WoE and IV for Numerical Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning numerical features for WOE and IV calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_numerical_variables(df, columns_list, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Bin the specified numerical columns into categories.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    bins (int or sequence, optional): Number of bins to create, or a sequence representing the bins.\n",
    "    labels (list, optional): Labels for the bins. Must be the same length as the resulting bins.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the new binned columns.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in columns_list:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # Convert the bin intervals to strings\n",
    "            df_copy[column+'_bin'] = pd.cut(df[column], bins=bins, labels=labels).astype(str)\n",
    "        else:\n",
    "            raise ValueError(f\"Column {column} is not numerical.\")\n",
    "        \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_to_bin = ['loan_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'emp_length', 'annual_inc', 'dti', 'inq_last_6mths', 'open_acc', 'total_acc']\n",
    "df_train = bin_numerical_variables(df_train, numerical_features_to_bin)\n",
    "\n",
    "# Create a list of binned features\n",
    "binned_numerical_features = [f+'_bin' for f in numerical_features]\n",
    "\n",
    "# Calculate WoE and IV for the binned features\n",
    "woe_iv_df_numerical = calculate_woe_iv(df_train, target_column, binned_numerical_features)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# display(woe_iv_df_numerical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": binned_numerical_features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dummy Features from Binned Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binned_numerical_columns = ['loan_amnt_bin', 'funded_amnt_inv_bin', 'int_rate_bin', 'installment_bin', 'emp_length_bin',\n",
    "#                                'annual_inc_bin', 'dti_bin', 'inq_last_6mths_bin', 'open_acc_bin', 'total_acc_bin']\n",
    "#df_train = add_dummy_variables(df_train, binned_numerical_columns)\n",
    "#df_train.drop(columns=binned_numerical_columns, inplace=True)                          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Binned Datetime Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new numerical features from datetime columns for WoE and IV analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume today's date to calculate years since.\n",
    "today = pd.to_datetime('today')\n",
    "\n",
    "def add_years_since_issue(df, issue_date_column='issue_d'):\n",
    "    \"\"\"\n",
    "    Calculates the number of years since the loan was issued.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in years\n",
    "    df['years_since_issue'] = (today - df[issue_date_column]).dt.days / 365.25\n",
    "\n",
    "def add_credit_history_length(df, earliest_credit_column='earliest_cr_line'):\n",
    "    \"\"\"\n",
    "    Calculates the length of the borrower's credit history in years.\n",
    "    \"\"\"\n",
    "    # Calculate the credit history length\n",
    "    df['credit_history_length'] = (today - df[earliest_credit_column]).dt.days / 365.25\n",
    "\n",
    "add_years_since_issue(df_train)\n",
    "add_credit_history_length(df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add binned numerical features `years_since_issue_bin` and `credit_history_length_bin` to `df_train` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_features = ['years_since_issue', 'credit_history_length']\n",
    "df_train = bin_numerical_variables(df_train, datetime_features)\n",
    "\n",
    "# Create a list of binned features\n",
    "binned_datetime_features = [f+'_bin' for f in datetime_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": binned_datetime_features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dummy features for datetime variables, keep the original datetime variables, and drop binned features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = add_dummy_variables(df_train, binned_datetime_features)\n",
    "df_train.drop(columns=binned_datetime_features, inplace=True)\n",
    "df_train.drop(columns=['issue_d', 'earliest_cr_line'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_and_plot_correlations(df, num_ranges):\n",
    "    # Compute Pearson correlations\n",
    "    correlations = df.corr(method='pearson')\n",
    "\n",
    "    # Flatten the correlation matrix and remove self correlations\n",
    "    corr_values = correlations.values.flatten()\n",
    "    corr_values = corr_values[~np.isnan(corr_values)]\n",
    "    corr_values = corr_values[corr_values != 1]\n",
    "\n",
    "    # Define the ranges for histograms\n",
    "    range_values = np.linspace(-1, 1, num_ranges + 1)\n",
    "\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_rows = (num_ranges + 1) // 2\n",
    "    num_cols = 2\n",
    "\n",
    "    # Create the subplots\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, squeeze=False)\n",
    "\n",
    "    # Plot histograms for each range\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if i < num_ranges:\n",
    "            lower_bound = range_values[i]\n",
    "            upper_bound = range_values[i + 1]\n",
    "\n",
    "            ax.hist(corr_values[(corr_values >= lower_bound) & (corr_values < upper_bound)], bins=50)\n",
    "            ax.set_xlabel('Correlation')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Correlations {lower_bound} to {upper_bound}')\n",
    "            ax.tick_params(axis='x', rotation=90)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    # Remove any empty subplots if the number of ranges is odd\n",
    "    if num_ranges % 2 != 0:\n",
    "        axs[-1, -1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "compute_and_plot_correlations(df_train, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of Features with the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_and_plot_correlations_with_target(df, target_col, num_ranges):\n",
    "    # Compute Pearson correlations with the target variable\n",
    "    correlations = df.corr()[target_col].drop(target_col)\n",
    "    \n",
    "    # Remove missing correlations\n",
    "    correlations = correlations.dropna()\n",
    "\n",
    "    # Define the ranges for histograms\n",
    "    range_values = np.linspace(-1, 1, num_ranges + 1)\n",
    "\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_rows = (num_ranges + 1) // 2\n",
    "    num_cols = 2\n",
    "\n",
    "    # Create the subplots\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, squeeze=False)\n",
    "\n",
    "    # Plot histograms for each range\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if i < num_ranges:\n",
    "            lower_bound = range_values[i]\n",
    "            upper_bound = range_values[i + 1]\n",
    "\n",
    "            ax.hist(correlations[(correlations >= lower_bound) & (correlations < upper_bound)], bins=50)\n",
    "            ax.set_xlabel('Correlation')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Correlations {lower_bound} to {upper_bound}')\n",
    "            ax.tick_params(axis='x', rotation=90)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    # Remove any empty subplots if the number of ranges is odd\n",
    "    if num_ranges % 2 != 0:\n",
    "        axs[-1, -1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "corr_with_target = compute_and_plot_correlations_with_target(df_train, 'default', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# We can now evaluate on the test set\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range', 'installment']  # Added 'installment'\n",
    "\n",
    "# Handle categorical features\n",
    "df_encoded = pd.get_dummies(df_multivariate, columns=categorical_features)\n",
    "\n",
    "# Split the data\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Define the model\n",
    "glm_model_fit = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = glm_model_fit.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = sm.add_constant(X_test)  # Adding a constant to the test data\n",
    "y_pred = results.predict(X_test)\n",
    "\n",
    "# You can then further analyze y_pred to measure model performance on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale variable X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Scale your variables\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y, X_scaled, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ValidMind Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training and testing datasets for model A\n",
    "vm_train_ds = vm.init_dataset(dataset=X_train, type=\"generic\", target_column='loan_status')\n",
    "vm_test_ds = vm.init_dataset(dataset=X_test, type=\"generic\", target_column='loan_status')\n",
    "\n",
    "# Initialize model A\n",
    "vm_model_A = vm.init_model(\n",
    "    model = glm_model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
