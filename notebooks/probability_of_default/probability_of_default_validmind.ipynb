{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Default Model using ValidMind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Connect Notebook to ValidMind Project\n",
    "- Step 2: Import Raw Data\n",
    "- Step 3: Data Description on Raw Data\n",
    "- Step 4: Data Preprocessing\n",
    "- Step 5: Data Description on Preprocessed Data \n",
    "- Step 6: Univariate Analysis\n",
    "- Step 7: Multivariate Analysis\n",
    "- Step 8: Model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect Notebook to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect Notebook to ValidMind Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 14:37:26,978 - INFO - api_client - Connected to ValidMind. Project: [3] PD Model - Initial Validation (cliwzqjgv00001fy6869rlav9)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_37c54 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_37c54_row0_col0, #T_37c54_row0_col1, #T_37c54_row0_col2, #T_37c54_row0_col3, #T_37c54_row1_col0, #T_37c54_row1_col1, #T_37c54_row1_col2, #T_37c54_row1_col3, #T_37c54_row2_col0, #T_37c54_row2_col1, #T_37c54_row2_col2, #T_37c54_row2_col3, #T_37c54_row3_col0, #T_37c54_row3_col1, #T_37c54_row3_col2, #T_37c54_row3_col3, #T_37c54_row4_col0, #T_37c54_row4_col1, #T_37c54_row4_col2, #T_37c54_row4_col3, #T_37c54_row5_col0, #T_37c54_row5_col1, #T_37c54_row5_col2, #T_37c54_row5_col3, #T_37c54_row6_col0, #T_37c54_row6_col1, #T_37c54_row6_col2, #T_37c54_row6_col3, #T_37c54_row7_col0, #T_37c54_row7_col1, #T_37c54_row7_col2, #T_37c54_row7_col3, #T_37c54_row8_col0, #T_37c54_row8_col1, #T_37c54_row8_col2, #T_37c54_row8_col3, #T_37c54_row9_col0, #T_37c54_row9_col1, #T_37c54_row9_col2, #T_37c54_row9_col3, #T_37c54_row10_col0, #T_37c54_row10_col1, #T_37c54_row10_col2, #T_37c54_row10_col3, #T_37c54_row11_col0, #T_37c54_row11_col1, #T_37c54_row11_col2, #T_37c54_row11_col3, #T_37c54_row12_col0, #T_37c54_row12_col1, #T_37c54_row12_col2, #T_37c54_row12_col3, #T_37c54_row13_col0, #T_37c54_row13_col1, #T_37c54_row13_col2, #T_37c54_row13_col3, #T_37c54_row14_col0, #T_37c54_row14_col1, #T_37c54_row14_col2, #T_37c54_row14_col3, #T_37c54_row15_col0, #T_37c54_row15_col1, #T_37c54_row15_col2, #T_37c54_row15_col3, #T_37c54_row16_col0, #T_37c54_row16_col1, #T_37c54_row16_col2, #T_37c54_row16_col3, #T_37c54_row17_col0, #T_37c54_row17_col1, #T_37c54_row17_col2, #T_37c54_row17_col3, #T_37c54_row18_col0, #T_37c54_row18_col1, #T_37c54_row18_col2, #T_37c54_row18_col3, #T_37c54_row19_col0, #T_37c54_row19_col1, #T_37c54_row19_col2, #T_37c54_row19_col3, #T_37c54_row20_col0, #T_37c54_row20_col1, #T_37c54_row20_col2, #T_37c54_row20_col3, #T_37c54_row21_col0, #T_37c54_row21_col1, #T_37c54_row21_col2, #T_37c54_row21_col3, #T_37c54_row22_col0, #T_37c54_row22_col1, #T_37c54_row22_col2, #T_37c54_row22_col3, #T_37c54_row23_col0, #T_37c54_row23_col1, #T_37c54_row23_col2, #T_37c54_row23_col3, #T_37c54_row24_col0, #T_37c54_row24_col1, #T_37c54_row24_col2, #T_37c54_row24_col3, #T_37c54_row25_col0, #T_37c54_row25_col1, #T_37c54_row25_col2, #T_37c54_row25_col3, #T_37c54_row26_col0, #T_37c54_row26_col1, #T_37c54_row26_col2, #T_37c54_row26_col3, #T_37c54_row27_col0, #T_37c54_row27_col1, #T_37c54_row27_col2, #T_37c54_row27_col3, #T_37c54_row28_col0, #T_37c54_row28_col1, #T_37c54_row28_col2, #T_37c54_row28_col3, #T_37c54_row29_col0, #T_37c54_row29_col1, #T_37c54_row29_col2, #T_37c54_row29_col3, #T_37c54_row30_col0, #T_37c54_row30_col1, #T_37c54_row30_col2, #T_37c54_row30_col3, #T_37c54_row31_col0, #T_37c54_row31_col1, #T_37c54_row31_col2, #T_37c54_row31_col3, #T_37c54_row32_col0, #T_37c54_row32_col1, #T_37c54_row32_col2, #T_37c54_row32_col3, #T_37c54_row33_col0, #T_37c54_row33_col1, #T_37c54_row33_col2, #T_37c54_row33_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_37c54\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_37c54_level0_col0\" class=\"col_heading level0 col0\" >Test Type</th>\n",
       "      <th id=\"T_37c54_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_37c54_level0_col2\" class=\"col_heading level0 col2\" >Description</th>\n",
       "      <th id=\"T_37c54_level0_col3\" class=\"col_heading level0 col3\" >ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row0_col0\" class=\"data row0 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row0_col1\" class=\"data row0 col1\" >Skewness</td>\n",
       "      <td id=\"T_37c54_row0_col2\" class=\"data row0 col2\" >The skewness test measures the extent to which a distribution of\n",
       "    values differs from a normal distribution. A positive skew describes\n",
       "    a longer tail of values in the right and a negative skew describes a\n",
       "    longer tail of values in the left.</td>\n",
       "      <td id=\"T_37c54_row0_col3\" class=\"data row0 col3\" >validmind.data_validation.Skewness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row1_col0\" class=\"data row1 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row1_col1\" class=\"data row1 col1\" >Duplicates</td>\n",
       "      <td id=\"T_37c54_row1_col2\" class=\"data row1 col2\" >The duplicates test measures the number of duplicate rows found in\n",
       "    the dataset. If a primary key column is specified, the dataset is\n",
       "    checked for duplicate primary keys as well.</td>\n",
       "      <td id=\"T_37c54_row1_col3\" class=\"data row1 col3\" >validmind.data_validation.Duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row2_col0\" class=\"data row2 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row2_col1\" class=\"data row2 col1\" >DatasetDescription</td>\n",
       "      <td id=\"T_37c54_row2_col2\" class=\"data row2 col2\" >Collects a set of descriptive statistics for a dataset</td>\n",
       "      <td id=\"T_37c54_row2_col3\" class=\"data row2 col3\" >validmind.data_validation.DatasetDescription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row3_col0\" class=\"data row3 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row3_col1\" class=\"data row3 col1\" >ScatterPlot</td>\n",
       "      <td id=\"T_37c54_row3_col2\" class=\"data row3 col2\" >Generates a visual analysis of data by plotting a scatter plot matrix for all columns\n",
       "    in the dataset. The input dataset can have multiple columns (features) if necessary.</td>\n",
       "      <td id=\"T_37c54_row3_col3\" class=\"data row3 col3\" >validmind.data_validation.ScatterPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row4_col0\" class=\"data row4 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row4_col1\" class=\"data row4 col1\" >TimeSeriesOutliers</td>\n",
       "      <td id=\"T_37c54_row4_col2\" class=\"data row4 col2\" >Test that find outliers for time series data using the z-score method</td>\n",
       "      <td id=\"T_37c54_row4_col3\" class=\"data row4 col3\" >validmind.data_validation.TimeSeriesOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row5_col0\" class=\"data row5 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row5_col1\" class=\"data row5 col1\" >TabularCategoricalBarPlots</td>\n",
       "      <td id=\"T_37c54_row5_col2\" class=\"data row5 col2\" >Generates a visual analysis of categorical data by plotting bar plots.\n",
       "    The input dataset can have multiple categorical variables if necessary.\n",
       "    In this case, we produce a separate plot for each categorical variable.</td>\n",
       "      <td id=\"T_37c54_row5_col3\" class=\"data row5 col3\" >validmind.data_validation.TabularCategoricalBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row6_col0\" class=\"data row6 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row6_col1\" class=\"data row6 col1\" >AutoStationarity</td>\n",
       "      <td id=\"T_37c54_row6_col2\" class=\"data row6 col2\" >Automatically detects stationarity for each time series in a DataFrame\n",
       "    using the Augmented Dickey-Fuller (ADF) test.</td>\n",
       "      <td id=\"T_37c54_row6_col3\" class=\"data row6 col3\" >validmind.data_validation.AutoStationarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row7_col0\" class=\"data row7 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row7_col1\" class=\"data row7 col1\" >DescriptiveStatistics</td>\n",
       "      <td id=\"T_37c54_row7_col2\" class=\"data row7 col2\" >Collects a set of descriptive statistics for a dataset, both for\n",
       "    numerical and categorical variables</td>\n",
       "      <td id=\"T_37c54_row7_col3\" class=\"data row7 col3\" >validmind.data_validation.DescriptiveStatistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row8_col0\" class=\"data row8 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row8_col1\" class=\"data row8 col1\" >PearsonCorrelationMatrix</td>\n",
       "      <td id=\"T_37c54_row8_col2\" class=\"data row8 col2\" >Extracts the Pearson correlation coefficient for all pairs of numerical variables\n",
       "    in the dataset. This metric is useful to identify highly correlated variables\n",
       "    that can be removed from the dataset to reduce dimensionality.</td>\n",
       "      <td id=\"T_37c54_row8_col3\" class=\"data row8 col3\" >validmind.data_validation.PearsonCorrelationMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row9_col0\" class=\"data row9 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row9_col1\" class=\"data row9 col1\" >TabularNumericalHistograms</td>\n",
       "      <td id=\"T_37c54_row9_col2\" class=\"data row9 col2\" >Generates a visual analysis of numerical data by plotting the histogram.\n",
       "    The input dataset can have multiple numerical variables if necessary.\n",
       "    In this case, we produce a separate plot for each numerical variable.</td>\n",
       "      <td id=\"T_37c54_row9_col3\" class=\"data row9 col3\" >validmind.data_validation.TabularNumericalHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row10_col0\" class=\"data row10 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row10_col1\" class=\"data row10 col1\" >HighCardinality</td>\n",
       "      <td id=\"T_37c54_row10_col2\" class=\"data row10 col2\" >The high cardinality test measures the number of unique\n",
       "    values found in categorical columns.</td>\n",
       "      <td id=\"T_37c54_row10_col3\" class=\"data row10 col3\" >validmind.data_validation.HighCardinality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row11_col0\" class=\"data row11 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row11_col1\" class=\"data row11 col1\" >MissingValues</td>\n",
       "      <td id=\"T_37c54_row11_col2\" class=\"data row11 col2\" >Test that the number of missing values in the dataset across all features\n",
       "    is less than a threshold</td>\n",
       "      <td id=\"T_37c54_row11_col3\" class=\"data row11 col3\" >validmind.data_validation.MissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row12_col0\" class=\"data row12 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row12_col1\" class=\"data row12 col1\" >RollingStatsPlot</td>\n",
       "      <td id=\"T_37c54_row12_col2\" class=\"data row12 col2\" >This class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.</td>\n",
       "      <td id=\"T_37c54_row12_col3\" class=\"data row12 col3\" >validmind.data_validation.RollingStatsPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row13_col0\" class=\"data row13 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row13_col1\" class=\"data row13 col1\" >DatasetCorrelations</td>\n",
       "      <td id=\"T_37c54_row13_col2\" class=\"data row13 col2\" >Extracts the correlation matrix for a dataset. The following coefficients\n",
       "    are calculated:\n",
       "    - Pearson's R for numerical variables\n",
       "    - Cramer's V for categorical variables\n",
       "    - Correlation ratios for categorical-numerical variables</td>\n",
       "      <td id=\"T_37c54_row13_col3\" class=\"data row13 col3\" >validmind.data_validation.DatasetCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row14_col0\" class=\"data row14 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row14_col1\" class=\"data row14 col1\" >TabularDescriptionTables</td>\n",
       "      <td id=\"T_37c54_row14_col2\" class=\"data row14 col2\" >Collects a set of descriptive statistics for a tabular dataset, for\n",
       "    numerical, categorical and datetime variables.</td>\n",
       "      <td id=\"T_37c54_row14_col3\" class=\"data row14 col3\" >validmind.data_validation.TabularDescriptionTables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row15_col0\" class=\"data row15 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row15_col1\" class=\"data row15 col1\" >AutoMA</td>\n",
       "      <td id=\"T_37c54_row15_col2\" class=\"data row15 col2\" >Automatically detects the MA order of a time series using both BIC and AIC.</td>\n",
       "      <td id=\"T_37c54_row15_col3\" class=\"data row15 col3\" >validmind.data_validation.AutoMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row16_col0\" class=\"data row16 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row16_col1\" class=\"data row16 col1\" >UniqueRows</td>\n",
       "      <td id=\"T_37c54_row16_col2\" class=\"data row16 col2\" >Test that the number of unique rows is greater than a threshold</td>\n",
       "      <td id=\"T_37c54_row16_col3\" class=\"data row16 col3\" >validmind.data_validation.UniqueRows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row17_col0\" class=\"data row17 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row17_col1\" class=\"data row17 col1\" >TooManyZeroValues</td>\n",
       "      <td id=\"T_37c54_row17_col2\" class=\"data row17 col2\" >The zeros test finds columns that have too many zero values.</td>\n",
       "      <td id=\"T_37c54_row17_col3\" class=\"data row17 col3\" >validmind.data_validation.TooManyZeroValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row18_col0\" class=\"data row18 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row18_col1\" class=\"data row18 col1\" >HighPearsonCorrelation</td>\n",
       "      <td id=\"T_37c54_row18_col2\" class=\"data row18 col2\" >Test that the pairwise Pearson correlation coefficients between the\n",
       "    features in the dataset do not exceed a specified threshold.</td>\n",
       "      <td id=\"T_37c54_row18_col3\" class=\"data row18 col3\" >validmind.data_validation.HighPearsonCorrelation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row19_col0\" class=\"data row19 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row19_col1\" class=\"data row19 col1\" >ACFandPACFPlot</td>\n",
       "      <td id=\"T_37c54_row19_col2\" class=\"data row19 col2\" >Plots ACF and PACF for a given time series dataset.</td>\n",
       "      <td id=\"T_37c54_row19_col3\" class=\"data row19 col3\" >validmind.data_validation.ACFandPACFPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row20_col0\" class=\"data row20 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row20_col1\" class=\"data row20 col1\" >TimeSeriesFrequency</td>\n",
       "      <td id=\"T_37c54_row20_col2\" class=\"data row20 col2\" >Test that detect frequencies in the data</td>\n",
       "      <td id=\"T_37c54_row20_col3\" class=\"data row20 col3\" >validmind.data_validation.TimeSeriesFrequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row21_col0\" class=\"data row21 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row21_col1\" class=\"data row21 col1\" >DatasetSplit</td>\n",
       "      <td id=\"T_37c54_row21_col2\" class=\"data row21 col2\" >Attempts to extract information about the dataset split from the\n",
       "    provided training, test and validation datasets.</td>\n",
       "      <td id=\"T_37c54_row21_col3\" class=\"data row21 col3\" >validmind.data_validation.DatasetSplit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row22_col0\" class=\"data row22 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row22_col1\" class=\"data row22 col1\" >SpreadPlot</td>\n",
       "      <td id=\"T_37c54_row22_col2\" class=\"data row22 col2\" >This class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.</td>\n",
       "      <td id=\"T_37c54_row22_col3\" class=\"data row22 col3\" >validmind.data_validation.SpreadPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row23_col0\" class=\"data row23 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row23_col1\" class=\"data row23 col1\" >TimeSeriesLinePlot</td>\n",
       "      <td id=\"T_37c54_row23_col2\" class=\"data row23 col2\" >Generates a visual analysis of time series data by plotting the\n",
       "    raw time series. The input dataset can have multiple time series\n",
       "    if necessary. In this case we produce a separate plot for each time series.</td>\n",
       "      <td id=\"T_37c54_row23_col3\" class=\"data row23 col3\" >validmind.data_validation.TimeSeriesLinePlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row24_col0\" class=\"data row24 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row24_col1\" class=\"data row24 col1\" >AutoSeasonality</td>\n",
       "      <td id=\"T_37c54_row24_col2\" class=\"data row24 col2\" >Automatically detects the optimal seasonal order for a time series dataset\n",
       "    using the seasonal_decompose method.</td>\n",
       "      <td id=\"T_37c54_row24_col3\" class=\"data row24 col3\" >validmind.data_validation.AutoSeasonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row25_col0\" class=\"data row25 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row25_col1\" class=\"data row25 col1\" >EngleGrangerCoint</td>\n",
       "      <td id=\"T_37c54_row25_col2\" class=\"data row25 col2\" >Test for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.</td>\n",
       "      <td id=\"T_37c54_row25_col3\" class=\"data row25 col3\" >validmind.data_validation.EngleGrangerCoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row26_col0\" class=\"data row26 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row26_col1\" class=\"data row26 col1\" >TimeSeriesMissingValues</td>\n",
       "      <td id=\"T_37c54_row26_col2\" class=\"data row26 col2\" >Test that the number of missing values is less than a threshold</td>\n",
       "      <td id=\"T_37c54_row26_col3\" class=\"data row26 col3\" >validmind.data_validation.TimeSeriesMissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row27_col0\" class=\"data row27 col0\" >DatasetMetadata</td>\n",
       "      <td id=\"T_37c54_row27_col1\" class=\"data row27 col1\" >DatasetMetadata</td>\n",
       "      <td id=\"T_37c54_row27_col2\" class=\"data row27 col2\" >Custom class to collect a set of descriptive statistics for a dataset.\n",
       "    This class will log dataset metadata via `log_dataset` instead of a metric.\n",
       "    Dataset metadata is necessary to initialize dataset object that can be related\n",
       "    to different metrics and test results</td>\n",
       "      <td id=\"T_37c54_row27_col3\" class=\"data row27 col3\" >validmind.data_validation.DatasetMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row28_col0\" class=\"data row28 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row28_col1\" class=\"data row28 col1\" >TimeSeriesHistogram</td>\n",
       "      <td id=\"T_37c54_row28_col2\" class=\"data row28 col2\" >Generates a visual analysis of time series data by plotting the\n",
       "    histogram. The input dataset can have multiple time series if\n",
       "    necessary. In this case we produce a separate plot for each time series.</td>\n",
       "      <td id=\"T_37c54_row28_col3\" class=\"data row28 col3\" >validmind.data_validation.TimeSeriesHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row29_col0\" class=\"data row29 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row29_col1\" class=\"data row29 col1\" >LaggedCorrelationHeatmap</td>\n",
       "      <td id=\"T_37c54_row29_col2\" class=\"data row29 col2\" >Generates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.</td>\n",
       "      <td id=\"T_37c54_row29_col3\" class=\"data row29 col3\" >validmind.data_validation.LaggedCorrelationHeatmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row30_col0\" class=\"data row30 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row30_col1\" class=\"data row30 col1\" >SeasonalDecompose</td>\n",
       "      <td id=\"T_37c54_row30_col2\" class=\"data row30 col2\" >Calculates seasonal_decompose metric for each of the dataset features</td>\n",
       "      <td id=\"T_37c54_row30_col3\" class=\"data row30 col3\" >validmind.data_validation.SeasonalDecompose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row31_col0\" class=\"data row31 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_37c54_row31_col1\" class=\"data row31 col1\" >ClassImbalance</td>\n",
       "      <td id=\"T_37c54_row31_col2\" class=\"data row31 col2\" >The class imbalance test measures the disparity between the majority\n",
       "    class and the minority class in the target column.</td>\n",
       "      <td id=\"T_37c54_row31_col3\" class=\"data row31 col3\" >validmind.data_validation.ClassImbalance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row32_col0\" class=\"data row32 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row32_col1\" class=\"data row32 col1\" >AutoAR</td>\n",
       "      <td id=\"T_37c54_row32_col2\" class=\"data row32 col2\" >Automatically detects the AR order of a time series using both BIC and AIC.</td>\n",
       "      <td id=\"T_37c54_row32_col3\" class=\"data row32 col3\" >validmind.data_validation.AutoAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_37c54_row33_col0\" class=\"data row33 col0\" >Metric</td>\n",
       "      <td id=\"T_37c54_row33_col1\" class=\"data row33 col1\" >TabularDateTimeHistograms</td>\n",
       "      <td id=\"T_37c54_row33_col2\" class=\"data row33 col2\" >Generates a visual analysis of datetime data by plotting histograms of\n",
       "    differences between consecutive dates. The input dataset can have multiple\n",
       "    datetime variables if necessary. In this case, we produce a separate plot\n",
       "    for each datetime variable.</td>\n",
       "      <td id=\"T_37c54_row33_col3\" class=\"data row33 col3\" >validmind.data_validation.TabularDateTimeHistograms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x295f86320>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validmind.tests import list_tests, load_test, describe_test\n",
    "\n",
    "list_tests(filter=\"data_validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Raw Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Lending Club Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the zip file\n",
    "# filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2011/lending_club_loan_data_2007_2011.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# ONLY FOR TESTING\n",
    "\n",
    "\n",
    "# Perform operations on the DataFrame as needed\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Description on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.data_validation.metrics import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Unused Variables\n",
    "\n",
    "Remove all the **Demographic** and **Customer Behavioural** features which is of no use for default analysis for credit approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-required columns\n",
    "# id - not required\n",
    "# member_id - not required\n",
    "# acc_now_delinq - empty\n",
    "# funded_amnt - not useful, funded_amnt_inv is useful which is funded to person\n",
    "# emp_title - brand names not useful\n",
    "# pymnt_plan - fixed value as n for all\n",
    "# url - not useful\n",
    "# desc - can be applied some NLP but not for EDA\n",
    "# title - too many distinct values not useful\n",
    "# zip_code - complete zip is not available\n",
    "# delinq_2yrs - post approval feature\n",
    "# mths_since_last_delinq - only half values are there, not much information\n",
    "# mths_since_last_record - only 10% values are there\n",
    "# revol_bal - post/behavioural feature\n",
    "# initial_list_status - fixed value as f for all\n",
    "# out_prncp - post approval feature\n",
    "# out_prncp_inv - not useful as its for investors\n",
    "# total_pymnt - post approval feature\n",
    "# total_pymnt_inv - not useful as it is for investors\n",
    "# total_rec_prncp - post approval feature\n",
    "# total_rec_int - post approval feature\n",
    "# total_rec_late_fee - post approval feature\n",
    "# recoveries - post approval feature\n",
    "# collection_recovery_fee - post approval feature\n",
    "# last_pymnt_d - post approval feature\n",
    "# last_credit_pull_d - irrelevant for approval\n",
    "# last_pymnt_amnt - post feature\n",
    "# next_pymnt_d - post feature\n",
    "# collections_12_mths_ex_med - only 1 value \n",
    "# policy_code - only 1 value\n",
    "# acc_now_delinq - single valued\n",
    "# application_type - single\n",
    "# pub_rec_bankruptcies - single valued for more than 99%\n",
    "# addr_state - may not depend on location as its in financial domain\n",
    "\n",
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"pymnt_plan\", \"url\", \"desc\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"initial_list_status\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\",\n",
    "                    \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\",\n",
    "                    \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"next_pymnt_d\", \"last_credit_pull_d\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"application_type\", \"addr_state\"]\n",
    "df_selected_vars = df.drop(columns=unused_variables)\n",
    "print(\"Features we are left with\",list(df_selected_vars.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_vars.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process `emp_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_emp_length(df, column='emp_length'):\n",
    "    # Define a mapping from original string values to numeric values\n",
    "    mapping = {'10+ years': 10, '< 1 year': 0, '1 year': 1, '2 years': 2, \n",
    "               '3 years': 3, '4 years': 4, '5 years': 5, '6 years': 6, \n",
    "               '7 years': 7, '8 years': 8, '9 years': 9, np.nan: np.nan}\n",
    "    \n",
    "    # Apply the mapping to the specified column\n",
    "    df[column] = df[column].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_vars = process_emp_length(df_selected_vars, 'emp_length')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df, columns):\n",
    "    # Specify the date format\n",
    "    date_format = \"%b-%y\"\n",
    "\n",
    "    # Iterate over the specified columns and convert to datetime\n",
    "    for column in columns:\n",
    "        df[column] = pd.to_datetime(df[column], format=date_format)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the specified columns to datetime\n",
    "columns_to_convert = ['issue_d']\n",
    "df_dates_fixed = convert_to_datetime(df_selected_vars, columns_to_convert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    return variables_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df_dates_fixed, min_missing_count)\n",
    "df_no_missing = df_dates_fixed.drop(columns=variables_to_drop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df_no_missing.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Loan Status `Current` \n",
    "\n",
    "Removing records with loan status as **`Current`**, as the loan is currently running and we can’t infer any information regarding default from such loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with loan_status as \"Current\"\n",
    "df_no_current = df_no_missing[df_no_missing[\"loan_status\"].apply(lambda x: False if x == \"Current\" else True)]\n",
    "\n",
    "# Update loan_status as Fully Paid to 0 and Charged Off to 1\n",
    "df_no_current[\"loan_status\"] = df_no_current[\"loan_status\"].apply(lambda x: 0 if x == \"Fully Paid\" else 1)\n",
    "\n",
    "# Convert 'emp_length' to string type\n",
    "df_no_current[\"emp_length\"] = df_no_current[\"emp_length\"].astype(str)\n",
    "\n",
    "# Update emp_length feature with continuous values as int\n",
    "# where (< 1 year) is assumed as 0 and 10+ years is assumed as 10 and rest are stored as their magnitude\n",
    "df_no_current[\"emp_length\"] = pd.to_numeric(df_no_current[\"emp_length\"].apply(lambda x: 0 if \"<\" in x else (x.split('+')[0] if \"+\" in x else x.split()[0])))\n",
    "\n",
    "# Look through the purpose value counts\n",
    "loan_purpose_values = df_no_current[\"purpose\"].value_counts() * 100 / df_no_current.shape[0]\n",
    "\n",
    "# Remove rows with less than 1% of value counts in particular purpose \n",
    "loan_purpose_delete = loan_purpose_values[loan_purpose_values < 1].index.values\n",
    "df_processed = df_no_current[[False if p in loan_purpose_delete else True for p in df_no_current[\"purpose\"]]]\n",
    "\n",
    "# Update int_rate, revol_util without % sign and as numeric type\n",
    "df_processed[\"int_rate\"] = pd.to_numeric(df_processed[\"int_rate\"].apply(lambda x:x.split('%')[0]))\n",
    "df_processed[\"revol_util\"] = pd.to_numeric(df_processed[\"revol_util\"].apply(lambda x:x.split('%')[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add New Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting month and year from issue_date\n",
    "df_processed['month'] = df_processed['issue_d'].apply(lambda x: x.month)\n",
    "df_processed['year'] = df_processed['issue_d'].apply(lambda x: x.year)\n",
    "\n",
    "# Get year from issue_d and replace the same\n",
    "df_processed[\"earliest_cr_line\"] = pd.to_numeric(df_processed[\"earliest_cr_line\"].apply(lambda x:x.split('-')[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning Continuous Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `loan_amnt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for loan_amnt range\n",
    "bins = [0, 5000, 10000, 15000, 20000, 25000, 36000]\n",
    "bucket_l = ['0-5000', '5000-10000', '10000-15000', '15000-20000', '20000-25000','25000+']\n",
    "df_processed['loan_amnt_range'] = pd.cut(df_processed['loan_amnt'], bins, labels=bucket_l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `int_rate` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'int_rate' to numeric\n",
    "df_processed['int_rate'] = pd.to_numeric(df_processed['int_rate'], errors='coerce')\n",
    "\n",
    "# Create bins for int_rate range\n",
    "bins = [0, 7.5, 10, 12.5, 15, 100]\n",
    "bucket_l = ['0-7.5', '7.5-10', '10-12.5', '12.5-15', '15+']\n",
    "\n",
    "# Using pd.cut to create 'int_rate_range' column\n",
    "df_processed['int_rate_range'] = pd.cut(df_processed['int_rate'], bins, labels=bucket_l)\n",
    "\n",
    "# Convert NaN to 'Unknown'\n",
    "df_processed['int_rate_range'] = df_processed['int_rate_range'].cat.add_categories('Unknown')\n",
    "df_processed['int_rate_range'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `annual_inc` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for annual_inc range\n",
    "bins = [0, 25000, 50000, 75000, 100000, 1000000]\n",
    "bucket_l = ['0-25000', '25000-50000', '50000-75000', '75000-100000', '100000+']\n",
    "df_processed['annual_inc_range'] = pd.cut(df_processed['annual_inc'], bins, labels=bucket_l)\n",
    "\n",
    "# Convert NaN to 'Unknown'\n",
    "df_processed['annual_inc_range'] = df_processed['annual_inc_range'].cat.add_categories('Unknown')\n",
    "df_processed['annual_inc_range'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `installment` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for installment range\n",
    "def installment(n):\n",
    "    if n <= 200:\n",
    "        return 'low'\n",
    "    elif n > 200 and n <=500:\n",
    "        return 'medium'\n",
    "    elif n > 500 and n <=800:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very high'\n",
    "\n",
    "df_processed['installment'] = df_processed['installment'].apply(lambda x: installment(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `dti` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for dti range\n",
    "bins = [-1, 5.00, 10.00, 15.00, 20.00, 25.00, 50.00]\n",
    "bucket_l = ['0-5%', '5-10%', '10-15%', '15-20%', '20-25%', '25%+']\n",
    "df_processed['dti_range'] = pd.cut(df_processed['dti'], bins, labels=bucket_l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Description on Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_df = vm.init_dataset(dataset=df_processed,\n",
    "                        target_column='loan_status')\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for amount of defaults in the data using countplot\n",
    "plt.figure()\n",
    "sns.countplot(y=\"loan_status\", data=df_processed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "metric = ClassImbalance(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.MissingValues import MissingValues\n",
    "metric = MissingValues(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularNumericalHistograms\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()\n",
    "\n",
    "HighCardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularDateTimeHistograms\n",
    "metric = TabularDateTimeHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan Defaults Ratio by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import LoanDefaultRatio\n",
    "\n",
    "# Select numerical and categorical features \n",
    "numerical_features = ['emp_length', 'month', 'year', 'earliest_cr_line', 'inq_last_6mths', 'revol_util', 'total_acc',\n",
    "                       'loan_amnt_range', 'int_rate_range', 'dti_range', 'installment', 'annual_inc_range']\n",
    "categorical_features = ['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status', 'purpose', 'open_acc', 'pub_rec']\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"loan_status_col\": \"loan_status\",\n",
    "    \"columns\": numerical_features + categorical_features\n",
    "}\n",
    "\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "metric = LoanDefaultRatio(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select variables for multivariate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = ['loan_status']\n",
    "selected_features = ['term', 'grade', 'purpose', 'pub_rec',\n",
    "                      'revol_util', 'funded_amnt_inv', 'int_rate', \n",
    "                      'annual_inc_range', 'dti', 'installment',\n",
    "                      'loan_amnt_range', 'annual_inc', 'loan_amnt',\n",
    "                      'earliest_cr_line']\n",
    "df_multivariate = df_processed.loc[:, selected_features + target_variable]\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_multivariate)\n",
    "test_context = TestContext(dataset=vm_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metric as custom test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Figure, Metric\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BivariateBarPlots(Metric):\n",
    "    \"\"\"\n",
    "    Generates a visual analysis of categorical data by plotting bivariate bar plots.\n",
    "    The input dataset and variable_pairs are required.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"bivariate_bar_plots\"\n",
    "    required_context = [\"dataset\"]\n",
    "    default_params = {\"variable_pairs\": None, \"loan_status_filter\": None}\n",
    "\n",
    "    def plot_bivariate_bar(self, variable_pairs, loan_status_filter):\n",
    "        figures = []\n",
    "        for x, hue in variable_pairs.items():\n",
    "            df = self.dataset.df\n",
    "            if loan_status_filter:\n",
    "                df = df[df[\"loan_status\"].isin(loan_status_filter)]\n",
    "\n",
    "            means = df.groupby([x, hue])[\"loan_status\"].mean().unstack().reset_index()\n",
    "            hue_categories = means.columns[1:]\n",
    "\n",
    "            n = len(hue_categories)\n",
    "            width = 1 / (n + 1)\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            color_palette = {\n",
    "                category: color\n",
    "                for category, color in zip(\n",
    "                    hue_categories, plt.cm.get_cmap(\"tab10\").colors\n",
    "                )\n",
    "            }\n",
    "\n",
    "            for i, hue_category in enumerate(hue_categories):\n",
    "                plt.bar(\n",
    "                    np.arange(len(means)) + i * width,\n",
    "                    means[hue_category],\n",
    "                    color=color_palette[hue_category],\n",
    "                    alpha=0.7,\n",
    "                    label=hue_category,\n",
    "                    width=width,\n",
    "                )\n",
    "\n",
    "            plt.title(x + \" by \" + hue)\n",
    "            plt.xlabel(x)\n",
    "            plt.ylabel(\"Loan Default Ratio\")\n",
    "            plt.xticks(ticks=np.arange(len(means)), labels=means[x], rotation=90)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            figures.append(\n",
    "                Figure(\n",
    "                    for_object=self, key=f\"{self.key}:{x}_{hue}\", figure=plt.figure()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        return figures\n",
    "\n",
    "    def run(self):\n",
    "        variable_pairs = self.params[\"variable_pairs\"]\n",
    "        loan_status_filter = self.params[\"loan_status_filter\"]\n",
    "\n",
    "        figures = self.plot_bivariate_bar(variable_pairs, loan_status_filter)\n",
    "\n",
    "        return self.cache_results(figures=figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: this does not work - from validmind.tests.data_validation.BivariateBarPlots import BivariateBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "variable_pairs = {'annual_inc_range': 'purpose', \n",
    "                  'term': 'purpose', \n",
    "                  'grade': 'purpose',\n",
    "                  'loan_amnt_range': 'purpose',\n",
    "                  'loan_amnt_range': 'term',\n",
    "                  'installment': 'purpose'}\n",
    "\n",
    "params = {\n",
    "    \"variable_pairs\": variable_pairs,\n",
    "    \"loan_status_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateBarPlots(test_context, params=params)\n",
    "#metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multivariate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Figure, Metric\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BivariateScatterPlots(Metric):\n",
    "    \"\"\"\n",
    "    Generates a visual analysis of categorical data by plotting bivariate scatter plots.\n",
    "    The input dataset and variable_pairs are required.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"bivariate_scatter_plots\"\n",
    "    required_context = [\"dataset\"]\n",
    "    default_params = {\"variable_pairs\": None, \"loan_status_filter\": None}\n",
    "\n",
    "    def plot_bivariate_scatter(self, variable_pairs, loan_status_filter):\n",
    "        figures = []\n",
    "        for x, y in variable_pairs.items():\n",
    "            df = self.dataset.df\n",
    "            if loan_status_filter:\n",
    "                df = df[df[\"loan_status\"] == loan_status_filter]\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            # Scatterplot using seaborn, with color variation based on 'loan_status'\n",
    "            # Create color mapping with rgba values, last value is alpha (transparency)\n",
    "            palette = {0: (0.8, 0.8, 0.8, 0.8), 1: 'tab:red'}\n",
    "            plot = sns.scatterplot(data=df, x=x, y=y, hue='loan_status', palette=palette, alpha=1)\n",
    "\n",
    "            # Change legend labels\n",
    "            legend_labels = ['Default' if t.get_text()=='1' else 'Non-default' for t in plot.legend_.texts[1:]]\n",
    "            plot.legend_.texts[1:] = legend_labels\n",
    "\n",
    "            plt.title(x + \" and \" + y)\n",
    "            plt.xlabel(x)\n",
    "            plt.ylabel(y)\n",
    "            plt.show()\n",
    "\n",
    "            figures.append(\n",
    "                Figure(\n",
    "                    for_object=self, key=f\"{self.key}:{x}_{y}\", figure=plt.figure()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        return figures\n",
    "\n",
    "    def run(self):\n",
    "        variable_pairs = self.params[\"variable_pairs\"]\n",
    "        loan_status_filter = self.params[\"loan_status_filter\"]\n",
    "\n",
    "        figures = self.plot_bivariate_scatter(variable_pairs, loan_status_filter)\n",
    "\n",
    "        return self.cache_results(figures=figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"variable_pairs\": variable_pairs,\n",
    "    \"loan_status_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateScatterPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bivariate Histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Figure, Metric\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BivariateHistograms(Metric):\n",
    "    \"\"\"\n",
    "    Generates a visual analysis of categorical data by plotting bivariate histograms.\n",
    "    The input dataset and variable_pairs are required.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"bivariate_histograms\"\n",
    "    required_context = [\"dataset\"]\n",
    "    default_params = {\"variable_pairs\": None, \"loan_status_filter\": None}\n",
    "\n",
    "    def plot_bivariate_histogram(self, variable_pairs, loan_status_filter):\n",
    "        figures = []\n",
    "        palette = {0: (0.5, 0.5, 0.5, 0.8), 1: 'tab:red'}\n",
    "\n",
    "        for x, y in variable_pairs.items():\n",
    "            df = self.dataset.df\n",
    "            if loan_status_filter:\n",
    "                df = df[df[\"loan_status\"] == loan_status_filter]\n",
    "\n",
    "            fig, axes = plt.subplots(2, 1)\n",
    "\n",
    "            for ax, var in zip(axes, [x, y]):\n",
    "                for loan_status, color in palette.items():\n",
    "                    subset = df[df['loan_status'] == loan_status]\n",
    "                    sns.histplot(subset[var],\n",
    "                                 ax=ax, \n",
    "                                 color=color,\n",
    "                                 edgecolor=None, \n",
    "                                 kde=True, \n",
    "                                 label='Default' if loan_status else 'Non-default')\n",
    "\n",
    "                ax.set_title(f\"Histogram of {var} by loan status\")\n",
    "                ax.set_xlabel(var)\n",
    "                ax.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            figures.append(\n",
    "                Figure(\n",
    "                    for_object=self, key=f\"{self.key}:{x}_{y}\", figure=plt.figure()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        return figures\n",
    "\n",
    "    def run(self):\n",
    "        variable_pairs = self.params[\"variable_pairs\"]\n",
    "        loan_status_filter = self.params[\"loan_status_filter\"]\n",
    "\n",
    "        figures = self.plot_bivariate_histogram(variable_pairs, loan_status_filter)\n",
    "\n",
    "        return self.cache_results(figures=figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"variable_pairs\": variable_pairs,\n",
    "    \"loan_status_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateHistograms(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multivariate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# We can now evaluate on the test set\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            loan_status   No. Observations:                29110\n",
      "Model:                            GLM   Df Residuals:                    29072\n",
      "Model Family:                Binomial   Df Model:                           37\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -10836.\n",
      "Date:                Tue, 20 Jun 2023   Deviance:                       21673.\n",
      "Time:                        14:58:39   Pearson chi2:                 2.88e+04\n",
      "No. Iterations:                   100   Pseudo R-squ. (CS):            0.06862\n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                            -1.7636      0.195     -9.024      0.000      -2.147      -1.381\n",
      "pub_rec                           0.2628      0.065      4.019      0.000       0.135       0.391\n",
      "revol_util                        0.0041      0.001      5.402      0.000       0.003       0.006\n",
      "funded_amnt_inv               -6.906e-05   6.43e-06    -10.742      0.000   -8.17e-05   -5.65e-05\n",
      "int_rate                          0.1803      0.018     10.130      0.000       0.145       0.215\n",
      "dti                               0.0036      0.003      1.251      0.211      -0.002       0.009\n",
      "annual_inc                    -1.678e-06   8.28e-07     -2.027      0.043    -3.3e-06   -5.56e-08\n",
      "loan_amnt                      6.582e-05   1.29e-05      5.117      0.000    4.06e-05     9.1e-05\n",
      "earliest_cr_line                  0.0015      0.000      3.577      0.000       0.001       0.002\n",
      "term_ 36 months                  -1.2282      0.097    -12.689      0.000      -1.418      -1.039\n",
      "term_ 60 months                  -0.5354      0.105     -5.086      0.000      -0.742      -0.329\n",
      "grade_A                           0.2330      0.131      1.774      0.076      -0.024       0.490\n",
      "grade_B                           0.1295      0.072      1.792      0.073      -0.012       0.271\n",
      "grade_C                          -0.0149      0.048     -0.310      0.757      -0.109       0.079\n",
      "grade_D                          -0.1825      0.055     -3.291      0.001      -0.291      -0.074\n",
      "grade_E                          -0.4876      0.082     -5.919      0.000      -0.649      -0.326\n",
      "grade_F                          -0.6230      0.123     -5.069      0.000      -0.864      -0.382\n",
      "grade_G                          -0.8181      0.176     -4.637      0.000      -1.164      -0.472\n",
      "purpose_car                      -0.5009      0.096     -5.200      0.000      -0.690      -0.312\n",
      "purpose_credit_card              -0.5038      0.061     -8.228      0.000      -0.624      -0.384\n",
      "purpose_debt_consolidation       -0.2255      0.043     -5.266      0.000      -0.309      -0.142\n",
      "purpose_home_improvement         -0.2415      0.072     -3.364      0.001      -0.382      -0.101\n",
      "purpose_major_purchase           -0.4023      0.082     -4.883      0.000      -0.564      -0.241\n",
      "purpose_medical                  -0.0162      0.119     -0.137      0.891      -0.248       0.216\n",
      "purpose_moving                    0.0303      0.129      0.235      0.814      -0.223       0.283\n",
      "purpose_other                     0.0257      0.058      0.442      0.659      -0.088       0.140\n",
      "purpose_small_business            0.5491      0.068      8.042      0.000       0.415       0.683\n",
      "purpose_wedding                  -0.4785      0.117     -4.086      0.000      -0.708      -0.249\n",
      "annual_inc_range_0-25000         -0.2186      0.272     -0.804      0.421      -0.751       0.314\n",
      "annual_inc_range_25000-50000     -0.4432      0.256     -1.734      0.083      -0.944       0.058\n",
      "annual_inc_range_50000-75000     -0.6433      0.243     -2.646      0.008      -1.120      -0.167\n",
      "annual_inc_range_75000-100000    -0.9030      0.232     -3.886      0.000      -1.358      -0.448\n",
      "annual_inc_range_100000+         -1.0669      0.208     -5.138      0.000      -1.474      -0.660\n",
      "annual_inc_range_Unknown          1.5113      1.300      1.163      0.245      -1.036       4.059\n",
      "loan_amnt_range_0-5000           -0.2127      0.137     -1.551      0.121      -0.482       0.056\n",
      "loan_amnt_range_5000-10000       -0.2696      0.093     -2.901      0.004      -0.452      -0.087\n",
      "loan_amnt_range_10000-15000      -0.3392      0.060     -5.685      0.000      -0.456      -0.222\n",
      "loan_amnt_range_15000-20000      -0.2927      0.065     -4.496      0.000      -0.420      -0.165\n",
      "loan_amnt_range_20000-25000      -0.2602      0.113     -2.307      0.021      -0.481      -0.039\n",
      "loan_amnt_range_25000+           -0.3893      0.201     -1.938      0.053      -0.783       0.004\n",
      "installment_high                 -0.4866      0.074     -6.568      0.000      -0.632      -0.341\n",
      "installment_low                  -0.4862      0.087     -5.605      0.000      -0.656      -0.316\n",
      "installment_medium               -0.4758      0.065     -7.286      0.000      -0.604      -0.348\n",
      "installment_very high            -0.3150      0.119     -2.658      0.008      -0.547      -0.083\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range', 'installment']  # Added 'installment'\n",
    "\n",
    "# Handle categorical features\n",
    "df_encoded = pd.get_dummies(df_multivariate, columns=categorical_features)\n",
    "\n",
    "# Split the data\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Define the model\n",
    "glm_model_fit = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = glm_model_fit.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = sm.add_constant(X_test)  # Adding a constant to the test data\n",
    "y_pred = results.predict(X_test)\n",
    "\n",
    "# You can then further analyze y_pred to measure model performance on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale variable X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Scale your variables\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y, X_scaled, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ValidMind Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 14:58:44,278 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-20 14:58:44,279 - INFO - dataset - Inferring dataset types...\n",
      "2023-06-20 14:58:44,940 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-20 14:58:44,941 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model type statsmodels.GLM is not supported at the moment.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_82191/3297485388.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize model A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m vm_model_A = vm.init_model(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglm_model_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvm_train_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/github/validmind/validmind-python/validmind/client.py\u001b[0m in \u001b[0;36minit_model\u001b[0;34m(model, train_ds, test_ds, validation_ds)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;34mf\"Model type {Model.model_library(model)}.{Model.model_class(model)} is not supported at the moment.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Model type statsmodels.GLM is not supported at the moment."
     ]
    }
   ],
   "source": [
    "# Initialize training and testing datasets for model A\n",
    "vm_train_ds = vm.init_dataset(dataset=X_train, type=\"generic\", target_column='loan_status')\n",
    "vm_test_ds = vm.init_dataset(dataset=X_test, type=\"generic\", target_column='loan_status')\n",
    "\n",
    "# Initialize model A\n",
    "vm_model_A = vm.init_model(\n",
    "    model = glm_model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-eEL8LtKG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
