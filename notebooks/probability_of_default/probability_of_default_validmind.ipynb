{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Default Model using ValidMind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Connect Notebook to ValidMind Project\n",
    "- Step 2: Import Raw Data\n",
    "- Step 3: Data Description\n",
    "- Step 4: Data Preparation\n",
    "- Step 5: Data Description on Preprocessed Data \n",
    "- Step 6: Univariate Analysis\n",
    "- Step 7: Multivariate Analysis\n",
    "- Step 8: Model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect Notebook to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect Notebook to ValidMind Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:47:33,810 - INFO - api_client - Connected to ValidMind. Project: [3] PD Model - Initial Validation (cliwzqjgv00001fy6869rlav9)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Raw Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Lending Club Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39717 entries, 0 to 39716\n",
      "Columns: 111 entries, id to total_il_high_credit_limit\n",
      "dtypes: float64(74), int64(13), object(24)\n",
      "memory usage: 33.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_91086/2449495096.py:4: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the zip file\n",
    "# filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2011/lending_club_loan_data_2007_2011.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Dataset Description "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:47:34,219 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-27 10:47:34,219 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac60b561d743405dbe07300e106b6f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>This section provides descriptive statistics for numerical, categorical and date…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Dataset Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:47:35,187 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-27 10:47:35,188 - INFO - dataset - Inferring dataset types...\n",
      "/Users/juanvalidmind/Dev/github/validmind-python/validmind/tests/data_validation/MissingValuesBarPlot.py:60: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([\"{:.1f}%\".format(x) for x in ax.get_yticks()])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a408ada7994975bbb8118210507602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of missing values by plotting bar plots with colored…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"xticks_fontsize\": 8}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Target Variable\n",
    "\n",
    "**Definition of Default**\n",
    "\n",
    "We categorizing `Fully Paid` loans as \"default = 0\" and `Charged Off` loans as \"default = 1\". This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk). \n",
    "\n",
    "Loans with `Current` status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add `default` Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_column(df, target_column):\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    # Convert target column to integer\n",
    "    df[target_column] = df[target_column].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_91086/1239657674.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[target_column] = df[target_column].astype(int)\n",
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_91086/2102852686.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_prep.drop(columns='loan_status', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "target_column = 'default'\n",
    "df_prep = add_target_column(df, target_column)\n",
    "\n",
    "# Drop 'loan_status' variable \n",
    "df_prep.drop(columns='loan_status', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unused Variables\n",
    "\n",
    "Remove all the **Demographic** and **Customer Behavioural** features which is of no use for default analysis for credit approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "                    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "                    \"pub_rec_bankruptcies\", \"chargeoff_within_12_mths\", \"tax_liens\", \"delinq_amnt\"]\n",
    "\n",
    "df_prep = df_prep.drop(columns=unused_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop\n",
    "\n",
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df_prep, min_missing_count)\n",
    "df_prep.drop(columns=variables_to_drop, axis=1, inplace=True)\n",
    "\n",
    "df_prep.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df_prep.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Data  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "We employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the `stratify = y` parameter is set, it ensures that the distribution of the target variable (`y`) in the test set is the same as that in the original dataset. \n",
    "\n",
    "This is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test \n",
    "X = df_prep.drop(target_column, axis = 1)\n",
    "y = df_prep[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 42, stratify = y)\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "\n",
    "Class imbalance is a common issue in credit risk scorecards and datasets like the Lending Club's. This imbalance arises when the number of defaulting loans (negative class) is significantly smaller than the number of loans that are paid off (positive class). Such imbalance can lead to biased models that favor the majority class, thus affecting predictive performance. \n",
    "\n",
    "Special techniques like oversampling, undersampling, or cost-sensitive learning are often needed to ensure that the minority class is appropriately represented during model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:47:36,739 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-27 10:47:36,740 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbc172069da47088e6396fce180d394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Class Imbalance ❌</h2>\\n            <p>The class imbalance test m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = ClassImbalance(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "We perform data cleaning after splitting the data into training and testing sets to prevent data leakage, maintain the test set's independence, and avoid overfitting. Data leakage can occur when information from the test set influences the training set, leading to overly optimistic performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def clean_date_columns(df: pd.DataFrame, columns: List[str], date_format: str = \"%b-%y\") -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Converts date columns to datetime format and create a new column \n",
    "    as a difference between today and the respective date.\n",
    "    \"\"\"\n",
    "    # Ensure the columns exist in the dataframe\n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    today_date = pd.to_datetime(datetime.now())\n",
    "    for column in columns:\n",
    "        # convert to datetime format\n",
    "        df[column] = pd.to_datetime(df[column], format=date_format)\n",
    "        # calculate the difference in months and add to a new column\n",
    "        df['mths_since_' + column] = round((today_date - df[column]) / np.timedelta64(1, 'M')).clip(lower=0)\n",
    "        # drop the original date column\n",
    "        df.drop(columns = [column], inplace = True)\n",
    "    return df\n",
    "\n",
    "def clean_term_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to remove 'months' string from the 'term' column and convert it to categorical\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].str.replace(' months', '')\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_rate_columns(df, column):\n",
    "    \"\"\"\n",
    "    Clean interest rate column. Remove the '%' sign and convert to numeric.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    column (str): Name of the interest rate column to be cleaned.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].str.replace('%', '')\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "\n",
    "def clean_emp_length_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to clean 'emp_length' column and convert it to categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column].fillna(value = 0, inplace=True)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_inq_last_6mths(df, column):\n",
    "    \"\"\"\n",
    "    Function to convert 'inq_last_6mths' column into categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>default</th>\n",
       "      <th>mths_since_earliest_cr_line</th>\n",
       "      <th>mths_since_issue_d</th>\n",
       "      <th>mths_since_last_pymnt_d</th>\n",
       "      <th>mths_since_last_credit_pull_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20010</th>\n",
       "      <td>12000</td>\n",
       "      <td>11950.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>7.66</td>\n",
       "      <td>374.16</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>7</td>\n",
       "      <td>OWN</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>13290.463640</td>\n",
       "      <td>13235.09</td>\n",
       "      <td>1290.46</td>\n",
       "      <td>4732.82</td>\n",
       "      <td>0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36771</th>\n",
       "      <td>5000</td>\n",
       "      <td>4523.570000</td>\n",
       "      <td>36</td>\n",
       "      <td>13.79</td>\n",
       "      <td>170.38</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>5</td>\n",
       "      <td>RENT</td>\n",
       "      <td>48996.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>5955.232865</td>\n",
       "      <td>5307.90</td>\n",
       "      <td>940.23</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36979</th>\n",
       "      <td>20000</td>\n",
       "      <td>9151.061533</td>\n",
       "      <td>36</td>\n",
       "      <td>14.74</td>\n",
       "      <td>690.74</td>\n",
       "      <td>D</td>\n",
       "      <td>D3</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>78000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>24866.391750</td>\n",
       "      <td>10285.37</td>\n",
       "      <td>4866.39</td>\n",
       "      <td>694.02</td>\n",
       "      <td>0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13221</th>\n",
       "      <td>1900</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>13.49</td>\n",
       "      <td>64.47</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>7</td>\n",
       "      <td>RENT</td>\n",
       "      <td>19392.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>2039.155521</td>\n",
       "      <td>2039.16</td>\n",
       "      <td>139.16</td>\n",
       "      <td>1654.65</td>\n",
       "      <td>0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>7000</td>\n",
       "      <td>6975.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>8.90</td>\n",
       "      <td>222.28</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>7292.361027</td>\n",
       "      <td>7266.32</td>\n",
       "      <td>292.36</td>\n",
       "      <td>6182.28</td>\n",
       "      <td>0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28142</th>\n",
       "      <td>7200</td>\n",
       "      <td>7100.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>11.12</td>\n",
       "      <td>236.13</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8501.348312</td>\n",
       "      <td>8383.27</td>\n",
       "      <td>1301.35</td>\n",
       "      <td>269.34</td>\n",
       "      <td>0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>6600</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>22.48</td>\n",
       "      <td>184.10</td>\n",
       "      <td>G</td>\n",
       "      <td>G2</td>\n",
       "      <td>2</td>\n",
       "      <td>RENT</td>\n",
       "      <td>38480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>9697.160007</td>\n",
       "      <td>9697.16</td>\n",
       "      <td>3097.16</td>\n",
       "      <td>62.19</td>\n",
       "      <td>0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28661</th>\n",
       "      <td>5000</td>\n",
       "      <td>4904.546865</td>\n",
       "      <td>36</td>\n",
       "      <td>13.61</td>\n",
       "      <td>169.95</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>59000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>7287.570000</td>\n",
       "      <td>7159.54</td>\n",
       "      <td>2272.57</td>\n",
       "      <td>1079.08</td>\n",
       "      <td>0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27357</th>\n",
       "      <td>10000</td>\n",
       "      <td>9875.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>13.23</td>\n",
       "      <td>228.71</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>42840.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>10743.870020</td>\n",
       "      <td>10609.57</td>\n",
       "      <td>743.87</td>\n",
       "      <td>9374.74</td>\n",
       "      <td>0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20539</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>14.54</td>\n",
       "      <td>688.81</td>\n",
       "      <td>D</td>\n",
       "      <td>D1</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>98000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>24797.565250</td>\n",
       "      <td>24797.57</td>\n",
       "      <td>4797.56</td>\n",
       "      <td>704.96</td>\n",
       "      <td>0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29997 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loan_amnt  funded_amnt_inv term  int_rate  installment grade sub_grade  \\\n",
       "20010      12000     11950.000000   36      7.66       374.16     A        A5   \n",
       "36771       5000      4523.570000   36     13.79       170.38     C        C5   \n",
       "36979      20000      9151.061533   36     14.74       690.74     D        D3   \n",
       "13221       1900      1900.000000   36     13.49        64.47     C        C2   \n",
       "5986        7000      6975.000000   36      8.90       222.28     A        A5   \n",
       "...          ...              ...  ...       ...          ...   ...       ...   \n",
       "28142       7200      7100.000000   36     11.12       236.13     B        B3   \n",
       "14973       6600      6600.000000   60     22.48       184.10     G        G2   \n",
       "28661       5000      4904.546865   36     13.61       169.95     C        C2   \n",
       "27357      10000      9875.000000   60     13.23       228.71     C        C1   \n",
       "20539      20000     20000.000000   36     14.54       688.81     D        D1   \n",
       "\n",
       "      emp_length home_ownership  annual_inc  ... total_acc   total_pymnt  \\\n",
       "20010          7            OWN     42000.0  ...        30  13290.463640   \n",
       "36771          5           RENT     48996.0  ...         9   5955.232865   \n",
       "36979         10       MORTGAGE     78000.0  ...        31  24866.391750   \n",
       "13221          7           RENT     19392.0  ...        21   2039.155521   \n",
       "5986           0           RENT     30000.0  ...        19   7292.361027   \n",
       "...          ...            ...         ...  ...       ...           ...   \n",
       "28142          1           RENT     65000.0  ...         7   8501.348312   \n",
       "14973          2           RENT     38480.0  ...         8   9697.160007   \n",
       "28661          0       MORTGAGE     59000.0  ...        34   7287.570000   \n",
       "27357          0           RENT     42840.0  ...        17  10743.870020   \n",
       "20539          0           RENT     98000.0  ...        14  24797.565250   \n",
       "\n",
       "      total_pymnt_inv  total_rec_int last_pymnt_amnt  default  \\\n",
       "20010        13235.09        1290.46         4732.82        0   \n",
       "36771         5307.90         940.23            7.38        0   \n",
       "36979        10285.37        4866.39          694.02        0   \n",
       "13221         2039.16         139.16         1654.65        0   \n",
       "5986          7266.32         292.36         6182.28        0   \n",
       "...               ...            ...             ...      ...   \n",
       "28142         8383.27        1301.35          269.34        0   \n",
       "14973         9697.16        3097.16           62.19        0   \n",
       "28661         7159.54        2272.57         1079.08        0   \n",
       "27357        10609.57         743.87         9374.74        0   \n",
       "20539        24797.57        4797.56          704.96        0   \n",
       "\n",
       "       mths_since_earliest_cr_line  mths_since_issue_d  \\\n",
       "20010                        300.0               149.0   \n",
       "36771                        452.0               170.0   \n",
       "36979                        453.0               171.0   \n",
       "13221                        283.0               145.0   \n",
       "5986                         253.0               141.0   \n",
       "...                            ...                 ...   \n",
       "28142                        201.0               156.0   \n",
       "14973                        326.0               146.0   \n",
       "28661                        307.0               157.0   \n",
       "27357                        356.0               155.0   \n",
       "20539                        219.0               150.0   \n",
       "\n",
       "       mths_since_last_pymnt_d  mths_since_last_credit_pull_d  \n",
       "20010                    125.0                           98.0  \n",
       "36771                    147.0                          147.0  \n",
       "36979                    135.0                          112.0  \n",
       "13221                    137.0                           86.0  \n",
       "5986                     135.0                           89.0  \n",
       "...                        ...                            ...  \n",
       "28142                    120.0                           86.0  \n",
       "14973                     97.0                           86.0  \n",
       "28661                     90.0                           90.0  \n",
       "27357                    148.0                          108.0  \n",
       "20539                    113.0                           96.0  \n",
       "\n",
       "[29997 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_date_columns(df_train, ['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "clean_rate_columns(df_train, 'int_rate')\n",
    "clean_rate_columns(df_train, 'revol_util')\n",
    "clean_emp_length_column(df_train, 'emp_length')\n",
    "clean_term_column(df_train, 'term')\n",
    "clean_inq_last_6mths(df_train, 'inq_last_6mths')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NaN values from `mths_since_last_pymnt_d` and `mths_since_last_credit_pull_d` before performing statistical tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(subset=['mths_since_last_pymnt_d', 'mths_since_last_credit_pull_d'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_columns(df):\n",
    "        numerical_columns = df.select_dtypes(\n",
    "            include=[\"int\", \"float\"]\n",
    "        ).columns.tolist()\n",
    "        return numerical_columns\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "        categorical_columns = df.select_dtypes(\n",
    "            include=[\"object\", \"category\", \"uint8\"]\n",
    "        ).columns.tolist()\n",
    "        return categorical_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:47:37,248 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-27 10:47:37,249 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977128eb36df4d8488cce67ef861a9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>This section provides descriptive statistics for numerical, categorical and date…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Squared Test on Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_squared_categorical_feature_selection(df, cat_vars, target):\n",
    "    \"\"\"\n",
    "    Performs a Chi-Squared test of independence for each categorical variable with the target.\n",
    "\n",
    "    :param df: DataFrame containing the data\n",
    "    :param cat_vars: list of column names which are categorical\n",
    "    :param target: target variable name\n",
    "    :return: DataFrame with p-values and Chi-squared statistic. Each row is a categorical variable, \n",
    "             columns are 'Variable', 'Chi-squared statistic', 'p-value'.\n",
    "    \"\"\"\n",
    "    # Ensure the columns exist in the dataframe\n",
    "    for var in cat_vars:\n",
    "        if var not in df.columns:\n",
    "            raise ValueError(f\"The column '{var}' does not exist in the dataframe.\")\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"The target column '{target}' does not exist in the dataframe.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for var in cat_vars:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(df[var], df[target])\n",
    "\n",
    "        # Perform the Chi-Square test\n",
    "        chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Add the result to the list of results\n",
    "        results.append([var, chi2, p])\n",
    "\n",
    "    # Convert results to a DataFrame and return\n",
    "    results_df = pd.DataFrame(results, columns=['Variable', 'Chi-squared statistic', 'p-value'])\n",
    "\n",
    "    # Sort by p-value in ascending order\n",
    "    results_df = results_df.sort_values(by='p-value')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Chi-squared statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grade</td>\n",
       "      <td>1181.030548</td>\n",
       "      <td>6.100681e-252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>1281.031756</td>\n",
       "      <td>2.644332e-247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>term</td>\n",
       "      <td>896.794176</td>\n",
       "      <td>4.883483e-197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>purpose</td>\n",
       "      <td>288.220979</td>\n",
       "      <td>6.982210e-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inq_last_6mths</td>\n",
       "      <td>187.916186</td>\n",
       "      <td>2.234132e-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>verification_status</td>\n",
       "      <td>70.193859</td>\n",
       "      <td>5.722649e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>addr_state</td>\n",
       "      <td>140.464282</td>\n",
       "      <td>9.212203e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>home_ownership</td>\n",
       "      <td>15.741778</td>\n",
       "      <td>3.385973e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emp_length</td>\n",
       "      <td>24.178570</td>\n",
       "      <td>7.140259e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Variable  Chi-squared statistic        p-value\n",
       "1                grade            1181.030548  6.100681e-252\n",
       "2            sub_grade            1281.031756  2.644332e-247\n",
       "0                 term             896.794176  4.883483e-197\n",
       "6              purpose             288.220979   6.982210e-54\n",
       "8       inq_last_6mths             187.916186   2.234132e-36\n",
       "5  verification_status              70.193859   5.722649e-16\n",
       "7           addr_state             140.464282   9.212203e-11\n",
       "4       home_ownership              15.741778   3.385973e-03\n",
       "3           emp_length              24.178570   7.140259e-03"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = get_categorical_columns(df_train)\n",
    "chi_squared_categorical_feature_selection(df_train, categorical_columns, target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA Test on Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 10:47:41,332 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-06-27 10:47:41,332 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fd2ed83b6e4a31a8a6b825b4843580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Perform an ANOVA F-test for each numerical variable with the target. The input d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "features = get_numerical_columns(df_train)\n",
    "params = {\"features\": features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ANOVAOneWayTable(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "def anova_numerical_features(df, num_vars, target):\n",
    "    \"\"\"\n",
    "    Performs an ANOVA F-test for each numerical variable with the target.\n",
    "\n",
    "    :param df: DataFrame containing the data\n",
    "    :param num_vars: list of column names which are numerical\n",
    "    :param target: target variable name\n",
    "    :return: DataFrame with p-values and F statistic. Each row is a numerical variable, \n",
    "             columns are 'Variable', 'F statistic', 'p-value'.\n",
    "    \"\"\"\n",
    "    # Ensure the columns exist in the dataframe\n",
    "    for var in num_vars:\n",
    "        if var not in df.columns:\n",
    "            raise ValueError(f\"The column '{var}' does not exist in the dataframe.\")\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"The target column '{target}' does not exist in the dataframe.\")\n",
    "    \n",
    "    # Ensure the target variable is not included in num_vars\n",
    "    if target in num_vars:\n",
    "        num_vars.remove(target)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for var in num_vars:\n",
    "        # Perform the ANOVA test\n",
    "        class_0 = df[df[target] == 0][var]\n",
    "        class_1 = df[df[target] == 1][var]\n",
    "        \n",
    "        f, p = f_oneway(class_0, class_1)\n",
    "\n",
    "        # Add the result to the list of results\n",
    "        results.append([var, f, p])\n",
    "\n",
    "    # Convert results to a DataFrame and return\n",
    "    results_df = pd.DataFrame(results, columns=['Variable', 'F statistic', 'p-value'])\n",
    "\n",
    "    # Sort by p-value in ascending order\n",
    "    results_df = results_df.sort_values(by='p-value')\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = get_numerical_columns(df_train)\n",
    "anova_numerical_features(df_train, numerical_columns, target=target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap Correlation of Numerical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HeatmapFeatureCorrelations import HeatmapFeatureCorrelations\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = HeatmapFeatureCorrelations(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations of Numerical Features with Target Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "features = get_numerical_columns(df_train)\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": features,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_categorical_features = ['term', 'sub_grade', 'emp_length', 'inq_last_6mths', 'addr_state']\n",
    "drop_numerical_features = ['installment', 'mths_since_last_credit_pull_d', 'total_rec_int',\n",
    "                           'mths_since_earliest_cr_line', 'mths_since_issue_d', 'open_acc',\n",
    "                           'pub_rec']\n",
    "\n",
    "df_train.drop(columns=drop_categorical_features+drop_numerical_features, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WoE and IV of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEIVTable import WOEIVTable\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "features = ['grade', 'purpose', 'verification_status', 'home_ownership']\n",
    "params = {\n",
    "    \"features\": features,\n",
    "    \"order_by\": [\"Feature\", \"IV\"]\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVTable(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEIVPlots import WOEIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "features = ['grade', 'purpose', 'verification_status', 'home_ownership']\n",
    "params = {\n",
    "    \"features\": features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WoE and IV of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_numerical_variables(df, columns_list, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Bin the specified numerical columns into categories.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    bins (int or sequence, optional): Number of bins to create, or a sequence representing the bins.\n",
    "    labels (list, optional): Labels for the bins. Must be the same length as the resulting bins.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the new binned columns.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in columns_list:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # Convert the bin intervals to strings\n",
    "            df_copy[column+'_bin'] = pd.cut(df[column], bins=bins, labels=labels).astype(str)\n",
    "        else:\n",
    "            raise ValueError(f\"Column {column} is not numerical.\")\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin numerical features for WoE and IV analysis\n",
    "#numerical_features_to_bin = get_numerical_columns(df_train)\n",
    "#df_train = bin_numerical_variables(df_train, numerical_features_to_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_columns = [f+'_bin' for f in numerical_features_to_bin]\n",
    "#woe_iv_df_numerical = calculate_woe_iv(df_train, target_column, categorical_columns)\n",
    "#display(woe_iv_df_numerical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_variables(df, columns_list):\n",
    "    \"\"\"\n",
    "    Generate dummy variables for specified columns in the DataFrame,\n",
    "    concatenate them with the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): DataFrame after processing.\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column + \"_\", drop_first=False)\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "categorical_columns = ['purpose', 'grade', 'verification_status', 'home_ownership']\n",
    "df_train = add_dummy_variables(df_train, categorical_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "clean_date_columns(df_test, ['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d'])\n",
    "clean_rate_columns(df_test, 'int_rate')\n",
    "clean_rate_columns(df_test, 'revol_util')\n",
    "clean_emp_length_column(df_test, 'emp_length')\n",
    "clean_term_column(df_test, 'term')\n",
    "clean_inq_last_6mths(df_test, 'inq_last_6mths')\n",
    "\n",
    "# Drop NaN values \n",
    "df_test.dropna(subset=['mths_since_last_pymnt_d', 'mths_since_last_credit_pull_d'], inplace=True)\n",
    "\n",
    "# Drop features\n",
    "df_test.drop(columns=drop_categorical_features + drop_numerical_features, inplace=True)\n",
    "\n",
    "# Dummy features \n",
    "categorical_columns = ['purpose', 'grade', 'verification_status', 'home_ownership']\n",
    "df_test = add_dummy_variables(df_test, categorical_columns)\n",
    "\n",
    "# Reindex df_train and df_test\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating Features and Target Variables for Training and Test Sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(target_column, axis=1)  \n",
    "y_train = df_train[target_column]  \n",
    "\n",
    "X_test = df_test.drop(target_column, axis=1)  \n",
    "y_test = df_test[target_column]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a GLM Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add constant to X_train for intercept term\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "result = model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "model_fit = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that X_test and y_test are your testing data and labels\n",
    "y_pred = model_fit.predict(X_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_fit.feature_importances_\n",
    "feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "display(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features\n",
    "selected_features = feature_importances.head(15)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = selected_features['Feature'].tolist()\n",
    "print(selected_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorecard Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Filter training and test data\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "# Instantiate the model\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = log_reg.coef_\n",
    "intercept = log_reg.intercept_\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients: \", coefficients)\n",
    "print(\"Intercept: \", intercept)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Model Fit Coefficients to Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate odds ratios\n",
    "odds_ratios = np.exp(coefficients).reshape(-1)\n",
    "\n",
    "# Define the scaling factor and base points\n",
    "scaling_factor = 20 / np.log(2)\n",
    "base_points = 500\n",
    "\n",
    "# Calculate the scores for each coefficient\n",
    "scores = scaling_factor * np.log(odds_ratios)\n",
    "scores = base_points - scores\n",
    "\n",
    "# Print the scores for each coefficient\n",
    "for feature, score in zip(selected_features, scores):\n",
    "    print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of Numerical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 'df' is your DataFrame and 'column_name' is the name of the column\n",
    "unique_values = df['inq_last_6mths'].unique()\n",
    "print(unique_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Cardinality of Categorical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots of Categorical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Ratios by Categorical Feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.DefaultRatioBarPlots import DefaultRatioBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": target_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = DefaultRatioBarPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Bar Plots of Default Ratios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateFeaturesBarPlots import BivariateFeaturesBarPlots\n",
    "\n",
    "# Pass target column to validmind dataset\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure the metric\n",
    "features_pairs = {'home_ownership': 'grade', \n",
    "                  'purpose': 'grade',\n",
    "                  'grade': 'verification_status'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "}\n",
    "\n",
    "metric = BivariateFeaturesBarPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots by Default Status"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateScatterPlots import BivariateScatterPlots\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateScatterPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Histograms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateHistograms import BivariateHistograms\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateHistograms(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Engineering "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dummy Catergorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_variables(df, columns_list):\n",
    "    \"\"\"\n",
    "    Generate dummy variables for specified columns in the DataFrame,\n",
    "    concatenate them with the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column + \":\", drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = add_dummy_variables(df_train, ['grade', 'home_ownership', 'verification_status', 'purpose'])\n",
    "# df_test = add_dummy_variables(df_test, ['grade', 'home_ownership', 'verification_status', 'purpose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the X_test DataFrame to match the column structure of the X_train DataFrame\n",
    "# df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight of Evidence (WoE) Binning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a modelling perspective, the **WoE** allows us to transform raw variables into a format which provides a more robust base for statistical analysis. Specifically, the WoE measures the predictive power of an individual class of a categorical variable, distinguishing between 'good' (non-defaulters) and 'bad' (defaulters) risks. This is accomplished by comparing the distribution of 'good' and 'bad' risks within a specific category to the overall 'good'/'bad' distribution. If the 'good'/'bad' ratio of a particular category is significantly divergent from the overall ratio, it suggests that category is a strong predictor of credit risk.\n",
    "\n",
    "**Information Value (IV)**, on the other hand, is a fundamental metric we use to quantify the predictive power of each input variable in our scorecards. The IV is calculated by taking the sum of the differences between the WoE of each category and the overall WoE, multiplied by the WoE of that category. In other words, IV measures the total amount of 'information' or predictive power a variable brings to the model. For example, variables with an IV between 0.1 and 0.3 provide a weak predictive power, those between 0.3 and 0.5 a medium predictive power, and those with an IV greater than 0.5 have strong predictive power. Therefore, we utilize the IV to prioritize variables for inclusion in the model and to ensure the model's stability and accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WoE and IV for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical_woe_iv_df = calculate_woe_iv(df_train, target_column, categorical_features)\n",
    "#display(categorical_woe_iv_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEIVPlots import WoEandIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": categorical_features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WoE and IV for Numerical Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning numerical features for WOE and IV calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_numerical_variables(df, columns_list, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Bin the specified numerical columns into categories.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    bins (int or sequence, optional): Number of bins to create, or a sequence representing the bins.\n",
    "    labels (list, optional): Labels for the bins. Must be the same length as the resulting bins.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the new binned columns.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in columns_list:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # Convert the bin intervals to strings\n",
    "            df_copy[column+'_bin'] = pd.cut(df[column], bins=bins, labels=labels).astype(str)\n",
    "        else:\n",
    "            raise ValueError(f\"Column {column} is not numerical.\")\n",
    "        \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_to_bin = ['loan_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'emp_length', 'annual_inc', 'dti', 'inq_last_6mths', 'open_acc', 'total_acc']\n",
    "df_train = bin_numerical_variables(df_train, numerical_features_to_bin)\n",
    "\n",
    "# Create a list of binned features\n",
    "binned_numerical_features = [f+'_bin' for f in numerical_features]\n",
    "\n",
    "# Calculate WoE and IV for the binned features\n",
    "woe_iv_df_numerical = calculate_woe_iv(df_train, target_column, binned_numerical_features)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# display(woe_iv_df_numerical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": binned_numerical_features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dummy Features from Binned Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binned_numerical_columns = ['loan_amnt_bin', 'funded_amnt_inv_bin', 'int_rate_bin', 'installment_bin', 'emp_length_bin',\n",
    "#                                'annual_inc_bin', 'dti_bin', 'inq_last_6mths_bin', 'open_acc_bin', 'total_acc_bin']\n",
    "#df_train = add_dummy_variables(df_train, binned_numerical_columns)\n",
    "#df_train.drop(columns=binned_numerical_columns, inplace=True)                          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Binned Datetime Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new numerical features from datetime columns for WoE and IV analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume today's date to calculate years since.\n",
    "today = pd.to_datetime('today')\n",
    "\n",
    "def add_years_since_issue(df, issue_date_column='issue_d'):\n",
    "    \"\"\"\n",
    "    Calculates the number of years since the loan was issued.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in years\n",
    "    df['years_since_issue'] = (today - df[issue_date_column]).dt.days / 365.25\n",
    "\n",
    "def add_credit_history_length(df, earliest_credit_column='earliest_cr_line'):\n",
    "    \"\"\"\n",
    "    Calculates the length of the borrower's credit history in years.\n",
    "    \"\"\"\n",
    "    # Calculate the credit history length\n",
    "    df['credit_history_length'] = (today - df[earliest_credit_column]).dt.days / 365.25\n",
    "\n",
    "add_years_since_issue(df_train)\n",
    "add_credit_history_length(df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add binned numerical features `years_since_issue_bin` and `credit_history_length_bin` to `df_train` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_features = ['years_since_issue', 'credit_history_length']\n",
    "df_train = bin_numerical_variables(df_train, datetime_features)\n",
    "\n",
    "# Create a list of binned features\n",
    "binned_datetime_features = [f+'_bin' for f in datetime_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update VM Dataset and Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"features\": binned_datetime_features,\n",
    "    \"label_rotation\": 90\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WoEandIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dummy features for datetime variables, keep the original datetime variables, and drop binned features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = add_dummy_variables(df_train, binned_datetime_features)\n",
    "df_train.drop(columns=binned_datetime_features, inplace=True)\n",
    "df_train.drop(columns=['issue_d', 'earliest_cr_line'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_and_plot_correlations(df, num_ranges):\n",
    "    # Compute Pearson correlations\n",
    "    correlations = df.corr(method='pearson')\n",
    "\n",
    "    # Flatten the correlation matrix and remove self correlations\n",
    "    corr_values = correlations.values.flatten()\n",
    "    corr_values = corr_values[~np.isnan(corr_values)]\n",
    "    corr_values = corr_values[corr_values != 1]\n",
    "\n",
    "    # Define the ranges for histograms\n",
    "    range_values = np.linspace(-1, 1, num_ranges + 1)\n",
    "\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_rows = (num_ranges + 1) // 2\n",
    "    num_cols = 2\n",
    "\n",
    "    # Create the subplots\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, squeeze=False)\n",
    "\n",
    "    # Plot histograms for each range\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if i < num_ranges:\n",
    "            lower_bound = range_values[i]\n",
    "            upper_bound = range_values[i + 1]\n",
    "\n",
    "            ax.hist(corr_values[(corr_values >= lower_bound) & (corr_values < upper_bound)], bins=50)\n",
    "            ax.set_xlabel('Correlation')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Correlations {lower_bound} to {upper_bound}')\n",
    "            ax.tick_params(axis='x', rotation=90)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    # Remove any empty subplots if the number of ranges is odd\n",
    "    if num_ranges % 2 != 0:\n",
    "        axs[-1, -1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "compute_and_plot_correlations(df_train, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of Features with the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_and_plot_correlations_with_target(df, target_col, num_ranges):\n",
    "    # Compute Pearson correlations with the target variable\n",
    "    correlations = df.corr()[target_col].drop(target_col)\n",
    "    \n",
    "    # Remove missing correlations\n",
    "    correlations = correlations.dropna()\n",
    "\n",
    "    # Define the ranges for histograms\n",
    "    range_values = np.linspace(-1, 1, num_ranges + 1)\n",
    "\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_rows = (num_ranges + 1) // 2\n",
    "    num_cols = 2\n",
    "\n",
    "    # Create the subplots\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, squeeze=False)\n",
    "\n",
    "    # Plot histograms for each range\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if i < num_ranges:\n",
    "            lower_bound = range_values[i]\n",
    "            upper_bound = range_values[i + 1]\n",
    "\n",
    "            ax.hist(correlations[(correlations >= lower_bound) & (correlations < upper_bound)], bins=50)\n",
    "            ax.set_xlabel('Correlation')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Correlations {lower_bound} to {upper_bound}')\n",
    "            ax.tick_params(axis='x', rotation=90)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    # Remove any empty subplots if the number of ranges is odd\n",
    "    if num_ranges % 2 != 0:\n",
    "        axs[-1, -1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "corr_with_target = compute_and_plot_correlations_with_target(df_train, 'default', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# We can now evaluate on the test set\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range', 'installment']  # Added 'installment'\n",
    "\n",
    "# Handle categorical features\n",
    "df_encoded = pd.get_dummies(df_multivariate, columns=categorical_features)\n",
    "\n",
    "# Split the data\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Define the model\n",
    "glm_model_fit = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = glm_model_fit.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = sm.add_constant(X_test)  # Adding a constant to the test data\n",
    "y_pred = results.predict(X_test)\n",
    "\n",
    "# You can then further analyze y_pred to measure model performance on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale variable X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Scale your variables\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y, X_scaled, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ValidMind Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training and testing datasets for model A\n",
    "vm_train_ds = vm.init_dataset(dataset=X_train, type=\"generic\", target_column='loan_status')\n",
    "vm_test_ds = vm.init_dataset(dataset=X_test, type=\"generic\", target_column='loan_status')\n",
    "\n",
    "# Initialize model A\n",
    "vm_model_A = vm.init_model(\n",
    "    model = glm_model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
