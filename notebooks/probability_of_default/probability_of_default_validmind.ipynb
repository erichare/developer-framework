{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Default Model using ValidMind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Connect Notebook to ValidMind Project\n",
    "- Step 2: Import Raw Data\n",
    "- Step 3: Data Description on Raw Data\n",
    "- Step 4: Data Preprocessing\n",
    "- Step 5: Data Description on Preprocessed Data \n",
    "- Step 6: Univariate Analysis\n",
    "- Step 7: Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect Notebook to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect Notebook to ValidMind Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d94fa th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d94fa_row0_col0, #T_d94fa_row0_col1, #T_d94fa_row0_col2, #T_d94fa_row0_col3, #T_d94fa_row1_col0, #T_d94fa_row1_col1, #T_d94fa_row1_col2, #T_d94fa_row1_col3, #T_d94fa_row2_col0, #T_d94fa_row2_col1, #T_d94fa_row2_col2, #T_d94fa_row2_col3, #T_d94fa_row3_col0, #T_d94fa_row3_col1, #T_d94fa_row3_col2, #T_d94fa_row3_col3, #T_d94fa_row4_col0, #T_d94fa_row4_col1, #T_d94fa_row4_col2, #T_d94fa_row4_col3, #T_d94fa_row5_col0, #T_d94fa_row5_col1, #T_d94fa_row5_col2, #T_d94fa_row5_col3, #T_d94fa_row6_col0, #T_d94fa_row6_col1, #T_d94fa_row6_col2, #T_d94fa_row6_col3, #T_d94fa_row7_col0, #T_d94fa_row7_col1, #T_d94fa_row7_col2, #T_d94fa_row7_col3, #T_d94fa_row8_col0, #T_d94fa_row8_col1, #T_d94fa_row8_col2, #T_d94fa_row8_col3, #T_d94fa_row9_col0, #T_d94fa_row9_col1, #T_d94fa_row9_col2, #T_d94fa_row9_col3, #T_d94fa_row10_col0, #T_d94fa_row10_col1, #T_d94fa_row10_col2, #T_d94fa_row10_col3, #T_d94fa_row11_col0, #T_d94fa_row11_col1, #T_d94fa_row11_col2, #T_d94fa_row11_col3, #T_d94fa_row12_col0, #T_d94fa_row12_col1, #T_d94fa_row12_col2, #T_d94fa_row12_col3, #T_d94fa_row13_col0, #T_d94fa_row13_col1, #T_d94fa_row13_col2, #T_d94fa_row13_col3, #T_d94fa_row14_col0, #T_d94fa_row14_col1, #T_d94fa_row14_col2, #T_d94fa_row14_col3, #T_d94fa_row15_col0, #T_d94fa_row15_col1, #T_d94fa_row15_col2, #T_d94fa_row15_col3, #T_d94fa_row16_col0, #T_d94fa_row16_col1, #T_d94fa_row16_col2, #T_d94fa_row16_col3, #T_d94fa_row17_col0, #T_d94fa_row17_col1, #T_d94fa_row17_col2, #T_d94fa_row17_col3, #T_d94fa_row18_col0, #T_d94fa_row18_col1, #T_d94fa_row18_col2, #T_d94fa_row18_col3, #T_d94fa_row19_col0, #T_d94fa_row19_col1, #T_d94fa_row19_col2, #T_d94fa_row19_col3, #T_d94fa_row20_col0, #T_d94fa_row20_col1, #T_d94fa_row20_col2, #T_d94fa_row20_col3, #T_d94fa_row21_col0, #T_d94fa_row21_col1, #T_d94fa_row21_col2, #T_d94fa_row21_col3, #T_d94fa_row22_col0, #T_d94fa_row22_col1, #T_d94fa_row22_col2, #T_d94fa_row22_col3, #T_d94fa_row23_col0, #T_d94fa_row23_col1, #T_d94fa_row23_col2, #T_d94fa_row23_col3, #T_d94fa_row24_col0, #T_d94fa_row24_col1, #T_d94fa_row24_col2, #T_d94fa_row24_col3, #T_d94fa_row25_col0, #T_d94fa_row25_col1, #T_d94fa_row25_col2, #T_d94fa_row25_col3, #T_d94fa_row26_col0, #T_d94fa_row26_col1, #T_d94fa_row26_col2, #T_d94fa_row26_col3, #T_d94fa_row27_col0, #T_d94fa_row27_col1, #T_d94fa_row27_col2, #T_d94fa_row27_col3, #T_d94fa_row28_col0, #T_d94fa_row28_col1, #T_d94fa_row28_col2, #T_d94fa_row28_col3, #T_d94fa_row29_col0, #T_d94fa_row29_col1, #T_d94fa_row29_col2, #T_d94fa_row29_col3, #T_d94fa_row30_col0, #T_d94fa_row30_col1, #T_d94fa_row30_col2, #T_d94fa_row30_col3, #T_d94fa_row31_col0, #T_d94fa_row31_col1, #T_d94fa_row31_col2, #T_d94fa_row31_col3, #T_d94fa_row32_col0, #T_d94fa_row32_col1, #T_d94fa_row32_col2, #T_d94fa_row32_col3, #T_d94fa_row33_col0, #T_d94fa_row33_col1, #T_d94fa_row33_col2, #T_d94fa_row33_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d94fa\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_d94fa_level0_col0\" class=\"col_heading level0 col0\" >Test Type</th>\n",
       "      <th id=\"T_d94fa_level0_col1\" class=\"col_heading level0 col1\" >Name</th>\n",
       "      <th id=\"T_d94fa_level0_col2\" class=\"col_heading level0 col2\" >Description</th>\n",
       "      <th id=\"T_d94fa_level0_col3\" class=\"col_heading level0 col3\" >ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row0_col0\" class=\"data row0 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row0_col1\" class=\"data row0 col1\" >Skewness</td>\n",
       "      <td id=\"T_d94fa_row0_col2\" class=\"data row0 col2\" >The skewness test measures the extent to which a distribution of\n",
       "    values differs from a normal distribution. A positive skew describes\n",
       "    a longer tail of values in the right and a negative skew describes a\n",
       "    longer tail of values in the left.</td>\n",
       "      <td id=\"T_d94fa_row0_col3\" class=\"data row0 col3\" >validmind.data_validation.Skewness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row1_col0\" class=\"data row1 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row1_col1\" class=\"data row1 col1\" >Duplicates</td>\n",
       "      <td id=\"T_d94fa_row1_col2\" class=\"data row1 col2\" >The duplicates test measures the number of duplicate rows found in\n",
       "    the dataset. If a primary key column is specified, the dataset is\n",
       "    checked for duplicate primary keys as well.</td>\n",
       "      <td id=\"T_d94fa_row1_col3\" class=\"data row1 col3\" >validmind.data_validation.Duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row2_col0\" class=\"data row2 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row2_col1\" class=\"data row2 col1\" >DatasetDescription</td>\n",
       "      <td id=\"T_d94fa_row2_col2\" class=\"data row2 col2\" >Collects a set of descriptive statistics for a dataset</td>\n",
       "      <td id=\"T_d94fa_row2_col3\" class=\"data row2 col3\" >validmind.data_validation.DatasetDescription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row3_col0\" class=\"data row3 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row3_col1\" class=\"data row3 col1\" >ScatterPlot</td>\n",
       "      <td id=\"T_d94fa_row3_col2\" class=\"data row3 col2\" >Generates a visual analysis of data by plotting a scatter plot matrix for all columns\n",
       "    in the dataset. The input dataset can have multiple columns (features) if necessary.</td>\n",
       "      <td id=\"T_d94fa_row3_col3\" class=\"data row3 col3\" >validmind.data_validation.ScatterPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row4_col0\" class=\"data row4 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row4_col1\" class=\"data row4 col1\" >TimeSeriesOutliers</td>\n",
       "      <td id=\"T_d94fa_row4_col2\" class=\"data row4 col2\" >Test that find outliers for time series data using the z-score method</td>\n",
       "      <td id=\"T_d94fa_row4_col3\" class=\"data row4 col3\" >validmind.data_validation.TimeSeriesOutliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row5_col0\" class=\"data row5 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row5_col1\" class=\"data row5 col1\" >TabularCategoricalBarPlots</td>\n",
       "      <td id=\"T_d94fa_row5_col2\" class=\"data row5 col2\" >Generates a visual analysis of categorical data by plotting bar plots.\n",
       "    The input dataset can have multiple categorical variables if necessary.\n",
       "    In this case, we produce a separate plot for each categorical variable.</td>\n",
       "      <td id=\"T_d94fa_row5_col3\" class=\"data row5 col3\" >validmind.data_validation.TabularCategoricalBarPlots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row6_col0\" class=\"data row6 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row6_col1\" class=\"data row6 col1\" >AutoStationarity</td>\n",
       "      <td id=\"T_d94fa_row6_col2\" class=\"data row6 col2\" >Automatically detects stationarity for each time series in a DataFrame\n",
       "    using the Augmented Dickey-Fuller (ADF) test.</td>\n",
       "      <td id=\"T_d94fa_row6_col3\" class=\"data row6 col3\" >validmind.data_validation.AutoStationarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row7_col0\" class=\"data row7 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row7_col1\" class=\"data row7 col1\" >DescriptiveStatistics</td>\n",
       "      <td id=\"T_d94fa_row7_col2\" class=\"data row7 col2\" >Collects a set of descriptive statistics for a dataset, both for\n",
       "    numerical and categorical variables</td>\n",
       "      <td id=\"T_d94fa_row7_col3\" class=\"data row7 col3\" >validmind.data_validation.DescriptiveStatistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row8_col0\" class=\"data row8 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row8_col1\" class=\"data row8 col1\" >PearsonCorrelationMatrix</td>\n",
       "      <td id=\"T_d94fa_row8_col2\" class=\"data row8 col2\" >Extracts the Pearson correlation coefficient for all pairs of numerical variables\n",
       "    in the dataset. This metric is useful to identify highly correlated variables\n",
       "    that can be removed from the dataset to reduce dimensionality.</td>\n",
       "      <td id=\"T_d94fa_row8_col3\" class=\"data row8 col3\" >validmind.data_validation.PearsonCorrelationMatrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row9_col0\" class=\"data row9 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row9_col1\" class=\"data row9 col1\" >TabularNumericalHistograms</td>\n",
       "      <td id=\"T_d94fa_row9_col2\" class=\"data row9 col2\" >Generates a visual analysis of numerical data by plotting the histogram.\n",
       "    The input dataset can have multiple numerical variables if necessary.\n",
       "    In this case, we produce a separate plot for each numerical variable.</td>\n",
       "      <td id=\"T_d94fa_row9_col3\" class=\"data row9 col3\" >validmind.data_validation.TabularNumericalHistograms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row10_col0\" class=\"data row10 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row10_col1\" class=\"data row10 col1\" >HighCardinality</td>\n",
       "      <td id=\"T_d94fa_row10_col2\" class=\"data row10 col2\" >The high cardinality test measures the number of unique\n",
       "    values found in categorical columns.</td>\n",
       "      <td id=\"T_d94fa_row10_col3\" class=\"data row10 col3\" >validmind.data_validation.HighCardinality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row11_col0\" class=\"data row11 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row11_col1\" class=\"data row11 col1\" >MissingValues</td>\n",
       "      <td id=\"T_d94fa_row11_col2\" class=\"data row11 col2\" >Test that the number of missing values in the dataset across all features\n",
       "    is less than a threshold</td>\n",
       "      <td id=\"T_d94fa_row11_col3\" class=\"data row11 col3\" >validmind.data_validation.MissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row12_col0\" class=\"data row12 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row12_col1\" class=\"data row12 col1\" >RollingStatsPlot</td>\n",
       "      <td id=\"T_d94fa_row12_col2\" class=\"data row12 col2\" >This class provides a metric to visualize the stationarity of a given time series dataset by plotting the rolling mean and rolling standard deviation. The rolling mean represents the average of the time series data over a fixed-size sliding window, which helps in identifying trends in the data. The rolling standard deviation measures the variability of the data within the sliding window, showing any changes in volatility over time. By analyzing these plots, users can gain insights into the stationarity of the time series data and determine if any transformations or differencing operations are required before applying time series models.</td>\n",
       "      <td id=\"T_d94fa_row12_col3\" class=\"data row12 col3\" >validmind.data_validation.RollingStatsPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row13_col0\" class=\"data row13 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row13_col1\" class=\"data row13 col1\" >DatasetCorrelations</td>\n",
       "      <td id=\"T_d94fa_row13_col2\" class=\"data row13 col2\" >Extracts the correlation matrix for a dataset. The following coefficients\n",
       "    are calculated:\n",
       "    - Pearson's R for numerical variables\n",
       "    - Cramer's V for categorical variables\n",
       "    - Correlation ratios for categorical-numerical variables</td>\n",
       "      <td id=\"T_d94fa_row13_col3\" class=\"data row13 col3\" >validmind.data_validation.DatasetCorrelations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row14_col0\" class=\"data row14 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row14_col1\" class=\"data row14 col1\" >TabularDescriptionTables</td>\n",
       "      <td id=\"T_d94fa_row14_col2\" class=\"data row14 col2\" >Collects a set of descriptive statistics for a tabular dataset, for\n",
       "    numerical, categorical and datetime variables.</td>\n",
       "      <td id=\"T_d94fa_row14_col3\" class=\"data row14 col3\" >validmind.data_validation.TabularDescriptionTables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row15_col0\" class=\"data row15 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row15_col1\" class=\"data row15 col1\" >AutoMA</td>\n",
       "      <td id=\"T_d94fa_row15_col2\" class=\"data row15 col2\" >Automatically detects the MA order of a time series using both BIC and AIC.</td>\n",
       "      <td id=\"T_d94fa_row15_col3\" class=\"data row15 col3\" >validmind.data_validation.AutoMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row16_col0\" class=\"data row16 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row16_col1\" class=\"data row16 col1\" >UniqueRows</td>\n",
       "      <td id=\"T_d94fa_row16_col2\" class=\"data row16 col2\" >Test that the number of unique rows is greater than a threshold</td>\n",
       "      <td id=\"T_d94fa_row16_col3\" class=\"data row16 col3\" >validmind.data_validation.UniqueRows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row17_col0\" class=\"data row17 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row17_col1\" class=\"data row17 col1\" >TooManyZeroValues</td>\n",
       "      <td id=\"T_d94fa_row17_col2\" class=\"data row17 col2\" >The zeros test finds columns that have too many zero values.</td>\n",
       "      <td id=\"T_d94fa_row17_col3\" class=\"data row17 col3\" >validmind.data_validation.TooManyZeroValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row18_col0\" class=\"data row18 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row18_col1\" class=\"data row18 col1\" >HighPearsonCorrelation</td>\n",
       "      <td id=\"T_d94fa_row18_col2\" class=\"data row18 col2\" >Test that the pairwise Pearson correlation coefficients between the\n",
       "    features in the dataset do not exceed a specified threshold.</td>\n",
       "      <td id=\"T_d94fa_row18_col3\" class=\"data row18 col3\" >validmind.data_validation.HighPearsonCorrelation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row19_col0\" class=\"data row19 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row19_col1\" class=\"data row19 col1\" >ACFandPACFPlot</td>\n",
       "      <td id=\"T_d94fa_row19_col2\" class=\"data row19 col2\" >Plots ACF and PACF for a given time series dataset.</td>\n",
       "      <td id=\"T_d94fa_row19_col3\" class=\"data row19 col3\" >validmind.data_validation.ACFandPACFPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row20_col0\" class=\"data row20 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row20_col1\" class=\"data row20 col1\" >TimeSeriesFrequency</td>\n",
       "      <td id=\"T_d94fa_row20_col2\" class=\"data row20 col2\" >Test that detect frequencies in the data</td>\n",
       "      <td id=\"T_d94fa_row20_col3\" class=\"data row20 col3\" >validmind.data_validation.TimeSeriesFrequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row21_col0\" class=\"data row21 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row21_col1\" class=\"data row21 col1\" >DatasetSplit</td>\n",
       "      <td id=\"T_d94fa_row21_col2\" class=\"data row21 col2\" >Attempts to extract information about the dataset split from the\n",
       "    provided training, test and validation datasets.</td>\n",
       "      <td id=\"T_d94fa_row21_col3\" class=\"data row21 col3\" >validmind.data_validation.DatasetSplit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row22_col0\" class=\"data row22 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row22_col1\" class=\"data row22 col1\" >SpreadPlot</td>\n",
       "      <td id=\"T_d94fa_row22_col2\" class=\"data row22 col2\" >This class provides a metric to visualize the spread between pairs of time series variables in a given dataset. By plotting the spread of each pair of variables in separate figures, users can assess the relationship between the variables and determine if any cointegration or other time series relationships exist between them.</td>\n",
       "      <td id=\"T_d94fa_row22_col3\" class=\"data row22 col3\" >validmind.data_validation.SpreadPlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row23_col0\" class=\"data row23 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row23_col1\" class=\"data row23 col1\" >TimeSeriesLinePlot</td>\n",
       "      <td id=\"T_d94fa_row23_col2\" class=\"data row23 col2\" >Generates a visual analysis of time series data by plotting the\n",
       "    raw time series. The input dataset can have multiple time series\n",
       "    if necessary. In this case we produce a separate plot for each time series.</td>\n",
       "      <td id=\"T_d94fa_row23_col3\" class=\"data row23 col3\" >validmind.data_validation.TimeSeriesLinePlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row24_col0\" class=\"data row24 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row24_col1\" class=\"data row24 col1\" >AutoSeasonality</td>\n",
       "      <td id=\"T_d94fa_row24_col2\" class=\"data row24 col2\" >Automatically detects the optimal seasonal order for a time series dataset\n",
       "    using the seasonal_decompose method.</td>\n",
       "      <td id=\"T_d94fa_row24_col3\" class=\"data row24 col3\" >validmind.data_validation.AutoSeasonality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row25_col0\" class=\"data row25 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row25_col1\" class=\"data row25 col1\" >EngleGrangerCoint</td>\n",
       "      <td id=\"T_d94fa_row25_col2\" class=\"data row25 col2\" >Test for cointegration between pairs of time series variables in a given dataset using the Engle-Granger test.</td>\n",
       "      <td id=\"T_d94fa_row25_col3\" class=\"data row25 col3\" >validmind.data_validation.EngleGrangerCoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row26_col0\" class=\"data row26 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row26_col1\" class=\"data row26 col1\" >TimeSeriesMissingValues</td>\n",
       "      <td id=\"T_d94fa_row26_col2\" class=\"data row26 col2\" >Test that the number of missing values is less than a threshold</td>\n",
       "      <td id=\"T_d94fa_row26_col3\" class=\"data row26 col3\" >validmind.data_validation.TimeSeriesMissingValues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row27_col0\" class=\"data row27 col0\" >DatasetMetadata</td>\n",
       "      <td id=\"T_d94fa_row27_col1\" class=\"data row27 col1\" >DatasetMetadata</td>\n",
       "      <td id=\"T_d94fa_row27_col2\" class=\"data row27 col2\" >Custom class to collect a set of descriptive statistics for a dataset.\n",
       "    This class will log dataset metadata via `log_dataset` instead of a metric.\n",
       "    Dataset metadata is necessary to initialize dataset object that can be related\n",
       "    to different metrics and test results</td>\n",
       "      <td id=\"T_d94fa_row27_col3\" class=\"data row27 col3\" >validmind.data_validation.DatasetMetadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row28_col0\" class=\"data row28 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row28_col1\" class=\"data row28 col1\" >TimeSeriesHistogram</td>\n",
       "      <td id=\"T_d94fa_row28_col2\" class=\"data row28 col2\" >Generates a visual analysis of time series data by plotting the\n",
       "    histogram. The input dataset can have multiple time series if\n",
       "    necessary. In this case we produce a separate plot for each time series.</td>\n",
       "      <td id=\"T_d94fa_row28_col3\" class=\"data row28 col3\" >validmind.data_validation.TimeSeriesHistogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row29_col0\" class=\"data row29 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row29_col1\" class=\"data row29 col1\" >LaggedCorrelationHeatmap</td>\n",
       "      <td id=\"T_d94fa_row29_col2\" class=\"data row29 col2\" >Generates a heatmap of correlations between the target variable and the lags of independent variables in the dataset.</td>\n",
       "      <td id=\"T_d94fa_row29_col3\" class=\"data row29 col3\" >validmind.data_validation.LaggedCorrelationHeatmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row30_col0\" class=\"data row30 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row30_col1\" class=\"data row30 col1\" >SeasonalDecompose</td>\n",
       "      <td id=\"T_d94fa_row30_col2\" class=\"data row30 col2\" >Calculates seasonal_decompose metric for each of the dataset features</td>\n",
       "      <td id=\"T_d94fa_row30_col3\" class=\"data row30 col3\" >validmind.data_validation.SeasonalDecompose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row31_col0\" class=\"data row31 col0\" >ThresholdTest</td>\n",
       "      <td id=\"T_d94fa_row31_col1\" class=\"data row31 col1\" >ClassImbalance</td>\n",
       "      <td id=\"T_d94fa_row31_col2\" class=\"data row31 col2\" >The class imbalance test measures the disparity between the majority\n",
       "    class and the minority class in the target column.</td>\n",
       "      <td id=\"T_d94fa_row31_col3\" class=\"data row31 col3\" >validmind.data_validation.ClassImbalance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row32_col0\" class=\"data row32 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row32_col1\" class=\"data row32 col1\" >AutoAR</td>\n",
       "      <td id=\"T_d94fa_row32_col2\" class=\"data row32 col2\" >Automatically detects the AR order of a time series using both BIC and AIC.</td>\n",
       "      <td id=\"T_d94fa_row32_col3\" class=\"data row32 col3\" >validmind.data_validation.AutoAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d94fa_row33_col0\" class=\"data row33 col0\" >Metric</td>\n",
       "      <td id=\"T_d94fa_row33_col1\" class=\"data row33 col1\" >TabularDateTimeHistograms</td>\n",
       "      <td id=\"T_d94fa_row33_col2\" class=\"data row33 col2\" >Generates a visual analysis of datetime data by plotting histograms of\n",
       "    differences between consecutive dates. The input dataset can have multiple\n",
       "    datetime variables if necessary. In this case, we produce a separate plot\n",
       "    for each datetime variable.</td>\n",
       "      <td id=\"T_d94fa_row33_col3\" class=\"data row33 col3\" >validmind.data_validation.TabularDateTimeHistograms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x287d39f60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validmind.tests import list_tests, load_test, describe_test\n",
    "\n",
    "list_tests(filter=\"data_validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Raw Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Lending Club Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the zip file\n",
    "# filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2011/lending_club_loan_data_2007_2011.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# ONLY FOR TESTING\n",
    "\n",
    "\n",
    "# Perform operations on the DataFrame as needed\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Description on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.data_validation.metrics import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Unused Variables\n",
    "\n",
    "Remove all the **Demographic** and **Customer Behavioural** features which is of no use for default analysis for credit approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-required columns\n",
    "# id - not required\n",
    "# member_id - not required\n",
    "# acc_now_delinq - empty\n",
    "# funded_amnt - not useful, funded_amnt_inv is useful which is funded to person\n",
    "# emp_title - brand names not useful\n",
    "# pymnt_plan - fixed value as n for all\n",
    "# url - not useful\n",
    "# desc - can be applied some NLP but not for EDA\n",
    "# title - too many distinct values not useful\n",
    "# zip_code - complete zip is not available\n",
    "# delinq_2yrs - post approval feature\n",
    "# mths_since_last_delinq - only half values are there, not much information\n",
    "# mths_since_last_record - only 10% values are there\n",
    "# revol_bal - post/behavioural feature\n",
    "# initial_list_status - fixed value as f for all\n",
    "# out_prncp - post approval feature\n",
    "# out_prncp_inv - not useful as its for investors\n",
    "# total_pymnt - post approval feature\n",
    "# total_pymnt_inv - not useful as it is for investors\n",
    "# total_rec_prncp - post approval feature\n",
    "# total_rec_int - post approval feature\n",
    "# total_rec_late_fee - post approval feature\n",
    "# recoveries - post approval feature\n",
    "# collection_recovery_fee - post approval feature\n",
    "# last_pymnt_d - post approval feature\n",
    "# last_credit_pull_d - irrelevant for approval\n",
    "# last_pymnt_amnt - post feature\n",
    "# next_pymnt_d - post feature\n",
    "# collections_12_mths_ex_med - only 1 value \n",
    "# policy_code - only 1 value\n",
    "# acc_now_delinq - single valued\n",
    "# application_type - single\n",
    "# pub_rec_bankruptcies - single valued for more than 99%\n",
    "# addr_state - may not depend on location as its in financial domain\n",
    "\n",
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"pymnt_plan\", \"url\", \"desc\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"initial_list_status\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\",\n",
    "                    \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\",\n",
    "                    \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"next_pymnt_d\", \"last_credit_pull_d\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"application_type\", \"addr_state\"]\n",
    "df_selected_vars = df.drop(columns=unused_variables)\n",
    "print(\"Features we are left with\",list(df_selected_vars.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_vars.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process `emp_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_emp_length(df, column='emp_length'):\n",
    "    # Define a mapping from original string values to numeric values\n",
    "    mapping = {'10+ years': 10, '< 1 year': 0, '1 year': 1, '2 years': 2, \n",
    "               '3 years': 3, '4 years': 4, '5 years': 5, '6 years': 6, \n",
    "               '7 years': 7, '8 years': 8, '9 years': 9, np.nan: np.nan}\n",
    "    \n",
    "    # Apply the mapping to the specified column\n",
    "    df[column] = df[column].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_vars = process_emp_length(df_selected_vars, 'emp_length')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df, columns):\n",
    "    # Specify the date format\n",
    "    date_format = \"%b-%y\"\n",
    "\n",
    "    # Iterate over the specified columns and convert to datetime\n",
    "    for column in columns:\n",
    "        df[column] = pd.to_datetime(df[column], format=date_format)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the specified columns to datetime\n",
    "columns_to_convert = ['issue_d']\n",
    "df_dates_fixed = convert_to_datetime(df_selected_vars, columns_to_convert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    return variables_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df_dates_fixed, min_missing_count)\n",
    "df_no_missing = df_dates_fixed.drop(columns=variables_to_drop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df_no_missing.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Loan Status `Current` \n",
    "\n",
    "Removing records with loan status as **`Current`**, as the loan is currently running and we can’t infer any information regarding default from such loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with loan_status as \"Current\"\n",
    "df_no_current = df_no_missing[df_no_missing[\"loan_status\"].apply(lambda x: False if x == \"Current\" else True)]\n",
    "\n",
    "# Update loan_status as Fully Paid to 0 and Charged Off to 1\n",
    "df_no_current[\"loan_status\"] = df_no_current[\"loan_status\"].apply(lambda x: 0 if x == \"Fully Paid\" else 1)\n",
    "\n",
    "# Convert 'emp_length' to string type\n",
    "df_no_current[\"emp_length\"] = df_no_current[\"emp_length\"].astype(str)\n",
    "\n",
    "# Update emp_length feature with continuous values as int\n",
    "# where (< 1 year) is assumed as 0 and 10+ years is assumed as 10 and rest are stored as their magnitude\n",
    "df_no_current[\"emp_length\"] = pd.to_numeric(df_no_current[\"emp_length\"].apply(lambda x: 0 if \"<\" in x else (x.split('+')[0] if \"+\" in x else x.split()[0])))\n",
    "\n",
    "# Look through the purpose value counts\n",
    "loan_purpose_values = df_no_current[\"purpose\"].value_counts() * 100 / df_no_current.shape[0]\n",
    "\n",
    "# Remove rows with less than 1% of value counts in particular purpose \n",
    "loan_purpose_delete = loan_purpose_values[loan_purpose_values < 1].index.values\n",
    "df_processed = df_no_current[[False if p in loan_purpose_delete else True for p in df_no_current[\"purpose\"]]]\n",
    "\n",
    "# Update int_rate, revol_util without % sign and as numeric type\n",
    "df_processed[\"int_rate\"] = pd.to_numeric(df_processed[\"int_rate\"].apply(lambda x:x.split('%')[0]))\n",
    "df_processed[\"revol_util\"] = pd.to_numeric(df_processed[\"revol_util\"].apply(lambda x:x.split('%')[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add New Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting month and year from issue_date\n",
    "df_processed['month'] = df_processed['issue_d'].apply(lambda x: x.month)\n",
    "df_processed['year'] = df_processed['issue_d'].apply(lambda x: x.year)\n",
    "\n",
    "# Get year from issue_d and replace the same\n",
    "df_processed[\"earliest_cr_line\"] = pd.to_numeric(df_processed[\"earliest_cr_line\"].apply(lambda x:x.split('-')[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning Continuous Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `loan_amnt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for loan_amnt range\n",
    "bins = [0, 5000, 10000, 15000, 20000, 25000, 36000]\n",
    "bucket_l = ['0-5000', '5000-10000', '10000-15000', '15000-20000', '20000-25000','25000+']\n",
    "df_processed['loan_amnt_range'] = pd.cut(df_processed['loan_amnt'], bins, labels=bucket_l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `int_rate` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'int_rate' to numeric\n",
    "df_processed['int_rate'] = pd.to_numeric(df_processed['int_rate'], errors='coerce')\n",
    "\n",
    "# Create bins for int_rate range\n",
    "bins = [0, 7.5, 10, 12.5, 15, 100]\n",
    "bucket_l = ['0-7.5', '7.5-10', '10-12.5', '12.5-15', '15+']\n",
    "\n",
    "# Using pd.cut to create 'int_rate_range' column\n",
    "df_processed['int_rate_range'] = pd.cut(df_processed['int_rate'], bins, labels=bucket_l)\n",
    "\n",
    "# Convert NaN to 'Unknown'\n",
    "df_processed['int_rate_range'] = df_processed['int_rate_range'].cat.add_categories('Unknown')\n",
    "df_processed['int_rate_range'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `annual_inc` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for annual_inc range\n",
    "bins = [0, 25000, 50000, 75000, 100000, 1000000]\n",
    "bucket_l = ['0-25000', '25000-50000', '50000-75000', '75000-100000', '100000+']\n",
    "df_processed['annual_inc_range'] = pd.cut(df_processed['annual_inc'], bins, labels=bucket_l)\n",
    "\n",
    "# Convert NaN to 'Unknown'\n",
    "df_processed['annual_inc_range'] = df_processed['annual_inc_range'].cat.add_categories('Unknown')\n",
    "df_processed['annual_inc_range'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `installment` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for installment range\n",
    "def installment(n):\n",
    "    if n <= 200:\n",
    "        return 'low'\n",
    "    elif n > 200 and n <=500:\n",
    "        return 'medium'\n",
    "    elif n > 500 and n <=800:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very high'\n",
    "\n",
    "df_processed['installment'] = df_processed['installment'].apply(lambda x: installment(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bins for `dti` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for dti range\n",
    "bins = [-1, 5.00, 10.00, 15.00, 20.00, 25.00, 50.00]\n",
    "bucket_l = ['0-5%', '5-10%', '10-15%', '15-20%', '20-25%', '25%+']\n",
    "df_processed['dti_range'] = pd.cut(df_processed['dti'], bins, labels=bucket_l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Description on Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_df = vm.init_dataset(dataset=df_processed,\n",
    "                        target_column='loan_status')\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for amount of defaults in the data using countplot\n",
    "plt.figure()\n",
    "sns.countplot(y=\"loan_status\", data=df_processed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "metric = ClassImbalance(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb72d5995d9459495c8d8bdc9a2e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Missing ❌</h2>\\n            <p>Test that the number of missing va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.MissingValues import MissingValues\n",
    "metric = MissingValues(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularNumericalHistograms\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()\n",
    "\n",
    "HighCardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularDateTimeHistograms\n",
    "metric = TabularDateTimeHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan Defaults Ratio by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import LoanDefaultRatio\n",
    "\n",
    "# Select numerical and categorical features \n",
    "numerical_features = ['emp_length', 'month', 'year', 'earliest_cr_line', 'inq_last_6mths', 'revol_util', 'total_acc',\n",
    "                       'loan_amnt_range', 'int_rate_range', 'dti_range', 'installment', 'annual_inc_range']\n",
    "categorical_features = ['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status', 'purpose', 'open_acc', 'pub_rec']\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"loan_status_col\": \"loan_status\",\n",
    "    \"columns\": numerical_features + categorical_features\n",
    "}\n",
    "\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "metric = LoanDefaultRatio(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select variables for multivariate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = ['loan_status']\n",
    "selected_features = ['term', 'grade', 'purpose', 'pub_rec',\n",
    "                      'revol_util', 'funded_amnt_inv', 'int_rate', \n",
    "                      'annual_inc_range', 'dti', 'installment',\n",
    "                      'loan_amnt_range', 'annual_inc', 'loan_amnt',\n",
    "                      'earliest_cr_line']\n",
    "df_multivariate = df_processed.loc[:, selected_features + target_variable]\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_multivariate)\n",
    "test_context = TestContext(dataset=vm_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateBarPlots import BivariateBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "variable_pairs = {'annual_inc_range': 'purpose', \n",
    "                  'term': 'purpose', \n",
    "                  'grade': 'purpose',\n",
    "                  'loan_amnt_range': 'purpose',\n",
    "                  'loan_amnt_range': 'term',\n",
    "                  'installment': 'purpose'}\n",
    "\n",
    "params = {\n",
    "    \"variable_pairs\": variable_pairs,\n",
    "    \"loan_status_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateBarPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bar Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_barplots(data, variable_pairs):\n",
    "    for x, hue in variable_pairs.items():\n",
    "        # Calculate the means\n",
    "        means = data.groupby([x, hue])['loan_status'].mean().unstack().reset_index()\n",
    "        hue_categories = means.columns[1:]  # get hue categories\n",
    "\n",
    "        n = len(hue_categories)  # number of hue categories\n",
    "        width = 1 / (n + 1)  # width of bars\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        # Create a color palette\n",
    "        color_palette = {category: color for category, color in zip(hue_categories, plt.cm.get_cmap('tab10').colors)}\n",
    "\n",
    "        for i, hue_category in enumerate(hue_categories):\n",
    "            plt.bar(np.arange(len(means)) + i * width, means[hue_category], color=color_palette[hue_category], \n",
    "                    alpha=0.7, label=hue_category, width=width)\n",
    "\n",
    "        plt.title(x + \" by \" + hue)\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(\"Loan Default Ratio\")\n",
    "        plt.xticks(ticks=np.arange(len(means)), labels=means[x], rotation=90)  # set x-tick labels\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_pairs = {'annual_inc_range': 'purpose', \n",
    "                  'term': 'purpose', \n",
    "                  'grade': 'purpose',\n",
    "                  'loan_amnt_range': 'purpose',\n",
    "                  'loan_amnt_range': 'term',\n",
    "                  'installment': 'purpose'}\n",
    "bivariate_barplots(df_multivariate, variable_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multivariate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bivariate_scatterplots(data, variable_pairs, loan_status_filter=None):\n",
    "    if loan_status_filter is not None:\n",
    "        # Apply the filter if it is specified\n",
    "        data = data[data['loan_status'] == loan_status_filter]\n",
    "        \n",
    "    for x, y in variable_pairs.items():\n",
    "        plt.figure()\n",
    "        \n",
    "        # Scatterplot using seaborn, with color variation based on 'loan_status'\n",
    "        # Create color mapping with rgba values, last value is alpha (transparency)\n",
    "        palette = {0: (0.8, 0.8, 0.8, 0.8), 1: 'tab:red'}\n",
    "        plot = sns.scatterplot(data=data, x=x, y=y, hue='loan_status', palette=palette, alpha=1) # set alpha to 1, transparency is managed by the color specification\n",
    "        \n",
    "        # Change legend labels\n",
    "        legend_labels = ['Default' if t.get_text()=='1' else 'Non-default' for t in plot.legend_.texts[1:]]  # Ignore the first text, which is \"loan_status\"\n",
    "        plot.legend_.texts[1:] = legend_labels\n",
    "\n",
    "        plt.title(x + \" and \" + y)\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "bivariate_scatterplots(df_multivariate, variable_pairs, loan_status_filter=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bivariate Histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_histograms(data, variable_pairs, loan_status_filter=None):\n",
    "    if loan_status_filter is not None:\n",
    "        # Apply the filter if it is specified\n",
    "        data = data[data['loan_status'] == loan_status_filter]\n",
    "\n",
    "    # Create color mapping with rgba values, last value is alpha (transparency)\n",
    "    palette = {0: (0.5, 0.5, 0.5, 0.8), 1: 'tab:red'}\n",
    "\n",
    "    for x, y in variable_pairs.items():\n",
    "        fig, axes = plt.subplots(2, 1,)\n",
    "\n",
    "        for ax, var in zip(axes, [x, y]):\n",
    "            for loan_status, color in palette.items():\n",
    "                subset = data[data['loan_status'] == loan_status]\n",
    "                sns.histplot(subset[var],\n",
    "                             ax=ax, \n",
    "                             color=color,\n",
    "                             edgecolor=None, \n",
    "                             kde=True, \n",
    "                             label='Default' if loan_status else 'Non-default')\n",
    "            \n",
    "            ax.set_title(f\"Histogram of {var} by loan status\")\n",
    "            ax.set_xlabel(var)\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "bivariate_histograms(df_multivariate, variable_pairs, loan_status_filter=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-eEL8LtKG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
