{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Default Model using ValidMind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Connect Notebook to ValidMind Project\n",
    "- Step 2: Import Raw Data\n",
    "- Step 3: Data Description on Raw Data\n",
    "- Step 4: Data Preprocessing\n",
    "- Step 5: Data Description on Preprocessed Data \n",
    "- Step 6: Univariate Analysis\n",
    "- Step 7: Multivariate Analysis\n",
    "- Step 8: Model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect Notebook to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect Notebook to ValidMind Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Raw Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Lending Club Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the zip file\n",
    "# filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2011/lending_club_loan_data_2007_2011.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.data_validation.metrics import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.MissingValues import MissingValues\n",
    "metric = MissingValues(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "#vm_df_train = vm.init_dataset(dataset=df_train,\n",
    "#                        target_column = target_column)\n",
    "#test_context = TestContext(dataset=vm_df_train)\n",
    "#metric = ClassImbalance(test_context)\n",
    "#metric.run()\n",
    "#metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Target Variable\n",
    "\n",
    "**Definition of Default**\n",
    "\n",
    "We categorizing `Fully Paid` loans as \"default = 0\" and `Charged Off` loans as \"default = 1\". This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk). \n",
    "\n",
    "Loans with `Current` status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add `default` Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_column(df, target_column):\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else 'Current')\n",
    "    # Remove rows where the target column is 'Current'\n",
    "    df = df[df[target_column] != 'Current']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'default'\n",
    "df_prep = add_target_column(df, target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unused Variables\n",
    "\n",
    "Remove all the **Demographic** and **Customer Behavioural** features which is of no use for default analysis for credit approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"pymnt_plan\", \"url\", \"desc\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"initial_list_status\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\",\n",
    "                    \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\",\n",
    "                    \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\", \"next_pymnt_d\", \"last_credit_pull_d\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"application_type\", \"addr_state\",\n",
    "                    \"pub_rec_bankruptcies\", \"chargeoff_within_12_mths\", \"tax_liens\", \"delinq_amnt\"]\n",
    "df_prep = df_prep.drop(columns=unused_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df_prep, min_missing_count)\n",
    "df_prep.drop(columns=variables_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df_prep.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Description after Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_df_prep = vm.init_dataset(dataset=df_prep)\n",
    "test_context = TestContext(dataset=vm_df_prep)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with loan_status as \"Current\"\n",
    "# df_prep = df_prep[df_prep[\"loan_status\"].apply(lambda x: False if x == \"Current\" else True)]\n",
    "\n",
    "# Update loan_status as Fully Paid to 0 and Charged Off to 1\n",
    "#df_no_current[\"loan_status\"] = df_no_current[\"loan_status\"].apply(lambda x: 0 if x == \"Fully Paid\" else 1)\n",
    "\n",
    "# Convert 'emp_length' to string type\n",
    "#df_no_current[\"emp_length\"] = df_no_current[\"emp_length\"].astype(str)\n",
    "\n",
    "# Update emp_length feature with continuous values as int\n",
    "# where (< 1 year) is assumed as 0 and 10+ years is assumed as 10 and rest are stored as their magnitude\n",
    "#df_no_current[\"emp_length\"] = pd.to_numeric(df_no_current[\"emp_length\"].apply(lambda x: 0 if \"<\" in x else (x.split('+')[0] if \"+\" in x else x.split()[0])))\n",
    "\n",
    "# Look through the purpose value counts\n",
    "#loan_purpose_values = df_no_current[\"purpose\"].value_counts() * 100 / df_no_current.shape[0]\n",
    "\n",
    "# Remove rows with less than 1% of value counts in particular purpose \n",
    "#loan_purpose_delete = loan_purpose_values[loan_purpose_values < 1].index.values\n",
    "#df_processed = df_no_current[[False if p in loan_purpose_delete else True for p in df_no_current[\"purpose\"]]]\n",
    "\n",
    "# Update int_rate, revol_util without % sign and as numeric type\n",
    "#df_processed[\"int_rate\"] = pd.to_numeric(df_processed[\"int_rate\"].apply(lambda x:x.split('%')[0]))\n",
    "#df_processed[\"revol_util\"] = pd.to_numeric(df_processed[\"revol_util\"].apply(lambda x:x.split('%')[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Sampling \n",
    "\n",
    "We employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the `stratify = y` parameter is set, it ensures that the distribution of the target variable (`y`) in the test set is the same as that in the original dataset. \n",
    "\n",
    "This is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test \n",
    "X = df_prep.drop(target_column, axis = 1)\n",
    "y = df_prep[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 42, stratify = y)\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Cleaning \n",
    "\n",
    "We perform data cleaning after splitting the data into training and testing sets to prevent data leakage, maintain the test set's independence, and avoid overfitting. Data leakage can occur when information from the test set influences the training set, leading to overly optimistic performance. \n",
    "\n",
    "Cleaning data separately ensures the test set acts as unseen data, mimicking real-world scenarios more accurately. It also helps to prevent the model from overfitting to the training data's specific characteristics. However, the same cleaning steps should be consistently applied to both training and test data, often facilitated by using saved parameters from the training data or employing pipelines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dates(df, column):\n",
    "    \"\"\"\n",
    "    Convert date columns to datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    column (str): Name of the date column to be processed.\n",
    "    \"\"\"\n",
    "    df[column] = pd.to_datetime(df[column], format = \"%b-%y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_int_rate(df, column):\n",
    "    \"\"\"\n",
    "    Clean interest rate column. Remove the '%' sign and convert to numeric.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    column (str): Name of the interest rate column to be cleaned.\n",
    "    \"\"\"\n",
    "    df[column] = df[column].str.replace('%', '')\n",
    "    df[column] = pd.to_numeric(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_emp_length(df, column):\n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "    df[column].fillna(value = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove 'months' string from the 'term' column and convert it to numeric\n",
    "def clean_loan_term(df, column):\n",
    "    df[column] = pd.to_numeric(df[column].str.replace(' months', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dates(df_train, 'earliest_cr_line')\n",
    "clean_dates(df_train, 'issue_d')\n",
    "clean_int_rate(df_train, 'int_rate')\n",
    "clean_emp_length(df_train, 'emp_length')\n",
    "clean_loan_term(df_train, 'term')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dates(df_test, 'earliest_cr_line')\n",
    "clean_dates(df_test, 'issue_d')\n",
    "clean_int_rate(df_test, 'int_rate')\n",
    "clean_emp_length(df_test, 'emp_length')\n",
    "clean_loan_term(df_test, 'term')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description after Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Data Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_df_test = vm.init_dataset(dataset=df_test)\n",
    "test_context = TestContext(dataset=vm_df_test)\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['grade', 'home_ownership', 'verification_status', 'purpose']\n",
    "numerical_features = ['loan_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'emp_length', 'annual_inc', 'dti', 'inq_last_6mths', 'open_acc', 'total_acc']\n",
    "date_time_columns = ['issue_d', 'earliest_cr_line']\n",
    "selected_columns = categorical_features + numerical_features + date_time_columns + [target_column]\n",
    " \n",
    "# Select these columns from X_train and X_test\n",
    "df_train = df_train[selected_columns]\n",
    "df_test = df_test[selected_columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularNumericalHistograms\n",
    "\n",
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Cardinality of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Datetime Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import TabularDateTimeHistograms\n",
    "metric = TabularDateTimeHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Ratios by Categorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.data_validation.metrics import DefaultRatioBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": target_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = DefaultRatioBarPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Bar Plots of Default Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateFeaturesBarPlots import BivariateFeaturesBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "features_pairs = {'home_ownership': 'grade', \n",
    "                  'purpose': 'grade',\n",
    "                  'grade': 'verification_status'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "}\n",
    "\n",
    "# Pass target column to validmind dataset\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "metric = BivariateFeaturesBarPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots by Default Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateScatterPlots import BivariateScatterPlots\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateScatterPlots(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.BivariateHistograms import BivariateHistograms\n",
    "\n",
    "features_pairs = {'int_rate': 'annual_inc', \n",
    "                  'funded_amnt_inv': 'dti', \n",
    "                  'annual_inc': 'funded_amnt_inv',\n",
    "                  'loan_amnt': 'int_rate',\n",
    "                  'int_rate': 'annual_inc',\n",
    "                  'earliest_cr_line': 'int_rate'}\n",
    "\n",
    "params = {\n",
    "    \"features_pairs\": features_pairs,\n",
    "    \"target_filter\": None\n",
    "}\n",
    "\n",
    "metric = BivariateHistograms(test_context, params=params)\n",
    "metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Engineering "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_variables(df, columns_list):\n",
    "    \"\"\"\n",
    "    Generate dummy variables for specified columns in the DataFrame,\n",
    "    concatenate them with the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column + \":\", drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_dummy_variables(df_train, ['grade', 'home_ownership', 'verification_status', 'purpose'])\n",
    "df_test = add_dummy_variables(df_test, ['grade', 'home_ownership', 'verification_status', 'purpose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the X_test DataFrame to match the column structure of the X_train DataFrame\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight of Evidence (WoE) Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_woe_iv(df, target_column, features=None):\n",
    "    \"\"\"\n",
    "    Calculate the Weight of Evidence (WoE) and Information Value (IV)\n",
    "    of categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed. It should contain the target column.\n",
    "    target_column (str): Name of the target column in the DataFrame.\n",
    "    features (list, optional): List of feature names for which WoE and IV is to be calculated. \n",
    "                               If None, all features in df will be used.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the WoE and IV for each category of the feature(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no specific features specified, use all columns in the DataFrame\n",
    "    if features is None:\n",
    "        features = df.drop(target_column, axis=1).columns.tolist()\n",
    "\n",
    "    # Create a dataframe to store WoE and IV values\n",
    "    master = []\n",
    "    \n",
    "    for feature in features:\n",
    "        lst = []\n",
    "        \n",
    "        # For each unique category in the feature\n",
    "        for val in df[feature].unique():\n",
    "            lst.append({\n",
    "                'Variable': feature,\n",
    "                'Value': val,\n",
    "                'All': df[df[feature] == val].count()[feature],\n",
    "                'Good': df[(df[feature] == val) & (df[target_column] == 0)].count()[feature],\n",
    "                'Bad': df[(df[feature] == val) & (df[target_column] == 1)].count()[feature]\n",
    "            })\n",
    "            \n",
    "        dset = pd.DataFrame(lst)\n",
    "        \n",
    "        # Calculate WoE and IV\n",
    "        dset['Distr_Good'] = dset['Good'] / dset['Good'].sum()\n",
    "        dset['Distr_Bad'] = dset['Bad'] / dset['Bad'].sum()\n",
    "        dset['WoE'] = np.log(dset['Distr_Good'] / dset['Distr_Bad'])\n",
    "        dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n",
    "\n",
    "        master.append(dset)\n",
    "    \n",
    "    master_dset = pd.concat(master, ignore_index=True)\n",
    "    \n",
    "    return master_dset.sort_values(by=['Variable', 'WoE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_woe_iv_df = calculate_woe_iv(df_train, target_column, categorical_features)\n",
    "display(categorical_woe_iv_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning numerical features for WOE and IV calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_numerical_variables(df, columns_list, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Bin the specified numerical columns into categories.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to be processed.\n",
    "    columns_list (list): List of column names to be processed.\n",
    "    bins (int or sequence, optional): Number of bins to create, or a sequence representing the bins.\n",
    "    labels (list, optional): Labels for the bins. Must be the same length as the resulting bins.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the new binned columns.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for column in columns_list:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # Convert the bin intervals to strings\n",
    "            df_copy[column+'_bin'] = pd.cut(df[column], bins=bins, labels=labels).astype(str)\n",
    "        else:\n",
    "            raise ValueError(f\"Column {column} is not numerical.\")\n",
    "        \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['loan_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'emp_length', 'annual_inc', 'dti', 'inq_last_6mths', 'open_acc', 'total_acc']\n",
    "df_train = bin_numerical_variables(df_train, numerical_features)\n",
    "\n",
    "# Create a list of binned features\n",
    "binned_features = [f+'_bin' for f in numerical_features]\n",
    "\n",
    "# Calculate WoE and IV for the binned features\n",
    "woe_iv_df_numerical = calculate_woe_iv(df_train, target_column, binned_features)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(woe_iv_df_numerical)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# We can now evaluate on the test set\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# First, we define the preprocessing steps\n",
    "numeric_features = ['pub_rec', 'revol_util', 'funded_amnt_inv', 'int_rate', 'dti', 'annual_inc', 'loan_amnt', 'earliest_cr_line']\n",
    "categorical_features = ['term', 'grade', 'purpose', 'annual_inc_range', 'loan_amnt_range', 'installment']  # Added 'installment'\n",
    "\n",
    "# Handle categorical features\n",
    "df_encoded = pd.get_dummies(df_multivariate, columns=categorical_features)\n",
    "\n",
    "# Split the data\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Define the model\n",
    "glm_model_fit = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = glm_model_fit.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test = sm.add_constant(X_test)  # Adding a constant to the test data\n",
    "y_pred = results.predict(X_test)\n",
    "\n",
    "# You can then further analyze y_pred to measure model performance on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale variable X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Scale your variables\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# Add a constant to the independent values\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y, X_scaled, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ValidMind Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training and testing datasets for model A\n",
    "vm_train_ds = vm.init_dataset(dataset=X_train, type=\"generic\", target_column='loan_status')\n",
    "vm_test_ds = vm.init_dataset(dataset=X_test, type=\"generic\", target_column='loan_status')\n",
    "\n",
    "# Initialize model A\n",
    "vm_model_A = vm.init_model(\n",
    "    model = glm_model_fit, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
