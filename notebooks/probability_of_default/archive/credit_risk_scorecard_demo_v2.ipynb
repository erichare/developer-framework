{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Scorecard Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you Begin\n",
    "To use the ValidMind Developer Framework with a Jupyter notebook, you need to install and initialize the client library first, along with getting your Python environment ready.\n",
    "\n",
    "If you don't already have one, you should also create a documentation project on the ValidMind platform. You will use this project to upload your documentation and test results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Client Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade validmind"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Client Library\n",
    "In a browser, go to the Client Integration page of your documentation project and click Copy to clipboard next to the code snippet. This code snippet gives you the API key, API secret, and project identifier to link your notebook to your documentation project.\n",
    "\n",
    "This step requires a documentation project. Learn how you can create one.\n",
    "\n",
    "Next, replace this placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"clk00h0u800x9qjy67gduf5om\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Credit risk Scorecard** model created from the Lending Club dataset is instrumental in computing the Probability of Default (PD), a key factor in ECL calculations. This scorecard assesses several credit characteristics of potential borrowers, like their credit history, income, outstanding debts, and more, each of which is assigned a specific score. By combining these scores, we derive a total score for each borrower, which translates into an estimated Point-in-Time (PiT) PD. The PiT PD reflects the borrower's likelihood of default at a specific point in time, accounting for both current and foreseeable future conditions.\n",
    "\n",
    "Additionally, for a holistic view of credit risk, it's essential to estimate the Lifetime PD. The Lifetime PD, as the name suggests, predicts the borrower's likelihood of default throughout the life of the exposure, taking into account potential future changes in the economic and financial conditions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Standard library imports\n",
    "import re\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# Data handling and analysis imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import inspect\n",
    "\n",
    "# Visualization imports\n",
    "%matplotlib inline\n",
    "\n",
    "# Scorecard development\n",
    "import scorecardpy as sc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_variables = [\n",
    "    \"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n",
    "    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n",
    "    'earliest_cr_line', 'issue_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, df, base_filename):\n",
    "    \"\"\"Save a model and a dataframe with a timestamp in the filename\"\"\"\n",
    "    # Get current date and time\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # Convert the current date and time to string\n",
    "    timestamp_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    filename = f'{base_filename}_{timestamp_str}.pkl'\n",
    "\n",
    "    # Save the model and dataframe\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump((model, df), file)\n",
    "        \n",
    "    print(f\"Model and dataframe saved as {filename}\")\n",
    "\n",
    "def get_numerical_columns(df):\n",
    "        numerical_columns = df.select_dtypes(\n",
    "            include=[\"int\", \"float\", \"uint\"]\n",
    "        ).columns.tolist()\n",
    "        return numerical_columns\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "        categorical_columns = df.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns.tolist()\n",
    "        return categorical_columns\n",
    "\n",
    "def add_target_column(df, target_column):\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    # Convert target column to integer\n",
    "    df[target_column] = df[target_column].astype(int)\n",
    "    return df\n",
    "\n",
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop\n",
    "\n",
    "def clean_term_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to remove 'months' string from the 'term' column and convert it to categorical\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].str.replace(' months', '')\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "def clean_emp_length_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to clean 'emp_length' column and convert it to categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column].fillna(value = 0, inplace=True)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "def clean_inq_last_6mths(df, column):\n",
    "    \"\"\"\n",
    "    Function to convert 'inq_last_6mths' column into categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "def compute_outliers(series, threshold=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return series[(series < lower_bound) | (series > upper_bound)]\n",
    "\n",
    "def remove_iqr_outliers(df, target_column, threshold=1.5):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols.remove(target_column)  # Exclude target_column from numerical columns\n",
    "    for col in num_cols:\n",
    "        outliers = compute_outliers(df[col], threshold)\n",
    "        df = df[~df[col].isin(outliers)]\n",
    "    return df\n",
    "\n",
    "def transform_woe_df(woe_df):\n",
    "    # Select and rename columns\n",
    "    transformed_df = woe_df[['variable', 'bin', 'count', 'count_distr', 'good', 'bad', 'badprob', 'woe', 'bin_iv', 'total_iv']].copy()\n",
    "    transformed_df.rename(columns={\n",
    "        'bin_iv': 'total_iv'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Create 'is_special_values' column (assuming there are no special values)\n",
    "    transformed_df['is_special_values'] = False\n",
    "    \n",
    "    # Transform 'bin' column into interval format and store it in 'breaks' column\n",
    "    transformed_df['breaks'] = transformed_df['bin'].apply(lambda x: '[-inf, %s)' % x if isinstance(x, float) else '[%s, inf)' % x)\n",
    "    \n",
    "    # Group by 'variable' to create bins dictionary\n",
    "    bins = {}\n",
    "    for variable, group in transformed_df.groupby('variable'):\n",
    "        bins[variable] = group\n",
    "    \n",
    "    return bins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Raw Data\n",
    "def import_raw_data(source): \n",
    "    print(\"Importing raw data from:\", source)\n",
    "    df_out = pd.read_csv(source)\n",
    "    print(f\"Data imported successfully with {df_out.shape[0]} rows and {df_out.shape[1]} columns.\")\n",
    "    return df_out\n",
    "\n",
    "def add_target_column(df, target_column):\n",
    "    # Check if 'loan_status' is in the DataFrame\n",
    "    if 'loan_status' not in df.columns:\n",
    "        raise ValueError(\"'loan_status' column not found in the DataFrame.\")\n",
    "\n",
    "    print(\"Converting 'loan_status' to target column...\")\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "\n",
    "    initial_row_count = df.shape[0]\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    removed_rows = initial_row_count - df.shape[0]\n",
    "    print(f\"Removed {removed_rows} rows with undefined 'loan_status' values.\")\n",
    "\n",
    "    # Convert target column to integer\n",
    "    df[target_column] = df[target_column].astype(int)\n",
    "    print(f\"Converted 'loan_status' to '{target_column}' and set its data type to integer.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def variables_with_min_missing(df, min_missing_count):\n",
    "    cols_to_drop = [col for col in df.columns if df[col].isnull().sum() / len(df) > min_missing_count / 100]\n",
    "    return cols_to_drop\n",
    "\n",
    "# Data Preparation: Add Definition of Default\n",
    "def add_default_definition(df, default_column, unused_variables):\n",
    "    print(\"Adding default definition...\")\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out = add_target_column(df_out, default_column)\n",
    "    print(\"Added target column based on default column.\")\n",
    "\n",
    "    # Drop 'loan_status' variable \n",
    "    df_out.drop(columns='loan_status', axis=1, inplace=True)\n",
    "    print(\"Dropped 'loan_status' column.\")\n",
    "\n",
    "    # Remove unused variables\n",
    "    df_out = df_out.drop(columns=unused_variables)\n",
    "    print(f\"Dropped {len(unused_variables)} unused columns.\")\n",
    "\n",
    "    # Remove missing values\n",
    "    min_missing_count = 80\n",
    "    variables_to_drop = variables_with_min_missing(df_out, min_missing_count)\n",
    "    df_out.drop(columns=variables_to_drop, axis=1, inplace=True)\n",
    "    print(f\"Dropped columns with more than {min_missing_count}% missing values.\")\n",
    "    df_out.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "    print(\"Dropped rows with missing 'emp_length'.\")\n",
    "    df_out.dropna(axis=0, subset=[\"revol_util\"], inplace=True)\n",
    "    print(\"Dropped rows with missing 'revol_util'.\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "# Data Sampling: Data Split \n",
    "def data_split(df, target_column):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Split data into train and test \n",
    "    X = df_out.drop(target_column, axis=1)\n",
    "    y = df_out[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(f\"Training data has {X_train.shape[0]} rows and {X_train.shape[1]} columns.\")\n",
    "    print(f\"Test data has {X_test.shape[0]} rows and {X_test.shape[1]} columns.\")\n",
    "\n",
    "    # Concatenate X_train with y_train to form df_train\n",
    "    df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Concatenate X_test with y_test to form df_test\n",
    "    df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "# EDA: Drop Unused Categories\n",
    "def drop_categories(df):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Initial count\n",
    "    initial_count = df_out.shape[0]\n",
    "\n",
    "    # Select rows where purpose is 'debt_consolidation' or 'credit_card'\n",
    "    df_out = df_out[df_out['purpose'].isin(['debt_consolidation', 'credit_card'])]\n",
    "    print(f\"Rows retained with purpose 'debt_consolidation' or 'credit_card': {df_out.shape[0]}\")\n",
    "\n",
    "    # Remove rows where grade is 'F' or 'G'\n",
    "    df_out = df_out[~df_out['grade'].isin(['F', 'G'])]\n",
    "    print(f\"Rows after removing grades 'F' or 'G': {df_out.shape[0]}\")\n",
    "\n",
    "    # Remove rows where sub_grade starts with 'F' or 'G'\n",
    "    df_out = df_out[~df_out['sub_grade'].str.startswith(('F', 'G'))]\n",
    "    print(f\"Rows after removing sub_grades starting with 'F' or 'G': {df_out.shape[0]}\")\n",
    "\n",
    "    # Remove rows where home_ownership is 'OTHER', 'NONE', or 'ANY'\n",
    "    df_out = df_out[~df_out['home_ownership'].isin(['OTHER', 'NONE', 'ANY'])]\n",
    "    print(f\"Rows after removing home_ownership values 'OTHER', 'NONE', or 'ANY': {df_out.shape[0]}\")\n",
    "\n",
    "    print(f\"Total rows dropped: {initial_count - df_out.shape[0]}\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# EDA: Drop Unused Features\n",
    "def drop_features(df, to_drop):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Before dropping\n",
    "    initial_cols = df_out.shape[1]\n",
    "    \n",
    "    df_out.drop(columns=to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # After dropping\n",
    "    after_drop_cols = df_out.shape[1]\n",
    "    \n",
    "    print(f\"Dropped {initial_cols - after_drop_cols} columns.\")\n",
    "    print(f\"Columns remaining after dropping: {after_drop_cols}\")\n",
    "\n",
    "    return df_out \n",
    "\n",
    "# EDA: Convert Features to WoE Values\n",
    "def convert_to_woe(df, woe_df, target_col):\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    # Placeholder for the transformation function - you'll need to define or import it\n",
    "    bins = transform_woe_df(woe_df)\n",
    "    \n",
    "    # Print how many features are getting transformed\n",
    "    print(f\"Converting {len(bins)} features to WoE values.\")\n",
    "    \n",
    "    # Make sure we don't transform the target column\n",
    "    if target_col in bins:\n",
    "        del bins[target_col]\n",
    "        print(f\"Excluded {target_col} from WoE transformation.\")\n",
    "    \n",
    "    # Apply the WoE transformation\n",
    "    df_out = sc.woebin_ply(df_out, bins=bins)\n",
    "    \n",
    "    print(f\"Successfully converted features to WoE values.\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "# Model Training: Add Constant\n",
    "def add_constant(df):\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Before adding constant\n",
    "    initial_cols = df_out.shape[1]\n",
    "\n",
    "    # Add constant\n",
    "    df_out = sm.add_constant(df_out)\n",
    "\n",
    "    # After adding constant\n",
    "    after_add_cols = df_out.shape[1]\n",
    "\n",
    "    print(f\"Added constant to dataframe. Number of columns went from {initial_cols} to {after_add_cols}.\")\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# Model Training: Train Model\n",
    "def train_model(df, target_column):\n",
    "    \n",
    "    # Ensure that the target column is in the DataFrame\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"'{target_column}' not found in DataFrame.\")\n",
    "\n",
    "    # Get X (features) and y (target) from df\n",
    "    X = df.drop(target_column, axis=1)  # Drop the target column to get features\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Define the model\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "\n",
    "    print(f\"Training the model with {X.shape[1]} features and {X.shape[0]} data points.\")\n",
    "\n",
    "    # Fit the model\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    print(\"Model trained successfully.\")\n",
    "\n",
    "    return model_fit\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import inspect\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up the logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='INFO: %(message)s')\n",
    "\n",
    "class Developer:\n",
    "    def __init__(self):\n",
    "        self.tasks_log = []\n",
    "        self.tasks_details = []\n",
    "        self.tasks = {}  # Dictionary to store tasks\n",
    "\n",
    "    def add_task(self, task_id, task):\n",
    "        \"\"\"Register a task.\"\"\"\n",
    "        if task_id in self.tasks:\n",
    "            raise ValueError(f\"Task ID '{task_id}' already exists!\")\n",
    "        self.tasks[task_id] = {'task': task}\n",
    "        return task_id\n",
    "\n",
    "    def get_caller_info(self, frame):\n",
    "        \"\"\"Fetch the calling line of code and the variable names.\"\"\"\n",
    "        code_context = inspect.getframeinfo(frame).code_context\n",
    "        line_of_code = code_context[0].strip() if code_context else \"\"\n",
    "        input_vars = {id(var): name for name, var in frame.f_locals.items()}\n",
    "        return line_of_code, input_vars\n",
    "\n",
    "    def get_task(self, task_id):\n",
    "        \"\"\"Retrieve task entry based on the task ID.\"\"\"\n",
    "        task_entry = self.tasks.get(task_id)\n",
    "        if not task_entry:\n",
    "            raise ValueError(f\"No task found for ID {task_id}\")\n",
    "        return task_entry\n",
    "\n",
    "    def execute_task(self, task_id, inputs=None, area_id=None):\n",
    "        if inputs is None:\n",
    "            inputs = []\n",
    "        \n",
    "        logging.info(f\"Executing task '{task_id}'...\\n\")\n",
    "        \n",
    "        frame = inspect.currentframe().f_back\n",
    "        line_of_code, input_vars = self.get_caller_info(frame)\n",
    "        input_var_names = [input_vars.get(id(inp), \"N/A\") for inp in inputs]\n",
    "        task_entry = self.get_task(task_id)\n",
    "        \n",
    "        result = task_entry['task'](*inputs)\n",
    "        \n",
    "        # Extract the variable name to which the result is assigned\n",
    "        output_match = re.search(r'^\\s*([\\w\\s,]+?)\\s*=', line_of_code)\n",
    "        output_var_name = output_match.group(1).replace(\" \", \"\") if output_match else \"N/A\"\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        end_time = datetime.datetime.now()\n",
    "        duration = (end_time - start_time).seconds\n",
    "        \n",
    "        # Log the task details internally\n",
    "        self.tasks_log.append(task_id)\n",
    "        self.tasks_details.append({\n",
    "            'Time': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'Area ID': area_id,\n",
    "            'Task ID': task_id,\n",
    "            'Input': \", \".join(input_var_names),\n",
    "            'Output': output_var_name,\n",
    "            'Duration': f\"{duration} seconds\"\n",
    "        })\n",
    "\n",
    "        return result\n",
    "\n",
    "    def show_lifecycle(self):\n",
    "        \"\"\"Display the model lifecycle details in a tabular format.\"\"\"\n",
    "        df = pd.DataFrame(self.tasks_details)\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Developer Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Developer class\n",
    "developer = Developer()\n",
    "\n",
    "# Set parameters\n",
    "default_column = \"default\"\n",
    "\n",
    "# Register developer tasks\n",
    "developer.add_task(\n",
    "    task_id=\"import_raw_data\", \n",
    "    task=import_raw_data,\n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"add_default_definition\",\n",
    "    task=add_default_definition,  \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"data_split\",\n",
    "    task=data_split, \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"drop_categories\",\n",
    "    task=drop_categories, \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"drop_features\",\n",
    "    task=drop_features, \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"convert_to_woe\",\n",
    "    task=convert_to_woe, \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"add_constant\",\n",
    "    task=add_constant, \n",
    ")\n",
    "\n",
    "developer.add_task(\n",
    "    task_id=\"train_model\",\n",
    "    task=train_model,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Raw Data: `df_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_url = \"https://vmai.s3.us-west-1.amazonaws.com/datasets/lending_club_loan_data_2007_2014.csv\"\n",
    "\n",
    "df_1 = developer.execute_task(\n",
    "    area_id=\"data_description\",\n",
    "    task_id=\"import_raw_data\", \n",
    "    inputs=[lending_club_url]   \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Dataset: `df_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.DescriptiveStatistics import DescriptiveStatistics\n",
    "\n",
    "vm_df_1 = vm.init_dataset(dataset=df_1)\n",
    "test_context_1 = TestContext(dataset=vm_df_1)\n",
    "\n",
    "metric = DescriptiveStatistics(test_context_1)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"fig_height\": 1100}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context_1, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Definition of Default: `df_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = developer.execute_task(\n",
    "    area_id=\"data_preparation\",\n",
    "    task_id=\"add_default_definition\", \n",
    "    inputs=[df_1, default_column, unused_variables]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Dataset: `df_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_df = vm.init_dataset(\n",
    "    dataset=df_2,\n",
    "    target_column=default_column)\n",
    "\n",
    "test_context_2 = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"fig_height\": 1100}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "metric = ClassImbalance(test_context_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.IQROutliersTable import IQROutliersTable\n",
    "\n",
    "num_features = get_numerical_columns(df_2)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"threshold\": 1.5\n",
    "        }\n",
    "\n",
    "metric = IQROutliersTable(test_context_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.IQROutliersBarPlot import IQROutliersBarPlot\n",
    "\n",
    "num_features = get_numerical_columns(df_2)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"threshold\": 1.5,\n",
    "          \"fig_width\": 500}\n",
    "\n",
    "metric = IQROutliersBarPlot(test_context_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the `stratify = y` parameter is set, it ensures that the distribution of the target variable (`y`) in the test set is the same as that in the original dataset. \n",
    "\n",
    "This is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split: `df_train_2` and `df_test_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2, df_test_2 = developer.execute_task(\n",
    "    area_id=\"data_sampling\",\n",
    "    task_id=\"data_split\", \n",
    "    inputs=[df_2, default_column]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Dataset: `df_train_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "vm_df_train_2 = vm.init_dataset(dataset=df_train_2,\n",
    "                                target_column=default_column)\n",
    "test_context_train_2 = TestContext(dataset=vm_df_train_2)\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context_train_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context_train_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context_train_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Categories: `df_train_3` and `df_test_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_3 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_categories\", \n",
    "    inputs=[df_train_2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_3 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_categories\", \n",
    "    inputs=[df_test_2]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Dataset: `df_train_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.TargetRateBarPlots import TargetRateBarPlots\n",
    "\n",
    "vm_df_train_3 = vm.init_dataset(\n",
    "    dataset=df_train_3, \n",
    "    target_column=default_column)\n",
    "\n",
    "test_context_train_3 = TestContext(dataset=vm_df_train_3)\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": default_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = TargetRateBarPlots(test_context_train_3, params=params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Features: `df_train_4` and `df_test_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['addr_state', 'total_rec_int', 'loan_amnt',\n",
    "                    'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n",
    "                    'total_pymnt_inv', 'last_pymnt_amnt']\n",
    "\n",
    "df_train_4 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_train_3, to_drop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_4 = developer.execute_task(\n",
    "    area_id=\"exploratory_data_analysis\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_test_3, to_drop]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Dataset: `df_train_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n",
    "\n",
    "vm_df_train_4 = vm.init_dataset(dataset=df_train_4, target_column=default_column)\n",
    "test_context_train_4 = TestContext(dataset=vm_df_train_4)\n",
    "\n",
    "cat_features = get_categorical_columns(df_train_4)\n",
    "params = {\"cat_features\": cat_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ChiSquaredFeaturesTable(test_context_train_4, params)\n",
    "metric.run()\n",
    "await metric.result.log() \n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n",
    "\n",
    "num_features = get_numerical_columns(df_train_4)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ANOVAOneWayTable(test_context_train_4, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context_train_4, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "params = {\"features\": None}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context_train_4, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Dataset: `df_train_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEBinTable import WOEBinTable\n",
    "\n",
    "metric = WOEBinTable(test_context_train_4)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"breaks_adj\": {\n",
    "        \"int_rate\": [5,10,15]}  \n",
    "     }\n",
    "\n",
    "metric = WOEBinTable(test_context_train_4, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.data_validation.WOEBinPlots import WOEBinPlots\n",
    "\n",
    "params = {\n",
    "    \"breaks_adj\": {\"int_rate\": [5,10,15]},\n",
    "    \"fig_height\": 500,\n",
    "}\n",
    "\n",
    "metric = WOEBinPlots(test_context_train_4, params=params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Features into WoE Values: `df_train_5` and `df_test_5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"breaks_adj\": {\n",
    "        \"int_rate\": [5,10,15]}  \n",
    "     }\n",
    "\n",
    "metric = WOEBinTable(test_context_train_4, params=params)\n",
    "metric.run()\n",
    "woe_dic = metric.result.metric.value['woe_iv']\n",
    "woe_df = pd.DataFrame(woe_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_5 = developer.execute_task(\n",
    "    area_id=\"feature_engineering\",\n",
    "    task_id=\"convert_to_woe\", \n",
    "    inputs=[df_train_4, woe_df, default_column]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_5 = developer.execute_task(\n",
    "    area_id=\"feature_engineering\",\n",
    "    task_id=\"convert_to_woe\", \n",
    "    inputs=[df_test_4, woe_df, default_column]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Constant: `df_train_6` and `df_test_6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_6 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"add_constant\", \n",
    "    inputs=[df_train_5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_6 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"add_constant\", \n",
    "    inputs=[df_test_5]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model 1: `model_fit_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit_1 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"train_model\", \n",
    "    inputs=[df_train_6, default_column]\n",
    ")\n",
    "\n",
    "print(model_fit_1.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Features: `df_train_7` and `df_test_7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "\n",
    "df_train_7 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_train_6, to_drop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_7 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"drop_features\", \n",
    "    inputs=[df_test_6, to_drop]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model 2: `model_fit_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit_2 = developer.execute_task(\n",
    "    area_id=\"model_training\",\n",
    "    task_id=\"train_model\", \n",
    "    inputs=[df_train_7, default_column]\n",
    ")\n",
    "\n",
    "# Save the model and train dataset for PD development \n",
    "save_data = False\n",
    "if save_data:\n",
    "    save_model(model_fit_2, df=df_train_7, base_filename='model_fit_glm_scorecard')\n",
    "\n",
    "print(model_fit_2.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Model: `model_fit_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VM dataset\n",
    "vm_df_train_7 = vm.init_dataset(\n",
    "    dataset=df_train_7,\n",
    "    target_column=default_column)\n",
    "vm_df_test_7 = vm.init_dataset(\n",
    "    dataset=df_test_7,\n",
    "    target_column=default_column)\n",
    "\n",
    "# Create VM model\n",
    "vm_model_fit_2 = vm.init_model(\n",
    "    model = model_fit_2, \n",
    "    train_ds=vm_df_train_7, \n",
    "    test_ds=vm_df_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionCoeffsPlot import RegressionCoeffsPlot\n",
    "\n",
    "test_context_models_fit_2 = TestContext(models = [vm_model_fit_2])\n",
    "\n",
    "metric = RegressionCoeffsPlot(test_context_models_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionModelsCoeffs import RegressionModelsCoeffs\n",
    "\n",
    "metric = RegressionModelsCoeffs(test_context_models_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogRegressionConfusionMatrix import LogRegressionConfusionMatrix\n",
    "\n",
    "test_context_model_fit_2 = TestContext(model= vm_model_fit_2)\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"cut_off_threshold\": 0.5,\n",
    "}\n",
    "\n",
    "metric = LogRegressionConfusionMatrix(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionROCCurve import RegressionROCCurve\n",
    "\n",
    "metric = RegressionROCCurve(test_context_model_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.GINITable import GINITable\n",
    "\n",
    "metric = GINITable(test_context_model_fit_2)\n",
    "metric.run()\n",
    "await metric.result.log() \n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegPredictionHistogram import LogisticRegPredictionHistogram\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Histogram of Probability of Default\",\n",
    "}\n",
    "\n",
    "metric = LogisticRegPredictionHistogram(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.LogisticRegCumulativeProb import LogisticRegCumulativeProb\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"title\": \"Cumulative Probability of Default\",\n",
    "}\n",
    "\n",
    "metric = LogisticRegCumulativeProb(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests.model_validation.statsmodels.ScorecardHistogram import ScorecardHistogram\n",
    "\n",
    "# Configure test parameters\n",
    "params = {\n",
    "    \"target_score\": 600,\n",
    "    \"target_odds\": 50,\n",
    "    \"pdo\": 20,\n",
    "    \"title\": \"Histogram of Credit Scores\",\n",
    "}\n",
    "\n",
    "metric = ScorecardHistogram(test_context_model_fit_2, params)\n",
    "metric.run()\n",
    "await metric.result.log()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Model Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifecycle = developer.show_lifecycle()\n",
    "display(lifecycle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
