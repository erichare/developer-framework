{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scorecard Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to ValidMind Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key and secret from environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to ValidMind Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:19:34,413 - INFO - api_client - Connected to ValidMind. Project: [3] PD Model - Initial Validation (cliwzqjgv00001fy6869rlav9)\n"
     ]
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"2494c3838f48efe590d531bfe225d90b\",\n",
    "  api_secret = \"4f692f8161f128414fef542cab2a4e74834c75d01b3a8e088a1834f2afcfe838\",\n",
    "  project = \"cliwzqjgv00001fy6869rlav9\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Lending Club Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_63268/861735461.py:2: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "filepath = '/Users/juanvalidmind/Dev/datasets/lending club/data_2007_2014/loan_data_2007_2014.csv'\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe Raw Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:19:36,856 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:19:36,857 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2891b81822e24d9cbc555d13ed63c215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>This section provides descriptive statistics for numerical, categorical and date…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "from validmind.tests.data_validation.TabularDescriptionTables import TabularDescriptionTables\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = TabularDescriptionTables(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Missing Values in Raw Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:19:43,206 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:19:43,207 - INFO - dataset - Inferring dataset types...\n",
      "/Users/juanvalidmind/Dev/github/validmind-python/validmind/tests/data_validation/MissingValuesBarPlot.py:60: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([\"{:.1f}%\".format(x) for x in ax.get_yticks()])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0324ece56254d549989fe72914575b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of missing values by plotting bar plots with colored…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.MissingValuesBarPlot import MissingValuesBarPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"threshold\": 80,\n",
    "          \"xticks_fontsize\": 8}\n",
    "\n",
    "metric = MissingValuesBarPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Target Variable\n",
    "\n",
    "**Definition of Default**\n",
    "\n",
    "We categorizing `Fully Paid` loans as \"default = 0\" and `Charged Off` loans as \"default = 1\". This binary classification is suitable for developing a credit scorecard, as it enables distinction between applicants likely to fulfill their credit obligations (low risk) and those likely to fail (high risk). \n",
    "\n",
    "Loans with `Current` status, which represents ongoing loans with an unresolved outcome, should be excluded from the model, as their final repayment status is still unknown and thus not suitable for a retrospective risk analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add `default` Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_63268/2473939582.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[target_column] = df[target_column].astype(int)\n",
      "/var/folders/tn/rbr74q892k13m82y37y396h40000gn/T/ipykernel_63268/2473939582.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns='loan_status', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def add_target_column(df, target_column):\n",
    "    # Assuming the column name is 'loan_status'\n",
    "    df[target_column] = df['loan_status'].apply(lambda x: 0 if x == \"Fully Paid\" else 1 if x == \"Charged Off\" else np.nan)\n",
    "    # Remove rows where the target column is NaN\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    # Convert target column to integer\n",
    "    df[target_column] = df[target_column].astype(int)\n",
    "    return df\n",
    "\n",
    "target_column = 'default'\n",
    "df = add_target_column(df, target_column)\n",
    "\n",
    "# Drop 'loan_status' variable \n",
    "df.drop(columns='loan_status', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Unused Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_variables = [\"id\", \"member_id\", \"funded_amnt\", \"emp_title\", \"url\", \"desc\", \"application_type\",\n",
    "                    \"title\", \"zip_code\", \"delinq_2yrs\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "                    \"revol_bal\", \"total_rec_prncp\", \"total_rec_late_fee\", \"recoveries\", \"out_prncp_inv\", \"out_prncp\", \n",
    "                    \"collection_recovery_fee\", \"next_pymnt_d\", \"initial_list_status\", \"pub_rec\",\n",
    "                    \"collections_12_mths_ex_med\", \"policy_code\", \"acc_now_delinq\", \"pymnt_plan\",\n",
    "                    \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"last_pymnt_d\", \"last_credit_pull_d\",\n",
    "                    'earliest_cr_line', 'issue_d']\n",
    "\n",
    "df = df.drop(columns=unused_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Variables with Large Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_with_min_missing(df, min_missing_percentage):\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "\n",
    "    # Get the variables where the percentage of missing values is greater than the specified minimum\n",
    "    variables_to_drop = missing_percentages[missing_percentages > min_missing_percentage].index.tolist()\n",
    "\n",
    "    # Also add any columns where all values are missing\n",
    "    variables_to_drop.extend(df.columns[df.isnull().all()].tolist())\n",
    "\n",
    "    # Remove duplicates (if any)\n",
    "    variables_to_drop = list(set(variables_to_drop))\n",
    "\n",
    "    return variables_to_drop\n",
    "\n",
    "min_missing_count = 80\n",
    "variables_to_drop = variables_with_min_missing(df, min_missing_count)\n",
    "df.drop(columns=variables_to_drop, axis=1, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, subset=[\"emp_length\"], inplace=True)\n",
    "df.dropna(axis=0, subset=[\"revol_util\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Type of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def clean_term_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to remove 'months' string from the 'term' column and convert it to categorical\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].str.replace(' months', '')\n",
    "    \n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "def clean_emp_length_column(df, column):\n",
    "    \"\"\"\n",
    "    Function to clean 'emp_length' column and convert it to categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "    \n",
    "    df[column] = df[column].replace('n/a', np.nan)\n",
    "    df[column] = df[column].str.replace('< 1 year', str(0))\n",
    "    df[column] = df[column].apply(lambda x: re.sub('\\D', '', str(x)))\n",
    "    df[column].fillna(value = 0, inplace=True)\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "def clean_inq_last_6mths(df, column):\n",
    "    \"\"\"\n",
    "    Function to convert 'inq_last_6mths' column into categorical.\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the dataframe\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The column '{column}' does not exist in the dataframe.\")\n",
    "\n",
    "    # Convert to categorical\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "clean_emp_length_column(df, 'emp_length')\n",
    "clean_term_column(df, 'term')\n",
    "clean_inq_last_6mths(df, 'inq_last_6mths')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_columns(df):\n",
    "        numerical_columns = df.select_dtypes(\n",
    "            include=[\"int\", \"float\", \"uint\"]\n",
    "        ).columns.tolist()\n",
    "        return numerical_columns\n",
    "\n",
    "def get_categorical_columns(df):\n",
    "        categorical_columns = df.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns.tolist()\n",
    "        return categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:19:48,692 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:19:48,692 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0285afd6b9b41f9a5af3703f05dc04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of the outliers for numeric variables. The input dat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.IQROutliersPlots import IQROutliersPlots\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "num_features = get_numerical_columns(df)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"threshold\": 1.5}\n",
    "\n",
    "metric = IQROutliersPlots(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Outliers using IQR Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_outliers(series, threshold=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    return series[(series < lower_bound) | (series > upper_bound)]\n",
    "\n",
    "def remove_iqr_outliers(df, target_column, threshold=1.5):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols.remove(target_column)  # Exclude target_column from numerical columns\n",
    "    for col in num_cols:\n",
    "        outliers = compute_outliers(df[col], threshold)\n",
    "        df = df[~df[col].isin(outliers)]\n",
    "    return df\n",
    "\n",
    "df = remove_iqr_outliers(df, target_column, threshold=1.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Method\n",
    "\n",
    "We employ stratified sampling to create our training and testing sets. Stratified sampling is particularly important in this context. When the `stratify = y` parameter is set, it ensures that the distribution of the target variable (`y`) in the test set is the same as that in the original dataset. \n",
    "\n",
    "This is crucial for maintaining a consistent representation of the target variable classes, especially important in scenarios where the classes are imbalanced, which is often the case in credit risk scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test \n",
    "X = df.drop(target_column, axis = 1)\n",
    "y = df[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 42, stratify = y)\n",
    "\n",
    "# Concatenate X_train with y_train to form df_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate X_test with y_test to form df_test\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance\n",
    "\n",
    "Class imbalance is a common issue in credit risk scorecards and datasets like the Lending Club's. This imbalance arises when the number of defaulting loans (negative class) is significantly smaller than the number of loans that are paid off (positive class). Such imbalance can lead to biased models that favor the majority class, thus affecting predictive performance. \n",
    "\n",
    "Special techniques like oversampling, undersampling, or cost-sensitive learning are often needed to ensure that the minority class is appropriately represented during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:00,831 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:00,832 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9684e6c0344e82b0e2532f8aa7db44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Class Imbalance ❌</h2>\\n            <p>The class imbalance test m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ClassImbalance import ClassImbalance\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "metric = ClassImbalance(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:01,476 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:01,476 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bdb6980439409da1dbcf65d5cf7c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of numerical data by plotting the histogram. The inp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TabularNumericalHistograms import TabularNumericalHistograms\n",
    "\n",
    "vm_df_train = vm.init_dataset(dataset=df_train)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "metric = TabularNumericalHistograms(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Cardinality of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50dc0e1d9d04d13a19c48841bd6e6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n            <h2>Cardinality ✅</h2>\\n            <p>The high cardinality test meas…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.HighCardinality import HighCardinality\n",
    "metric = HighCardinality(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plots of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f58e78c97f40879a962c1705b00d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of categorical data by plotting bar plots. The input…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.TabularCategoricalBarPlots import TabularCategoricalBarPlots\n",
    "metric = TabularCategoricalBarPlots(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Rate by Categorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column default is correct and contains only 1 and 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f406c5c35340b2bae82e9ceef89354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of loan default ratios by plotting bar plots. The in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.DefaultRateBarPlots import DefaultRateBarPlots\n",
    "\n",
    "# Configure the metric\n",
    "params = {\n",
    "    \"default_column\": target_column,\n",
    "    \"columns\": None\n",
    "}\n",
    "\n",
    "metric = DefaultRateBarPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Squared Test on Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:05,048 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:05,048 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426d56a4c5dd426c9a19c017274b6c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Perform a Chi-Squared test of independence for each categorical variable with th…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ChiSquaredFeaturesTable import ChiSquaredFeaturesTable\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "cat_features = get_categorical_columns(df_train)\n",
    "params = {\"cat_features\": cat_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ChiSquaredFeaturesTable(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA Test on Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:05,813 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:05,814 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b584ee432146ed8e2d026a363cb328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Perform an ANOVA F-test for each numerical variable with the target. The input d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.ANOVAOneWayTable import ANOVAOneWayTable\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "num_features = get_numerical_columns(df_train)\n",
    "params = {\"num_features\": num_features,\n",
    "          \"p_threshold\": 0.05}\n",
    "\n",
    "metric = ANOVAOneWayTable(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap Correlation of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:06,607 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:06,608 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf731fac0140f6aaf003c73a7b7234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Extracts the Pearson correlation coefficient for all pairs of numerical variable…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.PearsonCorrelationMatrix import PearsonCorrelationMatrix\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"declutter\": False,\n",
    "          \"features\": None,\n",
    "          \"fontsize\": 13}\n",
    "\n",
    "metric = PearsonCorrelationMatrix(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations of Numerical Features with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:07,369 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:07,370 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400c8a15ad2f4e7fa57cd11cbd8583b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of correlations between features and target by plott…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.FeatureTargetCorrelationPlot import FeatureTargetCorrelationPlot\n",
    "\n",
    "vm_df = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df)\n",
    "\n",
    "params = {\"features\": None,\n",
    "          \"fig_height\": 600}\n",
    "\n",
    "metric = FeatureTargetCorrelationPlot(test_context, params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_categorical_features = ['addr_state']\n",
    "drop_numerical_features = ['total_rec_int', 'loan_amnt',\n",
    "                           'funded_amnt_inv', 'dti', 'revol_util', 'total_pymnt', \n",
    "                           'total_pymnt_inv', 'last_pymnt_amnt',]\n",
    "\n",
    "df_train.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)\n",
    "\n",
    "# Update df_test \n",
    "df_test.drop(columns = drop_categorical_features + drop_numerical_features, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def encode_numerical_features(df):\n",
    "    \n",
    "    # term\n",
    "    df['term'] = df['term'].replace({' 36': '36M', ' 60': '60M'})\n",
    "\n",
    "    # emp_length_int\n",
    "    df['emp_length'] = df['emp_length'].replace('10+', '10')  # Replace '10+' with '10'\n",
    "    df['emp_length'] = pd.to_numeric(df['emp_length'], errors='coerce')  # Convert to numeric\n",
    "    df['emp_length'].fillna(-1, inplace=True)\n",
    "    bins = [0,1,2,3,5,8,10,999]\n",
    "    df['emp_length_bucket'] = pd.cut(df['emp_length'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='emp_length', inplace=True)\n",
    "\n",
    "    # inq_last_6mths\n",
    "    df['inq_last_6mths'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n",
    "    df['inq_last_6mths_bucket'] = pd.cut(df['inq_last_6mths'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='inq_last_6mths', inplace=True)\n",
    "    \n",
    "    # total_acc\n",
    "    df['total_acc'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 999]\n",
    "    df['total_acc_bucket'] = pd.cut(df['total_acc'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='total_acc', inplace=True)\n",
    "\n",
    "    # annual_inc\n",
    "    df['annual_inc'].fillna(-1, inplace=True)\n",
    "    df['annual_inc_1000'] = df['annual_inc']/1000\n",
    "    bins = [-1, 0, 10, 20, 30, 40, 50, 75, 100, 150, 250, 1000, 10000]\n",
    "    df['annual_inc_bucket'] = pd.cut(df['annual_inc_1000'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='annual_inc', inplace=True)\n",
    "    df.drop(columns='annual_inc_1000', inplace=True)\n",
    "    \n",
    "    # int_rate\n",
    "    df['int_rate'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 10, 25, 50]\n",
    "    df['int_rate_bucket'] = pd.cut(df['int_rate'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='int_rate', inplace=True)\n",
    "\n",
    "    # installment\n",
    "    df['installment'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 100, 200, 300, 400, 500, 750, 1000, 1500]\n",
    "    df['installment_bucket'] = pd.cut(df['installment'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='installment', inplace=True)\n",
    "\n",
    "    # open_acc\n",
    "    df['open_acc'].replace(\"N/A\", 1, inplace=True)\n",
    "    df['open_acc'].fillna(-1, inplace=True)\n",
    "    bins = [-1, 0, 1, 2, 3, 4, 5, 8, 10, 100]\n",
    "    df['open_acc_bucket'] = pd.cut(df['open_acc'], bins=bins, right=False, include_lowest=True)\n",
    "    df.drop(columns='open_acc', inplace=True)\n",
    "\n",
    "def find_categorical_features(df):\n",
    "    # Get the column names of features with the data type \"category\"\n",
    "    categorical_features = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    return categorical_features\n",
    "\n",
    "\n",
    "def convert_categorical_to_object(df):\n",
    "    # Find the categorical features\n",
    "    categorical_features = find_categorical_features(df)\n",
    "\n",
    "    # Convert the categorical features to object type\n",
    "    df[categorical_features] = df[categorical_features].astype(str)\n",
    "\n",
    "\n",
    "encode_numerical_features(df_train)\n",
    "convert_categorical_to_object(df_train)\n",
    "\n",
    "# Update df_test\n",
    "encode_numerical_features(df_test)\n",
    "convert_categorical_to_object(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight of Evidence (WoE) and Infomation Value (IV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:08,401 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:08,402 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b844820d4a54601b9f16ec4bdb7d8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Calculate the Weight of Evidence (WoE) and Information Value (IV) of categorical…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.WOEIVTable import WOEIVTable\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"order_by\": [\"Feature\", \"WoE\"]\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_iv_df = metric.result.metric.value['woe_iv']\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Coarsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def coarse_classing(df, mappings):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_new = df.copy()\n",
    "\n",
    "    # Loop through each feature and merge set\n",
    "    for feature, merge_sets in mappings.items():\n",
    "        for merge_set in merge_sets:\n",
    "            # Merge the specified categories into a new category\n",
    "            df_new[feature] = df_new[feature].apply(lambda x: f\"[{','.join(merge_set)}]\" if x in merge_set else x)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "def shorten_category_names(df, max_length=20, suffix=\"...\"):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for feature in df_new.columns:\n",
    "        # Check if the column has the \"object\" data type\n",
    "        if df_new[feature].dtype.name == 'object':\n",
    "            # Shorten long category names\n",
    "            df_new[feature] = df_new[feature].apply(lambda x: x[:max_length] + suffix if len(x) > max_length else x)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Create a dictionary of features and the sets to merge\n",
    "mappings = {\n",
    "    'sub_grade': [['B2','B3','B4','B5','C3','D1'], ['C1','C2','C4','C5'], ['D3','D4','D5','E3','G4'], ['E1','E2','E4','E5','F1','F2','F3','F4','G1','G2','G3','G5','F5']],\n",
    "    'grade': [['F','G']],\n",
    "    'purpose': [['wedding','major_purchase'], ['credit_card','car'], ['debt_consolidation','other','vacation'], ['medical','moving','house','educational'], ['renewable_energy','small_business']],\n",
    "    'home_ownership': [['MORTGAGE','OWN','RENT']],\n",
    "    'annual_inc_bucket': [['[250, 1000)','[100, 150)','[150, 250)','[1000, 10000)'], ['[50, 75)','[40, 50)'], ['[10, 20)','[0, 10)']],\n",
    "    'emp_length_bucket': [['[2, 3)','[40, 50)','[3, 5)','[1, 2)','[0, 1)','[5, 8)','[8, 10)']],\n",
    "    'inq_last_6mths_bucket': [['[4, 5)','[1, 2)'], ['[5, 10)','[3, 4)']],\n",
    "    'installment_bucket': [['[300, 400)','[200, 300)','[0, 100)'], ['[400, 500)', '[500, 750)']],\n",
    "    'total_acc_bucket': [['[20, 25)','[30, 35)','[15, 20)','[45, 50)','[40, 45)','[35, 40)','[10, 15)','[5, 10)']],\n",
    "    'open_acc_bucket': [['[5, 8)','[8, 10)','[10, 100)','[4, 5)'], ['[1, 2)','[2, 3)']]\n",
    "}\n",
    "\n",
    "df_train = coarse_classing(df_train, mappings)\n",
    "df_train = df_train[~df_train['home_ownership'].isin(['OTHER', 'NONE'])]\n",
    "df_train.drop(columns=\"home_ownership\", inplace=True)\n",
    "df_train = shorten_category_names(df_train, max_length=15, suffix=\"...\")\n",
    "\n",
    "# Update df_test\n",
    "df_test = coarse_classing(df_test, mappings)\n",
    "df_test = df_test[~df_test['home_ownership'].isin(['OTHER', 'NONE'])]\n",
    "df_test.drop(columns=\"home_ownership\", inplace=True)\n",
    "df_test = shorten_category_names(df_test, max_length=15, suffix=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:16,296 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:16,296 - INFO - dataset - Inferring dataset types...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c144ab031d741358427fc0f004f4aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Generates a visual analysis of the WoE and IV values distribution for categorica…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.data_validation.WOEIVPlots import WOEIVPlots\n",
    "\n",
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"fig_height\": 500,\n",
    "    \"fig_height\": 500,\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVPlots(test_context, params=params)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Features with WoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:21,465 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:21,465 - INFO - dataset - Inferring dataset types...\n"
     ]
    }
   ],
   "source": [
    "# Update vm dataset and test context\n",
    "vm_df_train = vm.init_dataset(dataset=df_train, \n",
    "                              target_column=target_column)\n",
    "test_context = TestContext(dataset=vm_df_train)\n",
    "\n",
    "# Configure test parameters\n",
    "\n",
    "params = {\n",
    "    \"features\": None,\n",
    "    \"order_by\": [\"Feature\", \"WoE\"]\n",
    "}\n",
    "\n",
    "# Run test\n",
    "metric = WOEIVTable(test_context, params=params)\n",
    "metric.run()\n",
    "woe_iv_df = metric.result.metric.value['woe_iv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categories(woe_df, original_df):\n",
    "    for feature in woe_df['Feature'].unique():\n",
    "        woe_categories = woe_df[woe_df['Feature'] == feature]['Category'].unique()\n",
    "        original_categories = original_df[feature].unique()\n",
    "        \n",
    "        # Check categories in WoE table that are not in original DataFrame\n",
    "        for category in woe_categories:\n",
    "            if category not in original_categories:\n",
    "                print(f\"Category '{category}' not found in feature '{feature}' in original DataFrame.\")\n",
    "                \n",
    "        # Check categories in original DataFrame that are not in WoE table\n",
    "        for category in original_categories:\n",
    "            if category not in woe_categories:\n",
    "                print(f\"Category '{category}' in feature '{feature}' not found in WoE table.\")\n",
    "\n",
    "                \n",
    "check_categories(woe_iv_df, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Category</th>\n",
       "      <th>All</th>\n",
       "      <th>Good</th>\n",
       "      <th>Bad</th>\n",
       "      <th>Distr_Good</th>\n",
       "      <th>Distr_Bad</th>\n",
       "      <th>WoE</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>verification_status</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>58436</td>\n",
       "      <td>49886</td>\n",
       "      <td>8550</td>\n",
       "      <td>0.444664</td>\n",
       "      <td>0.338574</td>\n",
       "      <td>0.272578</td>\n",
       "      <td>2.891799e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>verification_status</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>38235</td>\n",
       "      <td>30399</td>\n",
       "      <td>7836</td>\n",
       "      <td>0.270965</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>-0.135550</td>\n",
       "      <td>5.331848e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>verification_status</td>\n",
       "      <td>Verified</td>\n",
       "      <td>40770</td>\n",
       "      <td>31903</td>\n",
       "      <td>8867</td>\n",
       "      <td>0.284371</td>\n",
       "      <td>0.351127</td>\n",
       "      <td>-0.210868</td>\n",
       "      <td>1.407662e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>total_acc_bucket</td>\n",
       "      <td>[50, 999)</td>\n",
       "      <td>753</td>\n",
       "      <td>639</td>\n",
       "      <td>114</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.232475</td>\n",
       "      <td>2.746648e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>total_acc_bucket</td>\n",
       "      <td>[25, 30)</td>\n",
       "      <td>20878</td>\n",
       "      <td>17146</td>\n",
       "      <td>3732</td>\n",
       "      <td>0.152833</td>\n",
       "      <td>0.147784</td>\n",
       "      <td>0.033590</td>\n",
       "      <td>1.695709e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>total_acc_bucket</td>\n",
       "      <td>[[20, 25),[30, ...</td>\n",
       "      <td>114925</td>\n",
       "      <td>93722</td>\n",
       "      <td>21203</td>\n",
       "      <td>0.835401</td>\n",
       "      <td>0.839623</td>\n",
       "      <td>-0.005041</td>\n",
       "      <td>2.128088e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>total_acc_bucket</td>\n",
       "      <td>[0, 5)</td>\n",
       "      <td>885</td>\n",
       "      <td>681</td>\n",
       "      <td>204</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>-0.285789</td>\n",
       "      <td>5.738868e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>term</td>\n",
       "      <td>36M</td>\n",
       "      <td>117433</td>\n",
       "      <td>99441</td>\n",
       "      <td>17992</td>\n",
       "      <td>0.886378</td>\n",
       "      <td>0.712470</td>\n",
       "      <td>0.218406</td>\n",
       "      <td>3.798268e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>term</td>\n",
       "      <td>60M</td>\n",
       "      <td>20008</td>\n",
       "      <td>12747</td>\n",
       "      <td>7261</td>\n",
       "      <td>0.113622</td>\n",
       "      <td>0.287530</td>\n",
       "      <td>-0.928453</td>\n",
       "      <td>1.614657e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>A1</td>\n",
       "      <td>3623</td>\n",
       "      <td>3508</td>\n",
       "      <td>115</td>\n",
       "      <td>0.031269</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>1.926638</td>\n",
       "      <td>5.147019e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>A2</td>\n",
       "      <td>3846</td>\n",
       "      <td>3662</td>\n",
       "      <td>184</td>\n",
       "      <td>0.032642</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>1.499598</td>\n",
       "      <td>3.802287e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>A3</td>\n",
       "      <td>4488</td>\n",
       "      <td>4220</td>\n",
       "      <td>268</td>\n",
       "      <td>0.037615</td>\n",
       "      <td>0.010613</td>\n",
       "      <td>1.265372</td>\n",
       "      <td>3.416864e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>A4</td>\n",
       "      <td>6949</td>\n",
       "      <td>6429</td>\n",
       "      <td>520</td>\n",
       "      <td>0.057306</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>1.023514</td>\n",
       "      <td>3.757729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>A5</td>\n",
       "      <td>7204</td>\n",
       "      <td>6585</td>\n",
       "      <td>619</td>\n",
       "      <td>0.058696</td>\n",
       "      <td>0.024512</td>\n",
       "      <td>0.873213</td>\n",
       "      <td>2.985008e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>B1</td>\n",
       "      <td>7526</td>\n",
       "      <td>6725</td>\n",
       "      <td>801</td>\n",
       "      <td>0.059944</td>\n",
       "      <td>0.031719</td>\n",
       "      <td>0.636495</td>\n",
       "      <td>1.796509e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>[B2,B3,B4,B5,C3...</td>\n",
       "      <td>51452</td>\n",
       "      <td>43070</td>\n",
       "      <td>8382</td>\n",
       "      <td>0.383909</td>\n",
       "      <td>0.331921</td>\n",
       "      <td>0.145509</td>\n",
       "      <td>7.564754e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>[C1,C2,C4,C5]</td>\n",
       "      <td>28494</td>\n",
       "      <td>22407</td>\n",
       "      <td>6087</td>\n",
       "      <td>0.199727</td>\n",
       "      <td>0.241041</td>\n",
       "      <td>-0.188013</td>\n",
       "      <td>7.767462e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>D2</td>\n",
       "      <td>4788</td>\n",
       "      <td>3431</td>\n",
       "      <td>1357</td>\n",
       "      <td>0.030583</td>\n",
       "      <td>0.053736</td>\n",
       "      <td>-0.563656</td>\n",
       "      <td>1.305066e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>[D3,D4,D5,E3,G4...</td>\n",
       "      <td>11414</td>\n",
       "      <td>7746</td>\n",
       "      <td>3668</td>\n",
       "      <td>0.069045</td>\n",
       "      <td>0.145250</td>\n",
       "      <td>-0.743701</td>\n",
       "      <td>5.667393e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>[E1,E2,E4,E5,F1...</td>\n",
       "      <td>7657</td>\n",
       "      <td>4405</td>\n",
       "      <td>3252</td>\n",
       "      <td>0.039264</td>\n",
       "      <td>0.128777</td>\n",
       "      <td>-1.187761</td>\n",
       "      <td>1.063193e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>purpose</td>\n",
       "      <td>[wedding,major_...</td>\n",
       "      <td>5084</td>\n",
       "      <td>4402</td>\n",
       "      <td>682</td>\n",
       "      <td>0.039238</td>\n",
       "      <td>0.027007</td>\n",
       "      <td>0.373554</td>\n",
       "      <td>4.568939e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>purpose</td>\n",
       "      <td>home_improvemen...</td>\n",
       "      <td>7378</td>\n",
       "      <td>6241</td>\n",
       "      <td>1137</td>\n",
       "      <td>0.055630</td>\n",
       "      <td>0.045024</td>\n",
       "      <td>0.211516</td>\n",
       "      <td>2.243231e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>purpose</td>\n",
       "      <td>[credit_card,ca...</td>\n",
       "      <td>30359</td>\n",
       "      <td>25552</td>\n",
       "      <td>4807</td>\n",
       "      <td>0.227761</td>\n",
       "      <td>0.190354</td>\n",
       "      <td>0.179411</td>\n",
       "      <td>6.711226e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>purpose</td>\n",
       "      <td>[debt_consolida...</td>\n",
       "      <td>88233</td>\n",
       "      <td>71086</td>\n",
       "      <td>17147</td>\n",
       "      <td>0.633633</td>\n",
       "      <td>0.679008</td>\n",
       "      <td>-0.069164</td>\n",
       "      <td>3.138353e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>purpose</td>\n",
       "      <td>[medical,moving...</td>\n",
       "      <td>3825</td>\n",
       "      <td>3067</td>\n",
       "      <td>758</td>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.030016</td>\n",
       "      <td>-0.093459</td>\n",
       "      <td>2.503023e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>purpose</td>\n",
       "      <td>[renewable_ener...</td>\n",
       "      <td>2562</td>\n",
       "      <td>1840</td>\n",
       "      <td>722</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.028591</td>\n",
       "      <td>-0.555735</td>\n",
       "      <td>6.774204e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>open_acc_bucket</td>\n",
       "      <td>[3, 4)</td>\n",
       "      <td>2264</td>\n",
       "      <td>1860</td>\n",
       "      <td>404</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>0.015998</td>\n",
       "      <td>0.035686</td>\n",
       "      <td>2.074110e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>open_acc_bucket</td>\n",
       "      <td>[[5, 8),[8, 10)...</td>\n",
       "      <td>134377</td>\n",
       "      <td>109688</td>\n",
       "      <td>24689</td>\n",
       "      <td>0.977716</td>\n",
       "      <td>0.977666</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.552636e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>open_acc_bucket</td>\n",
       "      <td>[[1, 2),[2, 3)]</td>\n",
       "      <td>800</td>\n",
       "      <td>640</td>\n",
       "      <td>160</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>-0.104937</td>\n",
       "      <td>6.623300e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>int_rate_bucket</td>\n",
       "      <td>[5, 10)</td>\n",
       "      <td>31717</td>\n",
       "      <td>29436</td>\n",
       "      <td>2281</td>\n",
       "      <td>0.262381</td>\n",
       "      <td>0.090326</td>\n",
       "      <td>1.066373</td>\n",
       "      <td>1.834750e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>int_rate_bucket</td>\n",
       "      <td>[10, 25)</td>\n",
       "      <td>105724</td>\n",
       "      <td>82752</td>\n",
       "      <td>22972</td>\n",
       "      <td>0.737619</td>\n",
       "      <td>0.909674</td>\n",
       "      <td>-0.209659</td>\n",
       "      <td>3.607290e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>installment_bucket</td>\n",
       "      <td>[100, 200)</td>\n",
       "      <td>22754</td>\n",
       "      <td>19247</td>\n",
       "      <td>3507</td>\n",
       "      <td>0.171560</td>\n",
       "      <td>0.138875</td>\n",
       "      <td>0.211363</td>\n",
       "      <td>6.908542e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>installment_bucket</td>\n",
       "      <td>[[300, 400),[20...</td>\n",
       "      <td>68228</td>\n",
       "      <td>56523</td>\n",
       "      <td>11705</td>\n",
       "      <td>0.503824</td>\n",
       "      <td>0.463509</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>3.362261e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>installment_bucket</td>\n",
       "      <td>[[400, 500),[50...</td>\n",
       "      <td>42780</td>\n",
       "      <td>33725</td>\n",
       "      <td>9055</td>\n",
       "      <td>0.300611</td>\n",
       "      <td>0.358571</td>\n",
       "      <td>-0.176309</td>\n",
       "      <td>1.021882e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>installment_bucket</td>\n",
       "      <td>[750, 1000)</td>\n",
       "      <td>3679</td>\n",
       "      <td>2693</td>\n",
       "      <td>986</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>0.039045</td>\n",
       "      <td>-0.486476</td>\n",
       "      <td>7.316855e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>inq_last_6mths_bucket</td>\n",
       "      <td>[0, 1)</td>\n",
       "      <td>68522</td>\n",
       "      <td>57290</td>\n",
       "      <td>11232</td>\n",
       "      <td>0.510661</td>\n",
       "      <td>0.444779</td>\n",
       "      <td>0.138128</td>\n",
       "      <td>9.100138e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>inq_last_6mths_bucket</td>\n",
       "      <td>[[4, 5),[1, 2)]</td>\n",
       "      <td>41051</td>\n",
       "      <td>33181</td>\n",
       "      <td>7870</td>\n",
       "      <td>0.295762</td>\n",
       "      <td>0.311646</td>\n",
       "      <td>-0.052312</td>\n",
       "      <td>8.309021e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>inq_last_6mths_bucket</td>\n",
       "      <td>[2, 3)</td>\n",
       "      <td>18575</td>\n",
       "      <td>14608</td>\n",
       "      <td>3967</td>\n",
       "      <td>0.130210</td>\n",
       "      <td>0.157090</td>\n",
       "      <td>-0.187672</td>\n",
       "      <td>5.044666e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>inq_last_6mths_bucket</td>\n",
       "      <td>[[5, 10),[3, 4)...</td>\n",
       "      <td>9293</td>\n",
       "      <td>7109</td>\n",
       "      <td>2184</td>\n",
       "      <td>0.063367</td>\n",
       "      <td>0.086485</td>\n",
       "      <td>-0.311028</td>\n",
       "      <td>7.190312e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grade</td>\n",
       "      <td>A</td>\n",
       "      <td>26110</td>\n",
       "      <td>24404</td>\n",
       "      <td>1706</td>\n",
       "      <td>0.217528</td>\n",
       "      <td>0.067556</td>\n",
       "      <td>1.169365</td>\n",
       "      <td>1.753712e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grade</td>\n",
       "      <td>B</td>\n",
       "      <td>46890</td>\n",
       "      <td>40643</td>\n",
       "      <td>6247</td>\n",
       "      <td>0.362276</td>\n",
       "      <td>0.247377</td>\n",
       "      <td>0.381494</td>\n",
       "      <td>4.383341e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grade</td>\n",
       "      <td>C</td>\n",
       "      <td>35285</td>\n",
       "      <td>27660</td>\n",
       "      <td>7625</td>\n",
       "      <td>0.246550</td>\n",
       "      <td>0.301944</td>\n",
       "      <td>-0.202676</td>\n",
       "      <td>1.122702e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grade</td>\n",
       "      <td>D</td>\n",
       "      <td>20119</td>\n",
       "      <td>14244</td>\n",
       "      <td>5875</td>\n",
       "      <td>0.126965</td>\n",
       "      <td>0.232646</td>\n",
       "      <td>-0.605601</td>\n",
       "      <td>6.400006e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grade</td>\n",
       "      <td>E</td>\n",
       "      <td>7022</td>\n",
       "      <td>4208</td>\n",
       "      <td>2814</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.111432</td>\n",
       "      <td>-1.088851</td>\n",
       "      <td>8.049201e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grade</td>\n",
       "      <td>[F,G]</td>\n",
       "      <td>2015</td>\n",
       "      <td>1029</td>\n",
       "      <td>986</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>0.039045</td>\n",
       "      <td>-1.448545</td>\n",
       "      <td>4.327203e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>emp_length_bucket</td>\n",
       "      <td>[[2, 3),[40, 50...</td>\n",
       "      <td>97051</td>\n",
       "      <td>79245</td>\n",
       "      <td>17806</td>\n",
       "      <td>0.706359</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>2.230428e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>emp_length_bucket</td>\n",
       "      <td>[10, 999)</td>\n",
       "      <td>40390</td>\n",
       "      <td>32943</td>\n",
       "      <td>7447</td>\n",
       "      <td>0.293641</td>\n",
       "      <td>0.294896</td>\n",
       "      <td>-0.004264</td>\n",
       "      <td>5.349151e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>annual_inc_bucket</td>\n",
       "      <td>[[250, 1000),[1...</td>\n",
       "      <td>14480</td>\n",
       "      <td>12699</td>\n",
       "      <td>1781</td>\n",
       "      <td>0.113194</td>\n",
       "      <td>0.070526</td>\n",
       "      <td>0.473117</td>\n",
       "      <td>2.018680e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>annual_inc_bucket</td>\n",
       "      <td>[75, 100)</td>\n",
       "      <td>24799</td>\n",
       "      <td>21196</td>\n",
       "      <td>3603</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>0.142676</td>\n",
       "      <td>0.280815</td>\n",
       "      <td>1.298957e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>annual_inc_bucket</td>\n",
       "      <td>[[50, 75),[40, ...</td>\n",
       "      <td>71046</td>\n",
       "      <td>57342</td>\n",
       "      <td>13704</td>\n",
       "      <td>0.511124</td>\n",
       "      <td>0.542668</td>\n",
       "      <td>-0.059885</td>\n",
       "      <td>1.889029e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>annual_inc_bucket</td>\n",
       "      <td>[30, 40)</td>\n",
       "      <td>17872</td>\n",
       "      <td>13889</td>\n",
       "      <td>3983</td>\n",
       "      <td>0.123801</td>\n",
       "      <td>0.157724</td>\n",
       "      <td>-0.242169</td>\n",
       "      <td>8.215037e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>annual_inc_bucket</td>\n",
       "      <td>[[10, 20),[0, 1...</td>\n",
       "      <td>1594</td>\n",
       "      <td>1234</td>\n",
       "      <td>360</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>-0.259319</td>\n",
       "      <td>8.444300e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>annual_inc_bucket</td>\n",
       "      <td>[20, 30)</td>\n",
       "      <td>7650</td>\n",
       "      <td>5828</td>\n",
       "      <td>1822</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.072150</td>\n",
       "      <td>-0.328492</td>\n",
       "      <td>6.635975e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Feature            Category     All    Good    Bad  \\\n",
       "19    verification_status        Not Verified   58436   49886   8550   \n",
       "20    verification_status     Source Verified   38235   30399   7836   \n",
       "21    verification_status            Verified   40770   31903   8867   \n",
       "37       total_acc_bucket           [50, 999)     753     639    114   \n",
       "35       total_acc_bucket            [25, 30)   20878   17146   3732   \n",
       "34       total_acc_bucket  [[20, 25),[30, ...  114925   93722  21203   \n",
       "36       total_acc_bucket              [0, 5)     885     681    204   \n",
       "0                    term                 36M  117433   99441  17992   \n",
       "1                    term                 60M   20008   12747   7261   \n",
       "10              sub_grade                  A1    3623    3508    115   \n",
       "8               sub_grade                  A2    3846    3662    184   \n",
       "17              sub_grade                  A3    4488    4220    268   \n",
       "11              sub_grade                  A4    6949    6429    520   \n",
       "13              sub_grade                  A5    7204    6585    619   \n",
       "14              sub_grade                  B1    7526    6725    801   \n",
       "9               sub_grade  [B2,B3,B4,B5,C3...   51452   43070   8382   \n",
       "12              sub_grade       [C1,C2,C4,C5]   28494   22407   6087   \n",
       "18              sub_grade                  D2    4788    3431   1357   \n",
       "15              sub_grade  [D3,D4,D5,E3,G4...   11414    7746   3668   \n",
       "16              sub_grade  [E1,E2,E4,E5,F1...    7657    4405   3252   \n",
       "23                purpose  [wedding,major_...    5084    4402    682   \n",
       "24                purpose  home_improvemen...    7378    6241   1137   \n",
       "25                purpose  [credit_card,ca...   30359   25552   4807   \n",
       "22                purpose  [debt_consolida...   88233   71086  17147   \n",
       "26                purpose  [medical,moving...    3825    3067    758   \n",
       "27                purpose  [renewable_ener...    2562    1840    722   \n",
       "51        open_acc_bucket              [3, 4)    2264    1860    404   \n",
       "50        open_acc_bucket  [[5, 8),[8, 10)...  134377  109688  24689   \n",
       "52        open_acc_bucket     [[1, 2),[2, 3)]     800     640    160   \n",
       "44        int_rate_bucket             [5, 10)   31717   29436   2281   \n",
       "45        int_rate_bucket            [10, 25)  105724   82752  22972   \n",
       "46     installment_bucket          [100, 200)   22754   19247   3507   \n",
       "47     installment_bucket  [[300, 400),[20...   68228   56523  11705   \n",
       "48     installment_bucket  [[400, 500),[50...   42780   33725   9055   \n",
       "49     installment_bucket         [750, 1000)    3679    2693    986   \n",
       "31  inq_last_6mths_bucket              [0, 1)   68522   57290  11232   \n",
       "30  inq_last_6mths_bucket     [[4, 5),[1, 2)]   41051   33181   7870   \n",
       "32  inq_last_6mths_bucket              [2, 3)   18575   14608   3967   \n",
       "33  inq_last_6mths_bucket  [[5, 10),[3, 4)...    9293    7109   2184   \n",
       "2                   grade                   A   26110   24404   1706   \n",
       "3                   grade                   B   46890   40643   6247   \n",
       "4                   grade                   C   35285   27660   7625   \n",
       "5                   grade                   D   20119   14244   5875   \n",
       "6                   grade                   E    7022    4208   2814   \n",
       "7                   grade               [F,G]    2015    1029    986   \n",
       "29      emp_length_bucket  [[2, 3),[40, 50...   97051   79245  17806   \n",
       "28      emp_length_bucket           [10, 999)   40390   32943   7447   \n",
       "39      annual_inc_bucket  [[250, 1000),[1...   14480   12699   1781   \n",
       "40      annual_inc_bucket           [75, 100)   24799   21196   3603   \n",
       "38      annual_inc_bucket  [[50, 75),[40, ...   71046   57342  13704   \n",
       "42      annual_inc_bucket            [30, 40)   17872   13889   3983   \n",
       "43      annual_inc_bucket  [[10, 20),[0, 1...    1594    1234    360   \n",
       "41      annual_inc_bucket            [20, 30)    7650    5828   1822   \n",
       "\n",
       "    Distr_Good  Distr_Bad       WoE            IV  \n",
       "19    0.444664   0.338574  0.272578  2.891799e-02  \n",
       "20    0.270965   0.310300 -0.135550  5.331848e-03  \n",
       "21    0.284371   0.351127 -0.210868  1.407662e-02  \n",
       "37    0.005696   0.004514  0.232475  2.746648e-04  \n",
       "35    0.152833   0.147784  0.033590  1.695709e-04  \n",
       "34    0.835401   0.839623 -0.005041  2.128088e-05  \n",
       "36    0.006070   0.008078 -0.285789  5.738868e-04  \n",
       "0     0.886378   0.712470  0.218406  3.798268e-02  \n",
       "1     0.113622   0.287530 -0.928453  1.614657e-01  \n",
       "10    0.031269   0.004554  1.926638  5.147019e-02  \n",
       "8     0.032642   0.007286  1.499598  3.802287e-02  \n",
       "17    0.037615   0.010613  1.265372  3.416864e-02  \n",
       "11    0.057306   0.020592  1.023514  3.757729e-02  \n",
       "13    0.058696   0.024512  0.873213  2.985008e-02  \n",
       "14    0.059944   0.031719  0.636495  1.796509e-02  \n",
       "9     0.383909   0.331921  0.145509  7.564754e-03  \n",
       "12    0.199727   0.241041 -0.188013  7.767462e-03  \n",
       "18    0.030583   0.053736 -0.563656  1.305066e-02  \n",
       "15    0.069045   0.145250 -0.743701  5.667393e-02  \n",
       "16    0.039264   0.128777 -1.187761  1.063193e-01  \n",
       "23    0.039238   0.027007  0.373554  4.568939e-03  \n",
       "24    0.055630   0.045024  0.211516  2.243231e-03  \n",
       "25    0.227761   0.190354  0.179411  6.711226e-03  \n",
       "22    0.633633   0.679008 -0.069164  3.138353e-03  \n",
       "26    0.027338   0.030016 -0.093459  2.503023e-04  \n",
       "27    0.016401   0.028591 -0.555735  6.774204e-03  \n",
       "51    0.016579   0.015998  0.035686  2.074110e-05  \n",
       "50    0.977716   0.977666  0.000051  2.552636e-09  \n",
       "52    0.005705   0.006336 -0.104937  6.623300e-05  \n",
       "44    0.262381   0.090326  1.066373  1.834750e-01  \n",
       "45    0.737619   0.909674 -0.209659  3.607290e-02  \n",
       "46    0.171560   0.138875  0.211363  6.908542e-03  \n",
       "47    0.503824   0.463509  0.083400  3.362261e-03  \n",
       "48    0.300611   0.358571 -0.176309  1.021882e-02  \n",
       "49    0.024004   0.039045 -0.486476  7.316855e-03  \n",
       "31    0.510661   0.444779  0.138128  9.100138e-03  \n",
       "30    0.295762   0.311646 -0.052312  8.309021e-04  \n",
       "32    0.130210   0.157090 -0.187672  5.044666e-03  \n",
       "33    0.063367   0.086485 -0.311028  7.190312e-03  \n",
       "2     0.217528   0.067556  1.169365  1.753712e-01  \n",
       "3     0.362276   0.247377  0.381494  4.383341e-02  \n",
       "4     0.246550   0.301944 -0.202676  1.122702e-02  \n",
       "5     0.126965   0.232646 -0.605601  6.400006e-02  \n",
       "6     0.037508   0.111432 -1.088851  8.049201e-02  \n",
       "7     0.009172   0.039045 -1.448545  4.327203e-02  \n",
       "29    0.706359   0.705104  0.001778  2.230428e-06  \n",
       "28    0.293641   0.294896 -0.004264  5.349151e-06  \n",
       "39    0.113194   0.070526  0.473117  2.018680e-02  \n",
       "40    0.188933   0.142676  0.280815  1.298957e-02  \n",
       "38    0.511124   0.542668 -0.059885  1.889029e-03  \n",
       "42    0.123801   0.157724 -0.242169  8.215037e-03  \n",
       "43    0.010999   0.014256 -0.259319  8.444300e-04  \n",
       "41    0.051949   0.072150 -0.328492  6.635975e-03  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woe_iv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def woe_encoder(woe_df, original_df, target):\n",
    "    # Create a new DataFrame with the same columns as original_df\n",
    "    woe_encoded_df = pd.DataFrame(columns=original_df.columns, index=original_df.index)\n",
    "\n",
    "    # Loop through each feature-category and assign the corresponding WoE value as float\n",
    "    for feature in woe_df['Feature'].unique():\n",
    "        # Check that the feature exists in the original DataFrame\n",
    "        if feature not in original_df.columns:\n",
    "            print(f\"Feature {feature} not found in original DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        feature_woe = woe_df[woe_df['Feature'] == feature]\n",
    "        woe_dict = dict(zip(feature_woe['Category'], feature_woe['WoE']))\n",
    "\n",
    "        # Check that the categories exist in the original DataFrame\n",
    "        # Converting both to strings to avoid datatype issues\n",
    "        original_categories = original_df[feature].astype(str).unique()\n",
    "        woe_categories = feature_woe['Category'].astype(str).unique()\n",
    "        \n",
    "        # Two-way check:\n",
    "        # 1. For each category in the original DataFrame, check if it exists in the WoE DataFrame\n",
    "        missing_from_woe = [category for category in original_categories if category not in woe_categories]\n",
    "        if missing_from_woe:\n",
    "            print(f\"Categories {missing_from_woe} from original DataFrame not found in WoE DataFrame for feature {feature}.\")\n",
    "            \n",
    "        # 2. For each category in the WoE DataFrame, check if it exists in the original DataFrame\n",
    "        missing_from_original = [category for category in woe_categories if category not in original_categories]\n",
    "        if missing_from_original:\n",
    "            print(f\"Categories {missing_from_original} from WoE DataFrame not found in original DataFrame for feature {feature}.\")\n",
    "        \n",
    "        # Also converting original dataframe feature to string before replacement\n",
    "        woe_encoded_df[feature] = original_df[feature].astype(str).replace(woe_dict).astype(float)\n",
    "\n",
    "    # Check that the target exists in the original DataFrame\n",
    "    if target not in original_df.columns:\n",
    "        print(f\"Target {target} not found in original DataFrame. Returning None...\")\n",
    "        return None\n",
    "\n",
    "    # Add the target column to the new DataFrame\n",
    "    woe_encoded_df[target] = original_df[target]\n",
    "\n",
    "    return woe_encoded_df\n",
    "\n",
    "\n",
    "df_train = woe_encoder(woe_iv_df, df_train, target='default')\n",
    "\n",
    "# Update df_test\n",
    "df_test = woe_encoder(woe_iv_df, df_test, target='default')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit GLM Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:               137441\n",
      "Model:                            GLM   Df Residuals:                   137428\n",
      "Model Family:                Binomial   Df Model:                           12\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -59834.\n",
      "Date:                Wed, 05 Jul 2023   Deviance:                   1.1967e+05\n",
      "Time:                        13:20:27   Pearson chi2:                 1.38e+05\n",
      "No. Iterations:                     6   Pseudo R-squ. (CS):            0.07997\n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                    -1.4910      0.007   -199.474      0.000      -1.506      -1.476\n",
      "term                     -0.6012      0.017    -35.602      0.000      -0.634      -0.568\n",
      "grade                    -0.5400      0.029    -18.866      0.000      -0.596      -0.484\n",
      "sub_grade                -0.1907      0.031     -6.200      0.000      -0.251      -0.130\n",
      "verification_status      -0.2329      0.036     -6.557      0.000      -0.302      -0.163\n",
      "purpose                  -0.4942      0.049    -10.150      0.000      -0.590      -0.399\n",
      "emp_length_bucket        -2.0902      2.683     -0.779      0.436      -7.349       3.169\n",
      "inq_last_6mths_bucket    -0.4292      0.050     -8.595      0.000      -0.527      -0.331\n",
      "total_acc_bucket         -0.3940      0.222     -1.777      0.076      -0.829       0.040\n",
      "annual_inc_bucket        -1.2091      0.037    -33.107      0.000      -1.281      -1.138\n",
      "int_rate_bucket          -0.1150      0.024     -4.721      0.000      -0.163      -0.067\n",
      "installment_bucket       -1.3242      0.047    -28.064      0.000      -1.417      -1.232\n",
      "open_acc_bucket          -0.9450      0.736     -1.285      0.199      -2.387       0.497\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Create X_train, y_train and X_test, y_test\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n",
    "y_train = df_train[target_column]\n",
    "X_train = df_train.drop(target_column, axis=1)\n",
    "\n",
    "# Add constant to X_train for intercept term\n",
    "X_train = sm.add_constant(X_train)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Update df_test\n",
    "y_test = df_test[target_column]\n",
    "X_test = df_test.drop(target_column, axis=1)\n",
    "X_test = sm.add_constant(X_test)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_test = df_test.reindex(labels=df_train.columns, axis=1, fill_value=0)\n",
    "\n",
    "# Define the model\n",
    "model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "\n",
    "# Fit the model\n",
    "model_fit_glm = model.fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(model_fit_glm.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ValidMind Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:20:27,472 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:27,473 - INFO - dataset - Inferring dataset types...\n",
      "2023-07-05 13:20:28,648 - INFO - client - Pandas dataset detected. Initializing VM Dataset instance...\n",
      "2023-07-05 13:20:28,648 - INFO - dataset - Inferring dataset types...\n"
     ]
    }
   ],
   "source": [
    "# Cerate VM dataset\n",
    "vm_train_ds = vm.init_dataset(dataset=df_train,\n",
    "                        target_column=target_column)\n",
    "vm_test_ds = vm.init_dataset(dataset=df_test,\n",
    "                        target_column=target_column)\n",
    "\n",
    "# Create VM model\n",
    "vm_model_glm = vm.init_model(\n",
    "    model = model_fit_glm, \n",
    "    train_ds=vm_train_ds, \n",
    "    test_ds=vm_test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180f7fa4cbf346a58af9d97e745b4fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>A confusion matrix is a table that is used to describe the performance of a clas…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.sklearn.ConfusionMatrix import ConfusionMatrix\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = ConfusionMatrix(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3745dcfa7f84fffb21488b1149d1c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>A receiver operating characteristic (ROC), or simply ROC curve, is a graphical p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.RegressionROCCurve import RegressionROCCurve\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = RegressionROCCurve(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GINI and Kolmogorov-Smirnov (KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5da805bcc44885bf9dfb5a2f8b10f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Compute and display the AUC, GINI, and KS for train and test sets.</p>'), HTML(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validmind.tests.model_validation.statsmodels.GINITable import GINITable\n",
    "\n",
    "test_context = TestContext(model= vm_model_glm)\n",
    "metric = GINITable(test_context)\n",
    "metric.run()\n",
    "metric.result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard Development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Probability of Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pd(model_fit, X_train):\n",
    "\n",
    "    # Predict probabilities\n",
    "    probabilities = model_fit.predict(X_train)\n",
    "\n",
    "    # The probabilities are a 2D array with probabilities for the two classes.\n",
    "    # We are interested in the probability of default, which is the second column.\n",
    "    pd = probabilities\n",
    "\n",
    "    # Add PD as a new column in X_train\n",
    "    X_train['PD'] = pd\n",
    "\n",
    "    return X_train\n",
    "\n",
    "X_train_pd = compute_pd(model_fit_glm, X_train)\n",
    "df_train_pd = pd.concat([X_train_pd, y_train], axis=1)\n",
    "\n",
    "# Update df_test\n",
    "X_test_pd = compute_pd(model_fit_glm, X_test)\n",
    "df_test_pd = pd.concat([X_test_pd, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_pd_histogram(df_train, df_test, pd_col, target_col):\n",
    "    # Separate PD based on target column for training data\n",
    "    pd_train_0 = df_train[df_train[target_col] == 0][pd_col]\n",
    "    pd_train_1 = df_train[df_train[target_col] == 1][pd_col]\n",
    "\n",
    "    # Separate PD based on target column for testing data\n",
    "    pd_test_0 = df_test[df_test[target_col] == 0][pd_col]\n",
    "    pd_test_1 = df_test[df_test[target_col] == 1][pd_col]\n",
    "\n",
    "    # Create subplot\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n",
    "\n",
    "    # Create histograms for training data\n",
    "    trace_train_0 = go.Histogram(x=pd_train_0, opacity=0.75, name=f'Train {target_col} = 0')\n",
    "    trace_train_1 = go.Histogram(x=pd_train_1, opacity=0.75, name=f'Train {target_col} = 1')\n",
    "\n",
    "    # Create histograms for testing data\n",
    "    trace_test_0 = go.Histogram(x=pd_test_0, opacity=0.75, name=f'Test {target_col} = 0')\n",
    "    trace_test_1 = go.Histogram(x=pd_test_1, opacity=0.75, name=f'Test {target_col} = 1')\n",
    "\n",
    "    # Add traces to the subplots\n",
    "    fig.add_trace(trace_train_0, row=1, col=1)\n",
    "    fig.add_trace(trace_train_1, row=1, col=1)\n",
    "    fig.add_trace(trace_test_0, row=1, col=2)\n",
    "    fig.add_trace(trace_test_1, row=1, col=2)\n",
    "\n",
    "    # Update layout to overlay the histograms in each subplot\n",
    "    fig.update_layout(barmode='overlay', title_text='Histogram of Probability of Default')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_pd_histogram(df_train_pd,\n",
    "                  df_test_pd, \n",
    "                  pd_col='PD', \n",
    "                  target_col=target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_cumulative_pd(df_train, df_test, pd_col, target_col):\n",
    "    # Separate PD based on target column for training data\n",
    "    pd_train_0 = np.sort(df_train[df_train[target_col] == 0][pd_col])\n",
    "    pd_train_1 = np.sort(df_train[df_train[target_col] == 1][pd_col])\n",
    "\n",
    "    # Separate PD based on target column for testing data\n",
    "    pd_test_0 = np.sort(df_test[df_test[target_col] == 0][pd_col])\n",
    "    pd_test_1 = np.sort(df_test[df_test[target_col] == 1][pd_col])\n",
    "\n",
    "    # Calculate cumulative distributions\n",
    "    cumulative_pd_train_0 = np.cumsum(pd_train_0) / np.sum(pd_train_0)\n",
    "    cumulative_pd_train_1 = np.cumsum(pd_train_1) / np.sum(pd_train_1)\n",
    "    cumulative_pd_test_0 = np.cumsum(pd_test_0) / np.sum(pd_test_0)\n",
    "    cumulative_pd_test_1 = np.cumsum(pd_test_1) / np.sum(pd_test_1)\n",
    "\n",
    "    # Create subplot\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n",
    "\n",
    "    # Create line plots for training data\n",
    "    trace_train_0 = go.Scatter(x=pd_train_0, y=cumulative_pd_train_0, mode='lines', name=f'Train {target_col} = 0')\n",
    "    trace_train_1 = go.Scatter(x=pd_train_1, y=cumulative_pd_train_1, mode='lines', name=f'Train {target_col} = 1')\n",
    "\n",
    "    # Create line plots for testing data\n",
    "    trace_test_0 = go.Scatter(x=pd_test_0, y=cumulative_pd_test_0, mode='lines', name=f'Test {target_col} = 0')\n",
    "    trace_test_1 = go.Scatter(x=pd_test_1, y=cumulative_pd_test_1, mode='lines', name=f'Test {target_col} = 1')\n",
    "\n",
    "    # Add traces to the subplots\n",
    "    fig.add_trace(trace_train_0, row=1, col=1)\n",
    "    fig.add_trace(trace_train_1, row=1, col=1)\n",
    "    fig.add_trace(trace_test_0, row=1, col=2)\n",
    "    fig.add_trace(trace_test_1, row=1, col=2)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text='Cumulative Probability of Default')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_cumulative_pd(df_train_pd,\n",
    "                  df_test_pd, \n",
    "                  pd_col='PD', \n",
    "                  target_col=target_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Credit Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_credit_score(model_fit, X, target_score, target_odds, pdo):\n",
    "    # Create a copy of X\n",
    "    X_copy = X.copy()\n",
    "    \n",
    "    # Get logistic regression coefficients\n",
    "    beta = model_fit.params.values\n",
    "\n",
    "    # Get intercept (alpha)\n",
    "    alpha = model_fit.params[0]  # Intercept is the first parameter in statsmodels\n",
    "\n",
    "    # Calculate factor\n",
    "    factor = pdo / np.log(2)\n",
    "\n",
    "    # Calculate offset\n",
    "    offset = target_score - (factor * np.log(target_odds))\n",
    "\n",
    "    # Loop over each row in the copied data\n",
    "    for _, row in X_copy.iterrows():\n",
    "        # Initialize score for current row\n",
    "        score_i = 0\n",
    "\n",
    "        # Add contribution of each feature to the score\n",
    "        for i in range(1, len(beta)):  # Starting from 1 to skip the intercept\n",
    "            WoE_i = row[i]  # WoE for feature i, assuming intercept is in the first column\n",
    "            score_i += (beta[i] * WoE_i) * factor\n",
    "\n",
    "        # Add intercept's contribution to the score\n",
    "        score_i += alpha * factor\n",
    "\n",
    "        # Adjust the score scale using the offset\n",
    "        score_i += offset\n",
    "\n",
    "        # Add score to the new column in X_copy\n",
    "        X_copy.loc[row.name, 'score'] = score_i\n",
    "\n",
    "    return X_copy\n",
    "\n",
    "\n",
    "# Set target_score, target_odds, and pdo\n",
    "target_score = 600\n",
    "target_odds = 50\n",
    "pdo = 20\n",
    "\n",
    "# Compute credit scores and add to df_train\n",
    "X_train_scores = compute_credit_score(model_fit_glm, X_train_pd, target_score, target_odds, pdo)\n",
    "df_train_scores = pd.concat([X_train_scores, y_train], axis=1)\n",
    "\n",
    "# Update df_test \n",
    "X_test_scores = compute_credit_score(model_fit_glm, X_test_pd, target_score, target_odds, pdo)\n",
    "df_test_scores = pd.concat([X_test_scores, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_score_histogram(df_train, df_test, score_col, target_col):\n",
    "    # Separate scores based on target column for training data\n",
    "    scores_train_0 = df_train[df_train[target_col] == 0][score_col]\n",
    "    scores_train_1 = df_train[df_train[target_col] == 1][score_col]\n",
    "\n",
    "    # Separate scores based on target column for testing data\n",
    "    scores_test_0 = df_test[df_test[target_col] == 0][score_col]\n",
    "    scores_test_1 = df_test[df_test[target_col] == 1][score_col]\n",
    "\n",
    "    # Create subplot\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Train Data\", \"Test Data\"))\n",
    "\n",
    "    # Create histograms for training data\n",
    "    trace_train_0 = go.Histogram(x=scores_train_0, opacity=0.75, name=f'Train {target_col} = 0')\n",
    "    trace_train_1 = go.Histogram(x=scores_train_1, opacity=0.75, name=f'Train {target_col} = 1')\n",
    "\n",
    "    # Create histograms for testing data\n",
    "    trace_test_0 = go.Histogram(x=scores_test_0, opacity=0.75, name=f'Test {target_col} = 0')\n",
    "    trace_test_1 = go.Histogram(x=scores_test_1, opacity=0.75, name=f'Test {target_col} = 1')\n",
    "\n",
    "    # Add traces to the subplots\n",
    "    fig.add_trace(trace_train_0, row=1, col=1)\n",
    "    fig.add_trace(trace_train_1, row=1, col=1)\n",
    "    fig.add_trace(trace_test_0, row=1, col=2)\n",
    "    fig.add_trace(trace_test_1, row=1, col=2)\n",
    "\n",
    "    # Update layout to overlay the histograms in each subplot\n",
    "    fig.update_layout(barmode='overlay', title_text='Histogram of Scores')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "plot_score_histogram(df_train_scores, \n",
    "                     df_test_scores, \n",
    "                     score_col='score', \n",
    "                     target_col=target_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-framework",
   "language": "python",
   "name": "dev-framework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
