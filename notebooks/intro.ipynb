{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation\n",
    "\n",
    "* Load the library code from the local package directory\n",
    "* Load the API key and secret in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick hack to load local library code\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Load API key and secret from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidMind Python Library Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the ValidMind Library\n",
    "\n",
    "After creating an account with ValidMind, we can find the project's API key and secret in the settings page of the ValidMind dashboard:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/settings.png\" width=\"600\" height=\"300\" />\n",
    "\n",
    "The library credentials can be configured in two ways:\n",
    "\n",
    "- By setting the `VM_API_KEY` and `VM_API_SECRET` environment variables or\n",
    "- By passing `api_key` and `api_secret` arguments to the `init` function like this:\n",
    "\n",
    "```python\n",
    "vm.init(\n",
    "    api_key='<your-api-key>',\n",
    "    api_secret='<your-api-secret>',\n",
    "    project=\"cl2r3k1ri000009jweny7ba1g\"\n",
    ")\n",
    "```\n",
    "\n",
    "The `project` argument is mandatory since it allows the library to associate all data collected with a specific account project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "# Use api_host=\"https://api.dev.vm.validmind.ai/api/v1/tracking\" if you want to connect to the dev environment\n",
    "vm.init(\n",
    "    project=\"cl1jyv16o000809lg98gi9tie\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a demo dataset\n",
    "\n",
    "For this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\n",
    "\n",
    "We will train a sample model and demonstrate the following library functionalities:\n",
    "\n",
    "- Logging information about a dataset\n",
    "- Running data quality tests on a dataset\n",
    "- Logging information about a model\n",
    "- Logging training metrics for a model\n",
    "- Running model evaluation tests\n",
    "\n",
    "Before we logging any data on a new project, the ValidMind dashboard will let users know that they can automatically populate the different documentation sections by integrating the ValidMind into a model development environment:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/empty-data-description.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging general project metadata with `log_metadata`\n",
    "\n",
    "The ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`:\n",
    "\n",
    "```python\n",
    "vm.log_metadata(\"model_overview\", text=\"Testing\")\n",
    "```\n",
    "\n",
    "The `text` argument accepts Markdown formatted text as we'll see in the cell below. The documentation used for this model has been taken from the [Kaggle dataset](https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged metadata\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_overview = \"\"\"\n",
    "Testing writing metadata from the framework\n",
    "The ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`\n",
    "\n",
    "We aim to accomplish the following for this study:\n",
    "\n",
    "- Identify and visualize which factors contribute to customer churn\n",
    "- Build a prediction model that will perform the following:\n",
    "  - Classify if a customer is going to churn or not\n",
    "  - Preferably and based on model performance, choose a model that will attach a probability\n",
    "  to the churn to make it easier for customer service to target low hanging fruits in their\n",
    "  efforts to prevent churn\n",
    "\"\"\"\n",
    "\n",
    "vm.log_metadata(content_id=\"model_overview\", text=model_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard should now display the `Model Overview` section with the text we have provided from the library:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/model-overview.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a data quality test plan\n",
    "\n",
    "We will now run the default data quality test plan that will collect the\n",
    "following metadata from a dataset:\n",
    "\n",
    "- Field types and descriptions\n",
    "- Descriptive statistics\n",
    "- Data distribution histograms\n",
    "- Feature correlations\n",
    "\n",
    "and will run a collection of data quality tests such as:\n",
    "\n",
    "- Class imbalance\n",
    "- Duplicates\n",
    "- High cardinality\n",
    "- Missing values\n",
    "- Skewness\n",
    "\n",
    "ValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators.\n",
    "\n",
    "### Load our demo dataset\n",
    "\n",
    "Before running the test plan, we must first load the dataset into a Pandas DataFrame and initialize\n",
    "a ValidMind dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas dataset detected. Initializing VM Dataset instance...\n",
      "Inferring dataset types...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n",
    "\n",
    "vm_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    target_column=\"Exited\",\n",
    "    class_labels={\n",
    "        \"0\": \"Did not exit\",\n",
    "        \"1\": \"Exited\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and run the TabularDataQuality test plan\n",
    "\n",
    "We can now initialize the `TabularDataQuality` test plan. There are two ways to do this:\n",
    "\n",
    "1. By directly calling the test plan's constructor:\n",
    "\n",
    "```python\n",
    "dataset_tests = vm.test_plans.TabularDataset(\n",
    "    dataset=vm_dataset,\n",
    ")\n",
    "```\n",
    "\n",
    "2. Or by passing the test plan's ID to the `get_by_name` function. This will return an instance of the test plan class which we can initialize with the dataset object:\n",
    "\n",
    "```python\n",
    "DataQuality = vm.test_plans.get_by_name(\"tabular_data_quality\")\n",
    "\n",
    "dataset_tests = DataQuality(\n",
    "    dataset=vm_dataset,\n",
    ")\n",
    "```\n",
    "\n",
    "After initializing the test plan, we can run it by calling the `run` method:\n",
    "\n",
    "```python\n",
    "dataset_tests.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test plan 'tabular_dataset'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee85c17b8154765887f372e55bd874b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending results of test plan execution 'tabular_dataset' to ValidMind...\n",
      "|-- Running sub test plan - tabular_dataset_description\n",
      "Running test plan 'tabular_dataset_description'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b80d2869244d9698a59e5ea04030b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DatasetMetadata: dataset_metadata\n",
      "Running Metric: dataset_description\n",
      "Running Metric: dataset_correlations\n",
      "Sending results of test plan execution 'tabular_dataset_description' to ValidMind...\n",
      "Successfully logged dataset metadata and statistics.\n",
      "Successfully logged metrics\n",
      "|-- Running sub test plan - tabular_data_quality\n",
      "Running test plan 'tabular_data_quality'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d182d4f7166947f6a4f61e0ac0b73a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ThresholdTest: class_imbalance\n",
      "Running ThresholdTest: duplicates\n",
      "Running ThresholdTest: cardinality\n",
      "Running ThresholdTest: pearson_correlation\n",
      "Running ThresholdTest: missing\n",
      "Running ThresholdTest: skewness\n",
      "Running ThresholdTest: unique\n",
      "Running ThresholdTest: zeros\n",
      "Sending results of test plan execution 'tabular_data_quality' to ValidMind...\n",
      "Successfully logged test results for test: class_imbalance\n",
      "Successfully logged test results for test: duplicates\n",
      "Successfully logged test results for test: cardinality\n",
      "Successfully logged test results for test: pearson_correlation\n",
      "Successfully logged test results for test: missing\n",
      "Successfully logged test results for test: skewness\n",
      "Successfully logged test results for test: unique\n",
      "Successfully logged test results for test: zeros\n"
     ]
    }
   ],
   "source": [
    "dataset_tests = vm.test_plans.TabularDataset(\n",
    "    dataset=vm_dataset,\n",
    ")\n",
    "\n",
    "dataset_tests.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all test plans available in the developer framework\n",
    "\n",
    "We can find all the test plans available in the developer framework by calling the following functions:\n",
    "\n",
    "- All test plans: `vm.test_plans.list_plans()`\n",
    "- Describe a test plan: `vm.test_plans.describe_plan(\"tabular_dataset\")`\n",
    "- List all available tests: `vm.test_plans.list_tests()`\n",
    "\n",
    "As an example, here's the outpout `list_plans()` and `list_tests()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>ID                           </th><th>Name                        </th><th>Description                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>sklearn_classifier_metrics   </td><td>SKLearnClassifierMetrics    </td><td>Test plan for sklearn classifier metrics      </td></tr>\n",
       "<tr><td>sklearn_classifier_validation</td><td>SKLearnClassifierPerformance</td><td>Test plan for sklearn classifier models       </td></tr>\n",
       "<tr><td>sklearn_classifier           </td><td>SKLearnClassifier           </td><td>Test plan for sklearn classifier models that includes\n",
       "    both metrics and validation tests                                               </td></tr>\n",
       "<tr><td>tabular_dataset              </td><td>TabularDataset              </td><td>Test plan for generic tabular datasets        </td></tr>\n",
       "<tr><td>tabular_dataset_description  </td><td>TabularDatasetDescription   </td><td>Test plan to extract metadata and descriptive\n",
       "    statistics from a tabular dataset                                               </td></tr>\n",
       "<tr><td>tabular_data_quality         </td><td>TabularDataQuality          </td><td>Test plan for data quality on tabular datasets</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th>ID                           </th><th>Name                        </th><th>Description                                   </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>sklearn_classifier_metrics   </td><td>SKLearnClassifierMetrics    </td><td>Test plan for sklearn classifier metrics      </td></tr>\\n<tr><td>sklearn_classifier_validation</td><td>SKLearnClassifierPerformance</td><td>Test plan for sklearn classifier models       </td></tr>\\n<tr><td>sklearn_classifier           </td><td>SKLearnClassifier           </td><td>Test plan for sklearn classifier models that includes\\n    both metrics and validation tests                                               </td></tr>\\n<tr><td>tabular_dataset              </td><td>TabularDataset              </td><td>Test plan for generic tabular datasets        </td></tr>\\n<tr><td>tabular_dataset_description  </td><td>TabularDatasetDescription   </td><td>Test plan to extract metadata and descriptive\\n    statistics from a tabular dataset                                               </td></tr>\\n<tr><td>tabular_data_quality         </td><td>TabularDataQuality          </td><td>Test plan for data quality on tabular datasets</td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.test_plans.list_plans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Test Type    </th><th>ID                       </th><th>Name                        </th><th>Description                                                               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Metric       </td><td>dataset_correlations     </td><td>DatasetCorrelations         </td><td>Extracts the correlation matrix for a dataset. The following coefficients\n",
       "    are calculated:\n",
       "    - Pearson&#x27;s R for numerical variables\n",
       "    - Cramer&#x27;s V for categorical variables\n",
       "    - Correlation ratios for categorical-numerical variables                                                                           </td></tr>\n",
       "<tr><td>Metric       </td><td>dataset_description      </td><td>DatasetDescription          </td><td>Collects a set of descriptive statistics for a dataset                    </td></tr>\n",
       "<tr><td>Custom Test  </td><td>dataset_metadata         </td><td>DatasetMetadata             </td><td>Custom class to collect a set of descriptive statistics for a dataset.\n",
       "    This class will log dataset metadata via `log_dataset` instead of a metric.\n",
       "    Dataset metadat is necessary to initialize dataset object that can be related\n",
       "    to different metrics and test results                                                                           </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>class_imbalance          </td><td>ClassImbalanceTest          </td><td>Test that the minority class does not represent more than a threshold\n",
       "    of the total number of examples                                                                           </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>duplicates               </td><td>DuplicatesTest              </td><td>Test that the number of duplicates is less than a threshold               </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>cardinality              </td><td>HighCardinalityTest         </td><td>Test that the number of unique values in a column is less than a threshold</td></tr>\n",
       "<tr><td>ThresholdTest</td><td>pearson_correlation      </td><td>HighPearsonCorrelationTest  </td><td>Test that the Pearson correlation between two columns is less than a threshold\n",
       "\n",
       "    Inspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py                                                                           </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>missing                  </td><td>MissingValuesTest           </td><td>Test that the number of missing values is less than a threshold           </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>skewness                 </td><td>SkewnessTest                </td><td>Test that the skewness of a column is less than a threshold               </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>unique                   </td><td>UniqueRowsTest              </td><td>Test that the number of unique rows is greater than a threshold           </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>zeros                    </td><td>ZerosTest                   </td><td>Test that the number of zeros is less than a threshold                    </td></tr>\n",
       "<tr><td>Metric       </td><td>accuracy                 </td><td>AccuracyScore               </td><td>Accuracy Score                                                            </td></tr>\n",
       "<tr><td>Metric       </td><td>csi                      </td><td>CharacteristicStabilityIndex</td><td>Characteristic Stability Index between two datasets                       </td></tr>\n",
       "<tr><td>Metric       </td><td>confusion_matrix         </td><td>ConfusionMatrix             </td><td>Confusion Matrix                                                          </td></tr>\n",
       "<tr><td>Metric       </td><td>f1_score                 </td><td>F1Score                     </td><td>F1 Score                                                                  </td></tr>\n",
       "<tr><td>Metric       </td><td>pfi                      </td><td>PermutationFeatureImportance</td><td>Permutation Feature Importance                                            </td></tr>\n",
       "<tr><td>Metric       </td><td>psi                      </td><td>PopulationStabilityIndex    </td><td>Population Stability Index between two datasets                           </td></tr>\n",
       "<tr><td>Metric       </td><td>pr_curve                 </td><td>PrecisionRecallCurve        </td><td>Precision Recall Curve                                                    </td></tr>\n",
       "<tr><td>Metric       </td><td>precision                </td><td>PrecisionScore              </td><td>Precision Score                                                           </td></tr>\n",
       "<tr><td>Metric       </td><td>roc_auc                  </td><td>ROCAUCScore                 </td><td>ROC AUC Score                                                             </td></tr>\n",
       "<tr><td>Metric       </td><td>roc_curve                </td><td>ROCCurve                    </td><td>ROC Curve                                                                 </td></tr>\n",
       "<tr><td>Metric       </td><td>recall                   </td><td>RecallScore                 </td><td>Recall Score                                                              </td></tr>\n",
       "<tr><td>Custom Test  </td><td>shap                     </td><td>SHAPGlobalImportance        </td><td>SHAP Global Importance. Custom metric                                     </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>accuracy_score           </td><td>AccuracyTest                </td><td>Test that the accuracy score is above a threshold.                        </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>f1_score                 </td><td>F1ScoreTest                 </td><td>Test that the F1 score is above a threshold.                              </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>roc_auc_score            </td><td>ROCAUCScoreTest             </td><td>Test that the ROC AUC score is above a threshold.                         </td></tr>\n",
       "<tr><td>ThresholdTest</td><td>training_test_degradation</td><td>TrainingTestDegradationTest </td><td>Test that the training set metrics are better than the test set metrics.  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th>Test Type    </th><th>ID                       </th><th>Name                        </th><th>Description                                                               </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>Metric       </td><td>dataset_correlations     </td><td>DatasetCorrelations         </td><td>Extracts the correlation matrix for a dataset. The following coefficients\\n    are calculated:\\n    - Pearson&#x27;s R for numerical variables\\n    - Cramer&#x27;s V for categorical variables\\n    - Correlation ratios for categorical-numerical variables                                                                           </td></tr>\\n<tr><td>Metric       </td><td>dataset_description      </td><td>DatasetDescription          </td><td>Collects a set of descriptive statistics for a dataset                    </td></tr>\\n<tr><td>Custom Test  </td><td>dataset_metadata         </td><td>DatasetMetadata             </td><td>Custom class to collect a set of descriptive statistics for a dataset.\\n    This class will log dataset metadata via `log_dataset` instead of a metric.\\n    Dataset metadat is necessary to initialize dataset object that can be related\\n    to different metrics and test results                                                                           </td></tr>\\n<tr><td>ThresholdTest</td><td>class_imbalance          </td><td>ClassImbalanceTest          </td><td>Test that the minority class does not represent more than a threshold\\n    of the total number of examples                                                                           </td></tr>\\n<tr><td>ThresholdTest</td><td>duplicates               </td><td>DuplicatesTest              </td><td>Test that the number of duplicates is less than a threshold               </td></tr>\\n<tr><td>ThresholdTest</td><td>cardinality              </td><td>HighCardinalityTest         </td><td>Test that the number of unique values in a column is less than a threshold</td></tr>\\n<tr><td>ThresholdTest</td><td>pearson_correlation      </td><td>HighPearsonCorrelationTest  </td><td>Test that the Pearson correlation between two columns is less than a threshold\\n\\n    Inspired by: https://github.com/ydataai/pandas-profiling/blob/f8bad5dde27e3f87f11ac74fb8966c034bc22db8/src/pandas_profiling/model/correlations.py                                                                           </td></tr>\\n<tr><td>ThresholdTest</td><td>missing                  </td><td>MissingValuesTest           </td><td>Test that the number of missing values is less than a threshold           </td></tr>\\n<tr><td>ThresholdTest</td><td>skewness                 </td><td>SkewnessTest                </td><td>Test that the skewness of a column is less than a threshold               </td></tr>\\n<tr><td>ThresholdTest</td><td>unique                   </td><td>UniqueRowsTest              </td><td>Test that the number of unique rows is greater than a threshold           </td></tr>\\n<tr><td>ThresholdTest</td><td>zeros                    </td><td>ZerosTest                   </td><td>Test that the number of zeros is less than a threshold                    </td></tr>\\n<tr><td>Metric       </td><td>accuracy                 </td><td>AccuracyScore               </td><td>Accuracy Score                                                            </td></tr>\\n<tr><td>Metric       </td><td>csi                      </td><td>CharacteristicStabilityIndex</td><td>Characteristic Stability Index between two datasets                       </td></tr>\\n<tr><td>Metric       </td><td>confusion_matrix         </td><td>ConfusionMatrix             </td><td>Confusion Matrix                                                          </td></tr>\\n<tr><td>Metric       </td><td>f1_score                 </td><td>F1Score                     </td><td>F1 Score                                                                  </td></tr>\\n<tr><td>Metric       </td><td>pfi                      </td><td>PermutationFeatureImportance</td><td>Permutation Feature Importance                                            </td></tr>\\n<tr><td>Metric       </td><td>psi                      </td><td>PopulationStabilityIndex    </td><td>Population Stability Index between two datasets                           </td></tr>\\n<tr><td>Metric       </td><td>pr_curve                 </td><td>PrecisionRecallCurve        </td><td>Precision Recall Curve                                                    </td></tr>\\n<tr><td>Metric       </td><td>precision                </td><td>PrecisionScore              </td><td>Precision Score                                                           </td></tr>\\n<tr><td>Metric       </td><td>roc_auc                  </td><td>ROCAUCScore                 </td><td>ROC AUC Score                                                             </td></tr>\\n<tr><td>Metric       </td><td>roc_curve                </td><td>ROCCurve                    </td><td>ROC Curve                                                                 </td></tr>\\n<tr><td>Metric       </td><td>recall                   </td><td>RecallScore                 </td><td>Recall Score                                                              </td></tr>\\n<tr><td>Custom Test  </td><td>shap                     </td><td>SHAPGlobalImportance        </td><td>SHAP Global Importance. Custom metric                                     </td></tr>\\n<tr><td>ThresholdTest</td><td>accuracy_score           </td><td>AccuracyTest                </td><td>Test that the accuracy score is above a threshold.                        </td></tr>\\n<tr><td>ThresholdTest</td><td>f1_score                 </td><td>F1ScoreTest                 </td><td>Test that the F1 score is above a threshold.                              </td></tr>\\n<tr><td>ThresholdTest</td><td>roc_auc_score            </td><td>ROCAUCScoreTest             </td><td>Test that the ROC AUC score is above a threshold.                         </td></tr>\\n<tr><td>ThresholdTest</td><td>training_test_degradation</td><td>TrainingTestDegradationTest </td><td>Test that the training set metrics are better than the test set metrics.  </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.test_plans.list_tests()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `TabularDataset` test plan has finished running, we can view the results in the ValidMind dashboard:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/data-description.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset for training\n",
    "\n",
    "Before we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n",
    "\n",
    "- Dropping irrelevant variables\n",
    "- Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping irrelevant variables\n",
    "\n",
    "The following variables will be dropped from the dataset:\n",
    "\n",
    "- `RowNumber`: it's a unique identifier to the record\n",
    "- `CustomerId`: it's a unique identifier to the customer\n",
    "- `Surname`: no predictive power for this variable\n",
    "- `CreditScore`: we didn't observer any correlation between `CreditScore` and our target column `Exited`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables\n",
    "\n",
    "We will apply one-hot or dummy encoding to the following variables:\n",
    "\n",
    "- `Geography`: only 3 unique values found in the dataset\n",
    "- `Gender`: convert from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"Male\": 0, \"Female\": 1}\n",
    "df.replace({\"Gender\": genders}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n",
    "df.drop(\"Geography\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our model with the preprocessed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       1   42       2       0.00              1          1               1   \n",
       "1       1   41       1   83807.86              1          0               1   \n",
       "2       1   42       8  159660.80              3          1               0   \n",
       "3       1   39       1       0.00              2          0               0   \n",
       "4       1   43       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  Geography_France  Geography_Germany  \\\n",
       "0        101348.88       1                 1                  0   \n",
       "1        112542.58       0                 0                  0   \n",
       "2        113931.57       1                 1                  0   \n",
       "3         93826.63       0                 1                  0   \n",
       "4         79084.10       0                 0                  0   \n",
       "\n",
       "   Geography_Spain  \n",
       "0                0  \n",
       "1                1  \n",
       "2                0  \n",
       "3                0  \n",
       "4                1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation\n",
    "\n",
    "For training our model, we will **randomly** split the dataset in 3 parts:\n",
    "\n",
    "- `training` split with 60% of the rows\n",
    "- `validation` split with 20% of the rows\n",
    "- `test` split with 20% of the rows\n",
    "\n",
    "The `test` dataset will be our held out dataset for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.20)\n",
    "\n",
    "# This guarantees a 60/20/20 split\n",
    "train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "# For training\n",
    "x_train = train_ds.drop(\"Exited\", axis=1)\n",
    "y_train = train_ds.loc[:, \"Exited\"].astype(int)\n",
    "x_val = val_ds.drop(\"Exited\", axis=1)\n",
    "y_val = val_ds.loc[:, \"Exited\"].astype(int)\n",
    "\n",
    "# For testing\n",
    "x_test = test_df.drop(\"Exited\", axis=1)\n",
    "y_test = test_df.loc[:, \"Exited\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We will train a simple XGBoost model and set its `eval_set` to `[(x_train, y_train), (x_val, y_val)]` in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of \"in training\" metrics so model developers can provide additional context to model validators if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=10, enable_categorical=False,\n",
       "              eval_metric=['error', 'logloss', 'auc'], gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\n",
       "              max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.861875\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_proba(x_val)[:, -1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a model evaluation test plan\n",
    "\n",
    "We will now run a basic model evaluation test plan that is compatible with the model we have trained.\n",
    "Since we have trained an XGBoost model with a sklearn-like API, we will use the `SKLearnClassifier` test plan. This test plan will collect model metadata and metrics, and run a variety of model evaluation tests, according to the modeling objective (binary classification for this example).\n",
    "\n",
    "The following model metadata is collected:\n",
    "\n",
    "- Model framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\n",
    "- Model task details (e.g. binary classification, regression, etc.)\n",
    "- Model hyperparameters (e.g. number of trees, max depth, etc.)\n",
    "\n",
    "The model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n",
    "\n",
    "- AUC\n",
    "- Error rate\n",
    "- Logloss\n",
    "- Feature importance\n",
    "\n",
    "Similarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n",
    "\n",
    "- Simple training/test overfit test\n",
    "- Training/test performance degradation\n",
    "- Baseline test dataset performance test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize VM model object and train/test datasets\n",
    "\n",
    "In order to run our SKLearnClassifier test plan, we need to initialize ValidMind object instances for the trained model and the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas dataset detected. Initializing VM Dataset instance...\n",
      "Inferring dataset types...\n",
      "Pandas dataset detected. Initializing VM Dataset instance...\n",
      "Inferring dataset types...\n"
     ]
    }
   ],
   "source": [
    "vm_model = vm.init_model(model)\n",
    "vm_train_ds = vm.init_dataset(dataset=train_ds, type=\"generic\", target_column=\"Exited\")\n",
    "vm_test_ds = vm.init_dataset(dataset=test_df, type=\"generic\", target_column=\"Exited\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the `SKLearnClassifier` test plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test plan 'sklearn_classifier'...\n",
      "Generating predictions train dataset...\n",
      "Generating predictions test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ededf34150c8463d994cdca71a3d33f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending results of test plan execution 'sklearn_classifier' to ValidMind...\n",
      "|-- Running sub test plan - sklearn_classifier_metrics\n",
      "Running test plan 'sklearn_classifier_metrics'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311ec9ee505c471f8611614a16752403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ModelMetadata: model_metadata\n",
      "Running Metric: accuracy\n",
      "Running Metric: confusion_matrix\n",
      "Running Metric: f1_score\n",
      "Running Metric: pfi\n",
      "Running Metric: pr_curve\n",
      "Running Metric: precision\n",
      "Running Metric: recall\n",
      "Running Metric: roc_auc\n",
      "Running Metric: roc_curve\n",
      "Running Metric: csi\n",
      "Running Metric: psi\n",
      "Running SHAPGlobalImportance: shap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending results of test plan execution 'sklearn_classifier_metrics' to ValidMind...\n",
      "Successfully logged metrics\n",
      "|-- Running sub test plan - sklearn_classifier_validation\n",
      "Running test plan 'sklearn_classifier_validation'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9dc0dbde174a0f912fd049dabe286d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ThresholdTest: accuracy_score\n",
      "Running ThresholdTest: f1_score\n",
      "Running ThresholdTest: roc_auc_score\n",
      "Running ThresholdTest: training_test_degradation\n",
      "Sending results of test plan execution 'sklearn_classifier_validation' to ValidMind...\n",
      "Successfully logged test results for test: accuracy_score\n",
      "Successfully logged test results for test: f1_score\n",
      "Successfully logged test results for test: roc_auc_score\n",
      "Successfully logged test results for test: training_test_degradation\n"
     ]
    }
   ],
   "source": [
    "model_metrics = vm.test_plans.SKLearnClassifier(\n",
    "    model=vm_model,\n",
    "    train_ds=vm_train_ds,\n",
    "    test_ds=vm_test_ds,\n",
    ")\n",
    "\n",
    "model_metrics.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('validmind-Jp3s24zK-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5507f2e99c1cac96073e07e686bb64d511c5f1c7216ba7fc4306f43af6557f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
