{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation\n",
    "\n",
    "* Load the library code from the local package directory\n",
    "* Load the API key and secret in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick hack to load local library code\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Load API key and secret from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidMind Python Library Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the ValidMind Library\n",
    "\n",
    "After creating an account with ValidMind, we can find the project's API key and secret in the settings page of the ValidMind dashboard:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/settings.png\" width=\"600\" height=\"300\" />\n",
    "\n",
    "The library credentials can be configured in two ways:\n",
    "\n",
    "- By setting the `VM_API_KEY` and `VM_API_SECRET` environment variables or\n",
    "- By passing `api_key` and `api_secret` arguments to the `init` function like this:\n",
    "\n",
    "```python\n",
    "vm.init(\n",
    "    api_key='<your-api-key>',\n",
    "    api_secret='<your-api-secret>',\n",
    "    project=\"cl2r3k1ri000009jweny7ba1g\"\n",
    ")\n",
    "```\n",
    "\n",
    "The `project` argument is mandatory since it allows the library to associate all data collected with a specific account project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n",
    "vm.init(\n",
    "    # api_host=\"https://api.dev.vm.validmind.ai/api/v1/tracking\",\n",
    "    # project=\"cl2r3k1ri000009jweny7ba1g\"\n",
    "    project=\"cl1jyv16o000809lg98gi9tie\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a demo dataset\n",
    "\n",
    "For this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\n",
    "\n",
    "We will train a sample model and demonstrate the following library functionalities:\n",
    "\n",
    "- Logging information about a dataset\n",
    "- Running data quality tests on a dataset\n",
    "- Logging information about a model\n",
    "- Logging training metrics for a model\n",
    "- Running model evaluation tests\n",
    "\n",
    "Before we logging any data on a new project, the ValidMind dashboard will let users know that they can automatically populate the different documentation sections by integrating the ValidMind into a model development environment:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/empty-data-description.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging general project metadata with `log_metadata`\n",
    "\n",
    "The ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`:\n",
    "\n",
    "```python\n",
    "vm.log_metadata(\"model_overview\", text=\"Testing\")\n",
    "```\n",
    "\n",
    "The `text` argument accepts Markdown formatted text as we'll see in the cell below. The documentation used for this model has been taken from the [Kaggle dataset](https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged metadata\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_overview = \"\"\"\n",
    "Testing writing metadata from the framework\n",
    "The ValidMind library provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`\n",
    "\n",
    "We aim to accomplish the following for this study:\n",
    "\n",
    "- Identify and visualize which factors contribute to customer churn\n",
    "- Build a prediction model that will perform the following:\n",
    "  - Classify if a customer is going to churn or not\n",
    "  - Preferably and based on model performance, choose a model that will attach a probability\n",
    "  to the churn to make it easier for customer service to target low hanging fruits in their\n",
    "  efforts to prevent churn\n",
    "\"\"\"\n",
    "\n",
    "vm.log_metadata(content_id=\"model_overview\", text=model_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard should now display the `Model Overview` section with the text we have provided from the library:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/model-overview.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing a dataset with `analyze_dataset`\n",
    "\n",
    "The `analyze_dataset` function will collect the following metadata about the given dataset:\n",
    "\n",
    "- Field types and descriptions\n",
    "- Descriptive statistics\n",
    "- Data distribution histograms\n",
    "- Feature correlation\n",
    "\n",
    "and will run a collection of data quality tests such as:\n",
    "\n",
    "- Class imbalance\n",
    "- Duplicates\n",
    "- High cardinality\n",
    "- Missing values\n",
    "- Skewness\n",
    "\n",
    "ValidMind evaluates if the data quality metrics are within expected ranges. These thresholds or ranges can be further configured by model validators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas dataset detected. Initializing VM Dataset instance...\n",
      "Inferring dataset types...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n",
    "\n",
    "targets = vm.DatasetTargets(\n",
    "    target_column=\"Exited\",\n",
    "    class_labels={\n",
    "        \"0\": \"Did not exit\",\n",
    "        \"1\": \"Exited\",\n",
    "    }\n",
    ")\n",
    "\n",
    "vm_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    type=\"training\",\n",
    "    targets=targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating decriptive statistics...\n",
      "Preparing in-memory dataset copy...\n",
      "Calculating feature correlations...\n",
      "Preparing in-memory dataset copy...\n",
      "Logging dataset metadata to ValidMind...\n",
      "Successfully logged dataset metadata and statistics.\n",
      "Generating correlation plots...\n"
     ]
    }
   ],
   "source": [
    "vm.analyze_dataset(vm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data quality tests for \"training\" dataset...\n",
      "\n",
      "Preparing in-memory dataset copy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 34.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test suite has completed.\n",
      "Sending results to ValidMind...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged test results for test: class_imbalance\n",
      "Successfully logged test results for test: duplicates\n",
      "Successfully logged test results for test: cardinality\n",
      "Successfully logged test results for test: missing\n",
      "Successfully logged test results for test: skewness\n",
      "Successfully logged test results for test: zeros\n",
      "\n",
      "Summary of results:\n",
      "\n",
      "Test             Passed      # Passed    # Errors    % Passed\n",
      "---------------  --------  ----------  ----------  ----------\n",
      "class_imbalance  True               1           0         100\n",
      "duplicates       True               1           0         100\n",
      "cardinality      False              6           1     85.7143\n",
      "missing          True              14           0         100\n",
      "skewness         False              6           1     85.7143\n",
      "zeros            False              0           2           0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TestResults(category='data_quality', test_name='class_imbalance', params={'min_percent_threshold': 0.2}, passed=True, results=[TestResult(test_name=None, column='Exited', passed=True, values={0: 0.798, 1: 0.202})]),\n",
       " TestResults(category='data_quality', test_name='duplicates', params={'min_threshold': 1}, passed=True, results=[TestResult(test_name=None, column=None, passed=True, values={'n_duplicates': 0, 'p_duplicates': 0.0})]),\n",
       " TestResults(category='data_quality', test_name='cardinality', params={'num_threshold': 100, 'percent_threshold': 0.1, 'threshold_type': 'percent'}, passed=False, results=[TestResult(test_name=None, column='Surname', passed=False, values={'n_distinct': 2616, 'p_distinct': 0.327}), TestResult(test_name=None, column='Geography', passed=True, values={'n_distinct': 3, 'p_distinct': 0.000375}), TestResult(test_name=None, column='Gender', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_distinct': 4, 'p_distinct': 0.0005}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025}), TestResult(test_name=None, column='Exited', passed=True, values={'n_distinct': 2, 'p_distinct': 0.00025})]),\n",
       " TestResults(category='data_quality', test_name='missing', params={'min_threshold': 1}, passed=True, results=[TestResult(test_name=None, column='RowNumber', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CustomerId', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Surname', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='CreditScore', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Geography', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Gender', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Age', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Tenure', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Balance', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='NumOfProducts', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='HasCrCard', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='IsActiveMember', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'n_missing': 0, 'p_missing': 0.0}), TestResult(test_name=None, column='Exited', passed=True, values={'n_missing': 0, 'p_missing': 0.0})]),\n",
       " TestResults(category='data_quality', test_name='skewness', params={'max_threshold': 1}, passed=False, results=[TestResult(test_name=None, column='RowNumber', passed=True, values={'skewness': -0.005920679739677088}), TestResult(test_name=None, column='CustomerId', passed=True, values={'skewness': 0.010032280260684402}), TestResult(test_name=None, column='CreditScore', passed=True, values={'skewness': -0.06195161237091896}), TestResult(test_name=None, column='Age', passed=False, values={'skewness': 1.0245221429799511}), TestResult(test_name=None, column='Tenure', passed=True, values={'skewness': 0.007692043774702702}), TestResult(test_name=None, column='Balance', passed=True, values={'skewness': -0.13527693543111804}), TestResult(test_name=None, column='EstimatedSalary', passed=True, values={'skewness': 0.009510428002077728})]),\n",
       " TestResults(category='data_quality', test_name='zeros', params={'max_percent_threshold': 0.03}, passed=False, results=[TestResult(test_name=None, column='Tenure', passed=False, values={'n_zeros': 323, 'p_zeros': 0.040375}), TestResult(test_name=None, column='Balance', passed=False, values={'n_zeros': 2912, 'p_zeros': 0.364})])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.run_dataset_tests(vm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see all dataset related metadata and data quality metrics in the ValidMind dashboard:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/data-description.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset for training\n",
    "\n",
    "Before we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n",
    "\n",
    "- Dropping irrelevant variables\n",
    "- Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping irrelevant variables\n",
    "\n",
    "The following variables will be dropped from the dataset:\n",
    "\n",
    "- `RowNumber`: it's a unique identifier to the record\n",
    "- `CustomerId`: it's a unique identifier to the customer\n",
    "- `Surname`: no predictive power for this variable\n",
    "- `CreditScore`: we didn't observer any correlation between `CreditScore` and our target column `Exited`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables\n",
    "\n",
    "We will apply one-hot or dummy encoding to the following variables:\n",
    "\n",
    "- `Geography`: only 3 unique values found in the dataset\n",
    "- `Gender`: convert from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"Male\": 0, \"Female\": 1}\n",
    "df.replace({\"Gender\": genders}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n",
    "df.drop(\"Geography\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our model with the preprocessed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation\n",
    "\n",
    "For training our model, we will **randomly** split the dataset in 3 parts:\n",
    "\n",
    "- `training` split with 60% of the rows\n",
    "- `validation` split with 20% of the rows\n",
    "- `test` split with 20% of the rows\n",
    "\n",
    "The `test` dataset will be our held out dataset for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.20)\n",
    "\n",
    "# This guarantees a 60/20/20 split\n",
    "train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "# For training\n",
    "x_train = train_ds.drop(\"Exited\", axis=1)\n",
    "y_train = train_ds.loc[:, \"Exited\"].astype(int)\n",
    "x_val = val_ds.drop(\"Exited\", axis=1)\n",
    "y_val = val_ds.loc[:, \"Exited\"].astype(int)\n",
    "\n",
    "# For testing\n",
    "x_test = test_df.drop(\"Exited\", axis=1)\n",
    "y_test = test_df.loc[:, \"Exited\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We will train a simple XGBoost model and set its `eval_set` to `[(x_train, y_train), (x_val, y_val)]` in order to collect validation datasets metrics on every round. The ValidMind library supports collecting any type of \"in training\" metrics so model developers can provide additional context to model validators if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = LogisticRegression()\n",
    "# model.fit(\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(x_val)[:, -1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model performance `evaluate_model`\n",
    "\n",
    "The `evaluate_model` function will extract model metadata and metrics, and will run a collection of model evaluation tests, according to the model type, validation use case and custom validation requirements.\n",
    "\n",
    "The following model metadata is collected:\n",
    "\n",
    "- Model framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\n",
    "- Model task details (e.g. binary classification, regression, etc.)\n",
    "- Model hyperparameters (e.g. number of trees, max depth, etc.)\n",
    "\n",
    "The model metrics that are collected depend on the model type, use case, etc. For example, for a binary classification model, the following metrics could be collected (again, depending on configuration):\n",
    "\n",
    "- AUC\n",
    "- Error rate\n",
    "- Logloss\n",
    "- Feature importance\n",
    "\n",
    "Similarly, different model evaluation tests are run depending on the model type, use case, etc. For example, for a binary classification model, the following tests could be executed:\n",
    "\n",
    "- Simple training/test overfit test\n",
    "- Training/test performance degradation\n",
    "- Baseline test dataset performance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find an optimal threshold for the model before we evaluate it\n",
    "# We want to focus on a threshold that maximizes the F1 score since\n",
    "# we are interested in optimizing performance for the minority class\n",
    "y_pred = model.predict_proba(x_val)[:, -1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# Get the index of the largest F1 Score\n",
    "ix = argmax(fscore)\n",
    "threshold = thresholds[ix]\n",
    "print('Optimal threshold=%f, F1 Score=%.3f' % (threshold, fscore[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_metric = vm.Metric(\n",
    "    type=\"evaluation\",\n",
    "    scope=\"test\",\n",
    "    key=\"decision_threshold\",\n",
    "    value=[threshold]\n",
    ")\n",
    "\n",
    "vm.log_metrics([threshold_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = vm.evaluate_model(\n",
    "    model,\n",
    "    train_set=(x_train, y_train),    \n",
    "    val_set=(x_val, y_val),\n",
    "    test_set=(x_test, y_test),\n",
    "    # eval_opts={\"decision_threshold\": threshold}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('validmind-Jp3s24zK-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5507f2e99c1cac96073e07e686bb64d511c5f1c7216ba7fc4306f43af6557f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
