{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation\n",
    "\n",
    "* Load the SDK code from the local package directory\n",
    "* Load the API key and secret in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick hack to load local SDK code\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Load API key and secret from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidMind SDK Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the ValidMind SDK\n",
    "\n",
    "After creating an account with ValidMind, we can find the project's API key and secret in the settings page of the ValidMind dashboard:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/settings.png\" width=\"600\" height=\"300\" />\n",
    "\n",
    "The SDK credentials can be configured in two ways:\n",
    "\n",
    "- By setting the `VM_API_KEY` and `VM_API_SECRET` environment variables or\n",
    "- By passing `api_key` and `api_secret` arguments to the `init` function like this:\n",
    "\n",
    "```python\n",
    "vm.init(\n",
    "    api_key='<your-api-key>',\n",
    "    api_secret='<your-api-secret>',\n",
    "    project=\"cl2r3k1ri000009jweny7ba1g\"\n",
    ")\n",
    "```\n",
    "\n",
    "The `project` argument is mandatory since it allows the SDK to associate all data collected with a specific account project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "# For test environment use api_host=\"https://api.test.vm.validmind.ai/api/v1/tracking\"\n",
    "vm.init(project=\"cl2r3k1ri000009jweny7ba1g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a demo dataset\n",
    "\n",
    "For this simple demonstration, we will use the following bank customer churn dataset from Kaggle: https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data.\n",
    "\n",
    "We will train a sample model and demonstrate the following ValidMind SDK functionalities:\n",
    "\n",
    "- Logging information about a dataset\n",
    "- Running data quality tests on a dataset\n",
    "- Logging information about a model\n",
    "- Logging training metrics for a model\n",
    "- Running model evaluation tests\n",
    "\n",
    "Before we logging any data on a new project, the ValidMind dashboard will let users know that they can automatically populate the different documentation sections by integrating the ValidMind into a model development environment:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/empty-data-description.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging general project metadata with `log_metadata`\n",
    "\n",
    "The ValidMind SDK provides a function to log free-form metadata for a project. A list of preconfigured `content_id` can be used to select where in the dashboard documentation we want this metadata to be displayed. As an example, if a model developer wants to populate the `Model Overview` section for a project, they can use `model_overview` as the `content_id`:\n",
    "\n",
    "```python\n",
    "vm.log_dataset(\"model_overview\", text=\"Testing\")\n",
    "```\n",
    "\n",
    "The `text` argument accepts Markdown formatted text as we'll see in the cell below. The documentation used for this model has been taken from the [Kaggle dataset](https://www.kaggle.com/code/kmalit/bank-customer-churn-prediction/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged metadata\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_overview = \"\"\"\n",
    "We aim to accomplish the following for this study:\n",
    "\n",
    "- Identify and visualize which factors contribute to customer churn\n",
    "- Build a prediction model that will perform the following:\n",
    "  - Classify if a customer is going to churn or not\n",
    "  - Preferably and based on model performance, choose a model that will attach a probability\n",
    "  to the churn to make it easier for customer service to target low hanging fruits in their\n",
    "  efforts to prevent churn\n",
    "\"\"\"\n",
    "\n",
    "vm.log_metadata(content_id=\"model_overview\", text=model_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard should now display the `Model Overview` section with the text we have provided from the SDK:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/model-overview.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging information about a dataset with `log_dataset`\n",
    "\n",
    "The `log_dataset` SDK function will collect the following metadata about the given dataset:\n",
    "\n",
    "- Field types and descriptions\n",
    "- Descriptive statistics\n",
    "- Data distribution histograms\n",
    "- Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"notebooks/datasets/bank_customer_churn.csv\")\n",
    "\n",
    "targets = vm.DatasetTargets(\n",
    "    target_column=\"Exited\",\n",
    "    class_labels={\n",
    "        \"0\": \"Did not exit\",\n",
    "        \"1\": \"Exited\",\n",
    "    }\n",
    ")\n",
    "\n",
    "features = [\n",
    "    {\n",
    "        \"id\": \"RowNumber\",\n",
    "        \"type_options\": {\n",
    "            \"primary_key\": True,\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "vm_dataset = vm.log_dataset(df, \"training\", analyze=True, targets=targets, features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running data quality tests with `run_dataset_tests`\n",
    "\n",
    "When using `run_dataset_tests`, the SDK will compute some basic quality metrics on the dataset and evaluate if they are within expected ranges as configured by ValidMind and model validators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vm.run_dataset_tests(\n",
    "    df,\n",
    "    dataset_type=\"training\",\n",
    "    vm_dataset=vm_dataset,    \n",
    "    send=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see all dataset related metadata and data quality metrics in the ValidMind dashboard:\n",
    "\n",
    "<img src=\"https://vmai.s3.us-west-1.amazonaws.com/sdk-images/data-description.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset for training\n",
    "\n",
    "Before we train a model, we need to run some common minimal feature selection and engineering steps on the dataset:\n",
    "\n",
    "- Dropping irrelevant variables\n",
    "- Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping irrelevant variables\n",
    "\n",
    "The following variables will be dropped from the dataset:\n",
    "\n",
    "- `RowNumber`: it's a unique identifier to the record\n",
    "- `CustomerId`: it's a unique identifier to the customer\n",
    "- `Surname`: no predictive power for this variable\n",
    "- `CreditScore`: we didn't observer any correlation between `CreditScore` and our target column `Exited`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"CreditScore\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables\n",
    "\n",
    "We will apply one-hot or dummy encoding to the following variables:\n",
    "\n",
    "- `Geography`: only 3 unique values found in the dataset\n",
    "- `Gender`: convert from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"Male\": 0, \"Female\": 1}\n",
    "df.replace({\"Gender\": genders}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df[\"Geography\"], prefix=\"Geography\")], axis=1)\n",
    "df.drop(\"Geography\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our model with the preprocessed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation\n",
    "\n",
    "For training our model, we will **randomly** split the dataset in 3 parts:\n",
    "\n",
    "- `training` split with 60% of the rows\n",
    "- `validation` split with 20% of the rows\n",
    "- `test` split with 20% of the rows\n",
    "\n",
    "The `test` dataset will be our held out dataset for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.20)\n",
    "\n",
    "# This guarantees a 60/20/20 split\n",
    "train_ds, val_ds = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "# For training\n",
    "x_train = train_ds.drop(\"Exited\", axis=1)\n",
    "y_train = train_ds.loc[:, \"Exited\"].astype(int)\n",
    "x_val = val_ds.drop(\"Exited\", axis=1)\n",
    "y_val = val_ds.loc[:, \"Exited\"].astype(int)\n",
    "\n",
    "# For testing\n",
    "x_test = test_df.drop(\"Exited\", axis=1)\n",
    "y_test = test_df.loc[:, \"Exited\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We will train a simple XGBoost model and set its `eval_set` to `[(x_train, y_train), (x_val, y_val)]` in order to collect validation datasets metrics on every round. The ValidMind SDK supports collecting any type of \"in training\" metrics so model developers can provide additional context to model validators if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "xgb_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict_proba(x_val)[:, -1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging model metadata with `log_model`\n",
    "\n",
    "The `log_model` SDK function allows sending the following model metadata to the API:\n",
    "\n",
    "- Model framework and architecture (e.g. XGBoost, Random Forest, Logistic Regression, etc.)\n",
    "- Model task details (e.g. binary classification, regression, etc.)\n",
    "- Model hyperparameters (e.g. number of trees, max depth, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.log_model(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging model training metrics with `log_training_metrics`\n",
    "\n",
    "The SDK can automatically collect metrics that were captured by the framework during model training. The following are some example metrics that can be collected:\n",
    "\n",
    "- AUC\n",
    "- Error rate\n",
    "- Logloss\n",
    "- Feature importance\n",
    "\n",
    "When the framework has generated one metric value on each iteration/round, the SDK will collect all of the iteration values and group them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.log_training_metrics(xgb_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running model evaluation tests with `run_model_tests`\n",
    "\n",
    "When using `run_model_tests`, the SDK will run the appropriate model evaluation tests according to the following criteria:\n",
    "\n",
    "- Model use case\n",
    "- Model objective\n",
    "- Validation requirements set by the model validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = vm.run_model_tests(xgb_model, x_test, y_test, send=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e673a474f5e5c69884c79b5bbf26bd3f43f29ea8e4fba12a1f063946fe17172c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('validmind-sdk-Jp3s24zK-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
