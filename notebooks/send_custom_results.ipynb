{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Custom Metrics and Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick hack to load local SDK code\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Load API key and secret from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"http://localhost:3000/api/v1/tracking\",\n",
    "  api_key = \"e22b89a6b9c2a27da47cb0a09febc001\",\n",
    "  api_secret = \"a61be901b5596e3c528d94231e4a3c504ef0bb803d16815f8dfd6857fac03e57\",\n",
    "  # Use your project ID\n",
    "  project = \"...\"\n",
    ")\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Custom Metrics\n",
    "\n",
    "It is possible to send custom metrics to ValidMind without implementing a `Metric` class. The only requirement is to construct an instance of a `MetricResult`\n",
    "that can be sent to the ValidMind API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models import MetricResult\n",
    "\n",
    "accuracy_metric = MetricResult(\n",
    "    type=\"evaluation\",\n",
    "    scope=\"test_dataset\",    \n",
    "    key=\"my_custom_accuracy\",\n",
    "    value=0.666\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.log_metrics([accuracy_metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Custom Test Results\n",
    "\n",
    "It is possible to send custom test results to ValidMind without implementing a `ThresholdTest` class. The only requirement is to construct an instance of a `TestResults`\n",
    "that can be sent to the ValidMind API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models import TestResult, TestResults\n",
    "\n",
    "custom_params = {\n",
    "    \"min_percent_threshold\": 0.5\n",
    "}\n",
    "\n",
    "custom_test_result = TestResults(\n",
    "    category=\"model_performance\",\n",
    "    test_name=\"accuracy_threshold\",\n",
    "    params=custom_params,\n",
    "    passed=False,\n",
    "    results=[\n",
    "        TestResult(\n",
    "            passed=False,\n",
    "            values={\n",
    "                \"score\": 0.15,\n",
    "                \"threshold\": custom_params[\"min_percent_threshold\"],\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.log_test_results([custom_test_result])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Custom Metrics or ThresholdTests\n",
    "\n",
    "It is possible to implement custom metrics or threshold test classes. The are only two requirements for getting this to work:\n",
    "\n",
    "- We need to build a `TestPlan` that can execute the custom metric or threshold test (or add it to an existing `TestPlan`, TBD).\n",
    "- We need to implement a `run` method on the custom metric or threshold test class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Custom Metric\n",
    "\n",
    "The following example shows how to implement a custom metric that calculates the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Metric\n",
    "\n",
    "@dataclass\n",
    "class MeanMetric(Metric):\n",
    "    type = \"dataset\"\n",
    "    key = \"mean_of_values\"\n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "        return self.cache_results(mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Custom Metric\n",
    "\n",
    "It is possible to run a custom metric without running an entire test plan. This is useful for testing the metric before integrating it into a test plan.\n",
    "\n",
    "The key idea is to create a `TestContext` object and pass it to the metric initializer. When a test plan is executed, the `TestContext` is created by the `TestPlan` class and passed down to every associated metric and threshold test. However, when we want to test a metric in isolation, we need to create the `TestContext` ourselves.\n",
    "\n",
    "In this example we don't need to pass any arguments to the `TestContext` initializer, but it is possible to pass any arguments as required by `required_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_metric = MeanMetric(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_metric.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the results of the metric by accessing the `result` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestPlanMetricResult(figures=None, metric=MetricResult(type='dataset', scope='', key='mean_of_values', value=3.0, value_formatter=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metric.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metric.result.metric.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Custom ThresholdTest\n",
    "\n",
    "The following example shows how to implement a custom threshold test that fails if the mean of a list of numbers is greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import ThresholdTest\n",
    "\n",
    "@dataclass\n",
    "class MeanThresholdTest(ThresholdTest):\n",
    "    category = \"data_quality\"\n",
    "    name = \"mean_threshold\"\n",
    "    default_params = {\"mean_threshold\": 5}    \n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "\n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)        \n",
    "\n",
    "        passed = mean <= self.params[\"mean_threshold\"]\n",
    "        results = [\n",
    "            TestResult(\n",
    "                passed=passed,\n",
    "                values={\n",
    "                    \"mean\": mean,\n",
    "                    \"values\": values,\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return self.cache_results(results, passed=passed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Custom Threshold Test\n",
    "\n",
    "Similarly to custom metrics, it is also possible to run a custom threshold test without running an entire test plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestPlanTestResult(figures=None, test_results=TestResults(category='data_quality', test_name='mean_threshold', params={'mean_threshold': 5, 'values': [1, 2, 3, 4, 5]}, passed=True, results=[TestResult(test_name=None, column=None, passed=True, values={'mean': 3.0, 'values': [1, 2, 3, 4, 5]})]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from validmind.vm_models.test_context import TestContext\n",
    "\n",
    "test_context = TestContext()\n",
    "mean_threshold_test = MeanThresholdTest(test_context=test_context, params={\n",
    "    \"values\": [1, 2, 3, 4, 5]\n",
    "})\n",
    "mean_threshold_test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = mean_threshold_test.test_results.test_results\n",
    "test_results.passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed: True, values: {'mean': 3.0, 'values': [1, 2, 3, 4, 5]}\n"
     ]
    }
   ],
   "source": [
    "for result in test_results.results:\n",
    "    print(f\"passed: {result.passed}, values: {result.values}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Custom TestPlan\n",
    "\n",
    "The following example shows how to implement a custom test plan that executes the custom metric and threshold test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models import TestPlan\n",
    "\n",
    "class MyCustomTestPlan(TestPlan):\n",
    "    \"\"\"\n",
    "    Custom test plan\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"my_custom_test_plan\"\n",
    "    required_context = []\n",
    "    tests = [MeanMetric, MeanThresholdTest]\n",
    "\n",
    "my_custom_test_plan = MyCustomTestPlan(config={\n",
    "    \"mean_of_values\": {\n",
    "        \"values\": [1, 2, 3, 4, 5]\n",
    "    },\n",
    "    \"mean_threshold\": {\n",
    "        \"values\": [6, 7, 8, 9, 10]\n",
    "    }\n",
    "})\n",
    "my_custom_test_plan.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Figures with Metrics and Test Results\n",
    "\n",
    "It is possible to send figures with metrics and test results. The following example shows how to send a figure with a metric result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Figures with Custom Metrics\n",
    "\n",
    "Let's say we want to add a figure to our custom metric above. We can do this by adding a `figures` attribute to the `cache_results` method call. Let's modify our custom metric code to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from validmind.vm_models import Figure, Metric\n",
    "\n",
    "@dataclass\n",
    "class MeanMetricWithFigure(Metric):\n",
    "    type = \"dataset\"\n",
    "    key = \"mean_of_values_with_figure\"\n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)\n",
    "\n",
    "        # Create a random histogram with matplotlib\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n",
    "        ax.set_title(\"Histogram of random numbers\")\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Do this if you want to prevent the figure from being displayed\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        figure = Figure(key=self.key, figure=fig, metadata={})\n",
    "\n",
    "        return self.cache_results(mean, figures=[figure])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Figures with Custom Threshold Tests\n",
    "\n",
    "Similarly, we can add a figure to our custom threshold test by adding a `figures` attribute to the `cache_results` method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from validmind.vm_models import Figure, ThresholdTest\n",
    "\n",
    "@dataclass\n",
    "class MeanThresholdTestWithFigure(ThresholdTest):\n",
    "    category = \"data_quality\"\n",
    "    name = \"mean_threshold_with_figure\"\n",
    "    default_params = {\"mean_threshold\": 5}    \n",
    "\n",
    "    def run(self):\n",
    "        if \"values\" not in self.params:\n",
    "            raise ValueError(\"values must be provided in params\")\n",
    "\n",
    "        if not isinstance(self.params[\"values\"], list):\n",
    "            raise ValueError(\"values must be a list\")\n",
    "        \n",
    "\n",
    "        values = self.params[\"values\"]\n",
    "        mean = sum(values) / len(values)        \n",
    "\n",
    "        passed = mean <= self.params[\"mean_threshold\"]\n",
    "        results = [\n",
    "            TestResult(\n",
    "                passed=passed,\n",
    "                values={\n",
    "                    \"mean\": mean,\n",
    "                    \"values\": values,\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Create a random histogram with matplotlib\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(np.random.randn(1000), bins=20, color=\"blue\")\n",
    "        ax.set_title(\"Histogram of random numbers\")\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Do this if you want to prevent the figure from being displayed\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        figure = Figure(key=self.name, figure=fig, metadata={})        \n",
    "\n",
    "        return self.cache_results(results, passed=passed, figures=[figure])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a new test plan that includes our two new custom metrics and threshold tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models import TestPlan\n",
    "\n",
    "class MyCustomTestPlanWithFigures(TestPlan):\n",
    "    \"\"\"\n",
    "    Custom test plan\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"my_custom_test_plan_with_figures\"\n",
    "    required_context = []\n",
    "    tests = [MeanMetricWithFigure, MeanThresholdTestWithFigure]\n",
    "\n",
    "my_custom_test_plan_with_figures = MyCustomTestPlanWithFigures(config={\n",
    "    \"mean_of_values_with_figure\": {\n",
    "        \"values\": [1, 2, 3, 4, 5]\n",
    "    },\n",
    "    \"mean_threshold_with_figure\": {\n",
    "        \"values\": [6, 7, 8, 9, 10]\n",
    "    }\n",
    "})\n",
    "my_custom_test_plan_with_figures.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('validmind-Jp3s24zK-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5507f2e99c1cac96073e07e686bb64d511c5f1c7216ba7fc4306f43af6557f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
