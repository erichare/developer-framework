{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick hack to load local library code\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Load API key and secret from environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import validmind as vm\n",
    "\n",
    "# local clb168jzz0000928h07nbs1o9\n",
    "# staging clb5bf15i000b1smvawuoi4u0\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"https://api.staging.vm.validmind.ai/api/v1/tracking\",\n",
    "    project=\"clb5bf15i000b1smvawuoi4u0\"\n",
    "    # project=\"clb168jzz0000928h07nbs1o9\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset...\n",
      "Pandas dataset detected.\n",
      "Inferring dataset types...\n",
      "Preparing in-memory dataset copy...\n",
      "Calculating field statistics...\n",
      "Calculating feature correlations...\n",
      "Generating correlation plots...\n",
      "Successfully logged dataset metadata and statistics.\n",
      "Running data quality tests...\n",
      "Running data quality tests for \"training\" dataset...\n",
      "\n",
      "Preparing dataset for tests...\n",
      "Preparing in-memory dataset copy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping class_imbalance test because no target column is defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test suite has completed.\n",
      "Sending results to ValidMind...\n",
      "Successfully logged test results for test: duplicates\n",
      "Successfully logged test results for test: cardinality\n",
      "Successfully logged test results for test: missing\n",
      "Successfully logged test results for test: skewness\n",
      "Successfully logged test results for test: zeros\n",
      "\n",
      "Summary of results:\n",
      "\n",
      "Test         Passed      # Passed    # Errors    % Passed\n",
      "-----------  --------  ----------  ----------  ----------\n",
      "duplicates   False              0           1           0\n",
      "cardinality  True               6           0         100\n",
      "missing      True               9           0         100\n",
      "skewness     False              2           1     66.6667\n",
      "zeros        False              0           1           0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"notebooks/insurance_mortality/insurance_dataset.csv\")\n",
    "\n",
    "targets = vm.DatasetTargets(\n",
    "    target_column=\"Number_Of_Deaths\",\n",
    "    description=\"Number of deaths for a group of policyholders\",\n",
    ")\n",
    "\n",
    "analyze_results = vm.analyze_dataset(\n",
    "    dataset=df,\n",
    "    dataset_type=\"training\",\n",
    "    targets=targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.load('notebooks/insurance_mortality/mortality_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"notebooks/insurance_mortality/train_df.csv\")\n",
    "test_df = pd.read_csv(\"notebooks/insurance_mortality/test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(\"Number_Of_Deaths\", axis=1)\n",
    "y_train = train_df.loc[:, \"Number_Of_Deaths\"].astype(int)\n",
    "\n",
    "x_test = test_df.drop(\"Number_Of_Deaths\", axis=1)\n",
    "y_test = test_df.loc[:, \"Number_Of_Deaths\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging model metadata and parameters...\n",
      "Extracting training/validation set metrics from trained model...\n",
      "Refitting model...\n",
      "Successfully logged metrics\n",
      "Running model evaluation tests...\n",
      "Generating model predictions on test dataset...\n",
      "Generating model predictions on training dataset...\n",
      "Computing model evaluation metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GLMResults' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# This fails at \"Computing model evaluation metrics...\" when using a statsmodels model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m eval_results \u001b[39m=\u001b[39m vm\u001b[39m.\u001b[39;49mevaluate_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_set\u001b[39m=\u001b[39;49m(x_train, y_train),    \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     val_set\u001b[39m=\u001b[39;49m(x_test, y_test),    \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     test_set\u001b[39m=\u001b[39;49m(x_test, y_test),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andres/code/validmind-sdk/notebooks/quickstart.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/code/validmind-sdk/validmind/__init__.py:96\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, train_set, val_set, test_set, eval_opts, send)\u001b[0m\n\u001b[1;32m     91\u001b[0m log_training_metrics(\n\u001b[1;32m     92\u001b[0m     model, x_train\u001b[39m.\u001b[39mcopy(), y_train\u001b[39m.\u001b[39mcopy(), x_val\u001b[39m.\u001b[39mcopy(), y_val\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning model evaluation tests...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m eval_results \u001b[39m=\u001b[39m mod_evaluate_model(\n\u001b[1;32m     97\u001b[0m     model,\n\u001b[1;32m     98\u001b[0m     test_set\u001b[39m=\u001b[39;49mtest_set,\n\u001b[1;32m     99\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[1;32m    100\u001b[0m     eval_opts\u001b[39m=\u001b[39;49meval_opts,\n\u001b[1;32m    101\u001b[0m     send\u001b[39m=\u001b[39;49msend,\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m eval_results\n",
      "File \u001b[0;32m~/code/validmind-sdk/validmind/model_evaluation/__init__.py:43\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_set, train_set, val_set, eval_opts, send, run_cuid)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39melif\u001b[39;00m model_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGLMResultsWrapper\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Refitting again, need a big refactor for statsmodels support\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     refitted \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit()\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m evaluate_regression_model(\n\u001b[1;32m     44\u001b[0m         refitted, test_set, train_set, eval_opts, send, run_cuid\n\u001b[1;32m     45\u001b[0m     )\n",
      "File \u001b[0;32m~/code/validmind-sdk/validmind/model_evaluation/regression.py:136\u001b[0m, in \u001b[0;36mevaluate_regression_model\u001b[0;34m(model, test_set, train_set, eval_opts, send, run_cuid)\u001b[0m\n\u001b[1;32m    132\u001b[0m     y_train_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_train)\n\u001b[1;32m    134\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    135\u001b[0m results\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 136\u001b[0m     get_model_metrics(\n\u001b[1;32m    137\u001b[0m         model,\n\u001b[1;32m    138\u001b[0m         test_set,\n\u001b[1;32m    139\u001b[0m         test_preds\u001b[39m=\u001b[39;49m(y_pred, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    140\u001b[0m         train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[1;32m    141\u001b[0m         train_preds\u001b[39m=\u001b[39;49m(y_train_pred, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    142\u001b[0m         send\u001b[39m=\u001b[39;49msend,\n\u001b[1;32m    143\u001b[0m         run_cuid\u001b[39m=\u001b[39;49mrun_cuid,\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/code/validmind-sdk/validmind/model_evaluation/regression.py:65\u001b[0m, in \u001b[0;36mget_model_metrics\u001b[0;34m(model, test_set, test_preds, train_set, train_preds, send, run_cuid)\u001b[0m\n\u001b[1;32m     61\u001b[0m     evaluation_metric_result \u001b[39m=\u001b[39m metric_fn(\n\u001b[1;32m     62\u001b[0m         model, test_set, test_preds, linear\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     evaluation_metric_result \u001b[39m=\u001b[39m metric_fn(\n\u001b[1;32m     66\u001b[0m         model, test_set, test_preds, train_set, train_preds\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[39m# TBD: make sure the test functions always return a list\u001b[39;00m\n\u001b[1;32m     70\u001b[0m evaluation_metric_result \u001b[39m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     [evaluation_metric_result]\n\u001b[1;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(evaluation_metric_result, \u001b[39mlist\u001b[39m)\n\u001b[1;32m     73\u001b[0m     \u001b[39melse\u001b[39;00m evaluation_metric_result\n\u001b[1;32m     74\u001b[0m )\n",
      "File \u001b[0;32m~/code/validmind-sdk/validmind/metrics/regression.py:92\u001b[0m, in \u001b[0;36madjusted_r2_score\u001b[0;34m(model, test_set, test_preds, train_set, train_preds)\u001b[0m\n\u001b[1;32m     86\u001b[0m _, y_test_true \u001b[39m=\u001b[39m test_set\n\u001b[1;32m     87\u001b[0m y_test_pred, _ \u001b[39m=\u001b[39m test_preds\n\u001b[1;32m     89\u001b[0m adjusted_r2_train \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (\n\u001b[1;32m     90\u001b[0m     (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m r2_score_sklearn(y_train_true, y_train_pred))\n\u001b[1;32m     91\u001b[0m     \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(y_train_true) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     \u001b[39m/\u001b[39m (\u001b[39mlen\u001b[39m(y_train_true) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39;49mcoef_) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m adjusted_r2_test \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m r2_score_sklearn(y_test_true, y_test_pred)) \u001b[39m*\u001b[39m (\n\u001b[1;32m     96\u001b[0m     (\u001b[39mlen\u001b[39m(y_test_true) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m (\u001b[39mlen\u001b[39m(y_test_true) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mcoef_) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    100\u001b[0m     MetricResult(\n\u001b[1;32m    101\u001b[0m         api_metric\u001b[39m=\u001b[39mMetric(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     ),\n\u001b[1;32m    116\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/validmind-Jp3s24zK-py3.8/lib/python3.8/site-packages/statsmodels/base/wrapper.py:34\u001b[0m, in \u001b[0;36mResultsWrapper.__getattribute__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(results, attr)\n\u001b[1;32m     35\u001b[0m data \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdata\n\u001b[1;32m     36\u001b[0m how \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_attrs\u001b[39m.\u001b[39mget(attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GLMResults' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# This fails at \"Computing model evaluation metrics...\" when using a statsmodels model\n",
    "eval_results = vm.evaluate_model(\n",
    "    model,\n",
    "    train_set=(x_train, y_train),    \n",
    "    val_set=(x_test, y_test),    \n",
    "    test_set=(x_test, y_test),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('validmind-Jp3s24zK-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6 (default, Feb  4 2022, 16:50:30) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5507f2e99c1cac96073e07e686bb64d511c5f1c7216ba7fc4306f43af6557f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
